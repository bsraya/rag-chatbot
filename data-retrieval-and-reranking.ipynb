{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628dbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma:latest\")\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/rag\"\n",
    "collection_name = \"arxiv\"\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1e9d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0. Keywords: sparse attention, Sinkhorn normalization, block sparse attention, locality\n",
      "Key Objects: Queries, Keys, Key Blocks, Query Blocks, Sorting Network\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- How does Sinkhorn normalization contribute to the sparse attention mechanism?\n",
      "- Why is it beneficial to limit attention to key blocks instead of individual keys?\n",
      "- In what ways does this approach complement or differ from other sparse attention techniques like block local attention?\n",
      "---\n",
      "Summary:\n",
      "Sparse Sinkhorn Attention assigns key blocks to query blocks using a sorting network and Sinkhorn normalization, limiting each query's attention to keys within its assigned block to enhance the model's ability to capture locality.\n",
      "Original Text:\n",
      "Sparse Sinkhorn Attention [132] first splits queries and keys into several blocks and assigns a key block to each query block. Each query is only allowed to attend to the keys in the key block that is assigned to its corresponding query block. The assignment of key blocks is controlled by a sorting network, which uses Sinkhorn normalization to produce a doubly stochastic matrix as the permutation matrix representing the assignment. They use this content-based block sparse attention along with block local attention introduced in Sec. 4.1.1 to enhance the ability of the model to model locality.\n",
      "Contextualized Text:\n",
      "To further reduce computational complexity, Sparse Sinkhorn Attention divides queries and keys into blocks. Each query's attention is then limited to the keys within a specifically assigned key block. This assignment is managed by a sorting network leveraging Sinkhorn normalization to create a permutation matrix, which ultimately enhances the model's capability to model locality. [{'tags': ['attention', 'sparse', 'transformers', 'locality'], 'doc_id': 'dcaecbed-f382-4a74-9f9e-fed3f2ae75e9', 'summary': \"Sparse Sinkhorn Attention assigns key blocks to query blocks using a sorting network and Sinkhorn normalization, limiting each query's attention to keys within its assigned block to enhance the model's ability to capture locality.\", 'doc_type': 'text', 'entities': ['Sparse Sinkhorn Attention'], 'keywords': ['sparse attention', 'Sinkhorn normalization', 'block sparse attention', 'locality'], 'key_objects': ['Queries', 'Keys', 'Key Blocks', 'Query Blocks', 'Sorting Network'], 'contextual_text': \"To further reduce computational complexity, Sparse Sinkhorn Attention divides queries and keys into blocks. Each query's attention is then limited to the keys within a specifically assigned key block. This assignment is managed by a sorting network leveraging Sinkhorn normalization to create a permutation matrix, which ultimately enhances the model's capability to model locality.\", 'mentioned_images': [], 'section_hierarchy': {'h1': 'A Survey of Transformers', 'h2': '4 ATTENTION', 'h3': '4.1 Sparse Attention'}, 'hypothetical_questions': ['How does Sinkhorn normalization contribute to the sparse attention mechanism?', 'Why is it beneficial to limit attention to key blocks instead of individual keys?', 'In what ways does this approach complement or differ from other sparse attention techniques like block local attention?']}]\n",
      "----------------------------------------------------\n",
      "* 1. Keywords: gradient descent, objective function, parameter update\n",
      "Key Objects: Gradient Descent, Parameter Update, Objective Function\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- What are the three variants of gradient descent?\n",
      "- How does the amount of data used affect the accuracy of parameter updates?\n",
      "- What are the advantages and disadvantages of using more or less data for gradient descent?\n",
      "---\n",
      "Summary:\n",
      "Gradient descent has three variants that differ in the amount of data used to compute the objective function's gradient, resulting in a trade-off between parameter update accuracy and update time.\n",
      "Original Text:\n",
      "## 2 Gradient descent variants  \n",
      "There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.\n",
      "Contextualized Text:\n",
      "Three variants of gradient descent exist, each differing in the volume of data employed to calculate the gradient of the objective function. These variants involve a trade-off: more data generally leads to more accurate parameter updates, but also increases the time needed to perform each update. [{'tags': ['optimization', 'algorithm', 'machine learning'], 'doc_id': '940b1bfd-476a-482f-b931-3bf49dbd51ee', 'summary': \"Gradient descent has three variants that differ in the amount of data used to compute the objective function's gradient, resulting in a trade-off between parameter update accuracy and update time.\", 'doc_type': 'text', 'entities': [], 'keywords': ['gradient descent', 'objective function', 'parameter update'], 'key_objects': ['Gradient Descent', 'Parameter Update', 'Objective Function'], 'contextual_text': 'Three variants of gradient descent exist, each differing in the volume of data employed to calculate the gradient of the objective function. These variants involve a trade-off: more data generally leads to more accurate parameter updates, but also increases the time needed to perform each update.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'An overview of gradient descent optimization algorithms', 'h2': '2 Gradient descent variants'}, 'hypothetical_questions': ['What are the three variants of gradient descent?', 'How does the amount of data used affect the accuracy of parameter updates?', 'What are the advantages and disadvantages of using more or less data for gradient descent?']}]\n",
      "----------------------------------------------------\n",
      "* 2. Keywords: parameter importance, update sizes, optimization\n",
      "Key Objects: parameters, updates\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- How would one determine the 'importance' of a parameter?\n",
      "- What are the potential benefits of performing larger updates for some parameters and smaller updates for others?\n",
      "- How would this adaptation of update sizes impact the overall convergence behavior of the optimization process?\n",
      "---\n",
      "Summary:\n",
      "The next step in optimization is to adapt update sizes for each individual parameter based on their importance.\n",
      "Original Text:\n",
      "Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance.\n",
      "Contextualized Text:\n",
      "Having adapted our updates to the slope of the error function and accelerated SGD, the next step is to further refine the process by adapting our updates to each individual parameter to perform larger or smaller updates depending on their importance. [{'tags': ['optimization', 'SGD', 'parameters'], 'doc_id': '9cccbd90-e2c4-4551-9162-7805cb432f43', 'summary': 'The next step in optimization is to adapt update sizes for each individual parameter based on their importance.', 'doc_type': 'text', 'entities': [], 'keywords': ['parameter importance', 'update sizes', 'optimization'], 'key_objects': ['parameters', 'updates'], 'contextual_text': 'Having adapted our updates to the slope of the error function and accelerated SGD, the next step is to further refine the process by adapting our updates to each individual parameter to perform larger or smaller updates depending on their importance.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'An overview of gradient descent optimization algorithms', 'h2': '4 Gradient descent optimization algorithms', 'h3': '4.2 Nesterov accelerated gradient'}, 'hypothetical_questions': [\"How would one determine the 'importance' of a parameter?\", 'What are the potential benefits of performing larger updates for some parameters and smaller updates for others?', 'How would this adaptation of update sizes impact the overall convergence behavior of the optimization process?']}]\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"gradient descent\",\n",
    "    k=10,\n",
    "    filter={\"doc_type\": {\"$eq\": \"text\"}},\n",
    ")\n",
    "\n",
    "for index, doc in enumerate(results[:3]):\n",
    "    print(f\"* {index}. {doc.page_content} [{doc.metadata}]\")\n",
    "    print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\n",
    "    \"colbert-ir/colbertv2.0\"\n",
    ")\n",
    "base_retriever = vector_store.as_retriever()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=RAG.as_langchain_document_compressor(),\n",
    "    base_retriever=base_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What is Gradient Descent?\"\n",
    ")\n",
    "print(compressed_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
