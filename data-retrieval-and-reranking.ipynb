{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "628dbc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres import PGVector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"embeddinggemma:latest\")\n",
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/rag\"\n",
    "collection_name = \"arxiv\"\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b1e9d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0. Keywords: sparse attention, Sinkhorn normalization, block sparse attention, locality\n",
      "Key Objects: Queries, Keys, Key Blocks, Query Blocks, Sorting Network\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- How does Sinkhorn normalization contribute to the sparse attention mechanism?\n",
      "- Why is it beneficial to limit attention to key blocks instead of individual keys?\n",
      "- In what ways does this approach complement or differ from other sparse attention techniques like block local attention?\n",
      "---\n",
      "Summary:\n",
      "Sparse Sinkhorn Attention assigns key blocks to query blocks using a sorting network and Sinkhorn normalization, limiting each query's attention to keys within its assigned block to enhance the model's ability to capture locality.\n",
      "Original Text:\n",
      "Sparse Sinkhorn Attention [132] first splits queries and keys into several blocks and assigns a key block to each query block. Each query is only allowed to attend to the keys in the key block that is assigned to its corresponding query block. The assignment of key blocks is controlled by a sorting network, which uses Sinkhorn normalization to produce a doubly stochastic matrix as the permutation matrix representing the assignment. They use this content-based block sparse attention along with block local attention introduced in Sec. 4.1.1 to enhance the ability of the model to model locality.\n",
      "Contextualized Text:\n",
      "To further reduce computational complexity, Sparse Sinkhorn Attention divides queries and keys into blocks. Each query's attention is then limited to the keys within a specifically assigned key block. This assignment is managed by a sorting network leveraging Sinkhorn normalization to create a permutation matrix, which ultimately enhances the model's capability to model locality. [{'tags': ['attention', 'sparse', 'transformers', 'locality'], 'doc_id': 'dcaecbed-f382-4a74-9f9e-fed3f2ae75e9', 'summary': \"Sparse Sinkhorn Attention assigns key blocks to query blocks using a sorting network and Sinkhorn normalization, limiting each query's attention to keys within its assigned block to enhance the model's ability to capture locality.\", 'doc_type': 'text', 'entities': ['Sparse Sinkhorn Attention'], 'keywords': ['sparse attention', 'Sinkhorn normalization', 'block sparse attention', 'locality'], 'key_objects': ['Queries', 'Keys', 'Key Blocks', 'Query Blocks', 'Sorting Network'], 'contextual_text': \"To further reduce computational complexity, Sparse Sinkhorn Attention divides queries and keys into blocks. Each query's attention is then limited to the keys within a specifically assigned key block. This assignment is managed by a sorting network leveraging Sinkhorn normalization to create a permutation matrix, which ultimately enhances the model's capability to model locality.\", 'mentioned_images': [], 'section_hierarchy': {'h1': 'A Survey of Transformers', 'h2': '4 ATTENTION', 'h3': '4.1 Sparse Attention'}, 'hypothetical_questions': ['How does Sinkhorn normalization contribute to the sparse attention mechanism?', 'Why is it beneficial to limit attention to key blocks instead of individual keys?', 'In what ways does this approach complement or differ from other sparse attention techniques like block local attention?']}]\n",
      "----------------------------------------------------\n",
      "* 1. Keywords: gradient descent, objective function, parameter update\n",
      "Key Objects: Gradient Descent, Parameter Update, Objective Function\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- What are the three variants of gradient descent?\n",
      "- How does the amount of data used affect the accuracy of parameter updates?\n",
      "- What are the advantages and disadvantages of using more or less data for gradient descent?\n",
      "---\n",
      "Summary:\n",
      "Gradient descent has three variants that differ in the amount of data used to compute the objective function's gradient, resulting in a trade-off between parameter update accuracy and update time.\n",
      "Original Text:\n",
      "## 2 Gradient descent variants  \n",
      "There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.\n",
      "Contextualized Text:\n",
      "Three variants of gradient descent exist, each differing in the volume of data employed to calculate the gradient of the objective function. These variants involve a trade-off: more data generally leads to more accurate parameter updates, but also increases the time needed to perform each update. [{'tags': ['optimization', 'algorithm', 'machine learning'], 'doc_id': '940b1bfd-476a-482f-b931-3bf49dbd51ee', 'summary': \"Gradient descent has three variants that differ in the amount of data used to compute the objective function's gradient, resulting in a trade-off between parameter update accuracy and update time.\", 'doc_type': 'text', 'entities': [], 'keywords': ['gradient descent', 'objective function', 'parameter update'], 'key_objects': ['Gradient Descent', 'Parameter Update', 'Objective Function'], 'contextual_text': 'Three variants of gradient descent exist, each differing in the volume of data employed to calculate the gradient of the objective function. These variants involve a trade-off: more data generally leads to more accurate parameter updates, but also increases the time needed to perform each update.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'An overview of gradient descent optimization algorithms', 'h2': '2 Gradient descent variants'}, 'hypothetical_questions': ['What are the three variants of gradient descent?', 'How does the amount of data used affect the accuracy of parameter updates?', 'What are the advantages and disadvantages of using more or less data for gradient descent?']}]\n",
      "----------------------------------------------------\n",
      "* 2. Keywords: parameter importance, update sizes, optimization\n",
      "Key Objects: parameters, updates\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- How would one determine the 'importance' of a parameter?\n",
      "- What are the potential benefits of performing larger updates for some parameters and smaller updates for others?\n",
      "- How would this adaptation of update sizes impact the overall convergence behavior of the optimization process?\n",
      "---\n",
      "Summary:\n",
      "The next step in optimization is to adapt update sizes for each individual parameter based on their importance.\n",
      "Original Text:\n",
      "Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance.\n",
      "Contextualized Text:\n",
      "Having adapted our updates to the slope of the error function and accelerated SGD, the next step is to further refine the process by adapting our updates to each individual parameter to perform larger or smaller updates depending on their importance. [{'tags': ['optimization', 'SGD', 'parameters'], 'doc_id': '9cccbd90-e2c4-4551-9162-7805cb432f43', 'summary': 'The next step in optimization is to adapt update sizes for each individual parameter based on their importance.', 'doc_type': 'text', 'entities': [], 'keywords': ['parameter importance', 'update sizes', 'optimization'], 'key_objects': ['parameters', 'updates'], 'contextual_text': 'Having adapted our updates to the slope of the error function and accelerated SGD, the next step is to further refine the process by adapting our updates to each individual parameter to perform larger or smaller updates depending on their importance.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'An overview of gradient descent optimization algorithms', 'h2': '4 Gradient descent optimization algorithms', 'h3': '4.2 Nesterov accelerated gradient'}, 'hypothetical_questions': [\"How would one determine the 'importance' of a parameter?\", 'What are the potential benefits of performing larger updates for some parameters and smaller updates for others?', 'How would this adaptation of update sizes impact the overall convergence behavior of the optimization process?']}]\n",
      "----------------------------------------------------\n",
      "* 3. Keywords: self-attention, position-wise FFN, Transformer, efficiency, long-sequence compatibility\n",
      "Key Objects: self-attention, position-wise FFN, efficiency\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- Why is self-attention a bottleneck in long-sequence scenarios?\n",
      "- What are the key areas of focus when optimizing Transformers for efficiency?\n",
      "- How do researchers aim to make self-attention more compatible with long sequences?\n",
      "---\n",
      "Summary:\n",
      "Efforts to improve Transformer efficiency often focus on enhancing the long-sequence compatibility of self-attention and optimizing the computation and parameter efficiency of position-wise FFN for typical use cases.\n",
      "Original Text:\n",
      "of self-attention, as well as the computation and parameter efficiency of position-wise FFN for ordinary settings.\n",
      "Contextualized Text:\n",
      "To make Transformers more efficient, especially when dealing with long sequences, researchers focus on improving both the long-sequence compatibility of self-attention and the computation and parameter efficiency of position-wise FFN for typical usage scenarios. [{'tags': ['architecture', 'optimization', 'NLP'], 'doc_id': '2a23e58f-c063-4e5d-9ea4-50e9ca4f2875', 'summary': 'Efforts to improve Transformer efficiency often focus on enhancing the long-sequence compatibility of self-attention and optimizing the computation and parameter efficiency of position-wise FFN for typical use cases.', 'doc_type': 'text', 'entities': ['Transformer'], 'keywords': ['self-attention', 'position-wise FFN', 'Transformer', 'efficiency', 'long-sequence compatibility'], 'key_objects': ['self-attention', 'position-wise FFN', 'efficiency'], 'contextual_text': 'To make Transformers more efficient, especially when dealing with long sequences, researchers focus on improving both the long-sequence compatibility of self-attention and the computation and parameter efficiency of position-wise FFN for typical usage scenarios.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'A Survey of Transformers', 'h2': '2 BACKGROUND', 'h3': '2.3 Model Analysis'}, 'hypothetical_questions': ['Why is self-attention a bottleneck in long-sequence scenarios?', 'What are the key areas of focus when optimizing Transformers for efficiency?', 'How do researchers aim to make self-attention more compatible with long sequences?']}]\n",
      "----------------------------------------------------\n",
      "* 4. Keywords: batch gradient descent, gradient descent, cost function, training dataset, parameters, learning rate\n",
      "Key Objects: cost function, parameters, training dataset, gradient\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- Why is batch gradient descent slow compared to other optimization methods?\n",
      "- What are the limitations of batch gradient descent when dealing with datasets that exceed available memory?\n",
      "- How does the learning rate influence the parameter updates in batch gradient descent?\n",
      "---\n",
      "Summary:\n",
      "Batch gradient descent calculates the gradient of the cost function using the entire training dataset to update parameters, making it slow and unsuitable for large datasets.\n",
      "Original Text:\n",
      "### 2.1 Batch gradient descent  \n",
      "Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters Î¸ for the entire training dataset:  \n",
      "<!-- formula-not-decoded -->  \n",
      "As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that do not fit in memory. Batch gradient descent also does not allow us to update our model online , i.e. with new examples on-the-fly.  \n",
      "In code, batch gradient descent looks something like this:  \n",
      "```\n",
      "for i in range(nb_epochs): params_grad = evaluate_gradient(loss_function , data , params) params = params -learning_rate * params_grad\n",
      "```\n",
      "Contextualized Text:\n",
      "Vanilla gradient descent, also known as batch gradient descent, computes the gradient of the cost function with respect to model parameters using the entire training dataset. Because this process requires calculating gradients for the entire dataset before each parameter update, batch gradient descent can be slow and is often impractical for very large datasets. [{'tags': ['optimization', 'machine-learning', 'algorithm', 'gradient-descent'], 'doc_id': '4457eef6-3075-4e8c-a78f-aab951b34f7c', 'summary': 'Batch gradient descent calculates the gradient of the cost function using the entire training dataset to update parameters, making it slow and unsuitable for large datasets.', 'doc_type': 'text', 'entities': [], 'keywords': ['batch gradient descent', 'gradient descent', 'cost function', 'training dataset', 'parameters', 'learning rate'], 'key_objects': ['cost function', 'parameters', 'training dataset', 'gradient'], 'contextual_text': 'Vanilla gradient descent, also known as batch gradient descent, computes the gradient of the cost function with respect to model parameters using the entire training dataset. Because this process requires calculating gradients for the entire dataset before each parameter update, batch gradient descent can be slow and is often impractical for very large datasets.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'An overview of gradient descent optimization algorithms', 'h2': '2 Gradient descent variants', 'h3': '2.1 Batch gradient descent'}, 'hypothetical_questions': ['Why is batch gradient descent slow compared to other optimization methods?', 'What are the limitations of batch gradient descent when dealing with datasets that exceed available memory?', 'How does the learning rate influence the parameter updates in batch gradient descent?']}]\n",
      "----------------------------------------------------\n",
      "* 5. Keywords: gradient descent, optimization algorithms\n",
      "Key Objects: gradient descent optimization algorithms\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- What are the key advantages of gradient descent?\n",
      "- What types of machine learning problems are gradient descent algorithms typically used to solve?\n",
      "- Are there any limitations or challenges associated with using gradient descent?\n",
      "---\n",
      "Summary:\n",
      "This document provides an overview of gradient descent optimization algorithms, authored by Sebastian Ruder.\n",
      "Original Text:\n",
      "# An overview of gradient descent optimization algorithms  \n",
      "__Sebastian Ruder__  \n",
      "Insight Centre for Data Analytics, NUI Galway Aylien Ltd., Dublin ruder.sebastian@gmail.com\n",
      "Contextualized Text:\n",
      "This document provides an overview of gradient descent optimization algorithms. It is authored by Sebastian Ruder, who is affiliated with Insight Centre for Data Analytics, NUI Galway, and Aylien Ltd. in Dublin. [{'tags': ['optimization', 'machine learning'], 'doc_id': '35fa7c02-63e7-4eed-bc46-e04a9fae8ac2', 'summary': 'This document provides an overview of gradient descent optimization algorithms, authored by Sebastian Ruder.', 'doc_type': 'text', 'entities': [], 'keywords': ['gradient descent', 'optimization algorithms'], 'key_objects': ['gradient descent optimization algorithms'], 'contextual_text': 'This document provides an overview of gradient descent optimization algorithms. It is authored by Sebastian Ruder, who is affiliated with Insight Centre for Data Analytics, NUI Galway, and Aylien Ltd. in Dublin.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'An overview of gradient descent optimization algorithms'}, 'hypothetical_questions': ['What are the key advantages of gradient descent?', 'What types of machine learning problems are gradient descent algorithms typically used to solve?', 'Are there any limitations or challenges associated with using gradient descent?']}]\n",
      "----------------------------------------------------\n",
      "* 6. Keywords: key-value memory, linear projections, self-attention, memory compression\n",
      "Key Objects: key-value pairs, memory\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- Why is reducing the size of the key-value memory beneficial?\n",
      "- What are the trade-offs associated with using linear projections or pooling operations for memory compression?\n",
      "- How does the assumption of a fixed input sequence length limit the application of methods like Linformer?\n",
      "---\n",
      "Summary:\n",
      "Key-value pairs in attention mechanisms are often called a key-value memory, and techniques like Linformer and Poolingformer compress this memory to reduce computational complexity.\n",
      "Original Text:\n",
      "$^{7}$The key-value pairs are often referred to as a key-value memory (hence the name memory compression).  \n",
      "Linformer [142] utilizes linear projections to project keys and values from length n to a smaller length n$\\_{k}$ . This also reduces the complexity of self-attention to linear. The drawback of this approach is that an input sequence length has to be assumed and hence it cannot be used in autoregressive attention.  \n",
      "Poolingformer [165] adopts two-level attention that combines a sliding window attention and a compressed memory attention. The compressed memory module is used after the sliding window attention to increase the receptive field. They explore a few different pooling operations as the compression operation to compress the number of keys and values, including max pooling and pooling with Dynamic Convolution [146].\n",
      "Contextualized Text:\n",
      "In attention mechanisms, the key-value pairs used in the process are frequently referred to as a key-value memory. To reduce computational complexity, techniques like Linformer and Poolingformer compress this memory. Linformer utilizes linear projections to reduce the size of keys and values, while Poolingformer combines sliding window attention with a compressed memory module using operations like max pooling. [{'tags': ['attention', 'transformers', 'optimization', 'NLP'], 'doc_id': 'c604e71f-4cdb-4da4-9db6-a5f0c02365c3', 'summary': 'Key-value pairs in attention mechanisms are often called a key-value memory, and techniques like Linformer and Poolingformer compress this memory to reduce computational complexity.', 'doc_type': 'text', 'entities': ['Linformer', 'Poolingformer'], 'keywords': ['key-value memory', 'linear projections', 'self-attention', 'memory compression'], 'key_objects': ['key-value pairs', 'memory'], 'contextual_text': 'In attention mechanisms, the key-value pairs used in the process are frequently referred to as a key-value memory. To reduce computational complexity, techniques like Linformer and Poolingformer compress this memory. Linformer utilizes linear projections to reduce the size of keys and values, while Poolingformer combines sliding window attention with a compressed memory module using operations like max pooling.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'A Survey of Transformers', 'h2': '4 ATTENTION', 'h3': '4.3 Query Prototyping and Memory Compression'}, 'hypothetical_questions': ['Why is reducing the size of the key-value memory beneficial?', 'What are the trade-offs associated with using linear projections or pooling operations for memory compression?', 'How does the assumption of a fixed input sequence length limit the application of methods like Linformer?']}]\n",
      "----------------------------------------------------\n",
      "* 7. Keywords: dynamic mask attention, self-attention, machine translation, abstractive summarization, text data\n",
      "Key Objects: Dynamic Mask Attention Module, Self-attention Module, Token Representations\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- How does the dynamic mask attention module in MAN contribute to modeling locality in text data?\n",
      "- What are the conditions that influence the dynamic mask in MAN?\n",
      "- What are the benefits of using a dynamic mask versus a static mask in the attention mechanism?\n",
      "---\n",
      "Summary:\n",
      "Mask Attention Network (MAN) enhances Transformer blocks by adding a dynamic mask attention module, which improves performance in machine translation and abstractive summarization.\n",
      "Original Text:\n",
      "Mask Attention Network (MAN) [35] prepends a dynamic mask attention module to the selfattention module in each Transformer block. The mask is conditioned on token representations, the relative distance between tokens and head indices. The proposed dynamic mask attention is shown to effectively model locality in text data and the induced model consistently outperforms the baseline model in machine translation and abstractive summarization.\n",
      "Contextualized Text:\n",
      "To explore alternatives to the standard Transformer architecture, researchers have proposed modifications like the Mask Attention Network (MAN). MAN prepends a dynamic mask attention module to the self-attention module within each Transformer block. This module's mask is conditioned on token representations, the distance between tokens, and head indices. The resulting architecture demonstrates effective modeling of locality in text data and consistently outperforms baseline Transformer models in machine translation and abstractive summarization. [{'tags': ['architecture', 'NLP', 'transformer', 'attention'], 'doc_id': '12238e00-172a-41c6-b99d-41412e8cfc23', 'summary': 'Mask Attention Network (MAN) enhances Transformer blocks by adding a dynamic mask attention module, which improves performance in machine translation and abstractive summarization.', 'doc_type': 'text', 'entities': ['Transformer', 'MAN'], 'keywords': ['dynamic mask attention', 'self-attention', 'machine translation', 'abstractive summarization', 'text data'], 'key_objects': ['Dynamic Mask Attention Module', 'Self-attention Module', 'Token Representations'], 'contextual_text': \"To explore alternatives to the standard Transformer architecture, researchers have proposed modifications like the Mask Attention Network (MAN). MAN prepends a dynamic mask attention module to the self-attention module within each Transformer block. This module's mask is conditioned on token representations, the distance between tokens, and head indices. The resulting architecture demonstrates effective modeling of locality in text data and consistently outperforms baseline Transformer models in machine translation and abstractive summarization.\", 'mentioned_images': [], 'section_hierarchy': {'h1': 'A Survey of Transformers', 'h2': '6 ARCHITECTURE-LEVEL VARIANTS', 'h3': '6.5 Exploring Alternative Architecture'}, 'hypothetical_questions': ['How does the dynamic mask attention module in MAN contribute to modeling locality in text data?', 'What are the conditions that influence the dynamic mask in MAN?', 'What are the benefits of using a dynamic mask versus a static mask in the attention mechanism?']}]\n",
      "----------------------------------------------------\n",
      "* 8. Keywords: quadratic complexity, self-attention, divide-and-conquer, long sequences, Transformer\n",
      "Key Objects: sequences, segments, Transformer models\n",
      "Refers to Images: ./images/a-survey-to-transformers/image_14.png\n",
      "Hypothetical Questions:\n",
      "- Why does self-attention have quadratic complexity?\n",
      "- What are the advantages of using a divide-and-conquer approach compared to standard Transformer architectures?\n",
      "- How does decomposing sequences into segments help overcome limitations associated with long-range dependencies?\n",
      "---\n",
      "Summary:\n",
      "To address the quadratic complexity of self-attention and its impact on tasks requiring long-range context, a divide-and-conquer strategy can be employed, which involves decomposing long sequences into smaller segments that are processed by Transformer models.\n",
      "Original Text:\n",
      "### 6.4 Transformers with Divide-and-Conquer Strategies  \n",
      "The quadratic complexity of self-attention on sequences length can significantly limit the performance of some downstream tasks. For example, language modeling usually needs long-range context. Apart from the techniques introduced in Sec. 4, another effective way of dealing with long sequences is to use divide-and-conquer strategy, i.e., to decompose an input sequence into finer segments that can be efficiently processed by Transformer or Transformer modules. We identify two representative class of methods, recurrent and hierarchical Transformers, as illustrated in Fig. 13. These techniques can be understood as a wrapper for the Transformer model in which Transformer acts as an elementary component that is reused to process different input segments.  \n",
      "Fig. 13. Illustrations of recurrent and hierarchical Transformers.  \n",
      "\n",
      "Contextualized Text:\n",
      "Due to the quadratic complexity of self-attention with increasing sequence length, techniques like divide-and-conquer strategies are useful.  These strategies break down input sequences into smaller segments that can be efficiently processed by Transformer models, allowing for better handling of long-range context required in tasks like language modeling. [{'tags': ['architecture', 'NLP', 'transformer', 'efficiency'], 'doc_id': '84634acd-f65b-4131-8e99-c95fc7e095a3', 'summary': 'To address the quadratic complexity of self-attention and its impact on tasks requiring long-range context, a divide-and-conquer strategy can be employed, which involves decomposing long sequences into smaller segments that are processed by Transformer models.', 'doc_type': 'text', 'entities': ['Transformer'], 'keywords': ['quadratic complexity', 'self-attention', 'divide-and-conquer', 'long sequences', 'Transformer'], 'key_objects': ['sequences', 'segments', 'Transformer models'], 'contextual_text': 'Due to the quadratic complexity of self-attention with increasing sequence length, techniques like divide-and-conquer strategies are useful.  These strategies break down input sequences into smaller segments that can be efficiently processed by Transformer models, allowing for better handling of long-range context required in tasks like language modeling.', 'mentioned_images': ['./images/a-survey-to-transformers/image_14.png'], 'section_hierarchy': {'h1': 'A Survey of Transformers', 'h2': '6 ARCHITECTURE-LEVEL VARIANTS', 'h3': '6.4 Transformers with Divide-and-Conquer Strategies'}, 'hypothetical_questions': ['Why does self-attention have quadratic complexity?', 'What are the advantages of using a divide-and-conquer approach compared to standard Transformer architectures?', 'How does decomposing sequences into segments help overcome limitations associated with long-range dependencies?']}]\n",
      "----------------------------------------------------\n",
      "* 9. Keywords: attention heads, regularization, cosine distances, diversity\n",
      "Key Objects: attention heads, loss function\n",
      "Refers to Images: None\n",
      "Hypothetical Questions:\n",
      "- Why is it important to encourage diversity among attention heads?\n",
      "- How does maximizing cosine distances contribute to diversity in attention heads?\n",
      "- What is the purpose of dispersing the positions attended by multiple heads?\n",
      "---\n",
      "Summary:\n",
      "Li et al. [73] introduced a regularization term to the loss function that encourages diversity among attention heads by maximizing cosine distances and dispersing attended positions.\n",
      "Original Text:\n",
      "Li et al. [73] introduce an auxiliary disagreement regularization term into loss function to encourage diversity among different attention heads. Two regularization terms are respectively to maximize cosine distances of the input subspaces and output representations, while the last one is to disperse the positions attended by multiple heads with element-wise multiplication of the corresponding attention matrices.\n",
      "Contextualized Text:\n",
      "To improve multi-head attention, Li et al. [73] introduced an auxiliary disagreement regularization term into the loss function. This term encourages diversity among different attention heads by maximizing cosine distances between input subspaces and output representations, and by dispersing the positions attended by multiple heads using element-wise multiplication of their attention matrices. [{'tags': ['architecture', 'optimization', 'NLP'], 'doc_id': '96c4be5d-d5d9-48e5-b892-f8fbebbe6a91', 'summary': 'Li et al. [73] introduced a regularization term to the loss function that encourages diversity among attention heads by maximizing cosine distances and dispersing attended positions.', 'doc_type': 'text', 'entities': ['Transformer'], 'keywords': ['attention heads', 'regularization', 'cosine distances', 'diversity'], 'key_objects': ['attention heads', 'loss function'], 'contextual_text': 'To improve multi-head attention, Li et al. [73] introduced an auxiliary disagreement regularization term into the loss function. This term encourages diversity among different attention heads by maximizing cosine distances between input subspaces and output representations, and by dispersing the positions attended by multiple heads using element-wise multiplication of their attention matrices.', 'mentioned_images': [], 'section_hierarchy': {'h1': 'A Survey of Transformers', 'h2': '4 ATTENTION', 'h3': '4.6 Improved Multi-Head Mechanism'}, 'hypothetical_questions': ['Why is it important to encourage diversity among attention heads?', 'How does maximizing cosine distances contribute to diversity in attention heads?', 'What is the purpose of dispersing the positions attended by multiple heads?']}]\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = vector_store.similarity_search(\n",
    "    \"gradient descent\",\n",
    "    k=10,\n",
    "    filter={\"doc_type\": {\"$eq\": \"text\"}},\n",
    ")\n",
    "\n",
    "for index, doc in enumerate(results):\n",
    "    print(f\"* {index}. {doc.page_content} [{doc.metadata}]\")\n",
    "    print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5c70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
