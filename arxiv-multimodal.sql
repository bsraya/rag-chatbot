--
-- PostgreSQL database dump
--

\restrict noEU2KJ5VvOfSiHBugyfFICbUwQSBcuaqdUUSbidQYHzaJ9gpxseIzYIg8XT1mR

-- Dumped from database version 17.6 (Debian 17.6-1.pgdg13+1)
-- Dumped by pg_dump version 17.6 (Debian 17.6-1.pgdg13+1)

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- Name: vector; Type: EXTENSION; Schema: -; Owner: -
--

CREATE EXTENSION IF NOT EXISTS vector WITH SCHEMA public;


--
-- Name: EXTENSION vector; Type: COMMENT; Schema: -; Owner: 
--

COMMENT ON EXTENSION vector IS 'vector data type and ivfflat and hnsw access methods';


SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- Name: langchain_pg_collection; Type: TABLE; Schema: public; Owner: langchain
--

CREATE TABLE public.langchain_pg_collection (
    uuid uuid NOT NULL,
    name character varying NOT NULL,
    cmetadata json
);


ALTER TABLE public.langchain_pg_collection OWNER TO langchain;

--
-- Name: langchain_pg_embedding; Type: TABLE; Schema: public; Owner: langchain
--

CREATE TABLE public.langchain_pg_embedding (
    id character varying NOT NULL,
    collection_id uuid,
    embedding public.vector,
    document character varying,
    cmetadata jsonb
);


ALTER TABLE public.langchain_pg_embedding OWNER TO langchain;

--
-- Data for Name: langchain_pg_collection; Type: TABLE DATA; Schema: public; Owner: langchain
--

COPY public.langchain_pg_collection (uuid, name, cmetadata) FROM stdin;
abe8c200-bfa1-4355-947e-23ea618c310d	arxiv	null
\.


--
-- Data for Name: langchain_pg_embedding; Type: TABLE DATA; Schema: public; Owner: langchain
--

COPY public.langchain_pg_embedding (id, collection_id, embedding, document, cmetadata) FROM stdin;
35fa7c02-63e7-4eed-bc46-e04a9fae8ac2	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.023681432,-0.023794712,-0.048859235,-0.0066749356,-0.04292678,0.008501551,-0.0400172,0.06946708,0.069370344,-0.00978241,-0.024102485,-0.0479082,0.006146608,-0.0031217043,0.028838957,-0.032842625,-0.018852897,0.06625028,-0.010364224,-0.008153111,0.060190648,0.055598784,-0.024373325,0.0074787154,0.005060352,0.025669552,0.077661656,-0.0023143457,-0.015563058,-0.033784464,0.0069525004,-0.007954352,-0.032620158,0.015683433,0.009876843,-0.04481361,-0.0005008506,-0.011069612,0.011735493,-0.05204274,-0.041036427,0.04972722,0.0056932797,0.0009749687,-0.011137427,-0.045137204,-0.08946362,-0.044127475,0.0022145517,-0.018728917,0.017125322,0.031229345,-0.03246599,-0.0031451231,-0.02055877,-0.041315455,-0.012837647,-0.037099607,0.021983713,-0.0120988,-0.03921074,-0.008423555,-0.014913472,-0.057862572,0.043671973,-0.03257424,-0.026695881,0.022127675,0.06461881,0.1496407,-0.0035145904,-0.0023079535,-0.0031840524,-0.04408718,0.09806949,0.026189191,0.006814828,-0.0075348495,0.013705633,-0.005539198,-0.009157428,0.080324605,-0.035844143,-0.020730488,0.0777154,-0.009979358,-0.039554693,-0.051586166,0.08611948,-0.0726502,0.008413315,-0.037417386,-0.043172847,0.06633943,-0.00043405208,0.00034909812,0.009416832,-0.01299704,-0.006618333,-0.04262646,0.022584906,0.031974953,0.037803534,0.0861262,-0.005177686,-0.0027884853,-0.04774501,-0.032049414,0.020256937,0.00836094,-0.016956672,0.01609555,-0.030657861,0.0029382405,-0.026566736,-0.01068572,-0.047803134,0.009564114,0.0012578986,-0.0076117152,0.048162982,-0.008752379,-0.002058163,-0.018704565,0.03377173,0.06388079,-0.028236791,0.03850823,-0.015187073,0.011947426,0.059612855,-0.008217932,-0.009160756,-0.01308149,0.022262251,-0.027641216,0.08464282,-0.015168978,0.014078899,-0.0029107896,0.020670554,-0.059019215,-0.038304757,0.0207377,0.034663398,0.047180817,-0.0130382925,0.024913276,-0.009977871,0.055599958,0.02043055,0.045544986,0.043456722,-0.02333167,-0.039291147,-0.049568526,-0.050623335,0.03404992,-0.018163322,-0.038519412,-0.043419432,0.011746373,0.037018772,0.014953943,0.029569605,0.07549091,-0.016246097,0.03617318,-0.021016717,0.026259145,-0.0026956876,-0.038690854,0.008739956,0.016432352,0.008049743,-0.03604222,-0.0045520593,-0.014624713,-0.035071086,0.045218267,-0.018395467,0.037907764,-0.01915378,-0.06144724,0.0128807705,0.044697255,-0.02579876,0.011904407,-0.0073949057,-0.025028937,0.034444947,-0.004637088,-0.005665379,-0.035124112,0.06725439,0.0210288,0.028882908,-0.015323737,-0.0024822648,-0.005166978,0.008153217,0.01625818,-0.006674345,0.028607229,0.024240397,0.028006488,-0.0047450988,0.013767343,-0.06189106,0.04411754,-0.061308015,0.035488766,0.0013537006,-0.0026443512,-0.015351599,0.016340315,-0.030733153,-0.021047281,0.028311977,-0.0055372184,-0.07765539,0.006679207,0.00038151818,-0.019633897,-0.06140998,0.0015656595,-0.012203411,0.019721556,0.045294154,0.022946674,-0.030072061,0.032376133,-0.011026091,0.04363025,0.019379806,-0.0094451755,-0.011481019,0.0007877072,0.039356004,0.02091096,0.0399421,0.016507734,-0.00076991774,0.06562185,-0.0013272876,0.03363911,-0.0056127775,0.04278184,0.022630371,-0.013805761,-0.06959868,0.028549597,0.040512525,-0.021790914,0.0042571384,0.013408786,0.022009417,0.027232073,0.078890525,-0.015411149,0.009667039,-0.040610466,-0.06250069,0.0011191481,0.013875183,0.018616885,-0.01837652,0.012732462,-0.022264306,0.011591546,0.035350464,0.03671663,0.026407024,0.0055523445,0.019682012,0.036561485,-0.050088815,0.045183007,0.023919584,-0.011492495,0.028216042,0.015074751,0.04080274,-0.016341243,-0.00634778,-0.02413953,-0.030467095,-0.04348398,-0.05150583,7.255524e-05,0.0005483419,-0.031314053,0.06689874,0.0063727004,0.06082105,0.06281159,-0.03643947,-0.021994244,0.034137882,0.017879212,0.07679391,-0.04218102,0.061763186,-0.0011260966,5.8778774e-06,0.037580393,-0.05794339,0.006487629,0.014565668,0.015839517,0.06360297,0.04913571,-0.0060012285,-0.014583406,-0.015011489,0.027872968,0.03292504,0.022687135,-0.005330973,0.016909003,0.055006545,0.00432661,-0.032416124,0.046607308,0.006174479,-0.030267552,-0.013888613,-0.024491955,-0.082606144,0.018171366,-0.023781938,0.024342982,0.042168383,-0.0061186273,-0.016448282,-0.02918525,-0.032955322,0.018220583,0.047815803,-0.023295455,-0.003983262,-0.023587659,0.03202231,-0.027060183,-0.027469128,0.017357837,0.055163328,-0.00478163,-0.0074263723,-0.011734705,-0.0067839054,0.015217606,0.03649372,0.0425076,-0.02151666,-0.030203702,-0.050671216,-0.0049581793,0.0172719,-0.0065502403,0.043369323,-0.052259777,0.041322313,-0.077603646,0.0615477,0.026031539,-0.050020773,0.032535374,-0.02046798,0.0058768652,0.033124287,0.01128146,0.036336303,-0.07605366,0.0028286597,-0.0039840867,-0.012345766,0.05850348,0.02160828,-0.03266768,-0.0098561775,0.04479102,-0.013142583,-0.047022447,0.022796962,0.02117184,-0.0038482253,0.062390007,0.019264022,0.07445741,-0.03591392,-0.057728697,0.015515751,0.029658724,-0.038059916,-0.047535673,0.03900616,0.03134988,0.03310421,-0.05790437,-0.0021119637,0.014948448,-0.08466162,0.05855542,-0.026036713,-0.0051471223,-0.100085594,-0.009858769,-0.003932836,-0.009767996,0.06436929,-0.066899754,-0.03519185,-0.028639577,-0.06239293,-0.037059214,0.024083067,-0.00772229,-0.06538541,0.011234272,-0.049968805,0.08981484,0.053142693,0.035887707,-0.004883905,0.043113027,-0.018096283,0.01785088,-0.017095303,-0.019107733,0.017673546,-0.0018954985,-0.022501461,0.02871339,-0.013760003,-0.01836585,-0.068722606,0.057875387,0.008627706,0.047180086,0.06042889,-0.047276285,-0.013227634,-0.03007601,-0.020157082,0.017954111,0.0018272671,-0.039145086,-0.0033651923,0.029710565,0.016870234,0.0024262122,0.0266269,-0.05098222,-0.0053510405,0.0078105517,0.013007032,-0.024930606,-0.0021782615,-0.0028767635,-0.022271141,0.004112054,0.013778682,0.01397801,-0.0066389563,0.002770779,0.0067104297,-0.041866273,0.009364487,0.00037848105,0.0028806506,0.020058786,-0.023195328,0.020961735,0.0033269331,0.044233505,0.050712653,-0.00063062203,0.054889146,0.0066417307,-0.052612584,-0.04451186,0.06995067,0.029162524,0.062404294,-0.035682302,-0.0052498137,0.018347992,-0.07002187,0.015478447,-0.023345012,-0.031438343,0.0027439962,0.036154248,-0.010067152,-0.06978348,-0.005884386,0.01681454,0.07094446,-0.016948517,-0.06037244,0.020878926,0.047717907,0.004539942,-0.043482035,0.025258256,-0.0048629884,0.04980382,-0.005717155,0.043874558,0.013749536,0.006352781,0.019260036,0.026043475,-0.005466619,0.030175494,0.009221114,0.0041332166,-0.009994663,0.016715784,0.021875259,0.0035646644,-0.0041319607,0.03449361,0.022338452,0.021210128,-0.014343807,0.016738512,0.030166747,-0.03458793,0.0020174114,0.012117081,0.03492843,-0.027447212,-0.01971222,-0.059808094,-0.0004906923,-0.04042018,-0.0014195689,0.003032934,-0.07126524,-0.028845906,-0.009171014,0.043170292,0.055755045,-0.01645316,-0.037128102,-0.0057122586,-0.010723127,0.081857845,0.030151032,-0.024955269,0.047699727,-0.038950223,0.01116337,0.019352552,0.042693015,0.0054597915,0.010019512,0.02210807,0.014219523,0.029905716,-0.016238758,-0.028698303,0.00049365364,0.02002099,-0.04575204,-0.09666776,-0.06637042,0.006203139,0.012033029,0.038600475,0.017902091,-0.02908294,-0.03665412,0.10569119,0.045909975,0.039934274,-0.05583649,0.0147691965,-0.0040756674,0.012681905,0.0646859,-0.01639295,0.035638012,0.0676659,0.059388917,-0.038190506,0.034569517,0.025053453,0.022465477,-0.017916132,0.014747008,0.009313021,-0.041947845,-0.041227326,0.074924946,-0.062575154,0.038084205,-0.0053983065,-0.012444278,-0.015246849,-0.016560785,0.02258804,0.049361467,-0.02759007,0.016222462,0.025620447,-0.016857438,0.07808613,-0.086394735,0.015241043,0.04678396,-0.055990588,-0.05615793,0.025253806,-0.0016376246,-0.0050678905,-0.018518496,-0.027412998,0.04056034,0.04199427,-0.0298055,0.026302861,-0.07702465,-0.026298963,-0.0010052463,-0.03342222,-0.033300705,0.043933112,-0.0039194897,-0.002811358,0.07127136,-0.02399599,-0.017854735,-0.007981102,-0.019770186,-0.09001336,-0.014649222,0.05661737,-0.07697835,0.027354259,-0.003912137,0.05496744,-0.012768352,-0.024103869,-0.0623706,-0.043068584,0.049440786,0.0033445386,0.057556234,0.029338678,0.02809887,-0.049346477,-0.024358278,-0.053822592,-0.030668482,0.04773965,0.050421476,-0.0011596528,0.074501336,0.04070054,0.01792839,-0.06091471,-0.07246779,0.0017116248,0.0669993,0.014438622,-0.016159413,-0.01379603,-0.025854778,0.007627667,-0.028561203,-0.009610622,0.007072911,-0.027775357,-0.050249655,0.038805064,-0.0475022,-0.04042343,0.03708997,-0.01632641,-0.02068756,0.0641226,0.0072997743,-0.004713213,-0.067228414,-0.033918902,-0.011423729,-0.027359474,-0.049061134,0.02409362,0.0153322825,0.035596393,-0.025834413,0.048603665,0.011641563,-0.044910308,-0.030463846,-0.0006054011,-0.017854294,0.07739029,8.9168585e-05,-0.04523194,-0.032981116,0.029500835,-0.005985146,0.017313207,0.0100413095,0.018412545,-0.009687234,0.011197695,-0.0242688,-0.030413147,0.02831265,-0.025704054,0.010162441,0.05010848,0.0021627736,-0.020579886,-0.028678942,-0.04176107,0.021521278,0.023293411,0.012464372,0.07964855,0.013583831,0.03198014,0.023240825,0.0027477362,0.082919106,-0.019109013,-0.014551815,-0.025134722,0.022875432,0.017517433,-0.04308566,0.0012516232,0.09002392,-0.02761076,0.013050441,0.035859652,0.045026954,0.02988506,-0.045754686,0.045507047,-0.042180646,-0.008909043,0.0380629,0.041099757,0.0016633957,0.025823137,0.0015398874,-0.0059087384,-0.069503225,-0.060704734,-0.02359869,0.026710613,0.048542656,0.0015994027,-0.034570064,-0.012492113,0.0039353985,-0.004583647,0.005117789,0.013151807,0.00066147937,-0.012107597]	Keywords: gradient descent, optimization algorithms\nKey Objects: gradient descent optimization algorithms\nRefers to Images: None\nHypothetical Questions:\n- What are the key advantages of gradient descent?\n- What types of machine learning problems are gradient descent algorithms typically used to solve?\n- Are there any limitations or challenges associated with using gradient descent?\n---\nSummary:\nThis document provides an overview of gradient descent optimization algorithms, authored by Sebastian Ruder.\nOriginal Text:\n# An overview of gradient descent optimization algorithms  \n__Sebastian Ruder__  \nInsight Centre for Data Analytics, NUI Galway Aylien Ltd., Dublin ruder.sebastian@gmail.com\nContextualized Text:\nThis document provides an overview of gradient descent optimization algorithms. It is authored by Sebastian Ruder, who is affiliated with Insight Centre for Data Analytics, NUI Galway, and Aylien Ltd. in Dublin.	{"tags": ["optimization", "machine learning"], "doc_id": "35fa7c02-63e7-4eed-bc46-e04a9fae8ac2", "summary": "This document provides an overview of gradient descent optimization algorithms, authored by Sebastian Ruder.", "doc_type": "text", "entities": [], "keywords": ["gradient descent", "optimization algorithms"], "key_objects": ["gradient descent optimization algorithms"], "contextual_text": "This document provides an overview of gradient descent optimization algorithms. It is authored by Sebastian Ruder, who is affiliated with Insight Centre for Data Analytics, NUI Galway, and Aylien Ltd. in Dublin.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms"}, "hypothetical_questions": ["What are the key advantages of gradient descent?", "What types of machine learning problems are gradient descent algorithms typically used to solve?", "Are there any limitations or challenges associated with using gradient descent?"]}
936f9171-284d-44e4-a7ba-ba79fc2af815	abe8c200-bfa1-4355-947e-23ea618c310d	[0.012579752,-0.0085826535,-0.021843448,-0.0043420955,-0.016597243,0.051085547,-0.041617803,0.061243042,0.060937066,-0.02611522,-0.02753797,-0.008158085,0.0058396,0.010071164,-0.0268219,-0.00078603084,0.0013449285,0.07506023,-0.024172446,-0.02832782,0.085130826,0.04366405,-0.0060955407,0.029674307,0.027882395,0.001359997,0.028237982,0.003067678,0.0141676385,0.013525191,0.0077549326,-0.010018786,-0.008438658,0.015631245,0.018256405,0.038647704,0.0048979735,-0.001778911,0.005310782,-0.07347341,-0.03978161,0.0799643,-0.01977831,-0.004593309,-0.009327076,-0.035885293,-0.082431085,-0.040757224,-0.0075017884,-0.013005932,-0.015763966,0.038684808,-0.04284746,0.017750787,-0.03752279,-0.021036683,-0.02389496,-0.032262586,-0.008142435,-0.021445546,-0.05009921,0.014333367,0.022759939,-0.060708106,0.054430913,-0.060903404,-0.029534485,0.023462081,0.035501808,0.13505748,0.008803031,0.004192928,-0.025571568,-0.1079001,0.09382485,0.029079452,-0.017912937,-0.056364495,-0.003857012,-0.013773109,-0.013714617,0.048344735,-0.03876368,-0.032608435,0.059250798,0.02559813,-0.06215505,-0.05592637,0.07634394,-0.11217302,-0.013521947,0.015884649,-0.017694876,0.06125207,-0.030906942,0.0042121755,-0.00039527478,0.006662138,0.0026187918,-0.03909813,0.036946554,-0.0028436105,0.04128904,0.084426776,0.004131352,0.062521055,-0.03280929,-0.054441854,-0.013616046,0.03091255,-7.4172436e-05,0.010617824,-0.055153906,0.016020074,-0.014879728,-0.0097299125,-0.043574806,0.006895962,0.010358201,-0.05030089,0.03099636,-0.037547268,-0.007748097,-0.06253735,0.035460554,0.0405963,-0.0039023627,0.008126541,-0.039460517,-0.0077201696,0.019428678,-0.07173964,-0.017271772,-0.030893302,0.033641234,-0.009644688,0.049871862,-0.020996017,0.015361516,-0.012928523,-0.014030784,-0.026153805,-0.002014195,0.02071173,0.029526139,0.054473232,-0.043039218,-0.0008565654,1.0265643e-06,0.058587678,0.01618732,0.0007970553,0.03757878,-0.0048283604,-0.07278579,-0.057863887,-0.05482768,0.018444259,-0.023357792,0.00096129713,-0.048327267,-0.0124475015,0.025601858,0.016948165,0.03225136,0.07726019,0.02731119,0.038670808,-0.0063064373,-0.0038957533,-0.02503215,-0.012130932,0.040421814,-0.041437354,0.026771026,-0.021598617,-0.02799067,-0.035823666,-0.016096445,0.024902597,0.0037953749,0.022580717,-0.012850145,0.010278018,0.023665827,0.046982203,-0.027053816,-0.0019709908,0.0040409868,-0.015193417,0.042039547,-0.020423133,-0.0031426216,0.0032545018,0.068134606,0.026733829,0.011133011,-0.032040223,-0.0049041435,0.0031980667,0.019666234,0.044093475,-0.040792674,-0.008314508,0.012689019,-0.007941233,0.028246688,-0.007980136,-0.04708972,0.046859637,-0.06994324,0.02587434,-0.0011298924,0.055192836,0.03854096,0.0062639895,-0.03608764,-0.008237141,0.04885418,0.027746275,-0.06735732,0.016370682,0.0010178743,0.010099925,-0.021711884,-0.012481474,-0.050108775,0.002670247,0.052901663,0.0015556631,-0.016119083,0.02486582,-0.02775463,0.042495143,0.04710606,-0.012672666,-0.003195197,-4.957503e-05,0.027335308,0.07958582,0.02697341,-0.010098093,0.004135591,0.08638754,0.0008877267,0.012478718,0.02161611,0.056752384,0.018495942,0.007789865,-0.05909221,0.03988302,0.026973369,-0.0503064,0.024447652,-0.005204661,0.031003455,0.004894916,0.08092458,-0.047439575,-0.030900216,-0.06487546,-0.026223375,0.020659538,0.021442164,0.0032150028,0.06601972,0.019565387,-0.02635996,0.05294129,0.009705466,0.0259191,0.043054137,-0.010882849,0.00071094325,0.03111699,-0.010856858,0.007249206,0.042180102,0.012060823,0.008894968,0.026629569,0.05005978,-0.012640224,0.020845177,-0.014376462,-0.00014604091,-0.009717556,-0.05220123,-0.0025367215,0.013303261,-0.013886017,0.041240517,-0.0187814,0.006152202,0.05628062,-0.015389391,-0.04067409,0.043734368,0.041472394,0.060315378,-0.0021886888,0.01849863,0.039054543,0.0028581703,0.0037953265,-0.06233815,0.0050258706,0.009436761,0.017091028,0.027625753,-0.011958521,0.016459623,-0.003235532,-0.05284986,0.0007786638,0.05748181,-0.013098656,0.009829256,0.027661983,0.018984523,-0.0042004655,-0.02713706,0.017146762,0.0013660539,-0.015809247,0.018642815,-0.007247978,-0.12920131,0.023219619,-0.04098217,0.05125028,0.025885833,-0.004587573,-0.003576154,0.008752069,-0.050864764,0.012305649,0.03081944,0.026653135,-0.004659548,-0.036733814,0.06684403,-0.03033102,-0.005105286,0.017950473,0.008976653,0.0371002,0.028954284,0.034010172,0.026333824,0.042958155,0.061743055,-0.011478858,0.0066946964,-0.019881796,0.005681649,0.046932697,0.004609679,0.00032564267,0.020900626,-0.056776304,0.027842721,-0.041211624,0.017890207,0.00056258787,-0.0475507,0.0004948061,-0.011725785,-0.04480926,0.003415771,-0.013426408,0.028940925,-0.03323144,-0.011685297,0.054104146,0.04130453,0.038489476,0.024860924,-0.10697187,0.010697471,0.05359201,0.002325503,-0.05570916,0.0059076333,0.02556268,-0.004726457,0.04134902,0.05640761,0.0372663,-7.325043e-05,-0.015682306,0.03924269,0.030121867,-0.046593633,-0.007823269,0.05208701,0.02970587,0.019763762,-0.03659201,-0.022388257,0.025587296,-0.09635712,0.031698678,-0.02455502,-0.024471214,-0.06255559,-0.006113205,0.020866336,0.02228566,0.05602251,-0.057452064,-0.043020006,0.0018914763,-0.05216948,-0.023850916,-0.011318146,0.015540595,-0.063885905,-0.012414149,-0.014044269,0.089285254,0.055227045,0.03783736,0.027135113,0.020005897,-0.0044102804,0.0032928633,-0.01841783,-0.024107987,0.037565667,-0.041521296,-0.05494327,0.0157835,0.013208929,-0.021322407,-0.037931614,0.043809015,0.0036011431,0.048491254,0.047696806,-0.0072849193,0.03446132,0.02735581,0.015631394,0.009409932,-0.016812801,-0.029990437,-0.00763873,0.0067622815,0.012656786,-0.012046645,0.008548167,-0.01750514,-0.008219161,0.0054303566,0.02307449,-0.007999729,-0.025690636,0.026094086,-0.023317827,0.032815415,0.00493589,0.04691485,-0.002196518,-0.030245097,-0.0008251216,-0.08039845,0.011997036,0.02426665,-0.029316824,0.024418872,-0.017502092,0.05125011,0.008622819,0.047131997,0.035127614,0.044306863,0.09246969,0.008634641,-0.012533238,-0.038876344,0.048606347,0.037464008,0.013898493,-0.10256958,-0.055483896,-0.0057664183,-0.01913178,0.06521307,-0.034249,-0.029274203,-0.009497458,0.03681104,-0.039706584,-0.06532317,0.014776161,-0.013457505,0.06061475,0.005982991,-0.02531983,-0.01949044,0.08179353,0.03753065,0.0034165294,0.009662916,-0.013670651,0.04847374,0.0055620563,0.03150088,0.033936538,0.038945,0.03293312,0.0153360525,-0.01703934,0.018710127,0.029386733,-0.011801092,0.027682442,-0.013313538,0.03086534,0.031491745,0.021619985,-0.006734785,0.059289005,-0.004281448,-0.02558496,0.020619411,0.053763326,-0.053635743,0.0063180234,0.013678635,0.02159033,-0.008232192,-0.078377515,-0.06680475,-0.030412175,-0.042355698,0.004658916,-0.02871859,-0.04644053,-0.027592974,-0.00848348,0.010076961,0.07959288,-0.082026236,-0.038544964,0.031178461,-0.025131695,0.07951054,0.0027404407,0.015496578,0.07811285,-0.061994586,-0.013261026,0.006348552,0.022499498,0.0041056112,-0.023234267,0.043161526,0.04529099,0.019416953,0.010190138,-0.04602228,-0.020392293,-0.0027614515,-0.00096289295,-0.09326699,-0.0016360519,0.025688311,0.032849744,-0.00023282414,0.007730219,0.008076671,-0.009105624,0.06456928,0.061076257,0.014509552,-0.06752149,0.016198304,0.011841234,0.016615408,0.049626637,0.019399336,0.0327297,0.05116423,0.060357127,-0.042303022,0.027378023,0.024433255,-0.004554312,-0.03360536,-0.0077908994,-0.009672531,-0.054522477,-0.027573397,0.087501295,-0.007199893,0.028311,-0.022623196,-0.020906411,0.022901213,-0.03707973,-0.0323818,0.03206193,-0.0071555395,-0.0052424916,0.01505266,-0.07559353,0.06645946,-0.03511083,0.018600132,0.011880261,-0.039707042,-0.07345604,0.0012251463,-0.003779423,0.000565516,-0.00093805796,-0.023742137,0.016484072,0.03254933,-0.0045279423,0.0059762346,-0.019278774,0.010625289,-0.009603158,0.013808339,-0.050628062,0.030773122,0.046620354,-0.023689503,0.056545693,-0.017268984,-0.038427882,0.0029043246,-0.041652482,-0.08534338,-0.016608702,0.03416847,-0.0042360225,0.007842106,-0.0039010583,0.061141413,0.048821047,0.0005927057,-0.046606276,-0.022222163,0.011473837,0.001187014,0.055852033,0.106675,0.041794218,-0.04435875,-0.038536377,-0.027778074,-0.025942178,0.0646642,0.05853929,-0.015756132,0.07529929,0.0059838598,-0.015991958,-0.04687917,-0.055563696,0.029167525,0.08222482,0.03021618,0.0064796056,-0.0030802959,-0.030194055,-0.0064953505,0.015412109,-0.03615022,-0.002697665,-0.013872233,-0.017840233,0.0030274235,-0.032381695,-0.008177152,0.052121,-0.069801226,-0.012533003,0.06609067,0.008361746,-0.037063085,-0.023570655,-0.04035213,-0.027769113,-0.045848116,-0.003750731,-0.012743583,-0.022196302,-0.021518447,-0.013541262,0.029171238,0.035368815,0.0027699333,-0.012837049,0.047125153,0.021729218,0.051069748,0.014493575,-0.040628854,-0.045467388,0.05881963,-0.01054643,0.007694534,-0.03423639,0.037016243,0.012867643,0.0032582982,-0.034650493,-0.037797607,0.034933932,0.023359394,0.009111314,0.035320047,-0.03512478,0.0019823252,-0.028029643,-0.023671554,-0.018386137,0.0062577734,-0.03433635,0.055183522,-0.006472891,0.050209023,0.0013699838,0.0024345394,0.06517871,0.0354747,0.024215396,0.024910158,0.03309732,-0.0067457017,0.0034688413,0.020917116,0.04772162,-0.011418431,0.01728598,-0.02065753,0.023700833,0.019636104,-0.042260464,0.007367826,-0.030118521,-0.021315267,0.053947784,-0.005637711,-0.01030883,0.008694981,-0.0279816,-0.021092257,-0.050550517,-0.055374067,0.004168679,0.05222793,0.033786207,-0.017109333,-0.058468986,0.025817046,0.008547053,0.024687257,-0.05531428,0.00061574636,0.021513712,0.008517504]	Keywords: gradient descent, optimization algorithms, black-box optimizers\nKey Objects: gradient descent optimization algorithms\nRefers to Images: None\nHypothetical Questions:\n- Why are gradient descent algorithms often considered 'black boxes'?\n- What types of insights does this article aim to provide about gradient descent?\n- What are some of the topics that will be covered in this overview of gradient descent optimization?\n---\nSummary:\nThis article aims to provide readers with insights into the behavior of gradient descent optimization algorithms, addressing their frequent use as 'black-box' optimizers and lacking practical explanations of their strengths and weaknesses.\nOriginal Text:\n## Abstract\nGradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.\nContextualized Text:\nGradient descent optimization algorithms are increasingly popular but are often treated as 'black-box' optimizers due to a lack of practical explanations of their functionalities. This article aims to provide readers with insights into the behavior of these algorithms, covering variants, challenges, and optimization strategies.	{"tags": ["optimization", "algorithms", "deep-learning"], "doc_id": "936f9171-284d-44e4-a7ba-ba79fc2af815", "summary": "This article aims to provide readers with insights into the behavior of gradient descent optimization algorithms, addressing their frequent use as 'black-box' optimizers and lacking practical explanations of their strengths and weaknesses.", "doc_type": "text", "entities": [], "keywords": ["gradient descent", "optimization algorithms", "black-box optimizers"], "key_objects": ["gradient descent optimization algorithms"], "contextual_text": "Gradient descent optimization algorithms are increasingly popular but are often treated as 'black-box' optimizers due to a lack of practical explanations of their functionalities. This article aims to provide readers with insights into the behavior of these algorithms, covering variants, challenges, and optimization strategies.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "Abstract"}, "hypothetical_questions": ["Why are gradient descent algorithms often considered 'black boxes'?", "What types of insights does this article aim to provide about gradient descent?", "What are some of the topics that will be covered in this overview of gradient descent optimization?"]}
13bea49a-984e-463d-8b90-332aa7bbd968	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0098900255,-0.0020748198,-0.0093711065,-0.021627208,-0.069641665,0.019474272,-0.027830917,0.05361964,0.06425427,0.00241113,-0.011083827,0.03320986,0.0048359823,0.006848827,-0.023370193,0.015255584,-0.012040731,0.058839783,-0.015192457,-0.024488028,0.1067413,0.033037804,-0.0059508807,0.011503667,0.02675486,0.020328522,0.030052807,0.020601993,0.029203149,0.046482313,-0.034976985,0.022384524,-0.00031196835,-0.010267132,0.016018132,0.0022341914,0.007213722,-0.002195788,-0.018794106,-0.072004564,-0.0043422203,0.08862995,-0.01991515,-0.028311865,0.03666753,-0.019999025,-0.076689705,-0.033368364,-0.024946706,-0.0116853835,-0.053844515,0.0064986097,-0.024612116,0.032261483,-0.082928866,-0.0035831535,-0.0011666621,-0.014036924,0.0007591649,-0.002668377,-0.06467202,0.010061414,0.021961277,-0.030096347,0.07525932,-0.06397972,-0.026565623,0.041149434,0.03993932,0.13785677,0.007070436,-0.0011370135,-0.023895476,-0.10186881,0.09143374,0.026906626,-0.03194968,-0.048546176,0.01128349,-0.03351294,0.027352089,0.031982396,-0.038236123,-0.023298943,0.057290375,0.03292338,-0.08708107,-0.044034883,0.054588787,-0.040155932,-0.0020148775,0.037686277,-0.02376583,0.054458335,-0.011429097,-0.006256994,0.0010330855,-0.0022598088,-0.002595368,-0.025253385,0.03789392,0.027201531,0.055010356,0.106484696,-0.005145967,0.07006076,-0.019648133,-0.005158599,0.01075677,0.036026057,-0.02246364,0.027098302,-0.014664504,0.03117483,-0.012200313,-0.0019749987,-0.058302727,0.010718823,0.024906935,-0.04761534,0.04830333,-0.04098693,0.01165116,-0.035936322,0.029886601,0.073594026,0.014916186,-0.020411167,-0.00932986,-0.015983982,0.012604945,-0.037100177,-0.018272664,-1.1019205e-05,0.035469376,-0.01515799,0.019304277,-0.06828014,0.0043502115,0.016374463,-0.034078952,-0.036711615,0.0053060027,-0.015485612,0.016045498,0.08285039,-0.06350078,-0.0010345199,-0.0075528105,0.040330492,0.00020673685,0.019181829,0.03455771,-0.009818468,-0.05782028,-0.040576372,-0.039600518,0.0154821,0.005209639,-0.0032517056,-0.053956956,-0.016925896,0.028358541,0.053849243,0.046688892,0.10788868,0.032880682,0.019078765,0.031621654,0.02007569,-0.027802028,-0.013488304,0.059731673,-0.035620634,0.02073343,0.00088094326,0.0027491082,-0.013889096,-0.037458878,-0.002448059,0.012157061,0.060615733,-0.022177957,0.017197886,-0.018444844,0.020676669,-0.009281794,0.033945397,0.0018980125,-0.026625436,0.040407963,-0.029601613,-0.010429703,-0.00097093143,0.037158716,0.014612711,0.007898603,-0.02450335,0.015291053,-0.015500984,0.038718227,0.042509165,-0.05380341,-0.008403345,0.0026301495,-0.009071827,-0.03030683,-0.031080183,-0.056199852,0.04735941,-0.047787536,0.011759927,-0.022598606,0.04319834,0.05172395,0.0059267734,-0.02864126,-0.020863397,0.028111558,0.03705472,-0.034391783,0.035289187,0.017044552,-0.0037956794,-0.022089977,0.029974744,-0.041175514,-0.0060230372,0.055049755,0.028469767,-0.008796822,0.007947486,-0.046302263,0.069553226,0.02272138,-0.020513488,0.012573754,0.021053277,0.034399047,0.07148882,0.02599693,-6.6988914e-06,0.012748321,0.0908835,0.022577452,0.011883585,0.020348089,0.035994932,-0.012557695,-0.0024000206,-0.029535268,-0.0094854385,0.048704773,-0.0072223064,0.024065847,-0.010070939,0.05489713,0.026678182,0.07077354,-0.014014062,-0.04299682,-0.037113734,-0.021482714,0.026012113,0.04079591,0.04426022,0.0341091,0.07289803,-0.031883806,0.027207483,0.015064187,-0.0014542425,0.029905392,0.0059378683,-0.005550345,0.018331539,0.00070563843,-0.017876828,0.042844433,-0.0015268708,0.04889898,0.010170579,0.030209767,-0.033172656,-0.015089386,-0.03535078,-0.023373878,0.015874451,-0.0017461603,-0.03500343,-0.0027374236,-0.026030213,0.041756004,0.021344103,0.035972342,0.027293243,-0.0154912155,-0.017885335,0.037094872,0.004739711,0.034078397,0.0004947676,0.027405487,0.012605692,0.028303726,0.004584775,-0.07588871,0.03808795,0.038138464,0.051657904,0.03679508,0.016073331,0.0022981516,-0.0028127148,-0.028070442,-0.012378433,0.05891857,0.038307518,-0.0010149592,0.063101396,0.032430332,-0.025741534,0.0032262525,0.020771632,-0.027231464,-0.038905602,-0.0042761615,-0.016519722,-0.104140826,0.034205724,-0.035723098,0.07512075,0.011821988,0.008995838,-0.012271738,-0.0042717126,-0.04065519,-0.0019842244,0.03291916,0.06604827,-0.0075933007,-0.08723945,0.059823595,-0.050052743,0.013662711,-0.012321503,0.031673837,0.020620823,0.012215275,0.04879967,0.019475441,0.0382382,0.060096424,0.0009404207,0.00779881,0.0028982845,0.02888263,0.053048544,0.023164785,-0.0013150487,0.050316524,-0.022973327,0.049911562,-0.033955567,0.019988306,-0.027818229,-0.057985537,-0.03000586,0.049525827,-0.036694508,0.019442037,-0.015840454,0.023146393,-0.030763522,-0.015212453,0.06259832,0.036885727,0.03892648,0.03861292,-0.09537935,-0.007755554,0.065742835,-0.011067987,-0.009963414,0.012762689,0.01863403,-0.031961095,0.048500437,0.036501467,0.016016433,0.004870771,-0.0023871858,0.028463442,0.04917094,-0.044296596,-0.03047574,0.051186062,0.021007525,-0.009393707,0.003727925,0.02634523,0.01115782,-0.09131768,0.011168699,-0.024291122,-0.032874268,-0.041777916,0.013412755,0.03228428,0.017406031,0.036823392,-0.04188727,-0.041219,0.011699683,-0.05096069,-0.059445508,-0.018657016,-0.0042271065,-0.051671058,0.007868725,-0.009023553,0.046572223,0.06819386,0.00221284,0.026614783,-0.010812687,0.019600283,0.023215564,0.010703956,-0.00833539,0.015765365,-0.028249912,-0.022548182,0.032357357,0.023089085,-0.019400846,-0.02413166,0.059772562,-0.00406002,0.060020324,0.034267087,-0.023903392,0.069997974,0.011661974,-0.012241703,0.0042903614,0.01234214,-0.044750053,-0.033165034,0.019512927,-0.02006509,-0.046694595,-0.01809321,0.012615273,-0.014622604,-0.011531342,0.029524744,-0.0017941205,-0.010516779,0.05053257,0.0066470276,0.041130114,0.009917702,0.0019255205,-0.0057169995,-0.009849873,-0.021613464,-0.068697624,0.016331514,0.021051,-0.040734958,0.015238,-0.005359117,0.06602365,0.01364482,0.035079483,0.03833623,-0.041112307,0.06754858,0.013953716,-0.023012158,-0.00212739,0.067664504,0.065509744,0.0114267515,-0.046358395,-0.030937424,0.017482651,-0.010955747,0.023874512,0.020539487,-0.016440388,-0.03420331,-0.0013374287,-0.005578575,-0.05057511,-0.006184768,-0.013487384,0.037585653,0.0042078546,-0.029210856,-0.01098333,0.07918359,0.06254128,0.017674671,-0.025634233,0.029069973,0.061782505,0.027273862,-0.0013626803,0.05134803,0.026074637,0.053228617,0.008705286,0.008883623,0.0021062072,0.020951396,-0.02691976,0.026619706,-0.0016508128,-0.020051355,0.039039213,0.01952777,0.019003218,0.02006347,0.0055531966,-0.052105535,0.039919168,0.03889418,-0.04235379,0.01910698,-0.010843283,0.013777207,0.02853014,-0.06931693,-0.03128994,-0.046119366,-0.00072031113,0.0021656957,-0.030991541,0.0018294746,-0.032263905,0.0055640596,-0.00073761045,0.07545052,-0.082296304,0.012777475,0.06405449,-0.011171509,0.046227496,0.018969903,-0.00043071687,0.062381823,-0.056041967,-0.015492209,0.008700645,0.01853759,-0.012109138,-0.05124082,0.025347639,0.06287582,0.0136234,0.00992759,-0.06880513,-0.018966103,0.025991337,-0.0072228005,-0.049461376,0.018657623,0.033773392,0.018534638,0.0007044316,0.0023017204,0.03073879,-0.033121206,0.052886806,0.06171485,0.033898722,-0.057428736,0.033130098,0.007308463,0.029225152,0.050274864,0.04613038,0.024970077,0.013103178,0.028659219,-0.051304057,0.02772666,-0.011046074,-0.01771649,-0.05761752,0.015139019,-0.01093623,-0.06301017,-0.035120178,0.07815879,0.01240801,0.02197196,-0.017442431,-0.062414564,0.05462758,-0.03810633,-0.020566838,0.045688923,0.040355716,0.01356394,-0.011858756,-0.08708334,0.063789845,-0.0413608,0.0065581324,-0.007858776,-0.0009690488,-0.113455884,-0.003973974,0.010775925,-0.024010086,-0.0041931756,-0.013254226,0.016640637,0.019476064,0.036986656,0.028610297,0.0016396764,0.047153063,0.00013716797,0.024029218,-0.059555177,0.010059432,0.03676952,-0.028393809,0.057338975,-0.03745657,-0.064583875,0.00654746,-0.023377163,-0.07570553,0.01139754,0.023340225,-0.0031550035,0.0012006375,0.002367492,0.07083095,0.048156075,0.0060150432,-0.04999337,-0.054026745,0.00620385,0.0129838055,0.056060202,0.07580848,0.08807443,-0.03291061,-0.03346182,-0.0036294088,-0.022618793,0.060860902,0.03658142,-0.028831873,0.04698751,0.0068829134,-0.022097606,-0.023398034,-0.042287722,0.03749748,0.06754965,0.028811561,0.04975025,0.016325796,-0.038831405,-0.0058452785,-0.03386319,-0.040427558,0.0090644555,0.048528776,-0.026248869,0.04221947,0.038176812,-0.008901059,0.054499134,-0.016017474,0.008204102,0.06537376,0.037140362,-0.02037354,-0.01994188,-0.075171486,-0.02078933,-0.016735984,0.011590336,-0.026409896,-0.016331134,0.0027513863,0.032687,0.00869673,0.018232098,0.016564883,-0.02925309,0.045141283,0.04829035,0.036860254,-0.018411832,-0.038620472,-0.05921849,0.053993944,-0.02607862,0.014005155,-0.031648487,0.09399453,0.024386331,0.010583641,0.018838653,-0.029832149,0.04701785,0.0290941,-0.009299075,0.050514273,-0.026483493,0.031766564,-0.023175023,-0.023945978,-0.02244775,-0.014452341,-0.015468539,0.03405405,-0.00096615526,0.012992851,-0.029270962,-0.025361177,0.035550326,0.044964626,0.012017547,0.012300376,0.023575641,-0.017604524,-0.0077571752,0.04553449,0.03106204,0.02645867,-0.005249402,-0.028594565,0.010994894,0.052941415,-0.010149131,-0.010955582,-0.024415262,0.026678823,0.018344771,0.0138217155,0.015569505,0.010294663,-0.002002771,-0.026265917,-0.034032565,-0.0136202825,-0.0060713487,0.055579092,0.025770191,-0.026188897,-0.068000026,0.030983528,-0.010074579,0.04181058,-0.018594572,-0.0075923726,0.027042694,-0.0058328444]	Keywords: gradient descent, optimization, neural networks, deep learning\nKey Objects: optimization algorithms, neural networks\nRefers to Images: None\nHypothetical Questions:\n- Why is gradient descent so popular for neural network optimization?\n- What does it mean for optimization algorithms to be used as 'black boxes'?\n- What are some potential benefits of understanding the strengths and weaknesses of gradient descent optimization algorithms?\n---\nSummary:\nGradient descent is a widely used optimization algorithm, especially for neural networks, but the algorithms used to optimize it are often treated as 'black boxes' lacking practical explanation.\nOriginal Text:\n## 1 Introduction  \nGradient descent is one of the most popular algorithms to perform optimization and by far the most common way to optimize neural networks. At the same time, every state-of-the-art Deep Learning library contains implementations of various algorithms to optimize gradient descent (e.g. lasagne's 2 , caffe's 3 , and keras' 4 documentation). These algorithms, however, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by.\nContextualized Text:\nGradient descent is a popular optimization algorithm, and the most common method for optimizing neural networks. State-of-the-art deep learning libraries like lasagne, caffe, and keras provide various algorithms to optimize gradient descent, but these algorithms are frequently employed without clear explanations of their functionalities and limitations.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "13bea49a-984e-463d-8b90-332aa7bbd968", "summary": "Gradient descent is a widely used optimization algorithm, especially for neural networks, but the algorithms used to optimize it are often treated as 'black boxes' lacking practical explanation.", "doc_type": "text", "entities": ["lasagne", "caffe", "keras"], "keywords": ["gradient descent", "optimization", "neural networks", "deep learning"], "key_objects": ["optimization algorithms", "neural networks"], "contextual_text": "Gradient descent is a popular optimization algorithm, and the most common method for optimizing neural networks. State-of-the-art deep learning libraries like lasagne, caffe, and keras provide various algorithms to optimize gradient descent, but these algorithms are frequently employed without clear explanations of their functionalities and limitations.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "1 Introduction"}, "hypothetical_questions": ["Why is gradient descent so popular for neural network optimization?", "What does it mean for optimization algorithms to be used as 'black boxes'?", "What are some potential benefits of understanding the strengths and weaknesses of gradient descent optimization algorithms?"]}
7a68d403-86f2-43bc-ab31-00f020d52bf4	abe8c200-bfa1-4355-947e-23ea618c310d	[0.015247329,-0.032542445,-0.021093864,0.009735927,-0.04249219,0.04750231,-0.024384234,0.017762447,0.059010375,-0.053277124,0.0119015835,-0.027187878,-0.011780386,-0.03279157,-0.02948554,0.0046266187,-0.030391905,0.08861166,0.015922971,-0.041403633,0.06748161,0.039563585,0.0051291985,0.023646634,0.000101412916,-0.010074904,0.06763838,-0.0102096675,0.0060663326,-0.0033399658,-0.01101097,0.012098573,0.0008916093,0.0022654394,0.0021107378,0.03075514,-0.0027189592,0.010598821,-0.023546558,-0.05451699,-0.04294492,0.056054134,-0.024603695,-0.03682238,-0.03278781,-0.046049748,-0.0690064,-0.046867933,-0.002397165,-0.0066084787,0.0025904428,0.04605475,-0.031877954,0.020646537,-0.032218106,-0.012188337,-0.042057976,7.153288e-05,0.011975429,-0.03501865,-0.049254812,0.0257213,0.022082215,-0.04091583,0.057771575,-0.030289527,-0.020097086,0.037788767,0.027282275,0.124220654,0.020638775,0.009506318,-0.0060276827,-0.07513508,0.0952464,0.009644036,-0.02946612,-0.04436249,0.007158484,-0.009302239,-0.0022031644,0.033225484,-0.039690346,-0.03752692,0.06818773,0.0513964,-0.094138004,-0.047742397,0.08135841,-0.062040016,-0.029297438,0.01427096,0.00925716,0.0692877,-0.03380067,-0.020618537,-0.027655989,-0.0057360344,0.008650393,-0.04734875,0.032898705,-0.0143637555,0.01828833,0.08487554,-0.0045514475,0.044914152,-0.031782396,-0.017560735,-0.003590029,0.005139461,-0.009482151,0.005111231,-0.027863694,0.008846622,-0.0056175147,0.016763497,-0.020461237,0.008117213,0.008124829,-0.04936142,0.05265609,-0.034425747,-0.0035183565,-0.0442995,0.054029096,0.005198972,-0.02006738,0.016762504,-0.028414356,-0.01692174,0.00563992,-0.057229288,0.00989491,-0.039941676,0.036912978,-0.0040211813,0.051131424,0.014139762,-0.010951295,-0.005741195,-0.02164974,-0.057304185,-0.02562335,-0.010481277,0.0068723583,0.03370335,-0.036771826,0.01990094,-0.010658244,0.06658268,0.029001998,-0.026017053,0.035292152,-0.013424321,-0.05286932,-0.049843825,-0.03850169,0.004770854,-0.013182552,-0.017056538,-0.013046181,-0.007747176,0.017556,0.03608871,0.024965907,0.1265685,0.015729958,0.03883875,0.015882706,0.045901872,-0.0223514,-0.0064973948,0.009177965,-0.022428427,0.054835986,-0.0055570193,-0.0119970795,-0.030665062,-0.0036099649,0.007785124,-0.00019025136,0.017826188,-0.019926997,-0.007912673,0.0018047014,0.04170882,-0.041225,0.018338691,-0.004819463,-0.04211766,0.029708296,-0.0075544324,-0.017248487,0.025482006,0.030892942,0.015267708,0.0060770316,-0.032600604,0.007036712,-0.008640509,-0.01594373,0.04204245,-0.013878239,-0.019880349,-0.006988216,-0.03056582,0.010517797,-0.00055330235,-0.08211652,0.017689198,-0.046076916,0.048491035,-0.004421164,0.040183425,0.007431203,-0.026458934,-0.0708327,0.00056806125,0.04630721,0.00086481025,-0.09114812,-0.015457071,-0.020148395,0.009948032,-0.049082007,-0.009092971,-0.026239667,0.012622505,0.06042899,-0.01516847,-0.016395103,0.04435702,-0.02635386,0.04164497,0.037487,-0.0062693893,0.0093836095,0.00980402,0.027338076,0.049204163,0.033370074,-0.012117753,-0.028192658,0.10140786,-0.0017691351,-0.017329626,0.019406818,0.075878,-0.010832275,0.0052840104,-0.0629245,0.045108974,0.039403223,-0.031575732,-0.017509935,0.016560996,0.012285102,-0.014035708,0.093910076,-0.06065727,-0.024257433,-0.047575783,-0.024470517,0.008095321,0.014491889,0.016619448,0.041815974,0.022376208,-0.051430415,0.01708896,0.0018434272,0.03389192,0.020066835,-0.033053078,-0.0005695429,0.06799628,-0.032597892,0.008164614,0.03354186,-0.012069414,0.02252993,0.0049308096,0.036665995,-0.014511703,-0.0034067878,-0.023896817,0.02790442,-0.010894377,-0.04125606,0.011250902,-0.005390569,-0.004481473,0.06314004,-0.0071643186,-0.0160845,0.0502198,-0.014437894,-0.0029478827,0.021844864,0.0360406,0.082278945,-0.011690106,0.019448256,0.025994735,0.00018621844,0.004390513,-0.09884466,0.039020944,-4.9011855e-05,-0.0049997275,0.035872012,0.011464615,-0.0021805048,-0.023113117,-0.031157477,0.029278023,0.06370461,-0.040322345,0.017996099,0.011934454,0.032907687,-0.02652668,-0.0500214,0.03952593,0.004584372,-0.0016070198,0.0012316939,-0.01482762,-0.083152875,0.012183441,-0.07456498,0.041476198,0.030217567,-0.027558627,0.003488292,0.02245915,-0.06287076,0.009306873,0.029161109,0.017673554,0.007635167,-0.045895394,0.029412828,-0.04291422,-0.0015662439,0.011885285,0.007988777,0.021143992,0.029739534,0.037066273,-0.010526125,0.04372792,0.055568084,-0.0011981251,-0.0006020441,-0.015585395,0.0035431986,0.01843837,0.045075536,0.023809345,0.025765844,-0.051619593,0.037515543,-0.026918244,0.06386996,0.013735786,-0.058491077,0.010342138,-0.02296742,-0.014874443,-0.014125499,0.034474246,-0.0033408664,-0.06510466,-0.037897244,0.03198637,0.045226537,0.058805358,-0.018516619,-0.09984625,0.016019966,0.02895014,0.0017311993,-0.03979268,0.031645488,0.004196541,0.007359325,0.08474571,-0.0051720478,0.019939663,-0.027662074,-0.040605072,0.03609197,0.03142327,-0.038583707,0.0015900265,0.04180573,0.052575666,0.019952493,-0.05154518,-0.0072132396,0.026097467,-0.080654204,0.032138087,-0.023701856,-0.012310551,-0.083847396,0.0059662154,0.037437525,0.00037939055,0.05153668,-0.0392018,-0.03520036,-0.025168084,-0.05636191,-0.054831125,0.008060889,0.018981932,-0.04512847,0.022909913,-0.023992678,0.09016731,0.04715755,0.038369127,-0.0014535204,0.019231688,-0.0030884983,0.03042351,-0.025485832,-0.020613799,0.013591409,-0.022625979,-0.04242402,0.03441178,0.0034775755,0.00063369,-0.029038465,0.04950467,-0.0114507545,0.036768332,0.06601964,-0.0027620767,0.033635516,0.007310337,0.0027146395,0.041941084,0.0019235988,-0.030752959,-0.017635558,-0.0026991707,0.015800681,-0.012910613,0.023713656,-0.04634783,-0.008706665,0.019713512,0.019383214,-0.0069998084,-0.019870635,0.0030564491,-0.015318028,0.016957693,0.026103225,0.040410973,0.0051100035,-0.024192402,-0.017706664,-0.08143347,0.0053741764,0.012334856,-0.013653734,0.017530467,-0.029600166,0.036908284,-0.0029236265,0.031946495,0.031578504,0.0023889234,0.07768211,0.016773239,-0.02251672,-0.022997726,0.06115444,0.06502684,0.017410882,-0.078117415,-0.01188043,0.01817427,-0.056361504,0.021904504,-0.014787955,-0.0074421554,0.0012308822,0.043144755,-0.011199236,-0.06512455,0.03005086,-0.015033251,0.04522055,-0.006594924,-0.038360003,-0.03273405,0.10271306,0.033947807,-0.026842263,-0.010116555,-0.007250417,0.059892967,-0.01494227,0.038083717,0.063777864,0.04712309,0.0378119,0.010672531,0.0014895006,0.021744406,0.023533056,-0.030221904,0.03037565,-0.0069298795,-0.013177713,0.062349774,0.021622904,-0.0043138205,0.026264405,-0.026315227,0.010835016,0.020169798,0.065976344,-0.03340823,-0.01809952,0.01633195,-0.00012202598,0.012432908,-0.05574849,-0.05344458,-0.017536815,-0.040159862,0.033899598,-0.0042725215,-0.030083187,-0.021330236,-0.015172044,0.031139398,0.06413101,-0.07248152,-0.014966062,0.010768986,-0.02347195,0.0805146,0.012152692,-0.029258933,0.06937619,-0.07610675,-0.0008448413,0.0074335015,0.010151213,-0.026469057,-0.029027216,0.05903694,0.04496734,0.020987447,0.009497259,-0.009470259,-0.045785736,0.010415192,-0.008853282,-0.08040553,-0.012091871,0.03697674,0.016047703,-0.0058399052,-0.0043535684,0.023853075,-0.020976754,0.08253221,0.052151516,-0.0106933145,-0.08076948,0.0051799123,0.062748946,0.0026415004,0.05360987,0.013948883,0.018748453,0.032509174,0.081425875,-0.027508805,0.038240563,-0.028756164,0.0005870655,-0.05606176,0.006057507,-0.0018189796,-0.05071702,-0.031583916,0.073806986,-0.0180361,0.0410128,-0.002087697,-0.025434954,0.05883784,-0.028461328,-0.029971315,0.01791016,-0.0044482467,-0.00075247657,0.0065507987,-0.04964968,0.056025345,-0.05092244,0.0006887344,-0.0070358748,-0.028479086,-0.078958675,-0.013796996,-0.05153387,-0.015506631,-0.008731088,-0.045723844,0.012347766,0.03709411,-0.016588414,0.019456005,-0.041454084,0.0069358773,-0.021002414,-0.00028611516,-0.02818736,0.029392099,0.027677022,-0.027897656,0.058186907,-0.08645142,-0.028978085,0.034474954,-0.025800655,-0.08174935,0.0005963134,0.03463856,-0.052636113,0.006528113,-0.009526097,0.06953815,0.025739582,-0.017456993,-0.056862652,-0.05081476,0.024660401,-0.0034007484,0.058213137,0.0895676,0.041679014,-0.040985808,-0.013289325,-0.023587044,-0.03572405,0.011020364,0.025521731,-0.010906679,0.05954663,0.005212951,0.008676692,-0.057720236,-0.026588503,0.04117632,0.085454255,0.012606317,0.037522018,-0.022153426,-0.036556166,-0.029269943,-0.017559359,-0.022047289,0.001109026,-0.019179603,-0.024937965,-0.013012474,-0.03393153,-0.029299155,0.059299666,-0.04910939,0.0074686366,0.08592645,0.027545152,-0.042145785,-0.04707089,-0.041538708,0.002985262,-0.014724499,-0.007661498,-0.029052006,-0.002950785,0.016918685,-0.010614439,0.05586312,0.024589907,0.00332996,-0.023827614,0.042870358,0.0005512795,0.03983342,0.021608373,-0.024416631,-0.04383368,0.06420142,-0.0109647075,-0.001859497,-0.007430667,0.025690446,0.0010438185,-0.0013101342,-0.014541356,-0.027417894,0.021476159,0.016299978,-0.005007671,0.054679625,-0.014443918,-0.0041569546,-0.05770911,-0.0046029226,-0.03468685,0.03665616,-0.03195695,0.056495547,-0.023364156,0.046135936,-0.0037857506,-0.012671601,0.03825996,0.037446637,0.040207103,0.004749896,0.03635625,0.031912092,-0.014146291,0.047368903,0.043529574,0.015532457,0.0067243464,0.021240845,0.040638205,0.027168116,-0.044749875,0.043017182,-0.0063286317,-0.013108666,0.039101634,-0.00032792825,-0.019767975,-0.0116860615,-0.024793731,-0.033039052,-0.016643023,-0.06309912,0.019743314,0.06127849,0.05705094,-0.016269447,-0.06436324,0.025072407,-0.014734997,0.04197044,-0.049307905,0.017394282,0.025647838,-0.0020934294]	Keywords: gradient descent, optimization algorithms, neural networks, training\nKey Objects: gradient descent optimization algorithms, optimization\nRefers to Images: None\nHypothetical Questions:\n- What are some of the challenges encountered during training that necessitate specialized optimization algorithms?\n- How do different optimization algorithms address the identified challenges in gradient descent?\n- Why is optimizing gradient descent important for the performance of neural networks?\n---\nSummary:\nThis article aims to provide readers with an understanding of gradient descent optimization algorithms, outlining the structure of the paper and the topics that will be covered.\nOriginal Text:\nThis article aims at providing the reader with intuitions with regard to the behaviour of different algorithms for optimizing gradient descent that will help her put them to use. In Section 2, we are first going to look at the different variants of gradient descent. We will then briefly summarize challenges during training in Section 3. Subsequently, in Section 4, we will introduce the most common optimization algorithms by showing their motivation to resolve these challenges and how this leads to the derivation of their update rules. Afterwards, in Section 5, we will take a short look at algorithms and architectures to optimize gradient descent in a parallel and distributed setting. Finally, we will consider additional strategies that are helpful for optimizing gradient descent in Section 6.\nContextualized Text:\nThe purpose of this article is to give readers an intuitive understanding of the behavior of gradient descent optimization algorithms, which are commonly used to train neural networks. The article will be structured to first examine different variations of gradient descent, then address training challenges, and subsequently introduce optimization algorithms and strategies for both parallel and distributed settings.	{"tags": ["optimization", "deep learning", "algorithms"], "doc_id": "7a68d403-86f2-43bc-ab31-00f020d52bf4", "summary": "This article aims to provide readers with an understanding of gradient descent optimization algorithms, outlining the structure of the paper and the topics that will be covered.", "doc_type": "text", "entities": [], "keywords": ["gradient descent", "optimization algorithms", "neural networks", "training"], "key_objects": ["gradient descent optimization algorithms", "optimization"], "contextual_text": "The purpose of this article is to give readers an intuitive understanding of the behavior of gradient descent optimization algorithms, which are commonly used to train neural networks. The article will be structured to first examine different variations of gradient descent, then address training challenges, and subsequently introduce optimization algorithms and strategies for both parallel and distributed settings.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "1 Introduction"}, "hypothetical_questions": ["What are some of the challenges encountered during training that necessitate specialized optimization algorithms?", "How do different optimization algorithms address the identified challenges in gradient descent?", "Why is optimizing gradient descent important for the performance of neural networks?"]}
f783886b-e2f8-40ed-b4ee-adff527c3018	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.018222123,0.0048122513,-0.036872692,0.004058241,-0.040815327,0.015333813,0.0086202575,0.028702404,0.050861444,-0.021971552,-0.023900956,-0.030779151,0.009204947,0.0006264613,-0.019462423,-0.029662156,0.0010826028,0.11328785,0.011898936,0.01734178,0.07999445,0.032847222,-0.0054958686,0.0255975,-0.015087981,0.040634826,0.08168223,-0.008762372,0.0111938855,0.013207338,-0.040285073,0.031582605,-0.014266129,0.0019747538,-0.023459079,-0.030546766,-0.012987444,-0.004434367,-0.047292355,-0.039687112,-0.046529617,0.02435887,-0.04982825,-0.017344432,0.022805888,-0.05925135,-0.055057354,-0.03471244,0.00021151672,-0.042173572,-0.03765509,0.0761089,-0.022793425,0.031782664,-0.079722375,0.010428232,-0.01516112,-0.005066046,-0.019527312,-0.0032842464,-0.01975681,0.024291506,0.01665995,-0.06895487,0.09504495,-0.021763291,-0.042668786,0.02815107,0.043553747,0.123139076,-0.00719281,0.018238943,0.0044947546,-0.117830366,0.1268878,0.03165453,-0.017669266,-0.040035583,0.0033723242,-0.021293042,-0.007526325,0.045311652,-0.044901263,-0.02053607,0.07028005,0.05267747,-0.070584394,-0.013736655,0.07788511,-0.03714079,0.02063415,-0.0012378669,-0.02359062,0.018090378,0.028277528,0.011793444,0.004922336,-0.015470068,0.021608442,0.004699129,0.061387207,0.031788237,0.046957906,0.11037869,0.017675882,0.028815221,-0.041330148,-0.0035678989,0.003823605,0.03771676,0.012326947,0.031438313,-0.03406292,0.03452829,-0.03955999,-0.036229808,-0.043360393,0.022433905,-0.0006967289,-0.025879394,0.048504934,-0.016799543,0.0067980657,0.012164812,0.013307784,0.04814818,0.0036904144,0.007155127,-0.017077561,-0.0017496273,0.031961016,-0.056976978,-0.020871622,0.014593524,-0.0056516486,-0.010908586,0.025007868,-0.0046033296,0.016357265,0.03738587,-0.046039235,-0.075587206,-0.0072722365,0.01066933,-0.010856641,0.032245807,-0.05598593,0.03718604,-0.03416429,0.036384225,0.0094356295,-0.0007800323,0.039468665,-0.0006981335,-0.072534,-0.043065466,-0.05631804,0.07527501,-0.004150712,0.0018802788,-0.010467996,0.010692367,-0.014689905,0.08699038,0.035947245,0.11511193,-0.0019122387,0.029820582,0.023044998,0.028381968,0.008537798,-0.04015214,0.028397055,-0.02171207,-0.036341373,-0.0099219335,-0.010800154,0.0062146774,-0.033253644,0.011434603,-0.0061582075,0.044115122,0.01166538,-0.0180072,-0.04235892,0.033244994,0.004005777,0.028690713,-0.0038755212,-0.03794753,0.019047624,0.028204765,-0.0040660705,-0.008346061,0.048554372,0.0105188405,0.039243698,0.0028187993,0.016754718,-0.004413804,-0.0018608866,0.030853683,-0.037174895,-0.025407117,0.03096741,-0.023806274,-0.017604228,-0.002809535,-0.06084631,0.023395305,-0.03512454,0.024888245,0.010238433,0.022487152,0.034019466,-0.023958812,-0.054902915,-0.00733961,0.029856829,0.014971616,-0.0313618,0.022839468,0.00015832925,0.02139485,-0.05040603,0.013248831,-0.0313315,0.011338497,0.04364598,0.014404291,-0.03284411,0.049323715,-0.06259093,0.05522416,0.010411538,-0.013624527,0.0045461757,0.018286925,0.0043931752,0.033581503,0.029736416,0.008648429,-0.014422305,0.072554,0.02659772,0.028545754,0.032219432,0.040482596,-0.033205163,0.018145261,-0.02648532,-0.002473487,0.017175455,-0.051579926,2.1665981e-05,-0.0041184323,0.05675808,0.03136404,0.029466694,-0.032215364,-0.025856012,-0.019940665,-0.023345731,-0.035576507,0.024011835,0.056122027,0.00848413,0.017105987,-0.03422289,0.03621327,0.036484372,0.038372073,-0.022157716,0.010540525,0.030857453,0.052751794,-0.057595916,0.065639906,0.058163308,-0.03902398,0.057396192,-0.0040372903,0.013517936,0.005745461,-0.028132956,-0.0070335823,-0.04861848,0.025516834,-0.027271366,-0.021436289,0.008243844,-0.035563346,0.074779615,0.062860526,-0.00074668846,0.0039449655,-0.009017502,-0.04550562,0.029343367,0.033534285,0.05108692,-0.0073356465,0.054497924,0.0043167598,0.00790029,0.007614331,-0.091765426,0.04441402,0.03678041,0.04257726,0.023084352,0.008161155,-0.011563804,-0.01785989,-0.017815012,-0.027249286,0.046467632,-0.016024701,0.046214823,-0.0012194319,0.038658928,-0.0015659096,-0.045510605,0.032364633,0.015279039,-0.0012777918,-0.008264395,-0.050556496,-0.07255053,0.044345003,-0.018659575,0.022382831,0.047708575,0.026542764,-0.0005025419,-0.007911187,-0.03990542,0.033298064,-0.00779807,-0.008787116,0.004171325,-0.06657646,0.028423576,-0.0114068175,0.03144491,0.0010441154,0.02525415,0.032826353,-0.014338826,0.0908777,-0.00021658551,0.045254268,0.019302767,0.017633954,-0.018921878,0.0003059041,-0.07179947,0.033056147,0.04520529,0.007227951,0.05098451,-0.036303826,0.018099323,-0.05255726,0.07469822,0.023536712,-0.06538678,0.004587588,0.036418438,0.023164762,0.0007800245,0.016406426,-0.03214137,-0.05123119,0.002668644,0.04107785,0.08120347,0.027918132,-0.01636941,-0.10143383,0.027017105,0.041567378,-0.0028180564,-0.02324291,0.024042796,0.015091796,0.0073143546,0.06199638,-0.0018317774,0.06733673,0.00824934,-0.02974474,0.04136535,0.018501079,0.0013570039,-0.0020925596,0.0039407187,0.043715496,0.0015295146,0.012421915,0.01359112,-0.014685699,-0.07344505,0.019060986,0.0029956535,0.01444456,-0.03704742,-0.011330635,0.05782502,-0.03875133,0.04839692,-0.04434122,-0.035561007,-0.05040643,-0.0091682235,-0.036629286,-0.00846057,-0.0008408612,-0.045468736,0.0046774917,0.0010327589,0.050155614,0.022849806,0.06716039,-0.011884291,0.03905401,-0.0060782596,0.0076461732,-0.018182598,0.0083088605,0.00562524,-0.036495104,-0.005684182,0.049196023,0.012355901,-0.01918026,-0.06957591,0.10836706,-0.028604742,0.02130754,0.024398444,-0.00729862,0.040291764,-0.0058260285,-0.00905219,0.012200099,0.023487363,-0.0327834,-0.0069754124,0.019059313,0.026628643,-0.0150226345,-0.0046078037,0.027135711,-0.029021066,-0.018757777,0.0007881683,-0.027838586,-0.0055333963,0.015882932,-0.0003823376,0.06670541,0.01042388,0.030750874,-0.008891963,-0.020084476,-0.02541146,-0.045534074,0.0060463357,-0.0050985734,-0.01949559,0.033248335,-0.0025697087,-0.016341768,0.029900407,0.02060371,0.023777956,-0.017032119,0.048275746,-0.005731108,-0.015467585,-0.011278777,0.06709706,0.0128699485,0.041100606,-0.02856605,-0.0014264085,0.04445,-0.08230244,0.0098652225,0.014177581,-0.007517525,0.009751437,-0.014081384,0.0055088582,-0.055263184,0.052937284,-0.031475764,0.048240803,0.025622545,-0.063096784,-0.013724345,0.05870316,0.048797283,0.0083393995,0.035533637,0.046711776,0.09392463,0.0060889455,0.0071987063,-0.009667723,0.041308265,0.05542705,-0.03188581,-0.011881416,0.014104052,0.07943081,-0.0060785115,0.02798499,0.0049097156,-0.034556806,0.022300173,-0.018954402,-0.010396855,0.029649718,-0.008222681,-0.027290158,-0.0068219295,0.036527697,-0.061439715,-0.03203194,0.0023954834,-0.042263653,0.023934975,-0.054557335,-0.087669164,-0.0058535803,-0.01591298,-0.0023396385,-0.040069267,-0.0137675395,-0.021977745,0.01809324,-0.04244659,0.10376363,-0.022543361,0.037974257,0.045901384,0.021380885,0.045499176,0.03768535,-0.040019352,0.07845819,-0.051728062,0.003996266,0.027195407,0.017533628,-0.03571393,0.005046649,0.01780668,0.04441604,0.015155217,0.017158436,-0.022377053,-0.019742496,0.059147526,0.02749788,-0.066958874,-0.050433792,0.029428886,-0.02504435,0.040498562,-0.0118028885,-0.014118756,-0.04577505,0.09411595,0.01373788,0.019137278,-0.025644487,0.020595975,0.011142878,-0.0060157077,0.0681319,0.03238894,-0.0010966407,0.037244596,0.03921235,0.008147888,0.025896797,-0.018728543,0.019028882,-0.049641494,-0.0017018489,-0.0326347,-0.057167336,-0.037401937,0.1123826,-0.03431171,0.028100977,-0.026679667,-0.008496412,0.011616797,-0.028989151,0.009273833,0.060650323,-0.0058081346,0.027234577,0.002324796,-0.056157123,0.078760475,-0.051081482,0.017884245,0.0024771714,-0.025544403,-0.089996226,-0.024828315,-0.005507626,-0.059873834,-0.00539094,-0.010566119,0.02298329,0.022932155,-0.036308818,-0.002226133,-0.07146359,-0.021401336,0.012808921,-0.03756424,-0.010476797,0.0068635214,0.032574542,0.0015984639,0.04527792,-0.013877214,-0.02276621,-0.009238236,-0.0402221,-0.051216085,-0.0075882226,0.00354353,-0.06615174,0.033906877,-0.0017713235,0.0637657,0.031709522,-0.0018690735,-0.02790924,-0.05314064,0.023116095,0.015834047,0.025438953,0.049722414,0.09088104,-0.012026516,0.009708997,-0.040434293,0.018093685,0.029743407,0.022544343,-0.004407875,0.04063624,0.033943098,0.016780253,-0.010704286,-0.05219689,-0.008696509,0.02583731,0.052986402,0.028214894,-0.004482451,-0.051921204,-0.0012982172,-0.039704654,-0.005589567,0.04433132,0.019216308,-0.030492648,0.036933634,0.00362493,-0.022109045,0.029217206,-0.03536276,0.032610577,0.0655347,-0.0046901475,-0.0018426026,-0.033516802,-0.027274955,-0.01634167,-0.03696141,-0.0071794875,0.0015772748,-0.028108437,0.031231128,0.005979187,0.03340176,0.0052338433,-0.0017310979,-0.016337425,0.030140053,0.019105995,0.06927024,-0.024745904,-0.05369268,-0.06699829,0.07635051,0.005381921,0.0053780423,-0.032181814,0.06865457,-0.0015768048,0.02058301,-0.022183023,-0.025655512,0.02487984,0.0043137735,0.014451164,0.058514338,-0.046875745,0.03362432,-0.017339485,-0.020518344,-0.009123368,0.006764201,0.021724613,0.012953786,0.018925045,0.0015882131,0.008939921,-0.028682135,0.05069175,0.004756972,-0.028797364,-0.0005485916,0.024136096,-0.025180312,-0.05603244,0.046342026,0.081353836,-0.008975822,-0.0024853782,-0.008640697,0.035529297,0.002107516,-0.009551328,-0.020889457,0.0022448641,0.002151843,0.015118046,-0.0050053727,0.010433231,0.013899484,-0.025647376,-0.03630237,-0.017503083,-0.033474464,-0.05109984,0.036329266,0.061599787,-0.007274365,-0.029118448,-0.010943551,-0.027481323,0.009398204,-0.011670847,-0.029598894,-0.027723767,-0.01663688]	Keywords: gradient descent, objective function, learning rate, parameters\nKey Objects: objective function, parameters, gradient, learning rate\nRefers to Images: None\nHypothetical Questions:\n- What does it mean for gradient descent to find a 'local' minimum?\n- How does a larger learning rate affect the optimization process?\n- Why is it important to update parameters in the opposite direction of the gradient?\n---\nSummary:\nGradient descent minimizes an objective function by iteratively updating model parameters in the direction opposite to the gradient, with the learning rate determining the step size.\nOriginal Text:\nGradient descent is a way to minimize an objective function J (  ) parameterized by a model's parameters   R d by updating the parameters in the opposite direction of the gradient of the objective function   J (  ) w.r.t. to the parameters. The learning rate  determines the size of the steps we take to reach a (local) minimum. In other words, we follow the direction of the slope of the surface created by the objective function downhill until we reach a valley. 5  \n This paper originally appeared as a blog post at http://sebastianruder.com/ optimizing-gradient-descent/index.html on 19 January 2016.  \n2 http://lasagne.readthedocs.org/en/latest/modules/updates.html  \n3 http://caffe.berkeleyvision.org/tutorial/solver.html  \n4 http://keras.io/optimizers/  \n5 If you are unfamiliar with gradient descent, you can find a good introduction on optimizing neural networks at http://cs231n.github.io/optimization-1/ .\nContextualized Text:\nAs a foundational optimization algorithm, gradient descent minimizes an objective function (J()) by updating model parameters () in the direction opposite to the gradient of the objective function ( J()). The learning rate () controls the size of these steps, guiding the parameter updates downhill towards a minimum (local or otherwise).	{"tags": ["optimization", "algorithm", "neural networks"], "doc_id": "f783886b-e2f8-40ed-b4ee-adff527c3018", "summary": "Gradient descent minimizes an objective function by iteratively updating model parameters in the direction opposite to the gradient, with the learning rate determining the step size.", "doc_type": "text", "entities": [], "keywords": ["gradient descent", "objective function", "learning rate", "parameters"], "key_objects": ["objective function", "parameters", "gradient", "learning rate"], "contextual_text": "As a foundational optimization algorithm, gradient descent minimizes an objective function (J()) by updating model parameters () in the direction opposite to the gradient of the objective function ( J()). The learning rate () controls the size of these steps, guiding the parameter updates downhill towards a minimum (local or otherwise).", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "1 Introduction"}, "hypothetical_questions": ["What does it mean for gradient descent to find a 'local' minimum?", "How does a larger learning rate affect the optimization process?", "Why is it important to update parameters in the opposite direction of the gradient?"]}
940b1bfd-476a-482f-b931-3bf49dbd51ee	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0028787574,-0.014666711,-0.0019772805,0.042657502,-0.04011507,0.053701885,-0.048679586,0.027892828,0.074171446,-0.031927668,-0.0052132155,-0.068849385,-0.024557777,-0.019963,0.005106547,0.013574588,0.041307762,0.061303403,-0.03097254,0.0010125515,0.05383596,0.038103677,0.00585528,0.050891135,0.020026224,0.025488785,0.029774308,0.0498925,-0.013841892,0.030042835,0.00043601755,0.0076612234,-0.007338739,0.009293106,-0.0012716316,0.025512217,-0.023369743,-0.0669039,-0.032757908,-0.056396678,-0.040077195,0.029895471,-0.010221093,0.0022107263,-0.016323036,-0.06381407,-0.050232984,-0.042433966,-0.0059672776,0.0017277182,-0.028930979,0.08293283,-0.018443877,0.035810404,-0.01078469,-0.0054064393,0.019356083,0.0018095503,-0.036595326,-0.02518647,-0.015035912,0.0055784336,0.039829277,-0.03292656,0.04330646,-0.0229199,-0.016633026,0.029975709,0.035667647,0.14177114,-0.047756802,-0.012743543,-0.012191196,-0.074965134,0.1285025,0.048781313,-0.010163516,-0.04017193,0.010703913,-0.030766327,0.016281817,0.022320826,-0.031610608,-0.01871186,0.06147691,0.019795831,-0.08414317,-0.025809811,0.10425731,-0.044665482,2.9748557e-05,-0.006345716,0.027202696,0.057264138,-0.016050166,-0.028905043,0.024135174,-0.0019465409,0.0439052,-0.030961294,0.038339823,-0.019114072,0.008199919,0.09471938,0.004641812,0.025635177,-0.03634324,-0.030087225,0.041013528,-0.0036000984,0.026030846,0.03231259,-0.053810135,0.0031683126,-0.0145650925,-0.011971669,-0.01485333,-0.018946901,0.01694344,-0.066293344,0.044357542,-0.040785827,0.034638654,0.022422597,0.03161745,0.046127807,0.017144902,-0.02037727,-0.017693836,0.0040596244,0.0398285,-0.046127312,-0.009320412,-0.008211962,-0.010643574,0.022851152,0.038511846,0.023651214,0.012388232,-0.025008904,-0.0646115,-0.055097204,-0.010019257,0.022499453,0.0247945,0.033699717,-0.063532226,0.038473405,-0.01338752,0.049368806,0.06992434,0.0032610411,0.05490305,-0.012252865,-0.07108517,-0.046424035,-0.05428409,0.03701576,-0.023870086,0.026928587,-0.037856527,-0.037984554,-0.0036872076,0.04388537,0.057303075,0.09462202,-0.011073576,0.033316113,-0.003553877,0.002339771,-0.038270425,-0.04077889,0.028546095,-0.016213775,0.020584071,-0.016832866,-0.027837101,0.0056263143,-0.047045384,-0.011540446,0.019456254,0.020595388,-0.047688123,0.0038250422,-0.01167942,0.04580592,-0.042542696,0.051261205,-0.038676526,-0.007808262,0.013243819,-0.021351174,0.018056,-0.05602806,0.050074164,0.011212048,0.032927174,0.0035775402,0.023851346,0.012986899,-0.011004724,-0.0012143223,-0.037700947,-0.04086201,-0.03256333,-0.015955025,0.02495566,0.023496985,-0.024601147,0.053838335,-0.03060794,0.022877667,-0.006098947,0.047869615,0.028033344,-0.026865639,-0.07421987,0.002400524,0.030072141,-0.032042313,-0.029269349,0.027258389,-0.009449881,0.040584568,-0.029638613,0.0019889758,-0.04645769,-0.0051684882,0.0433214,-0.006152111,-0.023235293,0.047862183,-0.0602064,0.050150182,0.059359908,-0.047927484,0.0018279649,-0.017268948,0.06167542,0.03586071,0.049704093,0.036775623,-0.0036258812,0.07464434,0.009429918,0.003275274,0.00911215,0.02088093,0.013093203,-0.0027830685,-0.07028334,0.0010408307,0.02976022,-0.030496547,0.032466277,0.0049672034,0.017087957,0.015449531,0.038646083,-0.07916606,0.016156556,0.0028109178,-0.04246901,0.014510555,0.010638128,0.024520084,0.022011526,0.02470559,-0.03704721,0.04145122,0.051963646,0.014670605,-0.010878891,-0.0105174035,0.005004799,0.06064884,0.0073936153,0.051067784,0.02358649,-0.02084946,0.04212879,-0.008825726,0.02620552,0.020282239,0.0063568833,0.002029074,0.028003767,-0.0052499026,-0.08500566,0.013491006,0.012604519,-0.036626913,0.04640671,0.008852611,0.02326874,-0.007407773,-0.010016306,-0.03584975,0.09550744,0.029295307,0.045530763,-0.033265892,-0.05033987,-0.021495154,-0.007267607,0.031630896,-0.06437794,0.058878615,0.0016073387,-0.010328179,0.021915367,0.020068545,-0.011488046,0.0030829874,-0.0029812148,0.008242115,0.060822025,-0.013432119,-0.021978797,0.0007793879,0.042115748,-0.023932397,-0.04325015,0.040855855,0.0401839,-0.017207246,0.020880274,-0.029199762,-0.03966553,0.039883304,-0.044538856,0.062125187,0.05279255,-0.016891425,0.0017705811,0.021755172,-0.07877018,0.003358858,-0.016157836,-0.02722062,-0.021428695,-0.06979269,0.0074038347,-0.016895035,-0.005944778,0.0069127204,0.043486446,0.03566809,-0.0050809374,0.013386564,0.0050681885,0.045590118,0.059801962,0.027707322,0.022713086,-0.030202294,-0.014167658,0.04007529,0.058899015,-0.007912591,0.013332271,-0.038747735,0.021225115,-0.027968744,0.041470654,0.011001793,-0.047882386,-0.0044416287,-0.018402165,0.018384362,0.050576683,-0.029192697,0.045074634,-0.048742358,0.019001225,0.049832102,0.05676027,-0.0115321325,-0.0136110475,-0.07899141,0.01067954,-0.021425908,-0.028755553,-0.054165557,0.012911018,-0.032108575,0.03298952,0.05588736,0.027491257,0.095050655,-0.011735163,-0.0073538786,0.024383778,0.033046998,-0.03202691,-0.013386591,0.06569057,0.048476793,0.012263379,-0.04939978,-0.02190914,0.005205103,-0.06658161,0.034862723,-0.028768923,-0.008865791,-0.06239308,-0.016075233,0.038902767,-0.023252754,0.033266198,-0.038838863,-0.03929807,-0.034418058,-0.002007826,-0.042301767,-0.013465696,0.016352098,-0.038147036,-0.01916944,-0.053859606,0.08734706,0.06192515,0.003423295,0.009939278,0.017651964,-0.0022912286,-0.004008775,-0.013059916,-0.060920637,0.055160455,0.010451707,-0.036153,0.053610615,0.0043387925,-0.05852402,-0.046919428,0.07147072,-0.01182561,0.04995782,0.043991618,0.0045413645,0.020873638,-0.016361875,-0.0017836657,0.009763497,-0.033708822,-0.024649503,0.017796487,0.0063924748,0.008159433,0.015035418,0.07147559,-0.027522516,0.02091563,0.0140617415,0.018467644,-0.019859597,0.002924443,-3.7825925e-05,-0.016128423,0.0010695946,0.020968221,0.020958267,-0.025184678,-0.005903923,-0.008904893,-0.077502035,0.024582338,0.02578304,-0.006821362,0.023434332,-0.01775492,0.007242824,-0.058772154,0.03584101,0.02188016,0.058847893,0.07657878,0.032793295,-0.05726285,-0.015094751,0.040993135,0.010375022,0.02824441,-0.09785931,-0.023848735,-0.024924941,-0.019134907,0.039910045,-0.047493074,-0.05534217,-0.007451257,0.055178378,-0.035110187,-0.06730802,0.05297944,-0.00846984,0.035587337,-0.037912156,-0.035413258,-0.009043354,0.045487136,0.084047146,-0.02233643,0.033889126,-0.023899786,0.051967055,0.0062135514,0.014312936,0.007204749,0.0568075,0.022485731,-0.0024591414,-0.03275752,0.0071629677,-0.006080815,-0.020476703,0.038842455,-0.00083966966,0.020982774,0.0072705727,-0.014989652,-0.025022337,0.051156115,-0.0041442947,-0.0032653655,0.040793803,0.033991627,-0.046558976,-0.014585688,-0.005209565,-0.005223152,0.008240564,-0.019919554,-0.049525596,-0.02295648,-0.021899836,0.031198988,-0.008414381,-0.009230286,-0.023767684,0.0056115002,0.00812916,0.087249994,-0.04468755,0.0155811235,-0.038767762,0.003670611,0.046633527,0.02041838,0.016873226,0.059749488,-0.0728371,0.028722823,0.0047469246,0.03500134,0.010786533,-0.007311221,0.017141607,0.0031364323,0.028846264,0.035526533,-0.0434851,-0.029879857,0.048498113,0.012917958,-0.05996638,-0.03663312,0.032116015,0.00701155,0.013388837,-0.01951391,0.03588275,-0.013884602,0.1106263,0.037424248,0.017839149,-0.06369326,0.033124477,0.012665543,-0.005816934,0.073113784,0.049334683,0.0065571633,0.05180593,0.054147486,-0.009128211,-0.0024234324,0.019137468,-0.013403778,-0.0007880089,-0.03997966,-0.050564144,-0.0053338334,-0.07460435,0.050122943,-0.005111605,0.036398247,-0.0126869865,-0.003283477,0.01343282,-0.04750094,-0.012470301,0.01314447,-0.031365335,0.004148011,0.01973356,0.0001589772,0.06913763,-0.02925104,0.008606882,-0.036983654,-0.0055550844,-0.08206617,0.0065595885,-0.011491826,-0.016572362,-0.014717573,-0.056388997,0.03281354,-0.0037021495,0.0021470108,0.03425173,0.0072648856,-0.03275865,-0.002922846,-0.045431778,-0.005910416,0.011847351,0.00043194395,-0.008625618,0.041361056,-0.0182038,-0.070007645,0.028716927,-0.03351633,-0.06409545,0.029202834,-0.01116211,0.014705882,-0.003660435,-0.018110797,0.03466738,0.005160111,-0.03896314,-0.02685447,-0.059541337,0.042656407,-0.0020385704,0.06006445,0.090595625,0.021079501,-0.03157584,0.0069636307,-0.039307557,-0.003960591,0.045859497,0.047051005,0.0028456466,0.10042265,-0.00078838604,0.026219811,-0.0670925,-0.022581836,-0.05964049,0.07189593,0.052867495,0.031400915,0.014560993,-0.0039962623,0.016386196,-0.0073667304,-0.016015062,0.017023409,-0.006684429,0.022925815,-0.045630123,-0.021453986,-0.048308913,0.05583073,-0.045361463,0.006045269,0.10961106,0.010475717,-0.035676043,-0.04251185,-0.020179916,0.011882141,-0.0048862547,0.0052744513,-0.03490344,-0.012973231,0.005817326,-0.010922093,0.011546839,0.019649426,0.00015241356,-0.029070912,0.030849664,0.03400898,0.021271784,0.018208945,-0.03468082,-0.051531866,0.022906587,-0.005491593,0.023038294,-0.0431968,0.0042797984,-0.01714403,0.011870013,-0.008192253,-0.03794641,0.007446129,-0.0013792055,0.017235609,0.054807685,-0.03047914,0.0011208756,-0.034958817,0.008070127,0.009891004,-0.01976977,-0.0061155674,0.04769586,-0.013722092,0.037126333,-0.011446154,-0.012514404,0.026848143,0.016965285,0.008128448,0.021583915,0.0046500885,0.0012646938,-0.009543051,-0.010586089,0.029249422,-0.032441627,0.0010951657,-0.012960919,0.06777033,0.04154072,-0.007990971,-0.04398478,-0.0020419376,-0.05519422,0.03189539,0.01094091,-0.023943227,0.00431315,-0.009186653,-0.03357459,-0.04429921,-0.06519598,0.013121209,0.053969696,0.06760598,-0.046261426,-0.030167036,0.00029462663,0.022081543,-0.0004174088,-0.030233193,0.0066143414,-0.003082451,0.0067758285]	Keywords: gradient descent, objective function, parameter update\nKey Objects: Gradient Descent, Parameter Update, Objective Function\nRefers to Images: None\nHypothetical Questions:\n- What are the three variants of gradient descent?\n- How does the amount of data used affect the accuracy of parameter updates?\n- What are the advantages and disadvantages of using more or less data for gradient descent?\n---\nSummary:\nGradient descent has three variants that differ in the amount of data used to compute the objective function's gradient, resulting in a trade-off between parameter update accuracy and update time.\nOriginal Text:\n## 2 Gradient descent variants  \nThere are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.\nContextualized Text:\nThree variants of gradient descent exist, each differing in the volume of data employed to calculate the gradient of the objective function. These variants involve a trade-off: more data generally leads to more accurate parameter updates, but also increases the time needed to perform each update.	{"tags": ["optimization", "algorithm", "machine learning"], "doc_id": "940b1bfd-476a-482f-b931-3bf49dbd51ee", "summary": "Gradient descent has three variants that differ in the amount of data used to compute the objective function's gradient, resulting in a trade-off between parameter update accuracy and update time.", "doc_type": "text", "entities": [], "keywords": ["gradient descent", "objective function", "parameter update"], "key_objects": ["Gradient Descent", "Parameter Update", "Objective Function"], "contextual_text": "Three variants of gradient descent exist, each differing in the volume of data employed to calculate the gradient of the objective function. These variants involve a trade-off: more data generally leads to more accurate parameter updates, but also increases the time needed to perform each update.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "2 Gradient descent variants"}, "hypothetical_questions": ["What are the three variants of gradient descent?", "How does the amount of data used affect the accuracy of parameter updates?", "What are the advantages and disadvantages of using more or less data for gradient descent?"]}
4457eef6-3075-4e8c-a78f-aab951b34f7c	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0129966745,-0.02486495,-0.00548211,0.027812403,-0.04078207,0.05544264,-0.006233179,0.022285441,0.013610854,-0.029762011,-0.0070388564,5.6555677e-05,-0.010719621,-0.0017160804,-0.026328035,0.013192618,0.023352133,0.107513025,0.013381069,0.015933285,0.05797762,0.03707565,-0.023392182,0.031255767,-0.0016776695,0.024421098,0.057653196,0.020386947,-0.040519584,0.0034940508,-0.0032474236,0.010929795,-0.03881723,0.0013543462,-0.009581251,-0.010532467,0.008543506,-0.040090863,-0.018126769,-0.023909885,-0.042789165,0.05476461,-0.008087542,-0.046122797,-0.0053133573,-0.0707934,-0.077643804,-0.03447799,-0.0060737333,0.003023168,-0.032475237,0.04184289,-0.059367467,0.035895396,-0.06961168,-0.009334318,-0.0050382414,-0.012542459,-0.02644969,-0.013410788,-0.034367457,0.029920954,0.008982086,-0.026434707,0.027173795,-0.030380175,0.009231862,0.057889972,0.02779043,0.12633044,-0.025283748,0.0033614608,0.014231397,-0.09048854,0.13585374,0.027512496,-0.045328747,-0.059147954,-0.0074326047,-0.012950699,0.032929923,0.037557524,-0.037555892,-0.044102173,0.06498932,0.012026291,-0.0477384,-0.002911077,0.05752756,-0.06594743,0.010876356,0.029004361,-0.0019973228,0.022517908,-0.014501353,-0.019403351,0.041994132,-0.01938546,0.05936366,-0.010480079,0.07949847,-0.0045840605,0.031716365,0.12150934,0.0038379512,0.008324092,-0.031197244,-0.016381782,0.02169023,0.021136582,-0.023774266,0.073628396,-0.056336008,0.030007776,-0.058974333,0.005066666,-0.029413527,0.027868688,0.0017611264,0.02445599,0.047423594,-0.04787317,0.03642305,0.007797385,0.026764607,0.022278396,0.015850546,-0.001345975,-0.05792008,0.022766287,0.00498028,-0.05310603,-0.05466813,0.009493543,-0.0013042713,0.004796325,0.03592603,-0.0012989203,0.009282932,-0.004033511,-0.020422837,-0.048456255,-0.014033905,0.011672441,-0.017759185,0.02433802,-0.055280186,0.051030114,-0.029764792,0.06564684,0.04392205,0.005918643,0.05309664,-0.035913847,-0.07965013,-0.04991478,-0.040334065,0.014363863,-0.0052529667,-0.036336787,0.0003984888,-0.017105207,0.002986385,0.07906267,0.044002086,0.15765455,0.012138739,0.017162258,0.0009921237,0.02039564,-0.030220509,-0.037281137,0.04487372,-0.040621445,-0.02029919,-0.021573838,0.004141476,-0.03208099,-0.017351579,0.021552205,0.0068645785,0.070153534,-0.036609255,0.0052686287,-0.0033768045,0.054186106,0.0010852388,0.010518256,-0.03976085,-0.06544333,0.024700427,-0.0009734716,0.026288304,-0.035219125,0.03104944,-0.0041877474,0.03349296,0.006642554,-0.00034531121,-0.0011149086,0.02798697,0.054867037,-0.059530746,-0.051108453,0.060720246,-0.045832243,0.005349208,-0.033142574,-0.047400504,0.05775006,-0.063773535,0.016165648,0.038404312,0.02400731,0.013148774,-0.007628959,-0.027568402,-0.009713337,0.0433721,0.004641755,-0.039054397,0.010625655,-0.035397824,0.0462747,-0.0067895143,-0.0031142642,-0.028340371,-0.016260857,0.011292538,-0.039536584,-0.02970874,0.009044927,-0.040918414,0.046137035,0.009183923,0.011141779,0.021134596,-0.011115345,0.0146332225,0.026300527,0.04439341,0.0052733,-0.013152389,0.087090775,-0.01573277,0.003955249,0.04411696,0.038042467,0.018118482,0.0100877,-0.02552763,-0.002862628,0.004635998,-0.062345758,0.0068155886,-0.041189805,0.050797753,0.05256899,0.027531656,-0.067981504,-0.0010525581,0.0007655905,-0.008921225,-0.00042458938,-0.0071451236,0.023489313,0.029767757,0.01824058,-0.044113852,0.027637731,0.0070110774,0.031192489,-0.024353214,-0.011948775,-0.028415535,0.076791674,0.0019851134,0.025945107,0.058588438,-0.027642697,0.06969503,-0.012569161,0.011458966,0.003314547,-0.0013743719,-0.043621045,-0.058478314,-0.013918327,-0.024253067,0.017092634,0.017358435,-0.02708984,0.062280566,0.041570447,0.0113486145,0.0048948685,-0.0066812644,-0.029962877,0.051457107,0.017396485,0.07381444,-0.029211663,0.0049672695,-0.007060688,0.018779118,0.022723172,-0.066043474,0.04142988,0.005597844,0.011298186,0.04902055,0.012051542,-0.010556853,-0.025018845,-0.04598606,0.035013102,0.0763625,0.015906181,0.032221284,0.028265145,0.045505308,-0.00036630823,-0.01063506,0.032842483,-0.0067990995,-0.049498614,0.0016108792,-0.019566823,-0.10147548,0.09714648,-0.006787239,0.063501,0.06120317,0.004104235,0.00022608167,0.0075814673,-0.04960769,0.025943333,0.01848807,0.019525025,-0.014901888,-0.09029582,0.030485013,-0.01434202,0.028263431,0.024456538,0.021608748,0.009284391,-0.005874752,0.086528644,-0.028997641,0.054608084,0.0521947,0.043937713,0.004417169,-0.0049509523,-0.0144939525,0.015221261,0.03400964,-0.0074562756,0.018785538,-0.034861557,0.05918709,-0.040578537,0.06232706,0.042918608,-0.043797594,0.028491413,-0.004823847,-0.0046449453,0.0037475396,-0.020238418,0.01056986,-0.042296957,0.034115482,0.034968074,0.0010906435,0.009051093,-0.032559596,-0.050240565,0.016150875,0.031136638,-0.020990904,-0.05281061,0.01544587,0.020952147,0.0215054,0.08688456,0.0024762556,0.032854296,-0.017288292,0.015598841,0.028126163,0.022121027,-0.020719526,-0.03551748,0.017750032,0.065299965,-0.014911429,0.011730256,-0.0078230705,-0.022179633,-0.07589701,-0.020687982,0.0012386156,0.02069255,-0.075487375,-0.0089223245,0.021471811,-0.008117134,0.03419829,-0.08106879,-0.003615111,0.00021551651,-0.009227285,-0.036662117,-0.0417413,-0.03700393,-0.035753854,0.0107213175,-0.007838568,0.061970215,0.061591484,0.037148792,-0.023837114,0.011143217,-0.024350904,0.010662937,-0.009227004,-0.026137384,-0.008796129,-0.013352646,-0.028777856,0.061389152,0.021934412,-0.00957229,-0.053047147,0.10280086,-0.012255555,0.046567716,0.024177898,-0.01744971,0.03724487,0.017326629,-0.029674841,-0.008872651,0.025947114,0.02842209,-0.01546121,0.022110004,0.00390894,0.014834515,-0.01150278,-0.002840199,0.0061161364,-0.019389002,0.009289408,-0.042368315,0.0015431746,0.0070308573,0.012998233,0.011995286,0.01101516,0.025779894,0.0031710463,-0.034180358,-0.013914362,-0.05822758,0.0009596327,-0.043221626,0.00091030594,0.053929705,-0.05280085,0.033506393,-0.03759479,0.009792282,-0.006736126,0.04491403,0.026044063,0.022004098,-0.06551085,0.013731519,0.049650323,-0.0001370298,0.009996256,-0.040958904,-0.056241836,0.016840564,-0.04344304,0.044924002,-0.02594276,-0.006577924,0.016834993,0.009515059,0.0020792012,-0.027509276,0.026021346,-0.020524304,0.018002,0.0041856295,-0.040904626,-0.002577354,0.06349667,0.07631655,0.01638099,0.04556178,0.016937029,0.091027796,-0.0073002395,0.034738384,-0.017888248,0.060071632,-0.0010069385,0.008112036,0.005913965,-0.0026128863,0.018062616,0.023396142,0.010994587,0.02334219,0.004159664,0.019190283,-0.0274151,-0.029669862,0.062749185,-0.002211027,0.021614974,0.033419188,0.051996436,-0.089134365,-0.016292313,0.0031381962,-0.027484309,0.022221772,-0.04606278,-0.049194876,-0.034987193,0.008081416,-0.006042483,-0.01609244,-0.048261736,-0.06045949,0.015444329,-0.016724572,0.03880513,-0.026292024,0.03390147,0.045850627,-0.03877569,0.079985045,0.016859379,-0.025175001,0.046019316,-0.024540601,0.012094206,0.022302115,0.03322365,0.017614635,0.017228851,0.043056,0.0096356,-6.874918e-05,-0.022809923,-0.02670824,0.0022996666,0.03745154,-0.007608377,-0.05591339,-0.062002465,0.028757311,-0.019723179,0.008391675,-0.011019815,0.014141458,-0.0077974848,0.0646145,0.04538906,0.02428756,0.00494616,0.017535932,-0.0023782665,0.027556926,0.0902013,-0.009832079,0.017300414,0.083481245,0.0408007,-0.006854637,0.016126152,0.0030259036,-0.007716667,-0.024061954,-0.008201131,-0.046914335,-0.02848688,-0.049609065,0.05995128,-0.029797504,0.05422992,-0.021408211,-0.008679207,0.018528542,-0.053384885,-0.04332213,0.011850508,0.024215052,0.011548386,0.020250479,-0.036309067,0.079141624,-0.06197834,-0.020027634,-0.012590598,-0.0023682103,-0.03391028,-0.020895427,-0.0068165637,-0.0475107,-0.021717707,-0.030063238,0.042067964,0.02739052,-0.012956074,0.037536833,-0.04330431,0.0058556106,0.0038481923,-0.049107548,-0.050180335,0.015024668,0.018185338,-0.0028091166,0.080949925,-0.035993047,-0.04200633,0.03495518,-0.03048396,-0.073255494,-0.012827764,0.01588405,-0.026438601,0.03568375,0.032049783,0.04385835,0.0012130341,-0.057300877,-0.017744668,-0.07645191,0.0005289747,-0.014763712,0.08377917,0.061121047,0.015007195,-0.010548485,0.00024967673,-0.07115866,-0.016334696,0.047884956,0.036743272,-0.03332167,0.07033958,0.040930387,0.0025210974,-0.058972605,-0.047222085,-0.038793832,0.027692182,0.0178394,-0.0021866825,0.024620693,0.002402389,0.0095294425,-0.009958895,-0.0031319223,0.021089496,-0.012249505,-0.026296811,0.00078845304,0.005265134,-0.0020468966,0.038596872,-0.011409381,0.00039271984,0.069122784,-0.034543626,-0.012599417,-0.010075665,-0.010266936,-0.01623191,-0.029046714,0.02232868,-0.0025326961,-0.0075758,0.012333364,-0.0363844,0.046247687,0.013088981,-0.0067536905,-0.016214332,0.0069572064,0.017326275,0.045018747,-0.017914388,-0.06058055,-0.08663848,0.035914473,0.005945487,-0.01113596,-0.021486133,0.023391133,-0.022304595,-0.021199834,-0.012915793,-0.037512306,0.0042848284,-0.013158766,0.01884966,0.023548763,-0.027089529,0.036013637,-0.08338538,-0.043643508,0.022078278,-0.0023226042,-0.01420978,0.020985272,0.02835948,0.011965521,-0.030727835,-0.032941736,0.020732172,0.036347806,0.015753416,0.037986636,-0.009443167,0.018992644,-0.0100480225,0.009467134,0.049002398,-0.016904274,0.020379897,-0.009345578,0.0397848,-0.014395717,0.0063254433,-0.046195924,-0.021362048,-0.010667051,0.041288555,0.013284003,-0.0013110936,0.007315725,0.014604884,-0.03557629,-0.03775194,-0.024663111,-0.037288804,0.05215599,0.057952236,-0.03170782,-0.074866556,-0.020920923,-0.004864594,0.032756597,-0.012002634,-0.039489325,-0.012143611,-0.0036590735]	Keywords: batch gradient descent, gradient descent, cost function, training dataset, parameters, learning rate\nKey Objects: cost function, parameters, training dataset, gradient\nRefers to Images: None\nHypothetical Questions:\n- Why is batch gradient descent slow compared to other optimization methods?\n- What are the limitations of batch gradient descent when dealing with datasets that exceed available memory?\n- How does the learning rate influence the parameter updates in batch gradient descent?\n---\nSummary:\nBatch gradient descent calculates the gradient of the cost function using the entire training dataset to update parameters, making it slow and unsuitable for large datasets.\nOriginal Text:\n### 2.1 Batch gradient descent  \nVanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters  for the entire training dataset:  \n<!-- formula-not-decoded -->  \nAs we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that do not fit in memory. Batch gradient descent also does not allow us to update our model online , i.e. with new examples on-the-fly.  \nIn code, batch gradient descent looks something like this:  \n```\nfor i in range(nb_epochs): params_grad = evaluate_gradient(loss_function , data , params) params = params -learning_rate * params_grad\n```\nContextualized Text:\nVanilla gradient descent, also known as batch gradient descent, computes the gradient of the cost function with respect to model parameters using the entire training dataset. Because this process requires calculating gradients for the entire dataset before each parameter update, batch gradient descent can be slow and is often impractical for very large datasets.	{"tags": ["optimization", "machine-learning", "algorithm", "gradient-descent"], "doc_id": "4457eef6-3075-4e8c-a78f-aab951b34f7c", "summary": "Batch gradient descent calculates the gradient of the cost function using the entire training dataset to update parameters, making it slow and unsuitable for large datasets.", "doc_type": "text", "entities": [], "keywords": ["batch gradient descent", "gradient descent", "cost function", "training dataset", "parameters", "learning rate"], "key_objects": ["cost function", "parameters", "training dataset", "gradient"], "contextual_text": "Vanilla gradient descent, also known as batch gradient descent, computes the gradient of the cost function with respect to model parameters using the entire training dataset. Because this process requires calculating gradients for the entire dataset before each parameter update, batch gradient descent can be slow and is often impractical for very large datasets.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "2 Gradient descent variants", "h3": "2.1 Batch gradient descent"}, "hypothetical_questions": ["Why is batch gradient descent slow compared to other optimization methods?", "What are the limitations of batch gradient descent when dealing with datasets that exceed available memory?", "How does the learning rate influence the parameter updates in batch gradient descent?"]}
09121bec-e2b4-4b3b-8f29-b0548817e426	abe8c200-bfa1-4355-947e-23ea618c310d	[0.007979379,0.02317884,0.0026884873,0.028079871,-0.02769815,0.07946648,-0.0008553443,0.029937068,-0.0055742394,-0.036182195,-0.003402791,0.0028573107,-0.009093314,0.005821182,-0.050604023,0.021118581,0.002454618,0.072902225,-0.0035359547,0.035724834,0.08754763,0.04190359,-0.003924087,0.027885584,0.0034703996,0.045154024,0.054940406,0.038547173,-0.03104311,0.017645663,-0.011741266,0.01647494,-0.046890106,0.03643223,-0.019882504,-0.023311287,0.020997874,-0.030860411,-0.018953757,-0.038231466,-0.060059443,0.063063316,-0.025110418,-0.041328788,-0.009628292,-0.06748892,-0.067612305,-0.030708158,-0.0137304645,-0.0062862607,-0.023673857,0.03249137,-0.038428564,0.021048328,-0.074395224,0.0026541366,0.014901442,-0.0012249412,-0.017505392,-0.046067018,-0.04543058,0.015050964,0.016433014,-0.020729195,0.042311367,-0.045755778,-0.006249876,0.0575552,0.029917464,0.11441389,-0.03468397,0.018493697,-0.008324515,-0.075605355,0.104000635,0.035466462,-0.041941598,-0.07447047,-0.004244566,-0.012522688,0.02088258,0.030027257,-0.013444112,-0.04591148,0.07712014,0.016091118,-0.058564883,-0.01619242,0.06161843,-0.027558293,0.0008641466,0.01800838,-0.0062294016,0.008845443,-0.017357804,-0.021024035,0.03863426,-0.036180913,0.05378769,-0.011651035,0.052032,0.0045082006,0.047032803,0.11671868,-0.010871217,-0.00012420978,-0.03954945,-0.023743732,0.009619008,0.025884664,-0.026877636,0.10424739,-0.061229307,0.028451033,-0.06019887,0.018714488,-0.031332303,-0.0045839404,-0.016171953,-0.0026935285,0.06284514,-0.016332649,0.036499254,-0.006050448,0.04485843,0.018833242,0.012712684,-0.032938294,-0.04885376,0.0152225075,0.056436963,-0.0382252,-0.056542724,0.04255555,-5.190614e-05,-0.009369594,0.05308192,-0.00035318529,0.0075422716,0.023527242,-0.022745268,-0.04454762,-0.01934779,0.020309892,-0.010207893,0.0389241,-0.05474031,0.04966439,-0.0132455425,0.05571262,0.021871995,0.013266325,0.084778585,-0.024935884,-0.070197105,-0.058266778,-0.014912401,0.01434081,0.0150542995,-0.04930669,-0.025737092,-0.016550044,-0.014732614,0.07631911,0.051251348,0.14057882,-0.0062212762,0.019305272,0.020460794,0.05664756,-0.016115446,-0.030834984,0.056058772,-0.043307398,-0.012603655,-0.022628292,0.018000316,-0.029136285,-0.04432708,0.027657252,0.014271363,0.069467194,-0.04137701,0.025956424,-0.026357953,0.06623941,0.017440759,0.03268086,-0.029554486,-0.073441565,0.025992744,0.004482694,0.023254575,-0.012684435,0.034375805,0.0020247374,0.02543912,0.011386257,-0.004258195,-0.008397655,0.004141546,0.055068705,-0.033270482,-0.050557647,0.04555183,-0.029217025,-0.010577485,-0.02231963,-0.06990132,0.04374981,-0.073216505,0.037521847,0.032593306,0.030332612,-0.000685975,-0.020507062,-0.029292863,-0.025132217,0.056231298,0.0073312703,-0.037117593,0.020593472,-0.009577847,0.039983585,0.008622103,0.013449956,-0.018970793,-0.0058662696,0.030717276,-0.014234431,-0.0149345,0.040292364,-0.047821704,0.07414069,-0.024384055,-0.022109304,0.032651503,0.0066129845,-0.0130995605,-0.0075536426,0.05103548,-0.009999155,-0.004832023,0.07658995,-0.007131308,0.017164635,0.060539935,0.016619002,0.019517677,0.0109988935,-0.031134183,0.0011675389,0.014367953,-0.038103737,-0.0063627255,-0.051824693,0.05111055,0.07373711,0.044209138,-0.038633216,0.0062004114,-0.0073988084,0.014939915,-0.019413097,-0.03437098,0.045354035,0.030689543,0.026196174,-0.051291678,0.03491515,0.0068163704,0.027890423,-0.023534622,0.019823616,-0.031910017,0.035822835,0.008448267,0.023693992,0.053694393,-0.034687683,0.09066232,-0.0049275625,-0.0076209498,-0.005919285,0.0054136855,-0.041596513,-0.043602627,0.0029933292,-0.03515736,0.024354557,0.0038924173,-0.031309363,0.027316378,0.032837816,0.0138396695,0.003041467,0.0031191483,-0.015030962,0.032677747,0.02248424,0.08248312,-0.039761048,0.0068438235,0.0071613993,0.018492412,0.022320198,-0.071245365,0.032374263,0.012458427,0.021520797,0.038253717,-0.007788254,0.0030749375,-0.0033548686,-0.009382251,0.015635256,0.09177248,0.005307945,0.04230724,0.021570845,0.020679522,0.007438979,-0.0069532185,0.027476426,-0.010313294,-0.038816627,-0.0046701385,-0.03428807,-0.10026154,0.079314835,-0.023825333,0.05489526,0.052038897,-0.020573448,0.0032401374,0.016994318,-0.046585385,0.022092067,0.0049116886,0.021572283,-0.0022974678,-0.099090524,0.011753421,0.016288474,0.041798387,0.018874973,0.025591832,0.036815792,-0.008416002,0.10937946,-0.025311703,0.0861431,0.029119879,0.045025002,0.010162313,-0.008204976,-0.030172331,0.023856547,0.045845482,-0.01756742,0.027091555,-0.023044959,0.058138087,-0.021420706,0.041222457,0.038770467,-0.054711122,0.008301687,-0.00062570366,-0.0026032724,0.0076571736,-0.0061943755,0.011956322,-0.041778073,0.041896608,0.03502236,0.019395571,-0.017827708,-0.03754336,-0.08192923,0.011835841,0.034685962,-0.038166396,-0.030141953,0.040451262,-0.002468349,0.02365312,0.0802165,0.011365988,0.033940807,-0.029667858,0.0010749871,0.012594996,0.012951941,-0.024935992,-0.030587617,0.010240313,0.027661843,-0.0047732783,0.0010790569,0.028786242,-0.019390816,-0.07534045,0.0045923344,0.003760643,0.037698954,-0.04448223,-0.018093057,0.043508034,-0.011158525,0.031591505,-0.07313921,-0.026474632,-0.018031748,-0.0058492194,-0.013029075,-0.0299155,-0.029736953,-0.014754455,0.013974537,-0.014079417,0.033108585,0.050463103,0.044600125,-0.022366764,0.018352551,-0.03965421,0.014575428,-0.0076122736,-0.033401355,-0.020786608,-0.02242122,-0.038662184,0.04565261,0.013088271,-0.016334144,-0.013061745,0.0751095,-0.025194256,0.018673277,0.025636556,-0.012061244,0.043655895,0.020917559,-0.057616852,-0.018023826,0.032900568,0.0321576,-0.0016784332,0.015339334,0.0071717785,0.0061256625,-0.006305745,0.019813828,-0.0072406437,-0.027185177,-0.02190676,-0.041013133,0.003680502,-0.0021145213,0.0077294335,0.027921164,0.0060819136,0.029838994,0.0053135036,-0.05288621,-0.029548226,-0.052379135,-0.005538943,-0.026361147,-0.0023325905,0.042760067,-0.04652662,0.03420104,-0.024755022,0.043610387,-0.011339858,0.026224691,0.006981851,0.0014382601,-0.018572597,0.03091047,0.020326838,0.00543854,-0.0024197544,-0.016572684,-0.05908313,0.022617184,-0.044033848,0.050615765,0.0031347668,-0.018317655,0.035898093,-0.01406084,-0.00050386885,-0.024243232,0.046252873,0.0015166809,0.035012875,-0.017794143,-0.066119574,-0.0031528256,0.06267761,0.03486303,-0.0040106303,0.05173999,0.00088814256,0.067798406,0.0004017268,0.0374192,0.0043220646,0.076732814,0.024518562,-0.0003001861,0.030844169,0.021013884,0.010933771,0.023949118,0.028855834,0.03531498,-0.017125683,-0.004631142,-0.027400712,0.0028046821,0.06193092,0.023502398,0.016849658,0.03692194,0.026766518,-0.08814236,-0.026265651,-0.017419172,-0.029065913,0.022063842,-0.06018706,-0.047857627,-0.01682371,-0.010014332,-0.03507041,-0.026478058,-0.03715576,-0.054814707,0.02135046,-0.023486603,0.025909273,-0.03113543,0.035501357,0.04777957,-0.03409171,0.05028826,0.012920322,-0.021902231,0.048638307,-0.021388862,-0.011372907,-0.0017768128,0.038586825,0.009841173,0.020632952,0.035496257,0.03115926,0.007036645,-0.007821925,-0.031345315,-0.015161022,0.04425462,-0.028394239,-0.10104545,-0.06349047,0.018871954,-0.00871236,0.021718778,-0.014935273,0.015441341,-0.038357824,0.05027128,0.057053994,0.044085406,0.009548284,-0.004516093,-0.003997759,0.052484702,0.064444564,0.01121338,0.013464113,0.062430806,-0.004518311,-0.0108391205,-0.011236134,-0.0064658164,-0.013251658,-0.010611994,-0.022964366,-0.045563992,-0.055937894,-0.01786855,0.06204934,-0.06366179,0.064125806,-0.02614876,-0.012819236,0.018586796,-0.060952492,-0.031302508,0.0055490457,0.04182261,0.054343197,0.0407131,-0.010980118,0.08187103,-0.057245623,-0.03796804,-0.007691008,0.007784251,-0.04328493,-0.008925987,-0.007071466,-0.049339857,-0.0077568903,-0.029307468,0.03987268,0.01432078,-0.0130377915,0.013110675,-0.04042511,0.017450096,0.012314963,-0.020235823,-0.04645822,0.020973582,-0.0069296495,-0.018007176,0.057605755,-0.042812515,-0.026929313,0.014633079,-0.02647427,-0.12507744,0.0010173032,-0.003886157,-0.040918276,-0.0066161742,0.032052666,0.031448107,0.002946539,-0.05313316,-0.026897587,-0.06695198,0.0031045286,-0.0057830843,0.070368975,0.06423048,0.014994074,-0.012778455,-0.009196376,-0.048804186,-0.040729336,0.058037914,0.020894555,-0.062028684,0.057376735,0.041988943,0.002375386,-0.06061831,-0.037176292,-0.028807761,0.019512288,0.020894932,0.0074831108,0.03782203,-0.009942669,0.008017808,-0.03161137,-0.0034742667,0.004491559,0.008979787,-0.00557904,3.846978e-05,-0.004218803,-0.023612438,0.046102174,-0.01980085,0.022131305,0.05743534,-0.030673312,0.004911895,-0.007816222,-0.002821116,-0.010953809,-0.037322715,0.019670062,0.013707533,0.009291168,0.042048372,-0.04602066,0.019984707,-0.005572688,-0.018539999,-0.020091316,-0.0072892457,0.0275335,0.03363456,-0.0053185807,-0.046352923,-0.1046519,0.042632412,0.0013872962,-0.023125874,-0.018450974,0.04457828,-0.027565438,-0.0319245,-0.002589908,-0.030552177,-0.0116544105,0.0030399426,0.015078616,0.0013099451,-0.03860249,0.026241155,-0.07335461,-0.041418597,0.015494426,-0.033853438,-0.041493136,0.022926517,0.048936088,0.024850989,-0.049211428,-0.048187155,0.003288393,0.014502482,-0.0037736513,0.047153067,-0.016137892,0.008115087,-0.017641503,0.02738965,0.047566507,0.009148512,-0.0014588455,0.027393814,0.03584972,-0.012241068,-0.01846899,-0.051593117,-0.027938005,-0.0090656085,0.026643602,0.006888031,-0.005706484,-0.003095645,-0.0043094796,-0.047918074,-0.020906992,-0.020250231,-0.028139751,0.03822974,0.03893426,-0.0032350726,-0.055788014,-0.014811479,-0.002418624,0.030566184,-0.0075400835,-0.028793687,0.008411674,-0.010427006]	Keywords: batch gradient descent, gradient descent, cost function, parameters, learning rate, automatic differentiation\nKey Objects: parameters, cost function, gradient\nRefers to Images: None\nHypothetical Questions:\n- Why is batch gradient descent considered 'slow'?\n- What is the role of the 'learning rate' in the parameter update process?\n- How does automatic differentiation simplify the gradient calculation?\n---\nSummary:\nBatch gradient descent iteratively updates model parameters by computing the gradient of the cost function over the entire training dataset and adjusting parameters accordingly.\nOriginal Text:\n```\nfor i in range(nb_epochs): params_grad = evaluate_gradient(loss_function , data , params) params = params -learning_rate * params_grad\n```  \nFor a pre-defined number of epochs, we first compute the gradient vector params\\_grad of the loss function for the whole dataset w.r.t. our parameter vector params . Note that state-of-the-art deep learning libraries provide automatic differentiation that efficiently computes the gradient w.r.t. some parameters. If you derive the gradients yourself, then gradient checking is a good idea. 6  \nWe then update our parameters in the direction of the gradients with the learning rate determining how big of an update we perform. Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.\nContextualized Text:\nIn batch gradient descent, the model's parameters are updated by first calculating the gradient of the cost function with respect to these parameters for the entire training dataset. For a pre-defined number of epochs, the gradient vector, params_grad, is computed and the parameters are adjusted using a learning rate to determine the magnitude of the update. This iterative process, batch gradient descent, is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.	{"tags": ["optimization", "machine learning", "deep learning", "algorithm"], "doc_id": "09121bec-e2b4-4b3b-8f29-b0548817e426", "summary": "Batch gradient descent iteratively updates model parameters by computing the gradient of the cost function over the entire training dataset and adjusting parameters accordingly.", "doc_type": "text", "entities": [], "keywords": ["batch gradient descent", "gradient descent", "cost function", "parameters", "learning rate", "automatic differentiation"], "key_objects": ["parameters", "cost function", "gradient"], "contextual_text": "In batch gradient descent, the model's parameters are updated by first calculating the gradient of the cost function with respect to these parameters for the entire training dataset. For a pre-defined number of epochs, the gradient vector, params_grad, is computed and the parameters are adjusted using a learning rate to determine the magnitude of the update. This iterative process, batch gradient descent, is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "2 Gradient descent variants", "h3": "2.1 Batch gradient descent"}, "hypothetical_questions": ["Why is batch gradient descent considered 'slow'?", "What is the role of the 'learning rate' in the parameter update process?", "How does automatic differentiation simplify the gradient calculation?"]}
9108e63f-785e-4715-b857-fec37195dce8	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0102248285,0.0033952217,0.0026813035,0.050675187,-0.021604003,0.050412435,-0.0146078,0.040908683,0.017354999,-0.026384052,-0.031046228,-0.03488063,-0.018674633,-0.0011922352,-0.022989305,0.0012613613,0.023090748,0.10632391,-0.0028793297,-0.008294366,0.06489759,0.0095955245,0.017046457,0.007486854,-0.023805287,0.017174514,0.017732417,-0.0053716963,0.0012340429,0.0027807732,-0.04394451,0.017184842,-0.023896988,-0.009662286,0.02895499,-0.032680444,-0.004437661,-0.03129588,-0.026438907,-0.04555946,-0.041236274,0.023425933,-0.03768368,0.0074882377,0.0077586235,-0.09778702,-0.08541911,-0.020999555,0.01307252,-0.008936338,-0.00946884,0.03278522,-0.0389333,0.039658606,-0.07711122,0.03975436,-0.014591159,-0.016760046,0.0007640378,-0.03368078,-0.03821467,0.0077811414,0.0456141,-0.006644371,0.04543193,0.009537446,-0.0013165324,0.041163944,0.053772695,0.12562391,-0.023878325,0.043859847,-0.055940475,-0.08736126,0.10901743,0.04482268,-0.059039194,-0.04893421,-0.029608794,-0.009466655,0.028698906,0.062320843,-0.048717894,-0.005827183,0.050196093,0.0084660165,-0.06373574,-0.023105025,0.042071927,-0.027527053,-0.039539848,-0.006764994,-0.025959304,0.037370183,-0.026744267,-0.024896497,0.042976007,-0.019291153,0.002371995,-0.037384167,0.05598185,-0.013323905,0.051352672,0.10363743,-0.00013580857,0.01091076,-0.021537201,-0.020460736,-0.0075776717,0.0032394908,-0.0065270024,0.03122847,-0.07287845,0.011420267,-0.097116396,0.002896665,-0.033698212,-0.022221567,0.025353018,-0.021682443,0.016560158,0.01493815,-0.011656116,0.0048797303,0.034993816,0.059580818,-0.01106156,-0.029763613,-0.034197517,0.010122631,0.02479707,-0.039100718,-0.021740614,0.039863843,0.00010937115,-0.026990714,0.013093541,-0.02076771,0.020476872,0.0056521785,-0.05203772,-0.027935518,-0.025325013,-0.009115928,0.007830597,0.031388994,-0.07761641,0.041510325,-0.01603373,0.057458993,-0.0020451904,-0.009638705,0.045845836,-0.03497653,-0.05298812,-0.08174002,-0.035788685,0.044392,0.013211391,-0.016935123,-0.0153493555,-0.008964327,-0.0060828165,0.05511992,0.06749985,0.103378154,-0.024388729,0.00019164896,0.005252255,0.0037530796,-0.042132135,-0.042146757,0.053264655,-0.020528046,0.006078737,-0.031457026,0.010939552,-0.030986933,-0.019845579,0.023847563,-0.019508906,0.04411884,-0.02132382,0.008969329,0.00884409,0.073184095,-0.027173454,0.016062943,-0.00968351,-0.021187855,0.025565527,0.021556918,-0.0029033103,0.0041947337,0.082942024,0.020497372,0.029622655,-0.0052065104,-0.004605954,-0.006950293,-0.015272232,0.049546637,-0.052626807,-0.06776534,0.06422787,-0.031170828,-0.00020756637,-0.02889638,-0.041714225,0.023586309,-0.050254986,0.03413578,0.04925765,0.015907386,0.020719921,0.017942632,0.009330399,0.010677149,0.06763827,-0.015843406,-0.056720562,0.031864822,0.0070159794,0.04249115,-0.04495436,-0.014824659,0.017682362,0.0007340976,0.046478372,-0.004006163,-0.01486735,0.07032367,-0.032899905,0.059867132,2.9264325e-05,0.023005761,0.0067023127,0.044451408,0.0076891356,0.018337557,0.046980366,0.023690304,0.019109147,0.08628666,-0.019298857,-0.01892089,0.033396807,0.027463665,0.025882436,0.024538973,-0.02429911,0.018160257,0.024688177,-0.020991266,0.00076969253,-0.046203304,0.02579592,0.037796058,0.03317944,-0.058526106,-0.04138161,0.013257924,-0.005512286,0.058312796,0.0194984,-0.0070535983,0.022364106,0.0143911885,-0.056102924,0.051534966,0.01238118,0.024165314,-0.028141133,-0.02286851,-0.010243951,0.06691137,0.004370958,-0.0026488644,0.053472996,-0.014046114,0.049908653,0.023259677,0.038909193,0.00605976,-0.019384246,0.016490746,-0.034836806,-0.016511912,-0.0068550147,0.044694606,0.009987977,-0.051446423,0.03703271,-0.0046009175,-0.025833813,0.013270896,-0.009114334,-0.01903029,0.011637879,0.016333407,0.07602211,-0.037467714,0.016195182,-0.04505022,0.032153312,0.034536585,-0.071395524,0.010300494,0.02255445,0.023118006,0.052474324,-0.010773513,-0.00736944,-0.004272706,0.0026268198,0.04640667,0.08252184,0.0043950705,0.017658312,0.024828836,0.055954244,0.0050519044,-0.0189153,0.03386559,0.019725468,-0.017417565,0.023741117,-0.026373625,-0.051604934,0.088097535,-0.013019571,0.034481063,0.05156197,0.007008806,-0.019549854,0.010364146,-0.038030107,0.024135303,0.02733739,-0.010509291,-0.02793724,-0.044478305,0.025581134,0.0058350083,-0.01658015,0.014629037,-0.024356082,-0.008367601,-0.018227419,0.073598735,-0.02951831,0.06638068,0.040369853,0.051031794,-0.001505193,-0.0067692716,-0.0037048408,0.021247365,0.055784192,-0.052371413,0.04779635,-0.046925835,0.07253745,-0.04993245,0.037801392,0.015765168,-0.036778353,0.01805287,-0.023964878,-0.024274336,0.060662035,0.008992249,0.03437978,-0.06012526,0.03451931,0.042449888,0.046956543,-0.016159862,-0.023106618,-0.04532164,-0.0025118547,0.03974675,-0.072864525,-0.044837344,0.026507193,0.018748468,0.0045859506,0.08280579,-0.030060915,-0.0037643246,-0.06454297,-0.03682863,-0.0014315543,0.0074509736,-0.07229233,-0.042904306,0.037255336,0.02428945,0.021299364,0.00093545666,0.029608794,0.042085975,-0.075236484,0.02779283,-0.027783353,0.0016162387,-0.068670616,-0.016172407,0.046379536,-0.009583307,0.04222502,-0.07335844,-0.023795662,0.038363677,0.0036332833,0.0040652296,-0.05307911,-0.02192945,-0.029507345,-0.0039641666,-0.035235915,0.05485528,0.035043497,0.02338123,-0.007498126,0.024644587,-0.044757657,0.014039806,-0.026227143,-0.032569066,0.010687126,-0.0037437864,-0.027033748,0.09014687,0.022054741,-0.017260693,-0.025959825,0.071551144,-0.0257805,0.041631646,0.035531662,-0.052801833,0.015000633,0.0003498859,-0.026449114,-0.0012842951,0.02530661,0.020195384,-0.0034535273,-0.00594646,0.023808394,0.015250836,-0.008075253,-0.011092927,0.012728839,0.0056604077,0.023934314,-0.029473199,-0.0046991035,0.020555241,-0.0050353794,0.011871979,0.038740277,0.029301114,-0.023367202,0.00020007351,-0.03131559,-0.04198368,0.0016503583,-0.023194566,-0.04244415,0.058939695,-0.060664285,0.05324581,-0.020657219,0.012180133,0.006182021,0.04303534,0.028218025,0.025568949,-0.06140292,-0.009026968,0.031432215,0.003581103,-0.0069263605,-0.047013056,-0.047762707,-0.019613337,0.0019242602,0.010915634,-0.045512132,0.015531364,-0.0050658216,-0.0021228867,-0.014947502,-0.056201644,0.028488291,-0.0036318686,0.031388577,-0.025512176,-0.045168437,-0.0025037776,0.06409663,0.08236437,-0.012829054,0.028048886,0.013128199,0.069162905,0.0019060465,0.047976974,-0.038299613,0.025444424,-0.027311824,-0.00024888467,0.019472584,-0.00785276,0.0024485316,-0.015276227,0.0035965717,-0.0068077864,-0.025832955,0.034665428,-0.0011159292,0.0067760274,0.015038624,-0.00022277718,-0.015352395,0.019315157,0.06509278,-0.042422894,-0.04754896,-0.02467106,-0.014062176,-0.006263644,-0.059206177,-0.041398723,-0.02623602,0.008266952,-0.007180083,-0.018678246,-0.029607123,-0.08257568,-0.027944786,-0.0070578773,0.0384744,-0.014951017,0.037491716,0.0554435,-0.021943608,0.10050538,0.049807135,-0.027087726,0.009110236,0.00027479537,0.008928685,0.0011817721,0.0059576407,-0.005320609,0.022195684,0.033558328,-0.013914182,0.008423872,0.0025116545,-0.050567605,-0.016083794,0.0068640504,-0.00821789,-0.031249337,-0.074368164,-0.00084988534,0.010008281,0.009289617,-0.0005742562,7.1797054e-05,-0.06332063,0.07143728,0.055701517,0.017612696,0.0051498553,0.07045866,0.044609305,0.012395576,0.08032335,0.03455326,0.003117777,0.0661678,0.0066498835,-0.023117501,0.0060206326,-0.00010904915,0.031129517,-0.0013925035,-0.03213975,-0.030450504,0.012185779,-0.043272145,0.028873935,-0.045568008,0.033288177,-0.017672038,-0.04157618,0.03411396,-0.076658316,-0.05045391,0.029272445,-0.02516333,0.010958759,0.014481208,0.0043200287,0.042692028,-0.028915578,-0.034714285,-0.02092784,-0.024011603,-0.053562567,-0.043344773,-0.014374996,-0.015661752,-0.01530271,-0.018340291,0.02391584,-0.0054714293,-0.0588452,-0.0041066394,-0.054917913,0.02668651,-0.0077245543,-0.023166247,-0.054875188,0.02030802,0.00042075623,-0.016784275,0.0831679,-0.03778108,-0.05108932,0.044833615,-0.0050736,-0.07034962,0.006525929,0.032168154,0.010758383,0.027445022,0.0031249665,0.028136658,0.0028997664,-0.0025937317,-0.00694841,-0.049355187,0.017236063,-0.05067916,0.048043966,0.0772293,0.018186646,-0.015150678,-0.040409308,-0.065386936,0.01534614,0.039889332,0.027405389,-0.0021782261,0.047508102,0.052201565,0.047234852,-0.052777328,-0.04023953,0.001641243,0.04489174,0.0029270626,0.0036642326,0.049697846,-0.010720593,-0.0024243281,-0.01184444,-0.017033998,-0.0061623645,0.013891048,-0.039779026,-0.011911854,-0.040569905,-0.043302,0.05846097,0.0009931803,0.018918287,0.09032364,-0.017717788,-0.051009454,-0.07100434,0.006982035,-0.017061513,-0.0055045383,-0.006821869,-0.03920375,-0.03333808,0.022283442,-0.018602705,0.043965247,0.016705554,-0.055780232,-0.018958248,0.022234904,0.010396039,0.017714083,0.018395698,-0.076272406,-0.060293227,0.061518095,0.015023135,0.04506137,-0.03399421,-0.012584874,-0.021569105,-0.013441268,-0.0196574,0.0029625173,-0.040816445,0.011490323,0.021946734,0.014018228,-0.0066147083,0.03332312,-0.060606096,-0.020229097,0.001990311,0.019380806,-0.04147111,0.03272228,0.04468992,0.029459909,-0.023976363,0.010540416,0.05189104,0.0087827435,0.017526854,0.07058875,0.008201392,0.0073353047,-0.043926,0.0059426483,0.046658587,-0.0526529,0.018129414,-0.0043177675,0.084337994,-0.019996332,0.0011886465,-0.031575028,-0.03019653,-0.01198718,0.028494475,0.021991346,-0.031494804,0.009894594,0.049511384,-0.028878145,-0.030416306,-0.0040834756,-0.028013552,0.09385079,0.03402615,0.024086908,-0.01931957,-0.056823842,-0.026613224,0.021971656,-0.021443736,-0.0086956285,0.004587001,0.021053627]	Keywords: stochastic gradient descent, SGD, parameter updates, batch gradient descent, online learning\nKey Objects: training examples, parameters, gradients\nRefers to Images: None\nHypothetical Questions:\n- What is the main advantage of SGD compared to batch gradient descent?\n- Why does SGDs frequent updates with high variance cause the objective function to fluctuate?\n- In what scenarios would SGD be particularly beneficial for learning?\n---\nSummary:\nStochastic gradient descent (SGD) updates parameters for each training example, overcoming the redundancy of batch gradient descent and enabling faster online learning.\nOriginal Text:\n### 2.2 Stochastic gradient descent  \nStochastic gradient descent (SGD) in contrast performs a parameter update for each training example x ( i ) and label y ( i ) :  \n<!-- formula-not-decoded -->  \nBatch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily as in Figure 1.\nContextualized Text:\nUnlike batch gradient descent, which recomputes gradients for similar examples, stochastic gradient descent (SGD) performs a parameter update for each individual training example and its associated label. This approach eliminates redundant computations, making SGD significantly faster and suitable for online learning scenarios.	{"tags": ["optimization", "machine learning", "deep learning"], "doc_id": "9108e63f-785e-4715-b857-fec37195dce8", "summary": "Stochastic gradient descent (SGD) updates parameters for each training example, overcoming the redundancy of batch gradient descent and enabling faster online learning.", "doc_type": "text", "entities": [], "keywords": ["stochastic gradient descent", "SGD", "parameter updates", "batch gradient descent", "online learning"], "key_objects": ["training examples", "parameters", "gradients"], "contextual_text": "Unlike batch gradient descent, which recomputes gradients for similar examples, stochastic gradient descent (SGD) performs a parameter update for each individual training example and its associated label. This approach eliminates redundant computations, making SGD significantly faster and suitable for online learning scenarios.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "2 Gradient descent variants", "h3": "2.2 Stochastic gradient descent"}, "hypothetical_questions": ["What is the main advantage of SGD compared to batch gradient descent?", "Why does SGDs frequent updates with high variance cause the objective function to fluctuate?", "In what scenarios would SGD be particularly beneficial for learning?"]}
50ffa41c-456c-4d05-b0e3-fecb3c1b32ad	abe8c200-bfa1-4355-947e-23ea618c310d	[0.009692764,0.01017753,-0.0086200135,0.05452588,-0.010962306,0.0429823,-0.027987806,0.013448005,-0.0024458154,-0.014197567,-0.04116518,-0.0208965,-0.005765281,0.01834364,-0.025368972,0.009362593,0.00921709,0.08781479,0.016223831,-0.0063626748,0.07455823,0.043835003,0.007953613,0.021990279,-0.021813985,0.050406072,0.03565323,8.2171944e-05,0.013198749,0.0031810075,-0.046169464,0.039736748,-0.015901642,0.0054711048,0.008194678,-0.0027969845,0.02508769,-0.037691753,-0.022269892,-0.05813982,-0.03824287,0.018797828,-0.047424935,-0.0017188649,0.0044606766,-0.06861133,-0.059413686,-0.04411812,0.023488771,-0.02227757,-0.0077325287,0.055470612,-0.031871747,0.019226348,-0.07223067,0.04463094,-0.020801673,-0.011824485,-0.039606154,-0.013580023,-0.032100443,0.005332466,0.035634775,-0.029204704,0.06159649,-0.0520634,-0.018347526,0.05230471,0.022688461,0.106924795,-0.013397218,0.06856042,-0.05564123,-0.12738691,0.1011419,0.02981782,-0.040612604,-0.08153288,-0.018509155,-0.0016877344,0.026310055,0.039312873,-0.055224538,0.0019301396,0.03037885,0.010320918,-0.059153806,-0.026621167,0.07207988,-0.03067123,-0.008300957,-0.013273235,-0.010963084,0.034823522,-0.025934631,0.01187431,0.01999837,-0.021430127,0.0011427463,-0.008003154,0.07499227,-0.029588811,0.053483255,0.123586155,0.00032407913,0.018271279,-0.03727348,-0.05399424,-0.0033116185,-0.013774766,-0.009624686,0.04932026,-0.055102807,0.011542133,-0.06945244,0.020952974,-0.038800415,-0.004904997,-0.014066672,-0.048304476,0.04939483,-0.025490083,-0.029870776,0.026953878,0.058441203,0.06006504,-0.007436974,-0.0274037,-0.052560065,-0.0329227,0.018175984,-0.034292985,-0.05214919,0.026197456,0.004277696,-0.032858204,0.019562408,0.005061903,0.00074805546,0.004328055,-0.050741,-0.027263846,-0.05333651,0.001383585,0.021587558,0.015154468,-0.06830986,0.058011793,-0.027656635,0.050032675,0.005227981,-0.0012170458,0.06934086,-0.0019454301,-0.06262835,-0.082497016,-0.02288131,0.04368005,0.0046204366,-0.012528384,-0.015903119,-0.0017263243,-0.010606643,0.08102083,0.064596325,0.11433469,-0.016313257,0.016380673,0.008724323,0.00943076,-0.036654517,-0.025077343,0.024669617,-0.01251309,-0.004129395,-0.03237145,0.021649336,-0.0070775757,-0.017237471,0.004948875,-0.031741787,0.050812017,-0.031882912,-0.0115459,-0.0095093455,0.04469625,0.0015591966,0.019136159,0.0009968292,-0.024682868,-0.008069095,0.0005969106,0.013497172,0.029965192,0.051394124,0.015604282,0.020311179,0.016611421,0.01901325,-0.0016887375,0.00022123945,0.066291384,-0.021537002,-0.023340521,0.063910596,-0.03187569,0.012911444,-0.038386784,-0.06340244,0.009888495,-0.05743863,0.03791844,0.0379292,0.031797986,0.010044121,-0.0035950153,0.024907827,-0.013351475,0.066976845,0.006598742,-0.034211073,0.045677975,-0.0038201134,0.070621766,-0.009012893,-0.029694945,0.010006918,-0.015676053,0.02144135,-0.0035047808,-0.036980946,0.064339206,-0.05898271,0.06339805,-0.01313941,0.02106941,0.012179431,0.022748653,0.025528338,0.026918123,0.038591772,0.007715482,0.0004217153,0.0926572,-0.020635191,-0.012768674,0.03793598,0.015485539,0.022258738,0.022974974,-0.048387088,-0.009279059,0.014239926,-0.04852926,0.022286687,-0.05415007,0.05227326,0.06738098,0.015346249,-0.07602027,-0.022686418,-0.0025004593,-0.012605348,0.0106456615,0.0029373933,-0.013629305,0.01649874,6.936055e-05,-0.063166104,0.056254767,0.005609426,0.024066476,-0.0028711655,0.0024873314,-0.026161617,0.049583428,-0.0028325473,0.00292102,0.053204685,-0.04015372,0.048238825,0.0102985725,0.02916263,-0.0034973489,0.0003807046,0.034019943,-0.021188464,0.003841293,-0.046775293,0.05665635,0.0040005282,-0.04514296,0.0116301635,0.02278279,-0.03423473,0.011942643,0.011153319,-0.024366047,0.03225101,0.018190172,0.051126894,-0.038170967,0.048616346,-0.007570261,0.03131647,0.016031563,-0.07436223,0.009750575,0.0002140507,0.015355031,0.035032317,-0.02654503,0.02376457,0.01275561,-0.041488785,-0.0033152266,0.07871444,-0.003966496,0.027753707,0.011724331,-0.0155821005,-0.017128238,-0.0063254046,0.05835504,0.033053596,-0.019749735,0.005003069,-0.022779271,-0.072620384,0.084848866,-0.017904667,0.04124659,0.050367944,-0.0006672491,0.009666881,0.0030639474,-0.06699204,0.024105035,0.01857018,0.010469541,-0.007142773,-0.063021995,0.002865745,0.024256375,-0.0030502183,-0.0050861835,-0.036745705,0.0039695296,-0.033683434,0.08948246,-0.028771771,0.08849618,0.036716092,0.01430689,0.016543053,0.011039294,-0.033723556,0.04265802,0.0551504,-0.040049877,0.029347505,-0.044628628,0.051931504,-0.078679055,0.04918529,0.047861297,-0.028290628,0.010392237,-0.035751093,-0.0115674995,0.04190567,-0.008144248,0.047812756,-0.08086817,0.011976904,0.049306758,0.06787986,0.030257527,-0.011960754,-0.06497131,0.016590029,0.039879262,-0.04938705,-0.025293631,0.031769328,-0.00044738242,0.033615235,0.05895763,0.00960297,0.0038284021,-0.038971424,0.016033761,0.009638887,-0.036539566,-0.07657443,-0.03206615,0.01670896,0.036058396,0.0026905548,-0.032419503,-0.009490536,0.0385951,-0.0677509,0.042770833,-0.013177479,0.013016606,-0.030641142,-0.014751138,0.051092878,-0.038358904,0.019850349,-0.06430135,-0.040208604,0.011453194,0.00805883,0.02369979,-0.03948378,-0.008172172,-0.0200648,-0.032427397,-0.018295085,0.057610247,0.043315902,0.043431893,0.004384006,0.03533154,-0.026030734,0.027093822,-0.022170601,-0.031076975,0.0034309735,-0.0010452478,-0.045508537,0.074354835,0.038680088,-0.0037122553,0.00305167,0.07295088,-0.033483047,0.033379782,0.03202004,-0.014118751,0.053319074,0.012770607,-0.02270966,0.009153166,0.0091769835,0.009466933,0.011128361,-0.01742661,0.04210888,-0.011277232,-0.0050317217,-0.01096649,0.014857013,-0.01059502,0.017053897,-0.04733684,0.028348098,0.014228708,-0.014600317,0.038776644,-0.0032559235,0.03730787,-0.013925284,-0.025392754,-0.059767008,-0.0263166,0.007838131,-0.026933732,-0.06248028,0.042754505,-0.06504762,0.041324914,0.00988269,0.017271494,0.0066616624,0.033781357,0.03542522,0.011506393,-0.00033547598,-0.012151092,0.018432088,-2.428351e-05,0.01255064,-0.017106067,-0.045520976,-0.0037390564,0.003330265,0.023272995,-0.021127757,-0.009428071,0.0001558509,0.006329017,0.010226401,-0.042224724,0.04243591,-0.025813136,0.04482666,-0.031270456,-0.044632666,0.008363491,0.060423337,0.07621726,-0.010678555,0.031098804,0.0027021805,0.07748098,-0.018833028,0.030895837,-0.049680438,0.061216474,-0.00032624838,-0.020192865,0.017996987,0.060921717,0.033863567,-0.0058329417,0.018094543,-0.01784116,0.014449676,0.04013291,-0.0072918064,0.013558334,0.01742954,-0.014392122,-0.010264698,0.0032345166,0.03773388,-0.029035928,-0.06641053,-0.036651872,-0.008770231,-0.016126994,-0.08178857,-0.06164617,-0.023298915,-0.005162295,-0.015180092,-0.020515092,-0.04568643,-0.053534634,-0.01856246,-0.04214037,0.02880067,-0.012702949,0.03989957,0.07053633,-0.012416486,0.06661644,0.00796107,-0.009893204,0.007882813,-0.039033607,-0.021333264,-0.033317517,-0.0013027624,-0.022882065,0.027443139,0.007950082,-0.018647062,0.0016688369,0.0036689104,-0.052448492,-0.016077986,0.03364753,0.007444782,-0.053651214,-0.036546074,0.016129794,-0.019249387,-0.005648878,-0.01129023,0.0020194214,-0.06588221,0.07316062,0.05141292,-0.0021656544,0.013509985,0.06075398,0.014470228,0.00905277,0.05883908,0.023090519,0.032420166,0.05973238,0.028869381,0.012368622,0.005242906,0.009195561,-0.0014864069,-0.0007728668,-0.036105234,-0.038666565,-0.022422474,-0.02240711,0.03895746,-0.049212694,0.043018103,-0.033167236,-0.03361091,0.0071256435,-0.041042145,-0.041354794,0.02727499,-0.010137306,-0.00679855,0.023918947,-0.0062257205,0.0661682,-0.047151636,-0.018294869,0.00010224169,-0.022693496,-0.06034336,-0.021502763,0.020758219,-0.02811259,-0.033997264,-0.011241548,0.0005420276,0.0078087235,-0.04159357,-0.0031040497,-0.066595905,0.002541609,0.017981939,-0.050696857,-0.04461526,0.015050636,0.016328704,-0.014421248,0.051079374,-0.0026148907,-0.05432874,0.022036128,0.0061105737,-0.057959985,-0.012169058,0.026641183,-0.0068395594,0.028965471,0.023077704,0.024221642,-0.01574147,0.01807862,-0.012334595,-0.04314946,-0.021268908,-0.039403964,0.023755338,0.08241625,0.019190215,0.012774598,-0.06971387,-0.07602279,-0.019619294,0.04031774,0.037737705,0.001425408,0.042436156,0.047750846,0.022420062,-0.013082984,-0.042217758,0.03537342,0.038410228,-0.015625995,-0.00050814945,0.048350204,-0.036014244,0.006282775,-0.004666715,-0.0579996,0.0011097151,0.030210683,-0.038832884,0.019162375,-0.01511382,-0.02427554,0.07119859,-0.017590227,0.010536665,0.07924688,-0.006653194,-0.043992702,-0.05181101,-0.007186597,-0.011107936,-0.027266543,0.0014039809,-0.018803226,-0.02826314,0.013127388,-0.03000173,0.02069132,0.0011576655,-0.05866164,-0.016954035,0.013238861,0.029241659,0.0613092,0.026789278,-0.102530874,-0.08220147,0.08243493,0.024317427,0.0047965012,-0.031073796,0.022879144,-0.010299943,-0.05196749,-0.0036768387,0.0019537807,-0.037072048,0.018746635,0.024371715,0.0026312405,-0.04065283,0.0019613546,-0.058832724,-0.03505035,0.018655678,-0.0038918164,-0.037471954,0.018029157,0.060249463,0.015110552,-0.02146252,-0.007979705,0.030495543,0.01572231,-0.00019715494,0.061481357,0.008585709,0.020993108,-0.027942013,0.02063031,0.05937461,-0.057146937,-0.0084944265,0.017369062,0.0619322,-0.02231472,-0.019795582,0.008477021,-0.020560056,-0.022874385,0.011652234,0.011994243,-0.019886386,0.016691744,0.0070559336,-0.04046557,-0.02634971,-0.020304013,-0.040534608,0.041444253,0.029290544,0.010297677,0.01451165,-0.023495281,-0.040088262,0.01321263,-0.02473236,-0.01275223,-0.03520516,0.007655053]	Keywords: stochastic gradient descent, local minima, learning rate, convergence\nKey Objects: stochastic gradient descent, local minima, learning rate, convergence\nRefers to Images: None\nHypothetical Questions:\n- Why does SGD's fluctuation sometimes help during optimization?\n- How does decreasing the learning rate affect SGD's convergence behavior?\n- What is the trade-off between escaping local minima and converging to the exact minimum when using SGD?\n---\nSummary:\nStochastic Gradient Descent (SGD) fluctuations can help it escape local minima but complicate convergence, though decreasing the learning rate allows it to behave similarly to batch gradient descent.\nOriginal Text:\nWhile batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD's fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively. Its code fragment simply adds a loop over the training examples and evaluates the gradient w.r.t. each example. Note that we shuffle the training data at every epoch as explained in Section 6.1.  \n```\nfor i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function , example , params) params = params -learning_rate * params_grad\n```\nContextualized Text:\nUnlike batch gradient descent, which converges to the minimum of the parameter basin, Stochastic Gradient Descent (SGD) exhibits fluctuations during the optimization process. These fluctuations allow SGD to potentially jump to new and better local minima. However, this also complicates convergence to the exact minimum, as SGD tends to overshoot. Fortunately, gradually decreasing the learning rate enables SGD to converge similarly to batch gradient descent, and likely find a local or global minimum for both convex and non-convex optimization problems.	{"tags": ["optimization", "machine-learning", "algorithm"], "doc_id": "50ffa41c-456c-4d05-b0e3-fecb3c1b32ad", "summary": "Stochastic Gradient Descent (SGD) fluctuations can help it escape local minima but complicate convergence, though decreasing the learning rate allows it to behave similarly to batch gradient descent.", "doc_type": "text", "entities": [], "keywords": ["stochastic gradient descent", "local minima", "learning rate", "convergence"], "key_objects": ["stochastic gradient descent", "local minima", "learning rate", "convergence"], "contextual_text": "Unlike batch gradient descent, which converges to the minimum of the parameter basin, Stochastic Gradient Descent (SGD) exhibits fluctuations during the optimization process. These fluctuations allow SGD to potentially jump to new and better local minima. However, this also complicates convergence to the exact minimum, as SGD tends to overshoot. Fortunately, gradually decreasing the learning rate enables SGD to converge similarly to batch gradient descent, and likely find a local or global minimum for both convex and non-convex optimization problems.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "2 Gradient descent variants", "h3": "2.2 Stochastic gradient descent"}, "hypothetical_questions": ["Why does SGD's fluctuation sometimes help during optimization?", "How does decreasing the learning rate affect SGD's convergence behavior?", "What is the trade-off between escaping local minima and converging to the exact minimum when using SGD?"]}
cd5b36c0-a3f3-4983-8e50-69302a2f52ba	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0007708404,-0.009391138,-0.014291977,0.05719539,-0.016051173,0.051412378,0.0008010536,-0.0031192547,0.06342441,-0.029803876,-0.01851471,-0.017021034,-0.001633212,-0.00822505,-0.024121037,-0.019809766,0.0095707895,0.07198553,0.02889137,-0.021554455,0.07118662,0.05870498,0.007346154,-0.00042526287,-0.008202778,0.06211136,0.035533693,-0.021507058,0.020724729,-0.034560546,-0.02040884,0.03645993,-0.028947864,-0.027463015,0.032388173,-0.017161122,0.03322096,-0.020984437,-0.040714055,-0.04632644,-0.006958058,0.026621781,-0.01965316,0.021752656,1.3049614e-05,-0.10338269,-0.04707133,-0.034581468,0.0033218488,-0.013445329,-0.018090589,0.033111375,-0.015516722,0.04554271,-0.09635081,0.04650607,-0.026285833,-0.007755391,-0.03353435,-0.022056127,-0.050915465,-0.007843848,0.046569206,-0.014806368,0.06735743,-0.022534547,0.006469116,0.038164824,0.043426186,0.114115834,0.0077717775,0.05045571,-0.049847208,-0.10260918,0.10726831,0.05148163,-0.03787585,-0.046574187,-0.009228876,0.0069979727,0.00784747,0.042817343,-0.033179548,-0.02530669,0.06451904,0.020617459,-0.056712054,-0.019858075,0.08302275,-0.019368527,-0.048582673,0.013335848,-0.03881706,0.044646487,-0.009145726,0.0043808357,0.0150838625,-0.03296317,-0.012251209,0.008745721,0.041363515,0.007873244,0.04141449,0.13296203,-0.005704798,-0.0069243317,-0.03406562,0.0006735541,0.0049390057,-0.018653685,-0.03175282,0.049241345,-0.026825352,0.0049748844,-0.072009675,0.0044614263,-0.033043563,-0.0032522716,-0.008436123,-0.009587368,0.06237338,0.007348274,-0.05592294,0.028757028,0.040386867,0.04395205,7.0129994e-05,0.004056876,-0.039358307,-0.021433638,0.023723304,-0.05241317,-0.041478444,0.040911045,-0.006939708,-0.034002174,0.013782116,-0.029083798,0.005521904,0.015874295,-0.027541755,-0.039383996,-0.028855756,-0.032369897,-0.050747927,0.018178996,-0.06754367,0.04754372,-0.031346455,0.028833425,0.01581513,0.014645728,0.06239698,-0.060143523,-0.055762503,-0.07177524,-0.03540212,0.033937372,-0.025643542,-0.010973569,-0.022749096,-0.011592739,0.00942724,0.075193524,0.054908797,0.11645101,-0.007559546,-0.009478391,0.0007631566,0.008276253,-0.03813767,-0.03082176,0.044860065,-0.021898206,0.003584658,0.002084229,0.03806347,-0.014025401,-0.036830842,0.00993066,-0.053575177,0.044547632,-0.031417668,-0.0010551169,-0.0076412475,0.015126673,0.00860154,0.028841766,-0.010765455,-0.033348333,0.0139316935,-0.0076054735,-0.020940697,0.014278331,0.06510222,0.019066144,0.025754841,0.0070119086,0.022133647,0.0026161904,0.005914318,0.083280705,-0.031897135,-0.041598316,0.057641555,-0.046661984,0.02464345,-0.04144359,-0.08067307,0.0019011596,-0.036009192,0.002213689,0.043089245,0.019638414,0.016479177,-0.0033833487,-0.0033312503,0.0035040982,0.029112212,0.0090299295,-0.026327942,0.030421084,-0.00083778746,0.03821913,-0.0434162,-0.019394794,0.018779647,-0.0151475025,0.014208019,-0.01057764,-0.0027010313,0.052983772,-0.047784965,0.053387947,-0.0070093875,0.03625018,0.011377652,0.04463188,0.01499782,0.034240834,0.039166883,0.03222011,0.0054560318,0.07098886,-0.006009889,-0.012320193,0.01574537,0.047478948,-0.004982537,0.023957524,-0.013783164,-0.032170057,0.029840607,-0.029020453,-0.035421524,-0.04312596,0.032397557,0.053884197,0.029884782,-0.07005554,-0.028161252,0.011673986,-0.0038755955,0.03920748,0.029862503,0.025148034,0.004981611,0.024323376,-0.060961563,0.026612792,0.010242292,0.02025096,-0.0028433283,-0.0074387514,0.018758068,0.06870872,-0.022900438,-0.004320357,0.04503023,-0.023369726,0.041322134,0.014633624,0.026091563,0.0022748057,-0.05417118,0.023731543,-0.043317247,0.021690255,0.014715907,0.036811266,-0.01671497,-0.051063884,0.023085792,0.027882095,-0.039773688,0.0002949271,0.02267634,-0.0090924045,0.0058650402,0.004169693,0.057528842,-0.0376381,0.035836883,-0.047012392,0.046638936,0.024822894,-0.067654654,0.023745896,0.03329819,0.055808052,0.040139142,-0.023606881,0.018858047,0.004160684,0.020655705,0.017529633,0.06996196,0.0040208707,0.048990745,0.023510631,0.059837036,-0.028240921,0.0027353058,0.047723778,0.018213283,-0.013597144,0.0018782878,-0.046461213,-0.05843728,0.08115465,-0.03090616,0.03983749,0.035507377,0.032493655,0.0025467246,0.007609112,-0.02556923,0.014957948,0.05090728,-0.0074648503,-0.0147427125,-0.06914677,-0.008335753,0.022221688,0.011152513,0.0029801892,-0.023543956,-0.029434947,0.005113039,0.08256792,-0.041876055,0.062396582,0.02726338,0.034971718,0.025905231,0.03205294,-0.02233799,0.029628262,0.05883649,-0.024893016,0.049221408,-0.040652893,0.058572184,-0.06472251,0.028548019,0.031765025,-0.052354045,0.017947651,-0.004363115,0.002017839,0.033841763,0.018808208,0.022029541,-0.062459167,0.007903553,0.041032836,0.035447307,0.0025528101,-0.026656335,-0.07897796,0.009770211,0.015610086,-0.037230276,-0.007584219,0.040856652,0.004568057,0.006234316,0.099547915,-0.0041452735,0.0048581013,-0.03920863,0.0049794223,0.017588848,0.0268596,-0.07346182,-0.044605505,0.015269881,0.018511828,0.024600817,-0.020505635,0.004697521,0.016594049,-0.06742982,0.029829146,-0.019366736,0.0072215446,-0.052168157,-0.0047776774,0.054239348,-0.04959314,0.05396362,-0.08630729,-0.011043242,-0.0054695285,0.0107900845,0.010321722,-0.056451533,0.0029047423,-0.0062237023,0.011467428,-0.039245315,0.028836496,0.037463084,0.042654358,-0.019370366,0.0010282012,-0.008038236,0.012628715,-0.008501352,-0.038544875,-0.012356945,-0.004454834,-0.0061196885,0.091167636,0.022049753,-0.018224075,-0.04102261,0.08575371,-0.052667513,0.004778603,0.039822944,-0.024557652,0.037431337,-0.012870255,-0.015334577,-0.010214672,0.027879242,0.019594299,0.0047363276,-0.009341375,0.018343693,0.01069585,-0.010718967,0.00090510055,-0.004430442,-0.028748835,0.03411759,-0.0327728,-0.011088647,0.011612737,0.025241585,0.04687473,0.010224539,-0.004736422,-0.018453235,-0.008316014,-0.067841806,-0.042344745,-0.021804536,-0.0036643008,-0.059584994,0.028500367,-0.026960105,0.030272998,0.015473918,0.012483173,0.006318447,0.041055173,0.025868256,0.009486649,-0.02031742,0.011532058,0.0067971726,0.039466195,0.01939406,-0.011279508,-0.035233535,0.009207586,-0.027214853,0.016430713,-0.020919535,0.0014631114,0.0046834913,0.0002568261,0.011684997,-0.036164656,0.02333818,-0.0100013595,0.0089645125,-0.022537114,-0.0587781,-0.0038321072,0.074391395,0.09379132,-0.018272236,0.028091196,0.032598473,0.09192822,-0.024255112,0.019077554,0.004348559,0.030018989,-0.0038113343,-0.021696275,-0.0005012149,0.012984393,0.014588773,-0.029176785,0.021562176,0.004829738,-0.036739606,0.042511567,-0.014665328,0.02739265,-0.0005035851,-0.009880087,-0.016550984,0.026707206,0.063792855,-0.05025322,-0.04888656,-0.006429281,-0.033402063,-0.0071056234,-0.09156311,-0.051787093,-0.031121176,0.007415137,0.023774797,-0.022190936,-0.021666974,-0.05531436,-0.02627357,-0.03656701,0.0530118,-0.012455055,0.044738047,0.06700758,0.013023965,0.10007172,0.02916955,-0.054080483,-0.0017577913,-0.012763492,-0.0025169423,-0.021767022,0.014526159,-0.020732198,-0.0013443177,0.023036635,-0.010280961,0.038860884,-0.0025706466,-0.03505518,-0.022891575,-0.013384564,-0.013822001,-0.046678685,-0.045665123,-0.015103821,0.0051100403,0.0021640463,-0.03720045,-0.009338259,-0.06386118,0.104302295,0.063236125,0.028565776,-0.026922189,0.071653925,0.020129414,0.023282064,0.05397191,0.04070255,0.0379343,0.059392832,0.004759456,0.02434973,-0.0064681424,-4.7547648e-05,-0.011299797,-0.018075582,-0.03868174,-0.012632963,-0.025257569,-0.049572613,0.01986095,-0.01923933,0.016495176,0.0026153526,-0.05824409,0.03915939,-0.050233137,-0.01389698,0.03992192,-0.009700402,-0.0067085954,-0.009084513,-0.007046029,0.047261823,-0.06311469,-0.030911453,0.006011108,-0.024910396,-0.102594934,-0.021400075,0.0067285206,-0.011647965,-0.019937055,-0.009939498,0.023226626,0.03115742,-0.017128821,-0.005707712,-0.05265877,0.029088877,-0.0013468111,-0.041377105,-0.016131565,0.010992029,0.018072667,-0.01541454,0.06592821,-0.0393502,-0.055056587,0.020662325,0.001938093,-0.07272796,0.004915943,0.02182778,-0.037388925,0.009974648,-0.006365449,0.05785,0.01274621,-0.007790744,0.0009747145,-0.05242823,-0.028728956,-0.060884673,0.060498256,0.06285112,0.031625364,-0.009403666,-0.030519495,-0.07006803,0.017416585,0.0071582445,-0.00960344,0.0033799356,0.040286325,0.035752025,0.012820598,-0.005646509,-0.04036738,0.029155707,0.055061534,-0.008145302,0.026585586,0.047317855,-0.035665926,-0.024378132,-0.023093337,-0.05194802,0.029925443,0.014045275,-0.03376588,0.01764894,-0.008313799,-0.03529685,0.057600986,0.028370567,0.021163926,0.089610815,3.120851e-05,-0.039500576,-0.052337285,-0.010206694,-0.002249439,-0.0022292791,0.018411417,-0.02307104,-0.004833931,0.018868275,-0.00051732996,0.07290309,-0.001469381,-0.027426187,-0.028563172,0.0021224353,0.03299484,0.052958015,-0.0020753422,-0.08091952,-0.06858357,0.07009333,0.0014979481,0.0027773518,0.0007479363,0.012606513,-0.0060251667,-0.034796074,-0.0044679516,-0.00970695,-0.010656405,0.006142678,0.0014705361,0.018403908,-0.034349434,0.046188,-0.05492408,0.00096704304,-0.009838083,0.018084744,-0.013464041,0.0015386747,0.043676525,-0.0017108135,-0.0048166797,-0.003264206,0.048068672,0.016897988,0.0031424956,0.044731315,0.019072484,0.03473843,-0.04031333,0.038238127,0.061763752,-0.058382027,0.0149590885,-0.020383408,0.090315565,0.011608396,-0.0010378787,-0.01594623,-0.04134984,-0.02204924,0.0046102796,0.029670747,0.0036055725,0.00015971319,0.016301578,-0.05884231,-0.02832327,0.014761185,-0.04605589,0.06329327,0.029405732,0.034437433,-0.0100306505,-0.047409527,-0.04207823,0.020624556,-0.027488207,-0.018785851,-0.008923,0.0065666665]	Keywords: stochastic gradient descent, SGD, parameter updates, training examples, gradient evaluation\nKey Objects: training examples, model parameters, gradients, learning rate\nRefers to Images: ./images/an-overview-of-gradient-descent-optimization-algorithms/image_1.png\nHypothetical Questions:\n- Why is shuffling the training data important in SGD?\n- How does the learning rate affect the convergence behavior of SGD?\n- What are the trade-offs between using SGD versus batch gradient descent?\n---\nSummary:\nStochastic gradient descent (SGD) updates model parameters by evaluating the gradient with respect to each training example and adjusting the parameters accordingly.\nOriginal Text:\n```\nfor i in range(nb_epochs): np.random.shuffle(data) for example in data: params_grad = evaluate_gradient(loss_function , example , params) params = params -learning_rate * params_grad\n```  \n6 Refer to http://cs231n.github.io/neural-networks-3/ for some great tips on how to check gradients properly.  \nFigure 1: SGD fluctuation (Source: Wikipedia)  \n\nContextualized Text:\nIn stochastic gradient descent (SGD), model parameters are updated iteratively. For each epoch, the training data is shuffled, and then for each training example, the gradient of the loss function with respect to the parameters is evaluated. The parameters are subsequently adjusted by subtracting the learning rate multiplied by this gradient. This process, as described in the provided code fragment, allows for faster updates compared to batch gradient descent, but introduces fluctuations in the objective function.	{"tags": ["optimization", "machine learning", "algorithm", "deep learning"], "doc_id": "cd5b36c0-a3f3-4983-8e50-69302a2f52ba", "summary": "Stochastic gradient descent (SGD) updates model parameters by evaluating the gradient with respect to each training example and adjusting the parameters accordingly.", "doc_type": "text", "entities": ["Wikipedia", "http://cs231n.github.io/neural-networks-3/"], "keywords": ["stochastic gradient descent", "SGD", "parameter updates", "training examples", "gradient evaluation"], "key_objects": ["training examples", "model parameters", "gradients", "learning rate"], "contextual_text": "In stochastic gradient descent (SGD), model parameters are updated iteratively. For each epoch, the training data is shuffled, and then for each training example, the gradient of the loss function with respect to the parameters is evaluated. The parameters are subsequently adjusted by subtracting the learning rate multiplied by this gradient. This process, as described in the provided code fragment, allows for faster updates compared to batch gradient descent, but introduces fluctuations in the objective function.", "mentioned_images": ["./images/an-overview-of-gradient-descent-optimization-algorithms/image_1.png"], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "2 Gradient descent variants", "h3": "2.2 Stochastic gradient descent"}, "hypothetical_questions": ["Why is shuffling the training data important in SGD?", "How does the learning rate affect the convergence behavior of SGD?", "What are the trade-offs between using SGD versus batch gradient descent?"]}
ddc1d439-0036-4ccd-b651-deb5b1ac7447	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.05475798,-0.009496545,0.021252591,0.073384926,-0.038399793,-0.033410832,0.0046105245,0.041405376,0.04518531,-0.015367928,-0.022058006,-0.010118393,0.03473768,0.023223892,-0.025108539,0.07720106,0.04209908,0.022350287,-0.047973186,-0.013214586,0.029041944,0.024178684,-0.028943295,-0.001618071,0.032987606,0.048510976,-0.0034187671,-0.013324427,0.0065350276,0.013133081,0.007408856,0.016483676,0.009272937,0.046366718,-0.008939368,0.041834224,0.0070536323,-0.0154747795,0.0087556215,-0.0035385024,-0.08521097,0.027750064,-0.0022929981,0.03328134,0.028081518,-0.017062662,-0.072640024,-0.030119698,-0.0022804684,-0.038780294,-0.038438056,0.0315122,-0.051320337,0.021894902,-0.020441964,-0.017865533,-0.04486655,-0.045885615,-0.096987195,0.032893896,-0.023153916,-0.02419311,0.032151766,-0.027438428,0.017263025,-0.063943855,0.012691636,0.021486241,0.02659481,0.17870563,0.013633218,0.017413704,-0.04138892,-0.07183768,0.0786415,0.018115137,0.00214221,-0.032050066,0.004613128,0.0156963,0.017723648,0.05126533,-0.025676997,-0.02037165,0.07326811,0.03299601,-0.068495564,-0.057194024,0.06656582,-0.12463921,0.020908257,-0.020860726,0.03042276,0.044478253,-0.062129308,0.0059234276,-0.043007128,-0.043431744,0.00729385,-0.01643914,0.0143021885,-0.034411952,0.018625071,0.08462853,-0.015264583,-0.018872574,-0.004707509,-0.04791196,0.041967537,0.009397533,-0.047241062,0.023379613,-0.07255997,0.031244408,-0.0021124787,0.002260436,0.022853605,0.0074051735,0.015159355,0.016448108,0.017455278,-0.0007258892,0.0108793685,-0.0045590475,0.06495713,0.055957206,-0.009058819,-0.0033053388,-0.032044295,-0.016036177,0.0006062484,0.056730174,-0.05340882,0.02039231,-0.048199218,0.039746847,0.0110027455,-0.003928016,-0.02457995,-0.04154282,-0.01825381,-0.050730325,-0.031069716,0.016293691,0.06643026,0.03257672,-0.061272282,-0.04945578,0.012120872,0.1099873,0.028495468,0.035027783,0.01731038,0.008298468,-0.05039241,-0.048252366,-0.047451667,-0.020697424,-0.009058786,-0.03818133,-0.033355802,0.0022028626,0.009074662,0.061466385,0.061259657,0.09425197,0.023169734,0.018458433,-0.04228721,-0.013022845,-0.06140835,-0.037745662,0.019007707,-0.020892764,0.0009046675,-0.02824187,0.012676127,-0.015946979,-0.042330097,0.02508302,-0.042185996,0.060184866,-0.00947585,-0.0031173837,0.02404156,0.0441295,0.01777282,0.02438531,-0.005015777,-0.02354451,0.0032484357,-0.049564533,-0.023757469,-0.009147022,0.0062878914,0.034084253,0.0042227656,-0.00666365,-0.019808076,0.049654584,0.017613662,0.008465116,-7.671498e-05,-0.005817041,-0.0070949057,0.026911084,-0.029679112,0.061888263,-0.0017818458,0.059825297,0.012039447,0.028262671,-0.048425168,0.0371075,0.053568453,-0.013485333,0.001392675,0.04854,0.044221222,-0.01499968,-0.020517701,0.013942319,-0.0034056795,0.030867705,0.018593324,-0.04974244,-0.047447823,0.005802571,0.01539321,0.001688062,-0.0076931096,0.015172456,-0.042780224,0.06356778,0.0010719585,0.019093413,-0.02704366,0.027963907,0.074420884,0.035006065,0.026103485,-0.010455736,0.0044983523,0.04794587,-0.008389656,0.028918093,0.03641538,-0.0016258261,0.008590434,0.026389115,0.0016637637,0.047879018,0.033474814,-0.00941431,0.010391634,0.0063457033,-0.038243424,-0.011831029,0.014368058,-0.049900636,0.0061765066,-0.022130009,-0.05510784,0.036824252,0.009465109,0.014926202,0.056025114,0.01066783,-0.012388022,0.02687157,0.06267336,0.04880483,-0.009941283,-0.010132553,0.018065477,-0.025225481,-0.000911699,0.029912436,0.030475965,-0.016175883,0.015696345,0.0030053516,0.049815763,-0.011162249,0.0030409726,0.040697977,0.017382756,-0.0794187,-0.029899506,-0.025555637,-0.017011061,-0.023717621,0.012989763,0.0031253283,0.015228759,0.029103428,-0.014025334,-0.046992958,0.0052698134,0.056197137,0.04092313,-0.026517946,0.021317838,-0.015506112,0.033916067,0.00036253303,0.030211851,0.019399883,-0.0063002934,0.031221967,-0.010710964,0.0047721765,-0.047457334,0.023657506,0.0184804,0.0774929,0.0462549,-0.033515435,0.017555628,0.0351701,0.010599575,-0.021635162,0.01051132,0.03947552,0.10311874,0.02846776,0.0027601868,-0.046915635,-0.08973192,0.07260032,-0.021737188,0.016651357,0.08869418,0.0034196754,0.01462294,0.0007522328,-0.007332951,-0.015089486,0.006491664,0.00028787824,-0.031744666,-0.039532386,0.0056768907,0.0067378026,0.019241475,0.05985122,0.011865848,0.032864545,0.024625637,0.019634977,-0.0055258456,-0.04524837,-0.030797869,0.019930318,0.025916109,-0.04476676,0.00017441412,0.06318916,0.004140105,-0.030250994,0.012595047,-0.05228226,0.022400297,-0.071424305,0.018313142,0.004929845,-0.007780306,-0.022102952,-0.016680568,-0.0039508035,0.028962955,-0.025719523,0.04001787,-0.012541594,0.005857754,0.023422685,0.017162533,0.0065156855,-0.016102104,-0.032124788,-0.054837983,0.021608897,-0.027530639,-0.047472943,0.028682671,0.0051700007,-0.0036662875,0.039609455,-0.015181915,0.064540744,0.012351209,-0.027730063,-0.019455688,-0.026858557,-0.0560505,-0.009180079,-0.00631756,0.09087354,0.006345312,-0.038903683,0.0151170045,0.052429337,-0.071851484,0.039726425,-0.008498517,-0.040598083,-0.01250067,-0.0026750478,0.026767794,-0.017201576,0.04001404,-0.05544685,-0.019030008,0.011880571,-0.011079255,0.0020745213,-0.053878114,0.069700025,-0.023128388,-0.027000438,-0.02564901,0.0010952328,0.03610074,0.049993794,0.022080375,0.00970036,0.0084975455,0.08661578,0.031101454,-0.08285198,-0.0026635027,0.0041891914,-0.027969642,0.004694918,0.05978618,-0.008103447,-0.022090599,0.048624717,-0.00982074,0.08716652,0.015757328,-0.033961393,0.04085605,-0.044240933,0.012919108,-0.046281092,0.039298087,0.021913793,-0.011305513,-0.030573543,0.057671703,-0.025730113,-0.013752068,-0.03829532,-0.00788229,-0.00091514864,-0.0065874443,-0.038230956,-0.03517004,-0.0106509235,0.016978916,-0.0126621565,-0.06358184,-0.020070797,-0.021668412,-0.06224803,-0.028419038,-0.015115584,0.011849927,-0.016957272,-0.010874188,-0.027671695,-0.035328362,0.02525025,-0.046458043,0.04091215,0.061804183,0.016283022,0.03152134,0.030555585,-0.0101805795,-0.039651055,0.0337968,0.0053066504,-0.032382905,-0.040068604,0.0037371332,0.0059720045,-0.035431135,0.003673294,-0.012491744,0.01930756,0.028313914,0.029171292,-0.016861029,-0.03238137,0.0126038045,-0.035599343,-0.0028389897,0.008710215,-0.009022315,0.0016916231,0.013969363,0.0560905,0.011326533,0.055331334,-0.01991613,-0.0025473335,-0.020926574,0.0132394545,-0.009525362,0.016262319,0.035714276,0.029978517,-0.03360776,-0.013957586,-0.0018627867,-0.027602768,0.008464047,-0.008543555,0.06650759,0.018638663,0.0514076,0.04766783,0.04291658,-0.0035355864,-0.05423645,0.013628944,0.009960644,0.005518798,-0.0039531426,-0.01469949,0.011636251,-0.023906589,-0.05725417,-0.022411635,0.0014501113,-0.075379334,-0.008080626,-0.04676357,-0.00019767992,-0.046553336,-0.024330234,-0.041649617,0.08448598,0.009774251,-0.024664434,-0.03242219,-0.012107802,0.059134226,0.06787251,0.031087674,0.008518691,-0.004471331,0.040770587,0.01928238,0.011982186,0.011901708,0.05732178,-0.021421323,0.022357013,0.022175116,0.035872433,-0.10001617,-0.0016806453,0.00056029705,-0.0035621347,-0.0021839843,-0.047677953,-0.0034483315,0.013605167,0.019776199,-0.008982292,-0.04117258,-0.010362691,0.1261392,0.0966527,-0.012768868,0.009351568,0.036970463,0.0028190645,0.032657165,0.005193684,0.07808743,0.07441802,0.0060381377,0.0044496143,0.03470381,0.010370777,0.00028542292,0.014845271,-0.043790385,-0.052546367,-0.0071560717,-0.014329488,-0.035798967,0.034145325,0.0043024505,0.022525579,-0.04098162,-0.021140762,-0.04792108,-0.060326375,0.0015447509,0.07682067,-0.023804484,-0.046035692,0.021559466,-0.028628543,0.050416786,-0.0232397,-0.032298226,0.033070307,0.00359319,-0.016874794,-0.020207245,0.054609746,-0.014926049,-0.043348975,-0.04470937,0.05970041,0.05690385,0.0031423613,-0.052603308,-0.016458016,0.041108504,0.047242176,-0.034702104,-0.047462188,0.03791311,0.026072675,-0.01774968,0.066822834,0.03052698,-0.005625573,0.006163898,0.013396436,-0.055525333,-0.022495696,0.042857774,0.011843245,0.03999975,0.013648881,0.012264269,-0.031720527,-0.0008994543,0.015942218,0.025862308,0.049328603,-0.015299218,0.0028695385,0.08808597,0.005428643,-0.06320171,-0.008574275,-0.025412712,-0.068385616,-0.008897218,0.053336054,-0.000824489,0.05439373,0.012462315,-0.032361813,-0.038764507,-0.0865655,0.0106490385,0.06609503,-0.020091424,0.014484191,-0.0023266077,-0.023105338,-0.0232024,0.013563768,-0.010110436,0.004417491,0.017838519,0.040381204,-0.011755468,0.029949544,0.027691903,0.00048545815,-0.062192485,0.019020269,0.07288602,0.03163205,0.054904453,-0.042066995,-0.029915545,-0.036656145,-0.048848648,0.0074619437,-0.016321355,0.04601142,0.0064699114,-0.005898171,0.008664693,0.011637123,-0.047393616,-0.023908595,0.012283149,0.021981468,0.04331744,-0.00701913,-0.018566148,-0.041900534,0.07884824,-0.011988626,-0.0077178697,0.033382833,0.028545039,-0.023376225,-0.029243277,-0.011456887,-0.02506234,0.059006568,-0.04835272,-0.016074488,-0.052357003,0.002496133,-0.048039578,-0.0074456516,-0.011259886,0.03365196,0.022678137,0.04182054,0.07114173,0.0243992,-0.048283342,0.024804508,-0.031963967,0.03684859,-0.029512241,-0.02931819,0.0296071,-0.04398047,0.0027702746,-0.0044457824,-0.00065362995,0.013179037,-0.02453826,0.0013416603,-0.047826417,-0.010043664,-0.058524016,-0.013229166,0.0061080195,-0.06099273,-0.018848456,0.022682687,0.008424358,0.0047258935,0.0065621883,-0.052810557,-0.00820785,0.0047558453,0.006853526,-0.0416715,0.0032922227,-0.0034236498,-0.0032142822,-0.05317914,-0.061041262,0.024629993,0.03478379,-0.0046769595,0.025785705,-0.03468823,0.056274585]	Image title: Time Series Graph\nTags: time-series, graph, data, trend, sequence, fluctuation\nKey objects: Time Series, X-axis, Y-axis, Data Points, Trend Line\n---\nSummary:\nThis graph displays a time series data showing fluctuations around a decreasing trend. The x-axis represents time or a sequence number, while the y-axis shows the corresponding values, which predominantly range between -10 and -8.\nFull description:\nThe image depicts a line graph representing a time series. The x-axis ranges from approximately 0 to 3500, and the y-axis ranges from -10 to -4. The data points fluctuate significantly but exhibit a general decreasing trend over time.  The values oscillate above and below a decreasing path, suggesting some form of dynamic process. The graph showcases changes in values across the sequence, with noticeable peaks and valleys interspersed along the downward slope.\nText found in image:\n- -10\n- -8\n- -4	{"tags": ["time-series", "graph", "data", "trend", "sequence", "fluctuation"], "title": "Time Series Graph", "doc_id": "ddc1d439-0036-4ccd-b651-deb5b1ac7447", "source": "./images/an-overview-of-gradient-descent-optimization-algorithms/image_1.png", "summary": "This graph displays a time series data showing fluctuations around a decreasing trend. The x-axis represents time or a sequence number, while the y-axis shows the corresponding values, which predominantly range between -10 and -8.", "doc_type": "image", "key_objects": ["Time Series", "X-axis", "Y-axis", "Data Points", "Trend Line"], "parent_doc_id": "cd5b36c0-a3f3-4983-8e50-69302a2f52ba", "text_in_image": ["-10", "-8", "-4"], "contextual_description": "The image depicts a line graph representing a time series. The x-axis ranges from approximately 0 to 3500, and the y-axis ranges from -10 to -4. The data points fluctuate significantly but exhibit a general decreasing trend over time.  The values oscillate above and below a decreasing path, suggesting some form of dynamic process. The graph showcases changes in values across the sequence, with noticeable peaks and valleys interspersed along the downward slope."}
e93d6e21-abc7-4642-9f0a-51ffe9dbae56	abe8c200-bfa1-4355-947e-23ea618c310d	[0.019074474,-0.009272524,-0.0115074385,0.035900746,-0.029520605,0.045933492,0.0032103534,0.016596353,-0.0059248707,-0.041457627,0.0030493794,0.026551412,-0.041226096,-0.013194664,-0.024612686,0.034753587,0.003685629,0.087377965,0.014280093,0.0023268363,0.06597423,0.00957214,0.039032575,0.03573039,-0.004654764,0.014527428,0.03613246,0.035634253,-0.002234312,-0.008167671,-0.07004583,0.027327152,-0.03540836,-0.014987902,-0.012202155,-0.027468702,0.015326216,-0.019065574,-0.033696357,-0.03530685,-0.058183137,0.053435706,-0.0362716,-0.023747442,-0.005901901,-0.0928602,-0.08354008,-0.021165293,-0.033372283,-0.030089306,-0.003198846,0.0344372,-0.04280517,0.03793885,-0.07226971,0.026550049,0.009035958,0.0008880239,-0.01364386,-0.024675535,-0.06206762,0.0045225727,0.058573905,-0.021085741,0.06812197,-0.026443642,-0.046845503,0.033882417,0.04280576,0.105560765,-0.008644604,0.051878467,-0.00883953,-0.103093676,0.09530508,0.04953509,-0.05046117,-0.059053164,0.0031699291,-0.020128392,0.036375545,0.038466334,-0.0618121,-0.039406188,0.055516656,0.010582924,-0.0409,0.006197213,0.027312184,-0.006723987,-0.039811738,-0.0070452746,-0.044344082,0.04702358,-0.014911059,-0.0055894856,0.045445338,-0.046559915,0.050435953,-0.008539296,0.06348755,-0.03747425,0.04957314,0.12663527,-0.013105587,0.020279799,-0.057530064,-0.003559191,0.03337091,0.02484201,-0.043325853,0.08303991,-0.066184215,0.01212182,-0.05299187,-0.006789362,-0.02945956,0.013305406,0.009536288,-0.038807154,0.06814866,-0.032441527,0.025933733,-0.0054375236,0.005768034,0.016703928,-0.0338879,-0.031377453,-0.03380834,-0.014282076,0.05409452,-0.029525826,-0.041274905,0.03603124,-0.0031578965,-0.016422374,0.05066468,0.00869364,0.009141784,0.04031801,-0.051285833,-0.056078814,-0.0010834802,-0.015423686,0.00090542226,0.037702903,-0.056235902,0.038993333,0.001436172,0.04395274,0.002168953,-0.03170348,0.06928374,-0.017024543,-0.040347937,-0.061904598,-0.021399893,0.03621628,-0.007678009,-0.020424232,-0.024987128,-0.013941328,0.024391923,0.047997724,0.05051788,0.11171248,0.005443668,0.006549532,-0.0021586462,0.038429853,-0.021873103,-0.022339655,0.03689092,0.006649247,0.020070745,-0.030579817,0.010678896,-0.017649403,-0.0412704,0.00039922202,0.010278622,0.037793364,-0.04703769,0.004034851,-0.027946362,0.00827332,0.0018496473,0.015775388,-0.043344297,-0.044098444,0.016518975,0.014474803,-0.00266526,0.021279642,0.06298993,0.00026845417,0.0119653465,-0.0036193389,0.00674434,-0.036087826,-0.0064828973,0.0154382475,-0.049034048,-0.033490222,0.048452612,-0.033905424,-0.036904458,-0.048488617,-0.05777753,0.059642073,-0.072148375,0.047916852,0.02986224,0.030639892,-0.016180407,0.014210838,-0.023903944,-0.05436157,0.04819542,0.014087557,-0.038375396,0.04212613,-0.0028002644,0.023376867,-0.0012249106,-0.014742347,0.023123067,-0.01767407,0.038194645,-0.0049197343,-0.029995834,0.055631682,-0.03273044,0.08147165,-0.027417028,0.0050594206,0.011993125,0.025695965,-0.022820357,0.0095121795,0.034163196,0.048444133,-0.0071437713,0.06887167,-0.005516307,-0.025545971,0.03894966,0.03220047,0.02355767,0.033120725,-0.04728653,0.012753642,0.017258862,-0.016465355,0.0010955835,-0.050086334,0.04903909,0.044490002,0.01090192,-0.014434918,-0.034561045,-0.030554062,0.025435157,0.016507689,-0.0027308064,0.033540905,0.017875625,0.02509678,-0.06668692,0.031251397,-0.0037570505,0.022282075,-0.0036770578,-0.01475309,-0.034775954,0.052881807,0.021514809,0.016791493,0.023736224,0.011762974,0.083392955,0.001774534,0.04005729,-0.009542476,-0.011282574,-0.032842718,-0.016944386,0.025670523,-0.019964501,0.012262546,-0.00392683,-0.052711856,0.030042902,0.014361529,-0.0028713497,0.010459928,-0.0020842773,-0.038509317,0.03795608,0.010874174,0.078408085,-0.04152938,0.019042766,0.029997176,0.02163141,0.031255305,-0.060816787,0.06801969,0.027265899,0.016864454,0.034976054,0.014520043,0.004015682,0.00082824624,-0.010049285,0.014706977,0.109253466,0.0540577,0.005425911,0.03279899,0.021875149,-0.0033704122,0.0059949257,0.07190758,-0.011212144,-0.043648448,0.03400808,0.0062511214,-0.049715925,0.084455095,-0.0352709,0.034309417,0.043991745,-0.023582555,0.022635045,-0.0077163433,-0.03514217,-0.008542759,-0.002156715,0.015054626,0.028989807,-0.02904872,0.03252982,-0.024012089,-0.027806202,-0.0007852361,-0.015139517,0.0007780982,-0.006560642,0.09014422,-0.01758613,0.05887145,0.041602794,0.048189513,0.013180173,-0.009056713,0.0067854663,0.031266075,0.04725272,-0.024760505,0.019437201,-0.011206682,0.09326231,-0.018170184,0.024157383,0.07366509,-0.021955486,0.009881921,0.04085257,-0.01207581,0.036279593,0.018356398,0.018676734,-0.042187624,0.026221609,0.03944997,0.033697113,-0.020048397,-0.030853627,-0.063379444,0.013119075,0.033978857,-0.052802775,-0.04621354,0.038442053,-0.0063695437,0.034382608,0.04754152,0.006932512,-0.02295748,-0.016549176,-0.029263983,0.0076085604,0.002961576,-0.05145837,-0.024435794,0.033087477,0.038131915,-5.1347753e-05,0.022444323,0.006293687,0.052189894,-0.061961517,0.01752348,0.0010527853,0.0023820398,-0.06389038,-0.01583532,0.03334272,-0.027193192,0.044499867,-0.079597004,-0.015161481,-0.008045024,0.02237698,-0.001995091,0.038294014,-0.029216908,-0.02342756,-0.009035585,-0.016061014,0.038938772,0.040232092,0.025061356,0.0011377659,-0.0023663559,-0.026115378,0.02773032,-0.043684725,-0.009228032,0.01789057,-0.02145019,-0.047102842,0.08661928,-0.007279409,-0.0052623735,0.017699338,0.090219736,-0.04511268,0.03163467,0.030066393,-0.0018901756,0.060012873,0.02918378,-0.036073077,-0.002455155,0.042494856,0.021665037,0.012919286,0.026172655,0.033154327,-0.028483482,0.00068652007,-0.0032098289,0.03856757,0.0073021143,-0.022932302,-0.03133656,-0.010472192,0.008488112,0.04117287,0.02189334,0.014513221,0.032711465,-0.0021395613,-0.010782176,-0.015492688,-0.061524913,-0.019485699,0.0027186172,-0.017665168,0.08726411,-0.055338077,0.047982052,-0.011065601,0.009815977,0.044534285,0.013453017,0.032280963,-0.013003793,-0.021371746,-0.016103366,0.030392094,0.013435519,0.016514407,-0.035736274,-0.056799605,-0.0067057824,-0.018243125,0.028025603,0.00049426453,0.030056637,0.00570205,-0.0022589471,0.000978003,-0.037294004,0.027876707,0.009394918,0.03062827,-0.063089564,-0.040476885,-0.00648527,0.08229403,0.053507738,-0.01356362,0.02035556,0.02195413,0.062077742,0.028908407,0.036070116,-0.030798234,0.037734535,0.023774657,-0.028551744,0.04499152,0.054433554,0.008206049,-0.0009835551,0.01868886,0.020868992,-0.00069023407,0.028640376,-0.011391201,0.017655503,0.021246824,-0.026550427,0.013670785,0.019578997,0.04334822,-0.05036675,-0.04362502,-0.00733503,0.012462586,0.023850262,-0.047015935,-0.048459332,-0.027459595,-0.014982194,-0.015967896,-0.02050083,0.0044473596,-0.08980213,-0.011579216,0.0010518432,0.07011167,-0.062608235,0.030629447,0.06709503,-0.0015278895,0.065003835,0.020090302,0.010139293,0.03913254,-0.034896668,0.0033408848,0.028880676,0.0046188007,-0.012211272,-0.0023440146,0.034315232,0.014643271,0.0072083846,-0.021069495,-0.036114648,-0.04863326,0.026363907,-0.0037633474,-0.06467188,-0.046318304,0.043137774,-0.0037936214,0.029535582,-0.0029597615,0.007401753,-0.019312592,0.047571786,0.04631664,0.009350748,0.013527028,0.020078968,0.0133770555,0.013804395,0.06129147,0.03740411,0.019009586,0.06009499,-0.0011164803,-0.046530925,-0.0183149,-0.013069107,0.022370923,-0.0093590915,-0.021475162,-0.029496614,-0.0128152985,-0.05072763,0.043740954,-0.043522883,0.061141364,-0.040467314,-0.0795418,0.05370099,-0.0755834,-0.027975027,-0.0005639552,0.029929925,0.003221258,0.0045287125,-0.04374824,0.07007441,-0.074726366,-0.03578164,-0.0068699038,0.0021881538,-0.07439277,-0.029232003,-0.0016168572,-0.036378108,-0.01930998,-0.00820325,0.03923573,-0.0010698419,-0.008054763,0.029645242,-0.04052373,-0.011312565,0.002620651,-0.008857414,-0.012882226,0.0065688533,-0.0012644196,-0.0014357398,0.03275364,-0.08823968,-0.042848323,0.021953322,-0.02178723,-0.07139213,-0.017788397,0.004167615,-0.027007781,0.022982776,0.022046836,0.06304679,0.023358507,-0.03180995,-0.05952779,-0.05419315,-0.008617649,-0.036529977,0.032571092,0.055968948,0.02284083,-0.008876193,-0.050292972,-0.033649344,0.011413993,0.040759705,-0.0007737156,-0.029237233,0.06560735,0.02583316,0.0033096701,-0.009906271,-0.0509805,0.0035529577,0.06517352,-0.021360569,-0.017363494,0.060108278,-0.010811101,0.017731326,-0.037124917,0.0055630803,-0.023908649,0.031315885,-0.012381912,-0.013985421,-0.01330441,-0.051093016,0.03517595,-0.013577682,0.010122612,0.10526862,0.010689788,-0.039676026,-0.037039388,-0.017766895,-0.022456717,-0.013950474,0.020083928,-0.037988525,-0.029193478,0.0645703,-0.01077966,0.039713394,0.02411677,-0.031847533,-0.011846897,0.022355612,-0.01010149,0.037131835,0.033272713,-0.08023363,-0.08279673,0.07352025,0.0013078714,-0.0009349194,-0.0024571663,-0.007868348,-0.007781333,0.0031081273,2.1016343e-05,-0.0050738542,-0.0069374405,-0.012507982,0.021143287,0.008636826,-0.014551096,0.0463006,-0.03396299,-0.034332894,-0.002739758,-0.005665533,-0.030885935,0.031047095,0.024362352,0.031939346,-0.02313311,0.0047302623,0.0482461,0.016793163,0.0046757325,0.050921492,-0.031605124,-0.003461043,0.0013228318,0.010332475,0.03985131,-0.03145443,-0.025803426,0.039047316,0.08045829,0.047034107,-0.001643324,-0.027702246,-0.008508131,-0.030774852,0.04030043,-0.0067795794,0.011186085,-0.0009902297,-0.0010774595,-0.03381955,-0.017706802,-0.012087101,-0.08934176,0.06491497,0.013911415,0.024426522,-0.029717991,-0.016446851,-0.031865012,0.05895349,-0.0011825408,-0.037860606,-0.001144796,0.011825467]	Keywords: mini-batch gradient descent, stochastic gradient descent, batch gradient descent, parameter updates, matrix optimizations\nKey Objects: mini-batch, parameter updates, training examples\nRefers to Images: None\nHypothetical Questions:\n- What are the main advantages of mini-batch gradient descent compared to batch and stochastic gradient descent?\n- Why is it beneficial to reduce the variance of parameter updates during training?\n- How do optimized matrix operations contribute to the efficiency of mini-batch gradient descent?\n---\nSummary:\nMini-batch gradient descent combines the benefits of both stochastic and batch gradient descent by performing updates for every mini-batch of training examples, leading to stable convergence and efficient computation.\nOriginal Text:\n### 2.3 Mini-batch gradient descent  \nMini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples:  \n<!-- formula-not-decoded -->  \nThis way, it a) reduces the variance of the parameter updates, which can lead to more stable convergence; and b) can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. Common mini-batch sizes range between 50 and 256 , but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used. Note: In modifications of SGD in the rest of this post, we leave out the parameters x ( i : i + n ) ; y ( i : i + n ) for simplicity.  \nIn code, instead of iterating over examples, we now iterate over mini-batches of size 50 :  \n```\nContextualized Text:\nMini-batch gradient descent is a popular optimization algorithm that performs updates for every mini-batch of 'n' training examples, offering a balance between variance reduction and computational efficiency. This approach leverages optimized matrix operations found in modern deep learning libraries, making it the typical choice for training neural networks. Common mini-batch sizes range from 50 to 256.	{"tags": ["optimization", "machine learning", "deep learning", "SGD"], "doc_id": "e93d6e21-abc7-4642-9f0a-51ffe9dbae56", "summary": "Mini-batch gradient descent combines the benefits of both stochastic and batch gradient descent by performing updates for every mini-batch of training examples, leading to stable convergence and efficient computation.", "doc_type": "text", "entities": [], "keywords": ["mini-batch gradient descent", "stochastic gradient descent", "batch gradient descent", "parameter updates", "matrix optimizations"], "key_objects": ["mini-batch", "parameter updates", "training examples"], "contextual_text": "Mini-batch gradient descent is a popular optimization algorithm that performs updates for every mini-batch of 'n' training examples, offering a balance between variance reduction and computational efficiency. This approach leverages optimized matrix operations found in modern deep learning libraries, making it the typical choice for training neural networks. Common mini-batch sizes range from 50 to 256.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "2 Gradient descent variants", "h3": "2.3 Mini-batch gradient descent"}, "hypothetical_questions": ["What are the main advantages of mini-batch gradient descent compared to batch and stochastic gradient descent?", "Why is it beneficial to reduce the variance of parameter updates during training?", "How do optimized matrix operations contribute to the efficiency of mini-batch gradient descent?"]}
e0db6713-377d-4c90-a9e8-64b4c7a7e80c	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0024371166,-0.020941736,-0.017897101,0.068624295,-0.039962348,0.04440241,0.021857955,0.006294216,0.0065391134,-0.051747568,0.0032162636,0.00918597,-0.016128518,-0.022018047,-0.024894074,0.031354137,0.02706018,0.07314782,-0.00609835,-0.0001868678,0.04168165,0.015043227,0.04070159,0.03751838,0.012209204,0.04831763,0.052521363,0.06347706,-0.029168762,-0.01345066,-0.04810717,0.044996653,-0.036322672,-0.004356518,-0.01786811,-0.004026975,0.020976478,-0.031813767,-0.034068227,0.0018606025,-0.040350206,0.043426238,-0.01962562,-0.044056483,-0.025654338,-0.10659318,-0.08863024,-0.014931978,-0.058981493,-0.027650092,-0.003442699,0.07751695,-0.05150446,0.022155933,-0.058071855,0.023303049,0.0034637211,0.0131800985,-0.039145507,-0.025828516,-0.027099406,-0.014889964,0.050446626,-0.013543509,0.057339802,-0.004244765,0.0071353596,0.027989335,0.06535039,0.113167,0.0027332816,0.027608156,0.021746589,-0.09709044,0.10439326,0.041095093,-0.04417299,-0.07697087,0.006587726,-0.016466178,0.03194404,0.010491619,-0.050991703,-0.04826169,0.057650805,0.02760986,-0.022916365,0.025196835,0.042581346,-0.045616042,-0.021410929,0.0017437278,-0.048066147,0.004366006,-0.0037188306,0.0009241755,0.043742515,-0.042083606,0.04016933,0.027527308,0.07697547,-0.048230886,0.023493169,0.1098251,-0.030856526,0.008059453,-0.06287016,-0.009892558,0.010064739,-0.0035752023,-0.03055402,0.09091998,-0.059043575,0.018476505,-0.048153598,-0.011295391,-0.024706597,0.03196413,0.0043574115,-0.0013245057,0.08555726,-0.0102785295,0.032251906,0.019464182,0.005988176,0.012268395,-0.04038907,-0.038617026,-0.06941959,0.011854809,0.039626814,-0.028174857,-0.034070693,0.03462584,-0.02469938,-0.02766089,0.05557382,0.0091041755,0.009620657,0.026760496,-0.030120034,-0.053716578,-0.0011598542,0.024944639,0.0016008677,0.0065210355,-0.04754304,0.052898105,0.0026764637,0.03768591,-0.0039572828,-0.038041625,0.060634416,-0.036407072,-0.0371407,-0.064162016,-0.03320597,0.044129163,-0.0035932104,-0.028370092,0.0042636967,-0.035922125,0.0007652096,0.058555644,0.037417393,0.11596208,0.00044571023,0.011438333,0.0053894883,0.050602652,-0.01231095,-0.03130341,0.051787697,0.0025140094,0.015362105,-0.002875468,0.005630643,-0.005811099,-0.025703277,0.012627436,0.020836627,0.048528153,-0.056090217,0.013484944,-0.01852231,0.014455798,0.013491288,0.027374446,-0.052270867,-0.04628763,0.027350537,0.008924137,0.024280036,0.0040278495,0.041733302,0.0011042365,0.02727443,0.01034007,0.0015028198,0.0051705805,-0.00958199,0.01870403,-0.047696,-0.0016459884,0.058641683,-0.034564104,-0.009475756,-0.036325872,-0.08691016,0.051876556,-0.07665152,0.0035269111,0.04546236,-0.0024515195,-0.014093523,-0.0056564054,-0.034356955,-0.032154806,0.0433387,0.037079886,-0.0285606,0.04193842,-0.0033836968,0.02819053,0.023148766,-0.035718814,-0.016831797,-0.029199995,0.007953548,-0.027550858,-0.020979675,0.05247072,-0.03711672,0.061395846,-0.046514817,0.037680328,0.0077488907,0.004206371,-0.043401994,0.03165701,0.03233785,0.023461185,-0.009901823,0.06242985,-0.016419353,0.010785442,0.056396425,0.04436664,0.034196474,0.014335643,-0.045144793,0.0078700045,0.028764974,-0.060159482,-0.029244125,-0.041548055,0.024557257,0.07944864,-0.0019067719,-0.008729347,-0.0077888607,-0.016848553,0.035986025,-0.009454535,-0.0041079014,0.013649743,0.030079184,0.01974789,-0.06378292,0.029715987,-0.008979981,0.024825808,-0.045186605,-0.02147688,-0.026114782,0.06182223,0.008688003,0.042700473,0.022100382,-0.0036671387,0.057075188,-0.00023774238,0.023184849,0.010026301,-0.023839492,-0.044636425,-0.020322938,0.040067025,-0.022088619,0.028553214,0.014960999,-0.026340125,0.033688046,0.036052197,0.01136291,-0.008145668,-0.00400619,0.0062913876,0.034521688,0.029873285,0.06004853,-0.04598804,0.0025174995,0.031549897,0.001502923,0.009808942,-0.036088217,0.06203156,0.031142723,0.03328137,0.04838568,0.024207087,-0.009633729,0.0029547964,0.0022437267,0.024973406,0.10534743,0.009097614,0.023615073,0.043181803,-0.012014889,0.0064399815,0.0028068754,0.05831072,-0.024333732,-0.0361606,0.010878107,-0.0056099785,-0.097492,0.08270516,-0.018500654,0.030988583,0.05071066,0.014678054,-0.007745764,-0.0029034747,-0.04244593,-0.016148195,0.01255718,-0.003117171,0.00962133,-0.06521763,0.039589353,0.0035084635,0.0102535635,0.039575823,-0.014489001,0.007936708,-0.0020149376,0.07252091,-0.046436857,0.05987078,0.03046792,0.06266689,0.032183405,-0.013027943,-0.013858609,0.008128529,-0.0095526725,-0.007639957,0.00956161,-0.039151333,0.07502463,-0.0147088915,0.022349915,0.060963944,-0.006003816,0.04217845,0.015538718,0.008949937,0.038078945,0.035243962,0.013069636,-0.03873545,0.042247675,0.03283062,0.01812414,-0.009967828,-0.053183053,-0.027335187,-0.00023113155,0.021033376,-0.032209843,-0.034471367,0.041046154,-0.0023760193,0.03190322,0.071680285,0.013630822,-0.0020291454,-0.01653349,-0.017628051,0.022528673,0.027753308,-0.032608837,-0.025254432,0.011384924,0.03805185,0.014782799,0.009964192,-0.017985703,0.0067374157,-0.078202605,0.03389148,-0.017179236,0.030467015,-0.07915193,-0.017205307,0.025364814,-0.042284522,0.036836226,-0.08232542,-0.0017205083,-0.044949442,0.030688887,0.010924975,0.009330264,-0.0282101,-0.0057955952,0.002433935,-0.008427037,0.04345541,0.039631102,0.056462623,-0.00038189878,-0.0009992414,-0.033253714,0.00414945,-0.02160025,-0.034375567,0.009544824,-0.015062061,-0.019614095,0.09701273,0.015357121,-0.02504436,0.0025638328,0.09982577,-0.056091648,0.0022411712,0.010196799,-0.0010865513,0.027220653,0.0082751755,-0.044143137,-0.009622445,0.04978768,0.06192169,0.019809747,0.042827412,0.039609298,0.006272282,-0.0110311555,0.0037572044,0.027808763,-0.01982448,-0.025113733,-0.030108202,-0.0032659695,-0.015007584,0.04067129,0.011230734,0.00781242,0.043517914,-0.00015235422,-0.027787589,-0.015695341,-0.049065884,-0.04302893,-0.0067381333,0.020967176,0.08140489,-0.031834163,0.02473278,-0.010309269,-0.0015386484,0.035021923,0.03523673,0.02192386,0.0005642392,-0.025405627,-0.017991133,0.016502738,0.013252063,0.014811886,-0.03872696,-0.05882211,0.0048162406,-0.033774305,0.06924459,0.0035656719,0.02923138,0.056241393,0.006510091,0.010941731,-0.0023536668,0.016634937,0.0209628,0.021035763,-0.030734729,-0.032959126,-0.014225821,0.068050064,0.029189428,-0.032657407,0.03940593,0.015616208,0.0720811,0.017247446,0.032265406,-0.022424461,0.036787756,0.015509548,0.017714472,0.02600973,0.023298671,0.02544853,0.04153899,0.0011295051,0.04785758,-0.015988303,0.008075191,-0.03558202,0.0026637102,0.016117575,0.0051327655,0.024624763,-0.0016072124,0.050648954,-0.067247346,-0.05215477,-0.010763001,0.005207672,0.017467597,-0.034849595,-0.07089094,-0.007366682,-0.016874846,-0.011151723,-0.027884794,-0.020953506,-0.07038225,0.002679279,-0.008190529,0.08493826,-0.042922545,0.030085897,0.03173028,-0.019331608,0.08723186,0.035123643,0.015375596,0.059392743,-0.008843876,0.0013080694,0.019591266,0.0108147375,-0.016552461,-0.011215152,0.027888304,0.011894626,0.019662635,-0.019098815,-0.022430891,-0.043234285,0.031221032,-0.0037969537,-0.073119774,-0.061837785,0.051397495,-0.011451438,0.057029527,0.00072658487,-0.021946717,-0.006358963,0.035880595,0.04179832,0.04220041,-0.00021373069,0.023821678,-0.020029228,0.014920407,0.074938014,0.006338019,0.035346977,0.044703376,-0.00061466463,-0.01468815,-0.018007845,-0.009376699,0.02189047,0.0027589889,-0.049578324,-0.049344562,-0.00073820434,-0.045057733,0.015075459,-0.052660003,0.034949705,-0.019606795,-0.036849584,0.044789534,-0.04809129,-0.02381598,0.0027541863,0.039254937,0.022354,0.025242193,-0.0255699,0.076038815,-0.05018363,-0.015073169,0.004714701,0.0057373377,-0.061669823,-0.029831508,0.036319885,-0.020615371,0.0030221723,0.009638196,0.016262738,0.0050214003,-0.0024631948,0.04325314,-0.032184236,0.01727837,0.005649121,-0.03049531,0.0122715635,0.029069303,-0.005355699,-0.009215975,0.03753779,-0.043706335,-0.019688409,-0.0005745374,-0.004847638,-0.09202995,-0.022879068,0.02140356,-0.029690837,0.006866918,0.013870607,0.06990055,0.012797257,-0.032415878,-0.058997665,-0.06267248,-0.026008328,-0.058919676,0.056859158,0.082407184,0.0012630469,0.001190288,-0.021592418,-0.05405935,-0.0028199232,0.054251235,-0.0086380355,-0.046287917,0.049538158,0.030054092,0.014168525,-0.042310704,-0.07204974,-0.013854739,0.026940472,-0.0018908385,-0.03254456,0.049232833,-0.016725987,0.011532802,-0.020143792,-0.008968609,0.0013060713,-0.0045752972,0.017687326,-0.001914669,-0.030057225,-0.037314035,0.04671372,-0.017175023,0.0066861487,0.065388836,-0.0049853935,-0.020407991,-0.028219435,-0.012042692,-0.0035267589,-0.012211164,-0.0012444984,-0.025145147,-0.0073001813,0.04495836,-0.037728414,0.03413621,0.017815603,0.016966172,0.008532106,-0.0097949505,0.012041079,0.044867527,0.020986175,-0.068156585,-0.08181519,0.06413497,-0.00981224,0.009080307,0.0076036155,-0.024372576,-0.023067476,-0.015044691,-0.019282537,-0.014698818,-0.001543237,-0.035350397,0.0043823025,0.02635315,-0.030990882,0.049641404,-0.07168913,-0.021349909,0.0065904264,0.009859438,-0.00909375,0.017508462,0.019100497,-0.0062335245,-0.03791551,-0.010366947,0.064032696,0.018010078,-0.0158741,0.070591174,-0.0022717468,0.020283185,0.020993212,0.0317865,0.048829783,-0.016462658,-0.0111675225,0.047004342,0.06499887,0.031022634,-0.0028035357,-0.07606597,0.010458368,-0.054400273,0.028758446,-0.013327456,0.011746805,-0.008415816,0.010764517,-0.035647783,-0.0047525493,0.0047396976,-0.07922487,0.049274936,0.033261657,0.027898941,-0.026868893,-0.06584186,0.0040982063,0.043329585,-0.024069034,-0.050152704,-0.025261153,-0.013724121]	Keywords: mini-batch gradient descent, training data, model parameters, iteration, optimization\nKey Objects: mini-batches, model parameters, training data\nRefers to Images: None\nHypothetical Questions:\n- Why is it beneficial to use mini-batches instead of individual examples or the entire dataset?\n- What is the role of the learning rate in updating the model parameters?\n- How does `np.random.shuffle(data)` affect the training process?\n---\nSummary:\nThe code snippet demonstrates mini-batch gradient descent by iterating over mini-batches of training data to update model parameters.\nOriginal Text:\nIn code, instead of iterating over examples, we now iterate over mini-batches of size 50 :  \n```\nfor i in range(nb_epochs): np.random.shuffle(data) for batch in get_batches(data, batch_size=50): params_grad = evaluate_gradient(loss_function , batch , params) params = params -learning_rate * params_grad\n```\nContextualized Text:\nMini-batch gradient descent performs an update for every mini-batch of training examples. This approach reduces variance and leverages optimized matrix operations. The following code snippet illustrates this process: The code iterates over mini-batches of size 50, calculates the gradient for each batch, and updates the model parameters using a learning rate.	{"tags": ["optimization", "machine learning", "deep learning", "code"], "doc_id": "e0db6713-377d-4c90-a9e8-64b4c7a7e80c", "summary": "The code snippet demonstrates mini-batch gradient descent by iterating over mini-batches of training data to update model parameters.", "doc_type": "text", "entities": [], "keywords": ["mini-batch gradient descent", "training data", "model parameters", "iteration", "optimization"], "key_objects": ["mini-batches", "model parameters", "training data"], "contextual_text": "Mini-batch gradient descent performs an update for every mini-batch of training examples. This approach reduces variance and leverages optimized matrix operations. The following code snippet illustrates this process: The code iterates over mini-batches of size 50, calculates the gradient for each batch, and updates the model parameters using a learning rate.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "2 Gradient descent variants", "h3": "2.3 Mini-batch gradient descent"}, "hypothetical_questions": ["Why is it beneficial to use mini-batches instead of individual examples or the entire dataset?", "What is the role of the learning rate in updating the model parameters?", "How does `np.random.shuffle(data)` affect the training process?"]}
3dff6915-b80c-49c2-8eb8-f8ef840534bf	abe8c200-bfa1-4355-947e-23ea618c310d	[0.01083124,0.02202259,0.0023391144,0.040618215,-0.041173045,0.026598327,0.009424816,0.012783566,0.0022536074,-0.034865584,-0.0075014355,-0.0030963814,-0.036830433,-0.03922316,-0.03054486,0.02698788,0.038754657,0.0824117,0.0023031363,0.013710811,0.038789175,0.0032729334,0.009494024,0.041990284,-0.0035044302,0.053141598,0.053833403,0.02228264,0.0025125486,-0.013221859,-0.010375989,0.0007402162,-0.0020880594,0.003100415,-0.013928235,0.019583283,0.04423318,-0.02149962,-0.033530846,-0.046866134,-0.05821271,0.0686027,-0.031033952,-0.054351833,-0.038958743,-0.06509451,-0.043298256,-0.042285908,0.008561675,-0.02041745,-0.0035879875,0.03404221,-0.067350455,-0.005465718,-0.06066579,0.026147235,0.006788862,0.01792875,-0.00427711,-0.016708994,0.005290549,0.004867449,0.025417827,-0.014637735,0.079552874,-0.040734425,-0.023439867,0.031832542,0.033525016,0.101896815,-0.013405318,0.026118321,-0.04195002,-0.12527744,0.11296759,0.019968122,-0.054833323,-0.054513823,-0.024240414,-0.04003386,0.017328298,0.03747657,-0.055910684,-0.041719407,0.036402393,-0.0031460107,-0.06393339,0.010041972,0.026298251,-0.051872473,-0.005611344,0.0047547445,0.008217593,0.04288924,-0.027278297,-0.021906517,0.025745863,-0.052700706,0.05076813,-0.021360323,0.06539328,-0.02950201,0.024532396,0.11571209,-0.008495462,0.0040763244,-0.034334656,-0.040139753,0.008897283,0.0150676,0.0009340001,0.047093935,-0.065642454,0.00627894,-0.03707767,-0.011636302,-0.011722902,-0.0033857438,0.030473629,-0.005800452,0.07929632,-0.008570806,0.01753159,0.028803807,0.029669791,0.02351348,-0.050321206,-0.009314213,-0.049500365,0.021159314,0.029292563,-0.03024907,-0.04033921,0.03160125,-0.007042637,-0.010250994,0.052094582,0.046537355,0.0021176704,0.00034100638,-0.030601904,-0.041392315,-0.021475233,-0.008577159,0.014104206,0.013840722,-0.042580236,0.018895712,0.034470137,0.06724595,0.024057614,-0.03578363,0.06072909,-0.019523837,-0.051858526,-0.050225932,-0.053761646,0.014070243,-0.005641878,-0.020368807,-0.01884992,-0.014998844,0.0003216194,0.048655163,0.018857801,0.10547313,0.0027189413,0.030037701,-0.018333351,0.031610977,0.010588293,-0.020258944,0.011461211,-0.024674062,0.030349389,-0.026059534,-0.0012752959,-0.055571485,-0.009119873,0.018511295,-0.0068716714,0.076480635,-0.032546286,0.02485194,-0.03507048,0.015809294,0.021195091,0.014065147,-0.038926203,-0.045901526,0.035408285,0.004651102,-0.004550471,0.013866562,0.026316268,0.016691683,0.01151103,-0.010659586,-0.001287482,0.00032843646,0.01648551,0.025916673,-0.033913683,0.010812013,0.01788442,-0.047980364,-0.0051985113,-0.050517615,-0.044811264,0.03451432,-0.05945315,0.032871388,0.037100486,0.013255277,-0.009208252,0.007143464,-0.035465643,-0.041884568,0.043637965,0.01156326,-0.012399192,0.03710253,-0.006811192,0.032584675,0.044195775,-0.029045302,0.0037099505,0.023040796,0.042024292,-0.04947117,-0.03594596,0.056064144,-0.02772088,0.06562845,-0.024594486,0.02433344,0.013620815,-0.010443197,0.0036484797,0.01956953,0.02693694,0.03409277,-0.0069226245,0.07180605,-0.007954334,0.013847828,0.078650266,0.026087238,0.009887523,0.037610333,-0.053658597,-0.009075505,-0.0026219732,-0.032914355,-0.0075477725,-0.009766338,0.056635298,0.06889028,0.028150097,-0.02913173,-0.021190485,-0.052096948,0.024993168,-0.013984371,0.030492444,0.011121979,0.027281728,-0.014813324,-0.06299024,0.029512588,-0.008761609,0.043535907,-0.0015097747,-0.018778475,-0.04820618,0.07167888,-0.029900627,0.014290956,0.031798452,-0.050787702,0.04898676,0.010093247,0.013323946,-0.017177068,0.028412448,-0.024613032,-0.0061322707,0.016701702,-0.061407484,0.021847941,-0.022417188,-0.006371696,0.04806989,0.025445634,0.004455584,0.0023490528,0.0136030605,-0.026424155,0.043423012,0.035180237,0.07535209,-0.02187859,0.008971837,0.03315979,0.031984102,0.020832239,-0.06500627,0.043802787,-0.029033693,0.028560473,0.044023972,-0.007738333,-0.0075406204,-0.0041535753,-0.0286267,0.0260703,0.09206296,-0.008168795,0.03766872,0.022051916,-0.02779126,-0.012617039,-0.017659504,0.079398856,-0.0015499453,-0.034528513,0.022775173,-0.0008376308,-0.102079265,0.053388927,-0.034274545,0.07692419,0.04056619,-0.01741325,0.00542284,0.0039143614,-0.06902831,0.01485077,0.003074214,0.0057759373,0.024470013,-0.07964137,0.008590257,0.02096947,-0.019866744,0.01657731,-0.00884992,-0.004168039,-0.00010058859,0.100650705,-0.020792266,0.028880918,0.034445122,0.026567116,0.014787933,-0.016071526,-0.028701412,0.01934379,0.033340946,-0.013071646,0.020412086,-0.06467295,0.05357348,-0.019748336,0.045325365,0.06882454,-0.009168863,0.031300217,0.0052613113,-0.012335965,0.013665808,-0.007433529,0.027882246,-0.07498473,0.026042977,0.003780877,0.03493158,0.043728802,-0.056702577,-0.066533364,0.01874419,0.019013304,-0.026001798,-0.0014776333,0.043575782,0.005379835,0.031592473,0.06074626,0.017694227,0.010056894,-0.003265145,-0.0005003216,0.051635273,-0.0044861385,-0.029985836,-0.028997209,0.0385947,0.04542838,0.011770577,-0.022400547,-0.0070828456,0.037238173,-0.056179203,0.01644773,-0.0008845153,0.002309871,-0.07377943,-0.01833865,0.04803138,-0.01645206,0.0015391296,-0.0708683,-0.027628612,-0.009495898,0.00049562467,-0.027768334,0.01930203,-0.025043828,-0.028006557,-0.012733953,-0.005665335,0.07956752,0.036593746,0.06582855,0.013670996,-0.016386174,-0.02347041,0.007140035,-0.040113095,-0.02813356,0.025348846,-0.016447626,-0.04060825,0.06200902,0.0138784945,-0.015233393,0.0020056758,0.119699195,-0.026433758,0.018126396,0.027224258,-0.043467794,0.056132983,0.01540995,-0.013221023,0.0032318935,-0.0006982452,0.026475566,-0.028642561,0.007163383,0.05473066,-0.009578614,-0.015699133,-0.0027434623,0.003693524,-0.013295145,-0.011686538,-0.019285737,-0.024811339,0.028315673,0.020178793,-0.0067982255,0.0004856777,0.024171885,0.011007709,-0.054255717,-0.0195408,-0.054818515,0.009121431,-0.006986346,-0.0012611605,0.052944798,-0.049595147,0.039252017,-0.015274798,0.013385321,0.03463368,0.012573644,0.03321465,0.023917302,-0.043510195,-0.0500925,0.012942302,0.009454929,-0.022364657,-0.0048385533,-0.05323466,-0.028866336,-0.045170736,0.049058244,-0.017558651,-0.010904191,0.011557434,0.021898024,0.02770263,-0.038411804,0.018778592,-0.03157082,0.009551172,-0.04396315,-0.055803698,0.005852762,0.08868251,0.06575344,-0.012862559,0.037741043,-0.00151732,0.051637948,-0.016823387,0.058149602,-0.042032145,0.06798101,0.04262083,-0.00087655213,0.01388584,0.026019651,0.021588678,0.027952498,0.03812363,-0.003409554,0.042315163,0.0411612,0.00899732,0.024701271,0.00393425,-0.0024293363,-0.0019136778,-0.009654802,0.04300369,-0.10821355,-0.056110103,-0.02738866,0.027565686,0.014314848,-0.058951627,-0.07307526,0.036864776,-0.0367529,-0.018481517,-0.0067340285,-0.013450794,-0.05962798,0.020759143,-0.02227325,0.030635098,-0.045421075,0.030512968,0.03112201,-0.021917569,0.07914946,0.015966674,-0.018852472,0.049680706,-0.00017290434,0.001934528,-0.010924877,-0.0060554477,0.0032798224,0.029044053,0.046298955,0.018573467,0.018860891,0.00068187853,-0.006693959,-0.04403925,0.041841727,-0.021448374,-0.09848819,-0.02010784,0.062455006,0.018088393,0.023060134,0.030064795,0.0007640369,-0.005012604,0.060305465,0.025416506,0.00075681874,0.006287579,0.024287816,-0.0052006524,0.0045262454,0.054747775,0.01928758,0.0021947622,0.051323142,0.024480995,0.007170731,0.0068914145,0.0127869295,-0.012478594,-0.008176309,-0.009398055,-0.06550672,-0.03313046,-0.046132483,0.022324236,-0.023842383,0.08051951,-0.061561722,-0.04535577,0.0053683138,-0.019256348,-0.044249885,0.024999535,-0.0026391237,0.009632572,0.061792377,-0.029743344,0.059845768,-0.108724214,-0.013715717,-0.023413263,-0.0038706153,-0.055986244,-0.019873833,0.0035696873,-0.011490721,-0.0241893,-0.042723466,0.043095175,0.012538029,-0.013913731,0.0077209873,-0.0050930474,0.0072206557,-0.015823606,-0.06688448,-0.0051909746,0.004313719,0.020807069,-0.014542707,0.040595096,-0.07166844,-0.028118502,0.023465509,0.00020798222,-0.045807526,-0.04571512,0.025856506,-0.05215586,0.029935842,0.022676699,0.040155087,0.010352746,-0.04422841,-0.038801096,-0.089608796,0.0061469753,0.0020446114,0.06141087,0.07024845,-0.009149318,-0.013555093,-0.035043735,-0.030620957,-0.022019051,0.043596454,0.010114186,-0.021783788,0.06704107,0.032147948,-0.022364864,-0.025650993,-0.034427725,-0.017769292,0.014122355,-0.027567722,0.009820631,0.018598193,-0.04291982,0.02434431,-0.026853973,0.0030200717,-0.026898783,-0.0011229613,-0.0058603543,-0.014186609,-0.009040236,-0.01869992,0.009934043,-0.034627758,-0.012848512,0.09068448,0.01052768,-0.033080176,-0.011640864,-0.00989503,-0.0030582887,-0.022215318,0.033187874,-0.044920415,-0.012153039,0.021219768,-0.040906053,0.040065695,0.02309645,-0.004858052,-0.011267482,-0.017698515,0.014596978,0.030723495,0.052547365,-0.07573548,-0.081616014,0.03724572,-0.0016138057,-0.026592534,-0.013942786,0.0020115937,-0.0009235053,-0.03439854,-0.0337917,-0.039529473,-0.020629508,-0.031584173,0.03422393,-0.017083239,-0.04822246,0.009273257,-0.06489618,-0.044450287,-0.002525433,-0.012743537,-0.010517576,0.028243564,0.017978162,0.00022471484,-0.051837657,-0.01322286,0.07097054,0.029246103,-0.058783352,0.047721073,-0.006283785,0.01545963,-0.0075722937,0.017257785,0.06522284,-0.038441032,-0.0107813,0.044126987,0.069977686,0.011691407,0.0145008,-0.012582617,-0.035641883,-0.049120307,0.038081106,0.009828761,-0.025932988,0.025961652,0.0009792845,-0.060400564,-0.04262846,-0.015019055,-0.082566865,0.009662234,0.05386799,-0.02242187,-0.040206634,-0.03613027,-0.042795714,0.019546807,0.019485064,-0.048734546,-0.0041223224,-0.008789596]	Keywords: learning rate, mini-batch gradient descent, convergence, learning rate schedules, annealing\nKey Objects: learning rate, learning rate schedules, mini-batch gradient descent\nRefers to Images: None\nHypothetical Questions:\n- Why is choosing the right learning rate so difficult in mini-batch gradient descent?\n- How do learning rate schedules attempt to improve the convergence process?\n- What are the limitations of using pre-defined schedules for learning rates?\n---\nSummary:\nVanilla mini-batch gradient descent faces challenges including selecting an appropriate learning rate and the limitations of pre-defined learning rate schedules.\nOriginal Text:\n## 3 Challenges  \nVanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:  \n- Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.\n- Learning rate schedules [18] try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics [4].\nContextualized Text:\nVanilla mini-batch gradient descent, while offering a pathway to optimization, presents several challenges. One significant hurdle is choosing an appropriate learning rate; a rate that is too small slows convergence, while a rate that is too large can hinder it, causing fluctuations or divergence. Learning rate schedules, which adjust the learning rate during training, attempt to address this, but their pre-defined schedules and thresholds are unable to adapt to the specific characteristics of a dataset.	{"tags": ["optimization", "machine learning", "deep learning", "challenges"], "doc_id": "3dff6915-b80c-49c2-8eb8-f8ef840534bf", "summary": "Vanilla mini-batch gradient descent faces challenges including selecting an appropriate learning rate and the limitations of pre-defined learning rate schedules.", "doc_type": "text", "entities": [], "keywords": ["learning rate", "mini-batch gradient descent", "convergence", "learning rate schedules", "annealing"], "key_objects": ["learning rate", "learning rate schedules", "mini-batch gradient descent"], "contextual_text": "Vanilla mini-batch gradient descent, while offering a pathway to optimization, presents several challenges. One significant hurdle is choosing an appropriate learning rate; a rate that is too small slows convergence, while a rate that is too large can hinder it, causing fluctuations or divergence. Learning rate schedules, which adjust the learning rate during training, attempt to address this, but their pre-defined schedules and thresholds are unable to adapt to the specific characteristics of a dataset.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "3 Challenges"}, "hypothetical_questions": ["Why is choosing the right learning rate so difficult in mini-batch gradient descent?", "How do learning rate schedules attempt to improve the convergence process?", "What are the limitations of using pre-defined schedules for learning rates?"]}
67a392ba-b44a-410c-b9c5-1427ef329858	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0012686885,0.014089361,-0.020993954,0.049823172,-0.038770333,0.03243701,0.04012983,0.028036349,-0.0034373999,-0.024340373,0.00618946,-0.004301501,-0.048413943,-0.05062095,-0.02087615,-0.008500726,0.005056997,0.08788904,0.010025499,0.023829928,0.056559023,0.00029991334,0.01769262,0.006173716,-0.01820036,0.049322013,0.06784161,0.0026754283,0.021515694,-0.009510057,-0.047725677,0.009000608,-0.018405903,0.006587334,0.021796571,-0.002414,0.0061706444,-0.03849653,-0.02637441,-0.026492031,-0.058692075,0.04921285,-0.04004376,-0.029469378,-0.008562219,-0.09559971,-0.05181318,-0.022486547,-0.0012290698,-0.030315049,0.020424645,0.07529896,-0.027131421,0.032911066,-0.059612654,0.02679984,-0.012995672,-0.002177036,-0.0252873,-0.05522728,-0.047210615,0.013759753,0.03103129,-0.026718698,0.074551575,-0.025080048,-0.023927428,0.050485693,0.056668248,0.10610995,0.006348724,0.03875228,-0.022422565,-0.13965635,0.119768284,0.0058342167,-0.0530392,-0.05503631,-0.0021384382,-0.020333512,0.031027794,0.015785899,-0.021634862,-0.034477897,0.047239706,0.0076449867,-0.062923856,0.018698297,0.03720054,-0.018876556,-0.030181963,-0.018629063,-0.008972206,0.04145919,-0.0006329229,-0.020001816,0.0029223952,-0.047402874,0.021093106,0.0020942148,0.04454689,-0.014045482,0.027452629,0.122967795,0.012883121,0.011751197,-0.03030891,-0.026538286,0.004593827,0.0013817151,0.01794478,0.061407976,-0.033756826,0.015127751,-0.061573226,-0.0044989767,-0.024564736,0.017040491,0.010192151,-0.020480847,0.072084144,-0.027715134,0.004071732,0.030926693,-0.005865416,0.053202193,-0.03196084,-0.02134406,-0.043314125,-0.012977001,0.07530082,-0.04597826,-0.04059287,0.039870676,-0.015832506,-0.020825084,0.045948617,0.007242148,-0.007968722,0.0038114176,-0.041438326,-0.025691444,-0.03178074,0.0011838188,0.028196221,0.025897171,-0.064977355,0.037270978,0.00430592,0.027568875,0.022431158,-0.036945418,0.08977433,0.007883246,-0.012451609,-0.052323397,-0.040100917,0.04556742,-0.004087244,-0.016042944,-0.031561453,-0.009792922,-0.00608449,0.07651334,0.046875894,0.109799646,0.005961029,0.038245674,0.002040247,0.042796496,-0.0030511997,-0.033837344,0.023757447,-0.022987757,0.0031633708,-0.008734592,0.027683638,-0.028461805,-0.0049037496,0.0010894268,-0.006598686,0.06603272,-0.02077964,-0.010077296,-0.020950118,0.030243587,0.0048455903,0.04141476,-0.013667547,-0.042232912,0.03692593,0.013257743,-0.006500488,0.011093001,0.06132989,0.021943072,0.025471289,0.020635124,0.011837401,-0.021756457,0.0016214423,0.031309422,-0.032679655,-0.011716924,0.025482152,-0.056457963,-0.017335577,-0.07384734,-0.05937557,0.02931758,-0.054823626,0.03211829,0.020056013,0.009275709,-0.00706653,0.02290171,-0.024524912,-0.019136688,0.056638945,0.008254617,-0.046180323,0.01965331,0.00014528527,0.031948883,-0.015953358,-0.010331234,0.014724364,-0.022165902,0.027913868,-0.023473479,-0.010034216,0.04606577,-0.038174354,0.06863408,-0.0543683,0.028043272,0.008411365,0.027699944,0.008221898,0.011870811,0.03400774,0.027013203,-0.03455072,0.07620832,0.0056210295,0.024413455,0.06873999,0.038941044,0.020453593,0.013018155,-0.031683143,0.0043373774,0.03159429,-0.0074052345,-0.018449994,-0.004334713,0.04976592,0.04122754,0.035872962,-0.04053083,-0.032818295,-0.038161103,0.017384814,0.0026727212,0.024856417,0.03040332,0.010230004,-0.002084876,-0.067906246,0.017259305,0.023774717,0.01887619,-0.028402295,-0.0015174476,-0.04941491,0.06184524,-0.0019668904,0.029645303,0.03499457,-0.046037562,0.045513812,0.02057602,0.0098530855,-0.0018737924,0.006424203,0.011227988,-0.0077407043,0.006117819,-0.07647261,0.03654613,-0.017466435,-0.045638233,0.037609592,0.02016931,-0.013899209,-0.020378755,0.013126107,-0.015181694,-0.0031417464,0.02111961,0.06542716,-0.015986895,0.02969469,0.02236353,0.019726131,-0.0011462126,-0.08079302,0.034450613,-0.004200274,0.032329056,0.052192826,-0.0005525262,-0.0050835884,0.0018839854,-0.0011274122,0.029174583,0.102743536,0.023811959,-0.00295256,0.0343308,-0.005159059,0.038936466,-0.015386556,0.07504644,0.017552115,-0.002584262,0.027445404,-0.017495483,-0.09985909,0.059802838,-0.04408492,0.057381984,0.05878439,-0.015360253,-0.0046167304,0.008096905,-0.03182577,0.016410913,0.010595473,-0.027112527,0.023646634,-0.04907603,0.01022536,0.0028351867,0.0047138217,0.0012549998,-0.013967315,-1.704646e-05,-0.00882551,0.10470341,-0.018877301,0.018926844,0.014807215,0.03690845,0.01315624,0.0029781186,-0.019327274,0.025347447,0.045968212,-0.024342978,0.039660558,-0.052107207,0.049484592,-0.014792281,0.036589622,0.030764595,-0.0319676,0.045041427,0.024718734,-0.013797978,0.013253668,0.010363717,0.037210308,-0.058229007,0.029096555,0.04362484,0.047505163,0.060160447,-0.040902197,-0.08301905,-0.010820125,0.018019738,-0.03333817,-0.008489287,0.05435109,0.0019752136,0.02032465,0.06466648,-0.0009113103,-0.007791978,-0.008687446,0.0054250276,0.029868979,0.024018778,-0.030667853,-0.024913065,0.011559609,0.018629896,0.0055456683,0.006946412,0.0064843474,0.029720904,-0.060585737,0.02402531,-0.0014786482,0.0092524,-0.043669797,-0.024552811,0.0557562,-0.03891125,0.022323888,-0.09106743,-0.02846608,-0.015543626,0.003032979,-0.002426425,-0.0021028374,-0.0110758245,-0.022607068,-0.029528879,-0.02038786,0.07708283,0.033444595,0.044551507,0.016655393,-0.0043004276,-0.029040044,0.02039941,-0.029959276,0.0043807775,0.039181743,-0.0036555277,-0.030936884,0.06201975,-0.012422167,0.007202826,-0.004735497,0.105362356,-0.029653043,0.012951472,0.016223898,-0.040529266,0.03795129,0.02672503,-0.03788431,-0.0081985295,0.016251877,-0.011727966,-0.0082449345,0.008738292,0.03734181,-0.021038108,0.0119319,0.004662897,0.012915773,-0.019167699,-0.020776633,-0.030196832,-0.021729276,0.013174679,0.030137505,0.021159079,-0.0039220103,0.046155337,0.013780163,-0.026875881,-0.01646576,-0.053203057,-0.015098048,-0.024517423,-0.024665846,0.059660934,-0.017756032,0.022198353,0.01835227,0.016740322,0.034609586,0.018208573,0.062196173,-0.0045759864,-0.014380882,-0.014307164,0.026109362,0.006836598,0.005493948,-0.007882221,-0.026267523,-0.010033096,-0.032471173,0.021411328,-0.024735712,0.026258567,0.009155856,0.00087262184,0.017259637,-0.038454402,0.011289355,-0.023775533,0.029803129,-0.048745047,-0.057849962,-0.0015619749,0.07001865,0.08114616,0.008085028,0.025653793,0.016965711,0.06570608,-0.015995787,0.027483951,-0.011202068,0.057059772,0.04094278,-0.020708922,0.033953898,0.014840855,0.03776922,-0.0057819043,0.010602345,-0.019950189,0.028677644,0.0436218,-0.005300781,0.01065858,-0.017506663,0.021992221,-0.011444745,0.003675031,0.025854513,-0.07725085,-0.06766329,-0.03635806,0.018718785,0.0070969774,-0.08522616,-0.093892805,0.007813314,-0.048358593,-0.006456205,-0.021669941,-0.007492615,-0.0576207,0.018569052,0.0019566147,0.07579573,-0.031919107,0.022028878,0.06845421,0.00925075,0.07275312,0.018957973,-0.02886863,0.046529274,-0.01691698,-0.018299028,-0.02171883,-0.013361964,-0.024513727,0.035173815,0.03546639,0.010053777,-0.009918163,0.007429374,0.0131116565,-0.029099636,0.0068269353,-0.01346942,-0.07894819,-0.05001652,0.025199363,0.010574432,0.032305814,-0.002453719,-0.012814516,-0.010825801,0.04280486,-0.00014353049,0.00050207507,0.011114794,0.020594157,-0.020709893,0.006687768,0.06988379,0.00999097,-0.0096153775,0.056657556,0.008099113,-0.022849953,0.008015704,-0.012373802,0.014401153,-0.036515415,-0.0041754483,-0.05142509,-0.02001266,-0.03257258,0.043695502,-0.047084447,0.07697677,-0.03836365,-0.04661808,0.030690348,-0.0196147,-0.052066967,0.031919822,0.02506796,0.03473053,0.042734727,-0.0043581272,0.06396495,-0.08037911,-0.04810505,0.0048402306,0.01692166,-0.08235149,-0.031997424,0.02355477,-0.024002792,-0.01667055,-0.027533544,0.010349018,0.026583113,-0.050490748,0.0445686,-0.046899233,0.028953895,0.0037056874,-0.048179507,0.016554683,0.013728242,-0.009064487,-0.008042547,0.034980766,-0.06153579,-0.053254154,0.004567049,-0.035130493,-0.09045983,-0.014354187,0.033474073,-0.05156428,0.03705609,0.024186192,0.041714236,-0.005639855,-0.0016997377,-0.047079645,-0.07382323,-0.006560983,-0.013299655,0.028429655,0.09420102,0.010540385,-0.007510885,-0.025167365,-0.025736313,0.00028158203,0.02736469,0.032806013,-0.038237453,0.07597477,0.03460281,-0.010402616,-0.022501275,-0.03855805,-0.006691986,0.021109246,-0.0028910234,0.006546684,0.048540503,-0.033034854,-0.0132641215,-0.05545211,-0.0034502542,0.0012758962,0.017960142,-0.034237843,-0.006327228,-0.013210807,-0.046007928,0.007692524,-0.01693393,0.027970655,0.09491811,0.0061246133,-0.03713601,-0.020908521,-0.0032197172,0.011297853,-0.02680813,0.0039440086,-0.037327792,-0.013599795,0.0063280533,-0.03448063,0.078132816,0.043500517,-0.020932801,-0.032578837,0.01994472,0.028238479,0.05792228,0.050767556,-0.0665785,-0.074729785,0.0242889,0.011832833,-0.009800717,-0.021678451,0.022505078,-0.020514734,-0.018104972,-0.02077894,0.013239953,-0.00029403015,-0.004325486,0.019375142,0.02072967,-0.06302522,0.035118993,-0.055795502,0.00032829327,-0.0068283244,-0.0038383086,-0.041688353,0.018972231,0.01501261,0.0016919916,-0.016292421,0.00568756,0.0553481,0.02041547,-0.03320541,0.06570661,0.01623537,0.042438723,-0.033016425,0.024555922,0.07327551,-0.051944364,-0.026025914,0.033194397,0.087540835,-0.00066437165,-0.023622794,-0.048147764,-0.0129327485,-0.055084795,0.043860108,0.011894199,0.009444527,0.0034077573,-0.0225688,-0.050482817,-0.022510232,-0.020548517,-0.070552856,0.058682017,0.031010242,0.0009862359,0.009714102,-0.03852002,-0.05146041,0.021236815,0.044805504,-0.037080802,-0.01763449,-0.0001813559]	Keywords: learning rate, parameter updates, sparse data, feature frequencies\nKey Objects: learning rate, parameters, features\nRefers to Images: None\nHypothetical Questions:\n- Why is it problematic to apply the same learning rate to all parameters?\n- How would you adapt the learning rate to address the issue of varying feature frequencies?\n- What are the implications of updating rarely occurring features to a lesser extent?\n---\nSummary:\nA challenge with standard mini-batch gradient descent is that applying the same learning rate to all parameters may not be optimal, particularly when dealing with sparse data and features with varying frequencies.\nOriginal Text:\n- Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.\n- Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. [5] argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.\nContextualized Text:\nWhen using mini-batch gradient descent, a challenge arises because the same learning rate is applied to all parameter updates. When data is sparse and features have very different frequencies, this approach can be suboptimal; larger updates might be needed for rarely occurring features.	{"tags": ["optimization", "gradient descent", "machine learning"], "doc_id": "67a392ba-b44a-410c-b9c5-1427ef329858", "summary": "A challenge with standard mini-batch gradient descent is that applying the same learning rate to all parameters may not be optimal, particularly when dealing with sparse data and features with varying frequencies.", "doc_type": "text", "entities": [], "keywords": ["learning rate", "parameter updates", "sparse data", "feature frequencies"], "key_objects": ["learning rate", "parameters", "features"], "contextual_text": "When using mini-batch gradient descent, a challenge arises because the same learning rate is applied to all parameter updates. When data is sparse and features have very different frequencies, this approach can be suboptimal; larger updates might be needed for rarely occurring features.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "3 Challenges"}, "hypothetical_questions": ["Why is it problematic to apply the same learning rate to all parameters?", "How would you adapt the learning rate to address the issue of varying feature frequencies?", "What are the implications of updating rarely occurring features to a lesser extent?"]}
e5533967-07cb-4c4e-9ffe-5e646cfe3284	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0052372026,-0.011964463,-0.021006187,-0.009010907,-0.030008718,0.026957562,0.012437803,0.03527466,0.029590385,-0.04222829,-0.032008305,-0.018344488,0.016900893,0.010546781,-0.024487402,0.029088283,-0.003049794,0.0889928,0.0027997808,-0.02681287,0.09061676,0.01777595,-0.013964344,0.0390902,0.02460565,0.03863936,0.048468575,0.008288917,0.011591618,0.077647075,-0.0037063153,0.007842529,-0.021741157,0.019738074,0.01891735,0.01002861,0.0022663416,-0.015818615,0.033803757,-0.07234768,-0.040045615,0.0426249,-0.0525278,-0.012720233,0.03159344,-0.05152225,-0.09219353,-0.03209928,-0.0336581,-0.0127151245,-0.010327461,0.02255105,-0.021382583,0.02011823,-0.04879752,-0.0052810013,-0.006268996,-0.021171257,-0.015960399,-0.03039181,-0.06020835,0.045374043,-0.018444778,-0.03141211,0.061423477,-0.05257069,-0.027856525,0.06037265,0.015544461,0.106097616,0.026600309,-0.021477621,0.007775837,-0.061775934,0.09525199,0.041860964,-0.017178793,-0.047749892,-0.01946072,-0.0038807963,0.022698138,0.049739666,-0.018387685,-0.020484831,0.06339792,-0.00921573,-0.04940459,-0.021868559,0.07956455,-0.028908715,-0.012501493,0.023997676,-0.003954875,0.057599265,0.007883194,-0.026368164,-0.010410806,-0.027216142,0.026295932,-0.04459697,0.07746467,0.008385999,0.025504643,0.096508965,0.010883601,0.03142103,-0.030631546,-0.027606616,-0.011855416,0.061445653,-0.013760485,0.05457064,-0.03202178,0.030938525,-0.016990665,-0.04200595,-0.056287486,0.008064438,-0.008005233,-0.016663322,0.060240086,-0.037808407,-0.003556412,-0.04940834,0.06280176,0.050661314,0.00243062,0.017817937,-0.039684046,0.031866103,0.05153645,-0.021049866,-0.0138134705,-0.043400597,0.046155404,0.00753586,0.044471253,-0.012264687,-0.015949348,-0.00986806,0.004460489,-0.040063474,-0.03843554,0.009277381,0.007650245,0.09984986,-0.0687052,0.03947654,-0.009706922,0.045101933,0.02697546,-0.0013118925,0.07065525,-0.005362378,-0.0675355,-0.049861755,-0.0452406,-0.016619047,-0.04460134,-0.016828978,-0.062498,0.0015985848,0.028807132,0.044676367,0.035949916,0.09581945,0.03936988,0.04021009,0.0068977075,0.0051340335,-0.010397745,-0.000742516,0.020927344,-0.028586157,0.018064579,0.0045172176,0.021325916,-0.042085923,0.0074347253,0.053669356,0.01976443,0.020723328,-0.011703184,0.014031925,-0.0066535273,0.017773546,-0.0029279469,0.0375101,-0.016231332,-0.01861913,0.035179336,-0.01313796,0.0008423276,-0.0021114096,0.058821354,0.020935016,0.023988614,0.011857852,-0.00022980182,0.00042147253,0.013954,0.017952733,-0.03738293,-0.008862387,0.017111016,-0.03270395,-0.033284325,-0.028071932,-0.02850609,0.059371248,-0.050404042,0.049460206,0.020354096,0.045633152,0.013883651,0.012076863,-0.029872445,-0.015529363,0.040336493,0.021968316,-0.08905201,-0.033804543,0.024015427,-0.027816618,-0.038426362,0.006587026,-0.019861858,0.012156208,0.056304105,-0.0059372513,-0.025209162,0.004687736,-0.033847276,0.0605342,0.038540795,-0.018228974,0.013271353,-0.007192219,0.049227778,0.022544391,0.022374608,-0.013110328,-0.014321453,0.09935136,-0.023684165,0.001114585,0.012799139,0.039944742,0.031422786,-0.0012676931,-0.062011838,0.009209952,0.05664675,-0.04274291,0.028804602,-0.011504172,0.028845262,0.013766424,0.08460414,-0.02122768,-0.014859892,-0.039082352,-0.06073875,0.019015104,0.0122172665,0.008953643,0.03634284,0.017933,-0.0057464866,-0.0037061605,0.006942443,0.020984799,0.02504915,0.0020790217,-0.020136574,0.04529372,0.032092176,0.015608808,0.064348824,-0.050298642,0.018097203,0.05329482,0.048033122,-0.020487027,0.003303969,-0.037249457,-0.017074265,-0.028151143,-0.016139207,0.03421125,0.030309986,0.005516174,0.04480271,0.014797959,0.008464967,0.047215857,-0.010093035,0.011896039,0.02888232,0.050867308,0.06822496,-0.020022996,0.0110257035,-0.002670492,0.027572846,0.0199961,-0.10000851,0.045058765,0.023502992,0.041500926,0.063389584,-0.02160026,0.0064817425,-0.011783132,-0.009084353,0.027887398,0.047442198,0.016212443,-0.0003812876,0.04367752,0.05681867,0.01197102,-0.047686942,0.025996832,-0.024362994,-0.059147622,0.023687458,0.02194509,-0.110621125,0.0581855,-0.010233626,0.06730416,0.026756007,-0.014714944,-0.032466613,0.015501581,-0.021981768,-0.015572697,0.022597715,0.034568816,0.016387688,-0.07373318,0.042563777,-0.04440601,-0.03449339,0.026336087,0.01618378,0.031973917,0.028795814,0.0515412,-0.022612903,0.029177051,0.06301483,0.016670657,0.00426968,-0.0017208381,0.015877636,0.022486854,0.043323107,-0.0017530923,0.005421554,-0.05381868,0.05856259,-0.04103139,0.013002949,-0.0037846896,-0.02414836,-0.0015736223,0.03825901,-0.005058084,0.008597832,0.015478894,0.0221345,-0.05542368,0.0015613907,0.011181241,0.04138474,0.025639782,0.0049586543,-0.08700534,-0.010627966,0.01448695,-0.007839844,-0.03018641,0.035074603,0.02465735,0.0042748414,0.055309016,0.0071465285,0.039456416,-0.02712513,0.018012928,0.040742412,0.022674864,-0.009646087,0.016191382,0.042758938,0.08171202,0.0154584935,-0.058833193,-0.0070732655,-0.026646856,-0.07864819,0.029281821,-0.033157904,0.018906662,-0.071188144,0.054010384,0.026453871,-0.013807152,0.042390786,-0.06996864,-0.025120307,0.0028099434,-0.06177833,-0.037315615,-0.014917266,-0.021828858,-0.055813964,-0.0063484735,-0.04700896,0.07432711,0.06463357,0.034208108,0.005251874,0.0116713075,-0.050043926,-0.014304627,-0.019913733,-0.07777353,0.01823044,0.005884353,-0.038884394,0.04815221,-0.0052428194,-0.04623343,-0.02322758,0.034416072,0.014540737,0.04733393,0.087075785,-0.011532833,0.038746435,0.04421028,-0.01271827,-0.014047791,-0.012629375,-0.005847971,0.0074041258,0.009989115,0.01683656,-0.03107095,0.017926494,-0.029063476,-0.0065838397,-0.014134465,0.028799037,-0.005054936,-0.039502364,0.011550243,-0.0050920807,0.030701255,0.028814038,0.00074835884,0.009457554,-0.022783352,-0.009247706,-0.048019715,0.018203123,-0.015103503,-0.023612317,0.016513241,-0.035503086,0.057589144,-0.020440277,0.029484278,0.038281746,-0.015768426,0.05651347,0.050259613,-0.044760168,0.004061116,0.029294953,0.032671083,0.02625853,-0.027390854,-0.041931525,0.016212853,-0.052394014,0.038746316,-0.014547432,0.005931431,0.012278502,0.030894449,-0.0036593564,-0.059464116,0.011024454,0.016895836,0.05538468,0.017422842,-0.041828886,-0.042404316,0.037592508,0.06661656,0.0017521186,0.014521088,0.036655106,0.06801391,0.03680443,0.022367451,0.036521673,0.03677638,0.0009334694,0.014135567,0.028939193,0.010734006,0.0067689507,-0.02843122,0.022312712,-0.0015597546,0.019655323,-0.013692181,0.011862704,0.052808836,0.03274784,-0.014644226,0.0008123187,0.029325867,0.015720362,-0.014408888,-0.00052920496,0.00967613,-0.022846418,-0.0036976007,-0.066541046,-0.03619222,-0.010849703,-0.019233314,0.026857918,-0.04056669,-0.012411436,-0.04843741,0.035982355,0.037595402,0.059192494,-0.09072155,0.0032555046,0.033331502,-0.05157608,0.06464623,0.008730185,-0.04082098,0.046053547,-0.03778366,0.0024821293,-0.0058776406,0.038259562,0.0056594354,-0.004713522,0.0431912,0.036821466,0.022895837,-0.017650219,-0.041276313,-0.026346007,-0.014924318,-0.02697187,-0.07855137,-0.019027531,-0.004305946,0.0151871955,0.01061595,0.011593534,0.06148994,-0.034665693,0.05540959,0.04246509,-0.0015913614,-0.03930672,0.010994736,0.0432789,0.0017348564,0.058620352,-0.018522728,0.020694708,0.015847454,0.036842383,-0.04704324,0.009913409,0.031005522,-0.025183829,-0.038133886,-0.00028666557,-0.0073702005,-0.033624712,-0.02509292,0.094591394,-0.014887346,0.045885134,0.0063800253,-0.028398957,0.040238593,-0.039502945,-0.044266306,0.034293182,-0.0071611926,0.005053023,0.024656814,-0.04948047,0.07347292,-0.055741582,0.0028630374,-0.01555747,-0.03200138,-0.10413038,0.013298592,0.016282711,0.004190898,-0.01565535,-0.08224218,0.016653305,0.033550348,-0.00066906225,0.043070804,-0.033467405,0.01826896,-0.019169949,0.0060844137,-0.064181894,0.017666668,0.009004621,-0.033399403,0.053506363,-0.048632096,-0.039960545,-0.014251743,-0.011209675,-0.065249,0.0035822287,0.06550215,-0.0041908147,-0.05135934,0.0011602737,0.065496944,-0.010915946,-0.038287338,-0.015810875,-0.06309483,0.03743059,0.008222143,0.044157993,0.0519351,0.0524002,-0.04269089,-0.023066614,-0.047378495,-0.053967867,0.04157221,0.015757244,-0.012666008,0.04852616,0.039656274,-0.020914122,-0.057248976,-0.046875108,0.023163324,0.046640858,0.033303965,0.018548276,-0.024123382,-0.0125523275,-0.0065358225,-0.014292371,-0.031320754,0.012914935,0.011257631,-0.055145457,0.015720109,-0.01772501,-0.027254332,0.027038317,-0.051771257,0.017743595,0.06932338,0.02482389,-0.02393614,0.0056632822,-0.03646331,-0.007552982,-0.02261936,0.024745146,-0.018772025,-0.009939233,-0.0040782276,0.009842535,0.049493514,0.0037960822,0.016458504,-0.047217708,-0.0060899993,-0.00294858,0.06248847,0.017928705,-0.026102562,-0.053350013,0.03723938,-0.02756533,-0.029068448,-0.013791123,0.050225694,-0.0015762438,0.014632087,-0.002011892,-0.02641641,0.0095722955,-0.0054318397,-0.005737597,0.025153104,-0.011553787,0.01303944,-0.05155393,-0.043513194,-0.020842936,-0.01917304,-0.051807284,0.058349695,-0.0043457393,0.034910977,0.009486271,-0.0066381777,-0.0006865818,0.060593043,0.029675681,0.031462915,0.0387026,0.029656654,-0.021263458,0.007065688,0.066522345,0.018998291,-0.019276822,-0.02555504,0.008920587,0.031187214,-0.0050432477,-0.0035852313,-0.039446615,0.0029981704,0.032069623,0.012193269,0.022541903,0.027007185,-0.026028074,-0.03372077,-0.06628163,-0.068089806,0.011758259,0.06479738,0.029054843,-0.033462167,-0.08569649,0.05139641,-0.02956205,0.011033576,-0.022423398,-0.016150828,0.036189165,-0.014933114]	Keywords: gradient descent, optimization algorithms, deep learning, high-dimensional data\nKey Objects: optimization algorithms, Deep Learning community\nRefers to Images: None\nHypothetical Questions:\n- Why are certain optimization algorithms considered infeasible for high-dimensional datasets?\n- What are some examples of 'second-order methods' like Newton's method, and what makes them challenging to use?\n- What types of challenges motivate the need for specialized gradient descent optimization algorithms in deep learning?\n---\nSummary:\nThis section will outline gradient descent optimization algorithms commonly used in the Deep Learning community, focusing on practicality for high-dimensional datasets.\nOriginal Text:\n## 4 Gradient descent optimization algorithms  \nIn the following, we will outline some algorithms that are widely used by the Deep Learning community to deal with the aforementioned challenges. We will not discuss algorithms that are infeasible to compute in practice for high-dimensional data sets, e.g. second-order methods such as Newton's method 7 .\nContextualized Text:\nThis section introduces several gradient descent optimization algorithms frequently employed by the Deep Learning community. The focus will be on algorithms that are practical and feasible to compute even with high-dimensional datasets, and second-order methods such as Newton's method will not be covered due to computational limitations.	{"tags": ["optimization", "deep-learning", "algorithms"], "doc_id": "e5533967-07cb-4c4e-9ffe-5e646cfe3284", "summary": "This section will outline gradient descent optimization algorithms commonly used in the Deep Learning community, focusing on practicality for high-dimensional datasets.", "doc_type": "text", "entities": [], "keywords": ["gradient descent", "optimization algorithms", "deep learning", "high-dimensional data"], "key_objects": ["optimization algorithms", "Deep Learning community"], "contextual_text": "This section introduces several gradient descent optimization algorithms frequently employed by the Deep Learning community. The focus will be on algorithms that are practical and feasible to compute even with high-dimensional datasets, and second-order methods such as Newton's method will not be covered due to computational limitations.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms"}, "hypothetical_questions": ["Why are certain optimization algorithms considered infeasible for high-dimensional datasets?", "What are some examples of 'second-order methods' like Newton's method, and what makes them challenging to use?", "What types of challenges motivate the need for specialized gradient descent optimization algorithms in deep learning?"]}
6828452b-66eb-44ab-9312-565d0b7d36f9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01202233,0.00044703734,-0.037306186,0.03263948,-0.013959201,0.04782453,0.0037331244,0.02213272,0.008994987,-0.07295161,-0.023492265,-0.047833137,-0.0024803483,0.02181486,-0.046101853,0.0067066066,-0.017935792,0.07697395,-0.0107459305,-0.0065373136,0.039300192,0.015121161,0.029218296,0.010104986,-0.03290964,-0.0069700633,-0.00013595274,0.01024953,0.01727034,0.02971618,-0.023138765,-0.021288967,0.038754854,0.011241061,0.02181817,-0.01750144,-0.004716078,-0.037169576,0.0019675894,-0.04126455,-0.05198528,0.044597525,-0.022478417,0.024329487,0.025213683,-0.0930462,-0.03912514,-0.023348436,0.023855725,-0.013893637,-0.048357584,0.011334277,-0.08117474,0.04115531,-0.06556235,0.060968794,-0.054785788,0.013034625,-0.0050653755,-0.01500764,-0.03499647,0.020520912,0.026225507,-0.015912425,0.08004647,-0.03421605,0.013227992,0.059406914,0.021119798,0.12677833,-0.010787184,0.060432695,-0.008082405,-0.12144696,0.12594238,0.038941093,-0.03912227,-0.036528364,0.0034308182,-0.017079439,0.009212366,0.04111892,-0.051712804,-0.012228143,0.072309844,-0.0076864907,-0.055721693,0.0046845186,0.079670444,-0.05407888,-0.024698649,-0.024966456,-0.043149438,0.031841118,0.022144211,0.0021782573,0.016132096,-0.021853056,-0.02109014,-0.05153146,0.030777112,0.002669598,0.053043276,0.12289532,0.018975208,0.03860563,-0.003936285,-0.01740374,0.02289845,-0.00454884,-0.011132532,-0.019323919,-0.060418043,-0.022917418,-0.060812127,0.015952298,-0.020956311,0.021049377,-0.007496999,-0.047000475,0.013303545,-0.025590003,-0.021601807,-0.009351234,0.03252696,0.09692191,-0.02682097,-0.027758347,-0.021344254,-0.02503716,0.024344705,-0.022990419,-0.028057147,0.010978702,-0.010181183,-0.033672083,0.0050706733,0.030250244,-0.0049047605,0.020324126,-0.056648076,-0.024887526,-0.044290066,0.015918413,-0.007629861,0.047879744,-0.09516422,0.055385463,-0.023866547,0.0613955,0.016033405,-0.04081577,0.04375372,-0.035983246,-0.053011574,-0.042186107,-0.015191555,0.013491045,0.008212373,-0.045397546,0.0128288735,0.014920712,-0.0070115663,0.09359138,0.048272192,0.14867373,-0.0107399365,0.052348994,-0.0020926043,-0.0052810973,-0.0012662191,-0.04952777,0.0015277722,-0.051947366,-0.013129041,-0.022493796,0.011366987,-0.021301076,0.0044386364,0.0073461584,-0.0077872886,0.045887787,0.02467491,0.035216626,0.028323028,0.053471424,-0.0022464097,0.02971386,0.017056376,-0.0029017024,0.013813475,-0.0005458705,-0.00014908008,0.000529669,0.03834976,0.034436792,0.048756737,-0.01354503,0.012373938,-0.039031718,0.009722129,0.07453835,-0.025462126,-0.07586103,0.052198034,-0.031258397,0.0025490164,-0.022631379,-0.063782655,0.0062650456,-0.053370584,0.013067086,0.04203882,0.010449248,0.0521078,0.0784596,0.012191869,0.020292073,0.059530213,0.013325533,-0.03654844,0.02750438,-0.027805354,0.010231481,-0.014331555,-0.017774938,0.032630116,0.0018712631,0.057391085,0.0027804563,-0.014110866,0.049392734,-0.07263727,0.060996793,-0.017488968,0.023788659,-0.0053528044,0.021615194,0.050283603,0.0420804,0.018412123,-0.0019291299,-0.031436592,0.08080937,-0.0035862417,-0.050080456,0.031731613,0.049703453,0.009241904,-0.009794792,-0.07187883,-0.006049727,-0.00030218408,-0.021170598,0.021146642,-0.029599767,0.013410426,0.024943262,0.0330916,-0.04921653,-0.046793018,-0.024403842,-0.038692247,0.03711941,0.022053743,0.02350491,0.006227758,0.038018957,-0.05529573,0.019345624,0.031566676,-0.012943019,-0.021464158,-0.015232025,0.026296467,0.025370251,-0.0241864,-0.00096778135,0.055635355,-0.01594576,0.026814656,0.03024221,0.011566675,0.0029684873,-0.027002284,0.03369519,-0.025205532,-0.00950555,0.0023052252,0.011339317,-0.0077816085,-0.0195287,0.02917658,-0.014626259,-0.025775427,-0.027919482,-0.014228692,-0.04183809,-0.0034164635,0.01411,0.06833126,-0.017184861,0.03510345,-0.0024455555,-0.013441471,-0.022691533,-0.08960801,0.03983257,0.01909846,0.0015380343,0.07472898,-0.014565785,-0.03491147,0.01409099,-0.014294081,-0.0018601009,0.038534794,-0.033503853,-0.003068904,0.007142493,0.033351913,0.00066463056,-0.021242956,0.04732526,-0.011138867,0.019147541,-0.023546575,-0.05414237,-0.0811887,0.016588083,-0.032713663,0.0140726315,0.05369852,0.009578298,-0.0022106317,0.031764798,-0.04171871,0.01971958,0.0031035452,-0.034437757,-0.0072948043,-0.04864143,0.01948526,-0.011525198,0.014295735,0.015539542,-0.041390426,-0.025943108,-0.024608038,0.05267759,0.0044364505,0.032507252,0.053287383,0.024135476,0.028584933,0.010139423,-0.0025745204,0.030020876,0.039789435,-0.011780938,-0.012194669,-0.037781104,0.00995673,-0.06086465,0.002497985,0.032145143,-0.03042563,0.0021238886,-0.0033969714,-0.017384438,0.0056130663,-0.019106688,0.032349348,-0.021534972,0.0038918233,0.044409093,0.004550317,0.009005177,0.0043168636,-0.030568277,-0.02399284,0.034203533,-0.027365137,-0.053089127,0.048142992,-0.0018923183,0.0066614477,0.1120782,-0.011782192,-0.025239075,-0.029169852,-0.007594786,0.056825507,0.0026814288,-0.05575758,-0.021885553,0.032961648,0.04140598,0.020510472,-0.0062909485,0.008212843,0.025847124,-0.06966593,0.04646707,0.018613927,0.04300068,-0.06128038,0.0005895043,0.029401662,-0.004153326,0.019713013,-0.05809062,-0.05645107,0.028915556,-0.010037226,0.0068923435,-0.03732385,0.0010499933,-0.047118146,-0.048473753,-0.049316872,0.032841343,0.030015163,0.035776295,0.0461774,0.052327786,-0.0013515459,0.023546817,-0.0011224963,-0.049849432,-0.0072692614,0.03186889,-0.020083867,0.06451583,0.0054825856,0.0140091255,0.01753534,0.059341677,-0.015409206,0.043478195,0.050608154,-0.044167075,0.04749621,-0.020748623,-0.02237107,-0.010468554,0.081504785,0.031222552,0.032153822,-0.011897941,0.009425027,-0.021028144,-0.0021082405,0.007792261,0.010904118,-0.03291976,0.02809764,-0.06372582,-0.020479579,0.03216107,-0.024763575,0.04929296,-0.007696164,-0.005002843,0.008208033,-0.014584399,-0.023449264,-0.05457711,-0.005040829,-0.043097638,-0.08480797,0.026464637,-0.050482538,0.015526236,0.02428352,0.0008363206,0.04253046,0.050804667,0.08318952,0.032813825,-0.06868179,0.0077668717,-0.0024951347,0.06488749,0.050315384,-0.0423381,0.008333371,-0.0016947872,-0.034956384,-0.003163626,-0.02412684,0.01155533,-0.0656739,0.010592577,0.051934358,-0.0552387,0.028631257,-0.035352193,0.016775688,-0.047578193,0.0018369645,0.021499446,0.040292684,0.034099817,0.0028326032,-0.008433701,0.031912453,0.041272696,-0.020121781,0.021214312,-0.04232696,0.00065640255,0.0067977323,-0.056425124,0.01664341,0.008340196,0.0017220287,-0.025082594,-0.0021888656,-0.040893946,-0.0002669994,0.028028721,-0.02815429,-0.02633058,0.0046273116,-0.0103856735,-0.02074714,-0.006918722,0.028982634,-0.015424174,-0.011591385,0.041546915,0.0037926494,-0.01132258,-0.051745873,-0.041277867,-0.009906218,-0.024876425,0.012590551,0.0013653808,-4.484179e-06,-0.017927108,-0.014556579,-0.038253058,0.06272807,-0.028144624,0.044918314,0.063008025,-0.024416748,0.060602587,-0.0006132052,0.023905745,0.043043166,-0.004854776,-0.011065866,0.004018094,-0.009158141,-0.011571305,0.013287455,-0.008621833,0.021073978,0.006758473,0.022059055,-0.050320204,-0.01416216,0.010212708,-0.0036134422,-0.020528402,-0.026103497,-0.04544109,0.01652517,-0.01224741,-0.04076389,0.026129654,-0.025668625,0.08373538,0.051125333,0.01165128,-0.008919843,0.047083154,0.015136552,0.012760587,0.06373386,0.050976824,0.006941717,0.054341707,-0.0044222297,0.010425191,-0.017883893,-0.02111449,0.05431672,-0.04350172,-0.013204515,-0.03693421,-0.0010226703,-0.08030187,0.08093176,-0.035793778,-0.013173937,-0.043860875,-0.053074736,0.037613574,-0.067773424,-0.06277565,0.0282827,-0.045125842,-0.06410495,0.04830859,0.008923828,0.047789514,-0.027603751,-0.022287713,0.0075640352,-0.025432227,-0.07137412,-0.03816541,-0.0047911485,-0.017189905,-0.008933211,-0.05078053,0.0049320385,-0.036138922,-0.03470979,-0.00718418,-0.061581396,0.01565101,-0.042828787,-0.047684934,0.015758095,0.008936196,0.004989095,0.0016630345,0.050073028,-0.041145213,-0.048456647,0.019888928,0.01904565,-0.046289507,-0.00982104,0.08187032,-0.025741998,0.014621039,-0.038527753,0.024813486,0.016181339,-0.006054767,0.015097032,-0.040246118,0.044268254,-0.025229769,0.0308374,0.08401067,-0.0032511796,-0.017940452,-0.0485612,-0.04660558,-0.008645462,0.019612173,0.054394092,0.007294397,0.035089646,0.032877807,0.01200766,-0.007227882,-0.017116632,0.017054383,0.014376437,0.017620223,0.055065755,0.007363877,-0.049316145,0.005399208,-0.0038792007,-0.047963288,-0.0006233486,0.043416023,-0.069204286,0.027426817,-0.007820395,-0.0359167,0.04491021,0.014491771,0.021263208,0.0488849,0.010644196,-0.021185338,-0.045354757,-0.014616186,-0.008717386,-0.031327024,-0.010257223,-0.007967431,0.0011282748,-0.002030491,-0.007979328,0.050220855,-0.00858135,-0.06621879,-0.033769153,0.03282086,0.016118549,0.021106048,0.03193428,-0.021279337,-0.059489623,0.0406507,0.035763867,0.022743504,0.043648656,0.035637863,-0.024442526,-0.037105143,0.023754403,0.03377108,0.01791982,0.013915298,0.018434223,0.021050371,-0.016802946,0.017281191,-0.04786434,-0.028549992,-0.00072629907,0.034757346,-0.042295214,0.030501205,0.043632038,0.003014943,-0.021322086,-0.034343988,0.057702135,0.04544738,0.0010304888,0.07197691,0.010527248,0.034959357,-0.01422706,0.040234916,0.035801925,-0.08621723,0.029143676,-0.021962475,0.045202658,-0.03593553,-0.019973943,-0.0004305939,-0.038162593,0.0018774809,0.02575136,-0.032490976,0.013629951,-0.027075134,-0.0402072,-0.019396478,0.00023200313,-0.016388485,-0.05954946,0.0629678,0.04107529,-0.0328621,-0.022015704,-0.010373395,-0.029035134,0.036803283,-0.02985044,0.038483106,-0.018317753,0.0024706672]	Keywords: SGD, Momentum, ravines, oscillations, update vector\nKey Objects: ravines, update vector, Momentum\nRefers to Images: ./images/an-overview-of-gradient-descent-optimization-algorithms/image_2.png\nHypothetical Questions:\n- Why does SGD struggle with ravines?\n- How does Momentum improve upon the behavior of SGD in ravine-like surfaces?\n- What is the purpose of incorporating the past update vector when using Momentum?\n---\nSummary:\nMomentum is a technique designed to address the limitations of Stochastic Gradient Descent (SGD) by accelerating updates and reducing oscillations when navigating complex, ravine-like surfaces.\nOriginal Text:\n### 4.1 Momentum  \nSGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another [20], which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum as in Figure 2a.  \nFigure 2: Source: Genevieve B. Orr  \n  \nMomentum [17] is a method that helps accelerate SGD in the relevant direction and dampens oscillations as can be seen in Figure 2b. It does this by adding a fraction  of the update vector of the past time step to the current update vector 8  \n<!-- formula-not-decoded -->  \nThe momentum term  is usually set to 0 . 9 or a similar value.\nContextualized Text:\nStochastic Gradient Descent (SGD) can struggle with 'ravines,' areas with steep curves in one dimension, often found near local optima. To overcome this, Momentum is a technique that helps accelerate SGD updates and dampen oscillations. It achieves this by incorporating a fraction () of the previous update vector into the current update vector.	{"tags": ["optimization", "machine-learning", "algorithm"], "doc_id": "6828452b-66eb-44ab-9312-565d0b7d36f9", "summary": "Momentum is a technique designed to address the limitations of Stochastic Gradient Descent (SGD) by accelerating updates and reducing oscillations when navigating complex, ravine-like surfaces.", "doc_type": "text", "entities": ["SGD"], "keywords": ["SGD", "Momentum", "ravines", "oscillations", "update vector"], "key_objects": ["ravines", "update vector", "Momentum"], "contextual_text": "Stochastic Gradient Descent (SGD) can struggle with 'ravines,' areas with steep curves in one dimension, often found near local optima. To overcome this, Momentum is a technique that helps accelerate SGD updates and dampen oscillations. It achieves this by incorporating a fraction () of the previous update vector into the current update vector.", "mentioned_images": ["./images/an-overview-of-gradient-descent-optimization-algorithms/image_2.png"], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.1 Momentum"}, "hypothetical_questions": ["Why does SGD struggle with ravines?", "How does Momentum improve upon the behavior of SGD in ravine-like surfaces?", "What is the purpose of incorporating the past update vector when using Momentum?"]}
7e91853d-48dc-49a2-803d-f1d424fddb59	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.036907244,-0.010511996,-0.017595552,0.029148104,-0.08232515,0.049906544,-0.02239475,0.03212691,0.04993701,-0.01654632,-0.041312132,-0.033900946,-0.018812083,-0.011502097,-0.05309916,0.011032386,-0.0059909928,0.062949985,0.0013405316,-0.014500014,0.044788584,0.008646434,0.039950237,0.012997564,-0.0395092,0.010168088,0.0015800905,-0.018930903,0.010476172,0.021371087,-0.035408355,0.010197652,0.01181682,0.008239339,0.031785604,-0.023467762,-0.014494424,0.0072485404,-0.00036406802,-0.030240763,-0.031828478,0.0016961468,0.035372864,0.04589385,0.022855751,-0.04699845,-0.05941019,-0.040523626,0.015759356,-0.038596008,-0.015835209,0.03513728,-0.015677849,0.046528257,-0.07317175,0.05929056,-0.056663565,-0.014826312,-0.017936055,-0.035235606,-0.037281487,0.008124741,0.0383401,-0.023836851,0.046684865,-0.032275487,0.0071808402,0.027480025,0.0133521035,0.13627836,0.014342129,0.086644806,-0.018449808,-0.12293347,0.13483076,0.06168434,-0.027616689,-0.06327929,0.014391643,-0.009409032,0.03358903,0.049236804,-0.05893949,-0.00981397,0.048613276,-0.0063088625,-0.068754636,-0.01730168,0.044115536,-0.08201866,-0.0014351272,-0.048884876,-0.029915355,0.02734661,0.01708938,-0.030875884,-0.026798146,-0.047141023,-0.019852793,-0.030941445,0.06309397,-0.0031511628,0.035308555,0.093654044,0.026513191,0.021668144,0.018437784,-0.012267257,-0.002440968,-0.017340869,-0.02843968,-0.0019275995,-0.030189365,0.031377263,-0.029475655,0.01003457,-0.06619531,0.03897477,0.029209211,-0.05063349,0.02083408,-0.014791878,-0.028270412,-0.006476618,0.030636543,0.078930974,-0.024526417,-0.02901369,-0.019935181,-0.05123638,0.022723371,0.00030433477,-0.02199613,0.025667587,-0.0014884366,-0.00557094,-0.015955266,-0.00717932,0.011671973,0.006786724,-0.0552361,-0.027836205,-0.06560146,0.009492611,0.0060492274,0.043939188,-0.08583262,0.067183316,-0.015706176,0.06916541,0.020399826,-0.004511862,0.024330303,-0.027879685,-0.05338041,-0.07022653,-0.05927327,0.029994935,0.032486048,-0.05585599,-0.05486619,0.024438893,0.02918384,0.06455282,0.06829416,0.096120395,-0.050552268,0.010534527,-0.03879287,-0.031883862,-0.04114101,-0.04818377,0.027559817,-0.042003207,0.013730717,-0.010639352,0.02642049,-0.0049891584,-0.002582442,-0.012139714,-0.030150307,0.005631397,-0.0032542709,0.005273279,0.008449418,0.05593232,-0.017784134,0.0069981464,0.035031483,-0.032461327,0.009260665,0.018849488,-0.017777618,-0.03454777,0.072242826,0.01567659,0.03703845,0.01333688,0.00838325,-0.031495668,0.03573265,0.09981825,-0.06814161,-0.053614065,0.07846943,-0.00617222,0.0014799852,-0.026568998,-0.055323113,-0.0102421595,-0.0048338263,0.014518294,0.020989161,-0.016504696,0.022124901,0.035946164,0.003277083,0.006336094,0.034821246,-0.033007298,-0.07255554,-0.0065496974,0.003078991,0.0102591375,-0.027241522,0.0007162625,-0.009877546,-0.007333075,0.03235094,0.03811734,-0.012708873,0.08804044,-0.032073423,0.058233574,-0.011322591,0.052418284,0.0074019344,0.06017563,0.029681304,0.04204921,0.039418537,0.029142465,0.0024316553,0.06463473,0.012154953,-0.050309274,0.016968919,0.027855083,0.014039291,0.0033602058,-0.029889934,0.019495571,0.015426567,-0.012263865,-0.012443936,-0.006495425,0.028515423,0.012598352,0.021858195,-0.058767498,-0.04546035,-0.010084548,-0.031174315,0.056593947,-0.0028804105,0.025388451,0.042680383,0.012731584,-0.04924373,0.017868085,0.016542805,-0.0063600955,0.0023635023,-0.024671972,0.0008775199,0.046254516,-0.04652062,0.03339231,0.02818496,-0.012194544,0.0043830196,0.016934363,0.0018941464,0.019228376,-0.021826688,0.0382444,-0.0100754965,-0.04484527,-0.024232063,0.0019542682,0.030311016,-0.036885407,0.03670967,-0.017181996,-0.053843524,0.023141127,-0.009572797,-0.02661218,0.018681737,-0.0042193625,0.07413139,-0.040208843,0.04357629,-0.038578063,-0.002474754,-0.003267132,-0.055448104,-0.00049301697,0.013786157,0.054555956,0.08153278,-0.04284046,-0.0059521124,0.014115543,0.04812572,0.021134194,0.048264887,0.0043021846,0.02646892,0.027421724,0.07401024,0.006823159,-0.048003595,0.0755276,0.060006317,-0.010412008,0.01165279,-0.06352665,-0.0524792,0.07018981,-0.0355247,0.026605412,0.06632106,-0.007680688,0.019930096,-0.005660942,-0.01710996,0.0099385725,0.006944735,-0.037237857,-0.019584224,0.009793167,-0.0051454264,0.032207455,-0.0106837405,-0.008163421,-0.06836675,-0.02011326,-0.039436497,0.035148513,0.004072651,0.031216225,0.027263632,0.044236522,0.029548276,-0.0069692824,-0.03575936,0.045846358,0.051597103,-0.04831815,-0.0028152128,-0.017879931,0.022271682,-0.029678397,0.00015786928,0.023257824,-0.06735892,0.020585837,-0.03962909,0.009523671,0.005662801,-0.013163467,0.04935999,-0.017359272,0.013466097,0.07782459,0.02737222,-0.013154186,-0.020749988,-0.029801145,-0.039730787,0.029195502,-0.01872931,-0.04991509,-0.012604805,0.0023512912,-0.011091104,0.1127075,0.020510437,-0.012941939,-0.060548544,0.0018476191,0.013305428,-0.00551568,-0.048902143,-0.068011224,0.008297572,0.024852319,0.009577978,0.010143679,0.019634994,0.054995798,-0.065348625,0.016900478,-0.014508093,0.012718611,-0.019191563,-0.01690025,0.00277525,-0.029343508,0.06808913,-0.0761515,-0.0364584,-0.008806873,0.010615479,-0.025229594,-0.038273707,-0.012062507,-0.01283416,-0.022211086,-0.047326278,0.00347233,0.044336796,0.027830647,-0.025155358,0.030424504,0.0124847945,0.07302447,3.57925e-06,-0.016188031,0.015525924,0.049396377,0.01546474,0.0402486,0.04230858,0.026678298,-0.04236063,0.0682029,0.01908573,0.01171104,0.029356657,-0.0020000213,0.018983996,0.002827327,0.0021110387,-0.025801752,-0.014109846,0.0010263768,0.0066245054,0.022790572,0.045571778,0.0030673626,0.03909912,0.0020484217,0.020761728,-0.021456517,-0.017461456,-0.044690777,-0.003160063,0.0010799386,-0.014012607,0.029439192,0.01056553,0.005776799,0.0073390654,-0.009515174,-0.07070734,-0.05387532,-0.018413134,-0.005957498,-0.080993235,0.062005047,-0.03480849,-0.0058359234,-0.03036803,-0.006734342,-0.004014104,-0.016742561,0.062516764,0.05391838,-0.08028393,-0.02709267,0.010601931,0.028031675,0.03103628,-0.07142031,-0.033838484,0.00046349884,-0.028082024,0.01837772,-0.05139406,0.017384727,-0.036967386,0.01582685,0.012459287,-0.081920214,0.032048643,-0.018604849,-0.010066009,-0.043105662,0.0007150618,0.010674108,0.07839204,0.05260573,-0.02385837,0.0053299195,0.025174947,0.07129232,-0.057332937,0.03339299,-0.015213753,-0.0264336,-0.0068051917,0.033771086,0.002605415,-0.003348644,0.0022158495,-0.061336666,0.011877479,-0.045819413,-0.011919926,0.06178406,0.021113485,-0.02628737,0.0121990945,-0.01910733,-0.089355685,-0.0033984764,0.06246119,0.012803631,-0.025928825,-0.008735855,-0.0055055493,-0.03232722,-0.09659539,-0.013889965,0.0035294916,0.021099517,0.020592032,-0.018533086,-0.029437032,-0.065235846,-0.010851218,-0.016534712,0.02645288,-0.013196614,0.06268103,0.049132288,-0.013758433,0.027784992,0.007061993,0.02138808,0.02979496,-0.044906136,0.033355586,0.027780922,-0.00732654,-0.041653685,0.01297707,0.003661808,-0.020709077,-0.027361562,-0.0028587328,-0.044006933,-0.005447512,0.0019319049,-0.0024476273,-0.029477991,-0.04830179,-0.019754244,-0.0180737,-0.002330536,-0.006790496,0.028318068,-0.06361397,0.054802842,0.061589573,0.012794656,0.02965588,0.04968563,0.0016369091,0.02782776,0.03352339,0.04230144,0.01675182,0.06748004,0.010336663,0.019776419,-0.0045022485,-0.014871391,-0.0012999924,-0.014704769,-0.020436738,-0.053836882,0.015980106,-0.008935289,0.021610761,0.0035557088,-0.0035248718,-0.019188289,-0.056775834,-0.008966103,-0.03957706,-0.0039453562,0.03788597,-0.020333275,-0.028061774,0.016833104,-0.037147406,0.022940757,-0.01427419,-0.014070738,-0.018107694,0.0138853565,-0.06974906,-0.07563336,0.017403571,-0.044791494,-0.026951902,-0.029811967,0.020107739,-0.0030648261,-0.031604152,0.00816999,-0.061510224,0.017293533,-0.034727573,-0.065095775,-0.032644678,0.04077097,0.01202147,0.0062111085,0.0761583,-0.042037576,-0.031050544,0.021803258,0.007334624,-0.04796682,-0.01328911,0.048568852,-0.021102933,0.0332047,-0.015593111,0.04253114,0.0067692506,-0.011933528,-0.0010447691,-0.045266375,0.017246531,-0.06598935,0.022562537,0.05914219,0.0014024107,-0.033360723,-0.04874685,-0.051944815,-0.00031154676,0.01140295,0.041426554,-0.005076678,0.054748368,0.052450117,-0.0120383445,-0.0017385948,-0.022213671,-0.011549053,0.06278244,0.0052937237,0.030264296,0.024113609,-0.028484117,0.008130227,-0.018826485,-0.02677952,0.025203584,0.022725573,-0.026567701,0.0070380243,-0.021785082,-0.009042626,0.06070526,-0.018457633,0.050950523,0.046769388,0.013882185,0.001625285,-0.03302551,-0.0314058,0.03971457,-0.017714042,0.020493349,-0.0101634385,-0.02427401,0.0021122445,-0.03231811,0.04072967,0.0039353976,-0.053659372,-0.052735195,0.032522306,0.003463644,0.029649623,-0.008969101,-0.045075867,-0.054101646,0.07755746,0.030589838,0.008644321,-0.00086700724,-0.016169976,-0.03187369,-0.00045636692,0.026680972,-0.0017531093,0.0066035935,0.021443052,-0.026570374,-0.01598282,0.018269764,0.032499425,0.0027725513,-0.0025799426,0.041404396,0.0025276525,0.006073662,-0.008137363,0.038404286,0.0482642,0.047349796,-0.025700428,0.06425431,0.020166202,0.024774166,-0.001727467,-0.038877107,0.05846092,-0.0005359111,0.040493872,0.020822337,-0.05104525,0.024746638,-0.06809628,0.0683862,-0.053741578,0.039708916,0.03768916,-0.062438477,0.0034644778,0.022039687,-0.025828999,-0.00578604,-0.017876757,0.00084525056,-0.015558835,-0.009583988,-0.036008306,-0.015396593,0.052229747,0.012191758,0.005896428,-0.014525502,-0.032091137,-0.024850562,0.012920504,-0.028814742,-0.0041211336,-0.006084093,0.0073743677]	Image title: SGD with and without Momentum\nTags: sgd, optimization, momentum, neural-networks, cost-function, gradient-descent\nKey objects: Cost Function Landscape, Elliptical Contours, SGD Trajectory (without momentum), SGD Trajectory (with momentum)\n---\nSummary:\nThis diagram illustrates the difference between Stochastic Gradient Descent (SGD) without momentum and SGD with momentum. Both diagrams depict a cost function landscape represented by elliptical contours. The trajectories of the optimization process are shown as lines, highlighting how momentum can smooth out oscillations and accelerate convergence.\nFull description:\nThe diagram presents two visualizations of Stochastic Gradient Descent (SGD). Diagram (a) shows SGD without momentum. The path taken by the optimizer oscillates significantly due to the noisy gradient estimates, resulting in a meandering trajectory. Diagram (b) shows SGD with momentum. The path exhibits a smoother trajectory, as momentum helps to average out the noisy gradient estimates and dampen the oscillations. This allows the optimizer to move more consistently toward the minimum of the cost function.\nText found in image:\n- (a) SGD without momentum\n- (b) SGD with momentum	{"tags": ["sgd", "optimization", "momentum", "neural-networks", "cost-function", "gradient-descent"], "title": "SGD with and without Momentum", "doc_id": "7e91853d-48dc-49a2-803d-f1d424fddb59", "source": "./images/an-overview-of-gradient-descent-optimization-algorithms/image_2.png", "summary": "This diagram illustrates the difference between Stochastic Gradient Descent (SGD) without momentum and SGD with momentum. Both diagrams depict a cost function landscape represented by elliptical contours. The trajectories of the optimization process are shown as lines, highlighting how momentum can smooth out oscillations and accelerate convergence.", "doc_type": "image", "key_objects": ["Cost Function Landscape", "Elliptical Contours", "SGD Trajectory (without momentum)", "SGD Trajectory (with momentum)"], "parent_doc_id": "6828452b-66eb-44ab-9312-565d0b7d36f9", "text_in_image": ["(a) SGD without momentum", "(b) SGD with momentum"], "contextual_description": "The diagram presents two visualizations of Stochastic Gradient Descent (SGD). Diagram (a) shows SGD without momentum. The path taken by the optimizer oscillates significantly due to the noisy gradient estimates, resulting in a meandering trajectory. Diagram (b) shows SGD with momentum. The path exhibits a smoother trajectory, as momentum helps to average out the noisy gradient estimates and dampen the oscillations. This allows the optimizer to move more consistently toward the minimum of the cost function."}
36214953-f3d0-4089-983d-29e55f2a280b	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.024006004,-0.0057104146,-0.037155826,0.03559824,-0.033829797,0.019862926,0.0038099065,0.026123436,0.0049381447,-0.06293537,-0.02869355,-0.026773578,-0.026306499,0.039104655,-0.06470666,0.0030186959,-0.031654134,0.071323305,-0.0016948938,0.00064338965,0.039205294,0.010340804,0.030168053,0.032155592,-0.01817094,0.02328083,-0.017659783,0.018531518,-0.02144517,0.017380632,-0.018120663,-0.023492742,0.014656452,0.012820812,0.0034091943,-0.026845476,-0.04406332,-0.04300355,-0.0033917474,-0.021160215,-0.044001825,0.030155543,-0.02199237,0.011150998,0.022788618,-0.10968859,-0.029429073,-0.011578257,0.022201953,-0.010721404,-0.048196398,0.013964464,-0.051651157,0.029706312,-0.041388635,0.0569208,-0.064617015,0.0067158313,-0.02523045,-0.0036967937,-0.02038942,-0.011150392,0.055629622,-0.0010196797,0.1024035,-0.047797453,0.00911632,0.03484425,0.004961258,0.10483607,-0.010949928,0.08300045,0.0047223307,-0.11255807,0.10955969,0.05583276,-0.043254342,-0.05082249,0.025086844,-0.008566193,0.06021024,0.06739846,-0.043073393,0.013871545,0.07787825,-0.0022368731,-0.04052336,-0.056073617,0.056962393,-0.079529405,-0.02056601,-0.06756149,-0.010492721,0.031488672,-0.043016676,-0.023279402,-0.008820865,0.003801925,-0.024990607,0.0069570346,0.06760741,-0.0071893544,0.032570463,0.10262215,0.03032498,0.01548417,0.00517599,-0.031464543,0.01965136,-0.0064914483,-0.007457705,0.022894073,-0.07101492,-0.02951127,-0.041842,0.02819768,0.010601249,0.013666107,-0.0076724775,-0.058469508,0.020237971,-0.03969474,-0.0034595872,0.008451478,0.038758956,0.07063484,-0.012659751,0.008374931,-0.03421898,-0.022448255,0.025912667,0.0287338,-0.01810538,0.019969216,-0.023867294,-0.025714967,0.009459494,0.009642641,-0.00852597,-0.0069424883,-0.07964542,-0.013982847,-0.020113934,0.018072426,-0.004869328,0.033108108,-0.06912899,0.07365706,0.0037754674,0.04651822,0.019678295,-0.03631342,0.0766794,-0.011972745,-0.047946528,-0.046807505,-0.03442273,0.012523635,0.023867097,-0.018959636,-0.033765763,0.022185517,-0.01636202,0.09937954,0.04153939,0.13457942,-0.023031773,0.059527043,0.010540041,0.00022622773,-0.038957056,-0.05456208,0.046593837,-0.018621318,-0.0005144026,-0.0219727,0.020125352,0.003035963,-0.004817418,0.003870449,-0.013496406,0.041163113,0.015845926,0.013771506,0.04894936,0.09611156,-0.0057244906,0.024014251,0.018230462,-0.023002308,-0.008758357,0.008143222,-0.0077700955,-0.00035900524,0.057483096,0.03304796,0.036434162,-0.038487654,0.022584965,-0.022874763,0.015122689,0.032972604,-0.061583452,-0.056824278,0.05402702,-0.012824707,0.032790262,-0.026258485,-0.052784022,-0.0053037186,-0.03367712,0.052841328,0.059692997,-0.032969926,-0.005594515,0.03610253,-0.011062696,0.030774206,0.06001083,-0.019238392,-0.033000268,0.010672222,0.010771923,0.042755514,-0.0080931205,-0.0024877724,0.06508808,-0.022566501,0.05797157,-0.014593346,-0.05432149,0.06364433,-0.052352056,0.05115441,-0.054498143,0.021018384,-0.030775512,0.08408624,0.04555376,0.049711004,0.03401155,-0.0022210234,-0.027407577,0.070277825,-0.016444813,-0.058821436,0.064472,0.020773238,-0.015946044,0.020774588,-0.026063226,0.0068244026,-0.0074834954,-0.014307952,0.0076855626,-0.021732848,0.041776314,0.015594763,0.02875898,-0.071676284,-0.022753652,-0.012055579,-0.03794558,0.018313522,0.017950702,0.051731464,-0.0027922404,0.022252187,-0.029780323,0.02403185,0.039277013,0.023793451,-0.010707755,-0.0015301277,-0.0047766827,0.025626712,-0.015802825,0.034499306,0.028904418,-0.001045586,0.0057277125,0.012830685,0.028093737,0.00427282,0.003519527,0.037156418,-0.044485644,-0.014700052,-0.02475877,0.04522108,-0.024638973,-0.05437047,0.03629768,-0.011818612,-0.042812098,-0.021155037,-0.00025657704,-0.023913937,0.035792213,0.034546595,0.07196035,-0.03122351,0.055327732,-0.036170244,0.026039384,0.015734274,-0.07799889,0.006035541,-0.029618151,0.024162674,0.05521133,-0.018428002,-0.0031199106,-0.003407855,0.015026759,0.008459617,0.04037834,0.005482886,-0.002932003,0.0025823724,-0.0032638593,-0.027882516,-0.052812513,0.05791261,0.021680942,-0.0008440384,0.009765321,-0.045206163,-0.052384336,0.027873999,-0.044388097,0.027075987,0.05322169,-0.027874034,-0.0072055813,0.05702604,-0.035152312,0.0147157535,-0.015467548,-0.04351394,-0.035553865,-0.0246949,-0.0057802694,0.04606604,-0.008927325,-0.008558935,-0.040421177,0.0050484315,0.0024243772,0.08608428,0.012231935,0.055753347,0.050033044,0.031299677,0.02635367,-0.029716104,-0.043791115,0.0006686267,0.06528819,-0.040368788,-0.011231223,-0.022080708,-0.008544451,-0.06093196,0.028767796,0.0030691708,-0.036700115,-0.00021883994,-0.0045054597,-0.001794696,0.027426073,-0.0476811,0.037746448,-0.056893717,0.0280459,0.05642298,0.05510766,0.042624075,0.013353965,-0.04575048,-0.027217468,0.021112544,-0.012780849,-0.045550384,0.065426625,0.021869726,0.018395163,0.100605756,-0.009490955,-0.017841626,-0.031392068,-0.0035647382,0.023705814,-0.035359688,-0.012309176,-0.014667511,0.02879397,0.019887537,0.005878021,0.030101363,0.028109463,0.03636528,-0.060955804,0.03170168,0.016544618,0.01806246,-0.024382593,-0.048674174,0.051848218,0.002894108,0.033002805,-0.06415441,-0.033152096,-0.0117723895,0.017723843,-0.006207739,-0.028948518,-0.015401039,-0.023799356,-0.05420952,-0.058347017,0.058838516,0.0533708,0.060032576,0.0075833467,0.025653146,-0.012063788,0.07911788,0.015179655,-0.013605204,-0.0132846655,0.03418231,-0.008102618,0.055707823,0.0199645,0.022270573,0.005021923,0.056530032,-0.013069526,0.0333598,0.012598044,-0.033092335,0.037860215,-0.025992434,-0.01124872,0.013872648,0.060514074,0.015558312,0.02791061,0.01957883,0.02180389,-0.025877262,0.02628664,-0.02726297,0.042480636,-0.03640485,0.018000966,-0.02660267,0.0075533176,0.028831275,-0.0028200322,0.014384395,-0.0054112086,-0.014279531,-0.003768907,-0.02317293,-0.0611936,-0.03749896,0.007916294,-0.039281096,-0.09008045,0.015618996,-0.027048,0.02419218,0.024244532,0.023396945,0.020454168,0.010889794,0.041269965,0.026738154,-0.09340646,-0.009340946,0.019392557,0.0403064,0.030874364,-0.03331481,-0.007796908,-0.012499652,-0.044545908,0.0031048416,-0.023700077,0.004539318,-0.029982325,0.03608174,0.052861243,-0.04418737,0.014214805,-0.032418866,0.009372053,-0.027861526,0.027621644,0.029781012,0.03271827,0.04357146,-0.0010689537,0.03760485,-0.02535003,0.050649155,0.0054782243,0.018190792,-0.039236322,-0.0075799534,-0.022281425,-0.029373422,0.021944383,0.030559612,-0.0038215537,-0.026832828,0.02017037,-0.072273076,0.04280776,0.035708237,-0.01618286,-0.022555886,-0.012022951,0.01910399,-0.052578907,-0.0013530073,0.016114842,-0.023538599,-0.025322448,0.011660549,0.041053895,-0.0034671722,-0.046852283,-0.025207138,0.03827147,-0.061623335,-0.0002384628,0.025104411,0.037790623,-0.041254833,-0.012995372,-0.02379836,0.03681979,0.014412335,0.03967451,-0.017608447,-0.03191422,0.054809533,0.04641608,0.04227044,0.015338333,-0.013292597,0.000107093045,-0.040188164,-0.035198417,-0.014078586,0.05340444,-0.0074789315,-0.0044963798,-0.025625808,0.016056744,-0.035049487,0.02517511,0.0109446505,0.024657354,-0.037403632,-0.023437345,-0.015812593,-0.0029296416,0.016314087,-0.0522391,0.03868731,-0.04110403,0.07981086,0.07207327,-0.00035001748,0.013910508,0.067267716,0.020564716,0.036988728,0.05993735,0.023828136,0.03707683,0.042831477,-0.030936418,0.04381111,-0.008576577,-0.0167285,0.040018205,-0.035867047,-0.009709752,-0.012050884,0.008445868,-0.06421337,0.033749603,-0.043586943,0.022738216,-0.06674165,-0.04545067,0.03739872,-0.06568507,-0.07978167,0.009659054,-0.04685212,-0.037038706,0.015987253,-0.0057234685,0.017238598,-0.03225559,-0.018873528,0.0033177638,-0.03971429,-0.09649335,-0.051703256,0.0158219,-0.012313754,0.01861995,-0.030975267,0.013120598,0.0045004203,-0.011759547,-0.018309264,-0.056815807,-0.015516065,-0.02891004,-0.0040768813,-0.0035144472,-0.0028570988,-0.004006717,0.015260471,0.01628242,-0.050556086,-0.04945475,-0.015217888,0.011615853,-0.021143734,0.04483453,0.054595362,-0.010350291,0.020110799,-0.03565551,0.0017998522,-0.011015062,0.0022209806,0.009837703,-0.035481762,0.030388271,-0.01779907,0.005222739,0.055069998,-0.03651941,-0.016701734,-0.04295583,-0.048244204,0.012664207,0.013837173,0.0673854,-0.026934935,0.038414955,0.05842837,-0.017374383,-0.0042071287,-0.022846337,0.00519238,0.026045162,0.02494397,0.056552097,0.009774387,-0.04613486,0.0025931322,-0.03379311,-0.026812084,0.023076627,0.06382,-0.05187456,0.002450018,-0.026035411,-0.02621675,0.043947242,-0.035326306,0.045734774,0.066949956,0.0024437332,-0.025421148,-0.029628186,-0.014709898,0.0022516542,-0.038463783,0.012506049,-0.019447047,-0.034163255,-0.0032433067,0.008419384,-0.015655473,0.012026354,-0.07399962,0.0072547616,0.03334505,0.024981821,-0.021964047,0.03739155,-0.05726251,-0.038991377,0.020463247,0.032290496,0.0229695,0.034373537,0.032815754,-0.026839279,-0.04309066,-0.015018395,0.021194074,0.01267268,0.028394949,-0.013801163,0.01999076,-0.02186053,0.0064267004,-0.042023502,-0.016885461,0.019930784,-0.0014598629,-0.02871401,0.052493766,0.03679773,0.007170304,-0.0047637303,-0.02379548,0.05136063,-0.030714646,-0.0019149942,0.06409902,0.0034252463,0.03899642,0.00198757,0.016655164,0.018271819,-0.07979131,0.06734696,-0.0060883053,0.057391893,-0.006625779,-0.02856682,-0.00053694524,-0.03459285,-0.005175439,0.05329214,-0.019955913,-0.01161822,0.008514888,-0.012098066,-0.029585516,-0.003893438,-0.024964312,-0.054001506,0.0065335743,-0.01687467,-0.030347684,-0.0015406021,-0.046506096,-0.013634046,0.046049323,0.0032442026,0.045007218,0.01743402,-0.009111258]	Keywords: momentum, parameter updates, convergence, oscillation, gradient descent\nKey Objects: momentum term, parameter updates, gradients\nRefers to Images: None\nHypothetical Questions:\n- Why is it beneficial to accumulate momentum when updating parameters?\n- What is the effect of a  value close to 1 versus a value closer to 0?\n- How does momentum help overcome oscillations in gradient descent?\n---\nSummary:\nThe momentum term, typically set to 0.9, accelerates parameter updates by accumulating momentum from past gradients, leading to faster convergence and reduced oscillation.\nOriginal Text:\n<!-- formula-not-decoded -->  \nThe momentum term  is usually set to 0 . 9 or a similar value.  \nEssentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity, if there is air resistance, i.e.  &lt; 1 ). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.\nContextualized Text:\nTo address the challenges of standard Stochastic Gradient Descent (SGD) when navigating areas with steep directional changes, momentum is employed. The momentum term, often set to a value like 0.9, helps accelerate parameter updates. It accumulates momentum from past gradients, resulting in faster convergence and reduced oscillation. This can be visualized as pushing a ball down a hill, where the ball gains speed as it rolls.	{"tags": ["optimization", "machine learning", "deep learning"], "doc_id": "36214953-f3d0-4089-983d-29e55f2a280b", "summary": "The momentum term, typically set to 0.9, accelerates parameter updates by accumulating momentum from past gradients, leading to faster convergence and reduced oscillation.", "doc_type": "text", "entities": ["gradient descent"], "keywords": ["momentum", "parameter updates", "convergence", "oscillation", "gradient descent"], "key_objects": ["momentum term", "parameter updates", "gradients"], "contextual_text": "To address the challenges of standard Stochastic Gradient Descent (SGD) when navigating areas with steep directional changes, momentum is employed. The momentum term, often set to a value like 0.9, helps accelerate parameter updates. It accumulates momentum from past gradients, resulting in faster convergence and reduced oscillation. This can be visualized as pushing a ball down a hill, where the ball gains speed as it rolls.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.1 Momentum"}, "hypothetical_questions": ["Why is it beneficial to accumulate momentum when updating parameters?", "What is the effect of a  value close to 1 versus a value closer to 0?", "How does momentum help overcome oscillations in gradient descent?"]}
b30ab075-c0ad-4927-a6fb-a95adc981d42	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.009194084,-0.013959567,-0.041004065,0.010647503,-0.050589286,0.06003247,0.035258878,0.025488174,0.052402124,-0.07033186,0.017151242,-0.020281352,-0.014120183,-0.020425668,-0.063505985,-0.019223275,-0.0125350505,0.07201629,0.024349978,0.035940073,0.06971266,0.028861456,0.024625834,0.029200092,0.018569086,0.048771504,0.03350486,-0.0027936467,0.026109252,0.005444423,-0.0046833954,-0.032501217,-0.0016177915,0.023162927,-0.015161509,-0.014039568,-0.06269437,-0.0577382,-0.02924445,-0.0427945,-0.03810291,0.044941258,-0.029796092,0.0102029415,0.017577525,-0.033457257,-0.0043125753,-0.057188522,0.027661253,-0.023713214,-0.03072233,0.06887017,-0.032014925,0.040847156,-0.047494996,0.05453995,-0.04644317,0.034796875,-0.028807396,-0.031500004,-0.049300946,-0.0013154373,0.04779437,-0.050318975,0.082179785,-0.025432754,-0.039200798,0.031541333,0.06850673,0.11181504,0.0052498863,0.044218782,0.05967462,-0.06752092,0.12599555,0.048013628,-0.007674466,-0.029646598,0.022571756,0.011545414,0.045063864,0.028816897,-0.022989543,-0.062096033,0.08284211,-0.004415902,-0.04368821,-0.03778394,0.067315646,-0.0407607,0.012078166,-0.036488023,-0.02999368,-0.018430572,-0.017541552,-0.009791157,-0.04207734,-0.009993863,0.0011555073,-0.00454914,0.034431044,0.026679078,0.032461394,0.112372436,0.033618312,0.017408114,-0.03632209,-0.043622628,0.028310573,-0.009126975,-0.044509947,0.05332143,-0.070599735,-0.023398353,-0.0048778886,-0.024159156,-0.033784665,0.057883725,-0.025093613,-0.019299252,0.04647141,-0.07712378,0.0074390005,-0.00096543535,0.012470363,0.058847826,0.028076854,-0.021428084,-0.042092733,0.015644701,0.033091895,-0.0013974154,-0.029419659,-0.016227735,0.021929186,-0.029788336,0.045207005,-0.027612986,-0.026952095,0.023352468,-0.08617568,-0.023958879,0.0015789577,0.020280186,-0.027867615,0.0328005,-0.054702353,0.025822017,0.021746334,0.052895997,0.046235863,-0.04891497,0.063738406,-0.023610163,-0.07696033,-0.03656144,-0.0071429377,0.033751376,0.007826011,0.011727865,-0.0015666466,0.036711916,0.014559446,0.081368655,-0.0013259838,0.15557116,-0.0054903687,0.029449373,0.006897818,-0.03895041,-0.013906658,-0.038346726,0.03996271,-0.029523833,-0.009607676,-0.044028245,0.018005235,0.02067427,-0.028276766,0.05177069,-0.017188096,0.002729586,0.0034719098,-0.03157562,0.01144394,0.03527067,0.024303282,0.023079911,0.01909199,-0.036401276,0.023817299,-0.0134357605,0.012267313,-0.025432382,0.04245263,0.051759534,0.052578345,0.0039849943,0.021680422,-0.05732519,0.022376742,0.054055803,-0.006534474,-0.044762503,0.020293916,0.022654532,0.018238239,-0.04215256,-0.059684798,0.040429633,0.009524163,0.06085937,0.020658355,0.05108704,-0.00146129,0.0359689,-0.031272985,-0.017300393,0.0009129561,0.025338009,-0.030248191,0.0071386346,-0.0079353945,0.0058273496,-0.044069804,0.038644668,0.010722364,-0.036273874,0.04949246,0.011501394,-0.012516174,0.0738932,-0.07488929,0.014194758,-0.03539471,-0.02092758,-0.006097398,0.058916956,0.028498478,0.08067438,0.03058103,0.0459978,-0.016492704,0.09121822,0.021632822,-0.015700124,-0.019089451,0.016944107,0.015558408,-0.0009811904,-0.029209638,-0.034479875,0.00044555898,-0.026982768,-0.016710429,-0.030301316,0.049873997,-0.024183875,0.06685233,-0.032980703,-0.00053467724,-0.019657806,-0.036421694,0.004252673,-0.027511239,0.04497823,0.010483385,0.015561228,0.005613728,0.044573598,0.044655398,0.021557039,0.0076680756,-0.000292019,0.04329315,0.041690364,0.009951583,0.019404128,0.026620818,-0.012077522,0.03953943,0.0071282224,0.005971043,-0.002232915,-0.033670142,0.025175799,-0.05050371,-0.0011599589,-0.010661729,-0.005077834,0.0037902733,-0.02840088,0.02272653,0.043546278,-0.024204701,0.005936651,-0.0008747489,0.0011109712,0.029266374,0.07144203,0.091672145,0.0146743385,0.028360574,0.037469946,0.011362119,-0.0043214383,-0.07853676,0.051649306,0.0067070476,0.037315134,0.03422705,-0.018978864,-0.037140872,-0.042709336,-0.0130674215,0.04954958,0.040569417,-0.037308592,-0.002920937,0.023164185,0.03233633,-0.028587377,-0.039433625,0.037189048,-0.0046668584,-0.007238882,0.023559349,-0.07074471,-0.027166322,0.07538959,-0.067154944,0.027174199,0.045379348,-0.008122677,-0.02975183,0.040948816,-0.048251234,0.01651869,0.019275695,-0.021956721,-0.019579424,-0.06552632,-0.008100504,-0.018559784,-0.00962261,0.007693911,0.014165822,0.028808715,-0.008692626,0.06481621,-0.014666457,0.07209019,0.022771405,0.019773321,0.038497504,-0.0039628195,-0.029851967,0.017710129,0.048560012,-0.012605785,0.025290001,-0.037220888,-0.00662183,-0.07106156,0.04226858,0.024454832,-0.031647813,-0.0030959994,0.0329611,0.028576974,-0.014440288,-0.0015595134,0.035802983,-0.033661757,0.018476533,0.026211137,0.0023663824,-0.013893067,-0.017348278,-0.07302548,0.02078907,0.0058505437,-0.024989327,-0.04845922,0.017288422,-0.0057435357,0.04522666,0.097633414,0.02065665,-0.00094422937,0.0053928224,-0.0064818333,0.04283069,0.01156719,-0.0064425943,-0.007392259,0.025182122,0.04030825,0.033908594,0.011884665,0.029446205,0.049565345,-0.062446292,-0.0073973984,0.04404478,0.012118528,-0.040956136,-0.024562025,0.033002608,-0.003970823,0.010950468,-0.057378195,-0.063994765,-0.021295708,0.019746123,-0.011460189,-0.025632486,-0.034137297,-0.036750942,-0.04486261,-0.006635789,0.042362276,0.037144117,0.028659426,-0.015856922,0.015575143,-0.02184624,0.02792927,0.038714867,-0.028336108,-0.010540391,-0.015353114,0.0073768217,0.0339511,0.009462792,-0.0050244974,-0.08310524,0.06336927,0.0031050604,0.020173114,0.0101061985,-0.010907848,0.019367853,0.010558619,-0.017319178,0.024692722,0.039660484,0.0045772633,0.03674359,0.027864048,0.011165125,0.023965228,0.0014759348,0.03809582,0.0756471,-0.008298286,0.033632807,-0.01501522,-0.005025824,0.03611212,-0.008511779,0.03694524,-0.021204773,0.025449598,-0.0112943435,-0.037366297,-0.05698733,-0.049775,-0.0204196,-0.012571063,-0.05411846,0.030931951,-0.01157984,-0.0039811614,-0.022617236,0.007169133,0.035993576,-0.0048851664,0.022758514,-0.0020343561,-0.03940318,0.0066233347,0.020097448,0.02955743,0.056521192,0.0071089566,0.018627485,0.0233119,-0.06526475,0.059195593,0.0064746346,0.004295554,-0.021689627,0.025797999,0.01942529,-0.03417446,0.059664097,-0.046919856,0.019595433,0.005318452,-0.001905531,-0.0047361506,0.022952218,0.03533794,0.016490491,0.0024013617,0.033133876,0.07558493,0.007679994,0.018599145,-0.0073762587,0.0053327,0.033871137,-0.055053655,-0.007454553,0.06502494,0.03346127,0.013675704,0.05447771,-0.049610306,-0.0124608055,0.039823018,-0.04128691,-0.026973836,0.07468134,0.01935951,-0.035106633,-0.03580702,0.023707883,-0.035690345,0.014199536,0.013326823,-0.031439897,0.020526674,-0.03827827,-0.04616694,0.005749402,-0.0463773,0.0060693957,-0.031338226,-0.0041156975,-0.027018776,0.013037505,-0.039761804,0.06468722,0.0031962262,0.0025306647,0.025498852,0.010114666,0.014119713,0.0540341,0.0015825471,0.059282333,-0.040153418,-0.022515114,0.050490484,0.019088566,0.006510855,0.022742277,0.010148484,0.05298439,-0.022185914,-0.009026495,-0.06214321,0.03504473,0.0077117146,-0.019814359,-0.077245906,-0.022512931,-0.032290123,-0.020907262,0.03283374,-0.07337234,-0.013517718,-0.021657536,0.08726358,0.067761235,0.021670781,-0.00047405492,0.04129616,-0.0145970145,0.022039982,0.07001154,0.023898385,0.0076649915,0.04233854,-0.0028952265,0.0148824155,0.009056921,0.0022882167,-0.016314803,-0.0066714673,-0.010947152,-0.030926282,-0.00076566264,-0.059289802,0.08581329,-0.03631161,0.04152385,-0.034253754,-0.056841638,0.0022234863,-0.029786874,-0.033996653,0.030284548,-0.0072491397,0.0008504049,0.0031742076,-0.030104978,0.051953882,-0.044665202,-0.046634365,-0.00408939,-0.017194593,-0.11065439,-0.03616719,-0.016157899,-0.067496516,0.0029003904,-0.06351762,0.0047534807,0.015441288,-0.0398349,0.0023299993,-0.040220067,-0.010084568,-0.032748066,-0.012683031,-0.01460016,-0.00881087,-0.038526475,0.002945074,0.050747536,-0.054642312,-0.0384381,-0.02605641,0.017185321,-0.04325274,-0.0048584244,0.053931974,-0.0077850237,0.042153675,-0.038435407,0.03421121,0.001770374,-0.010006546,0.023175724,-0.069785036,0.042910498,0.036546074,-0.0032034498,0.053859267,0.03458664,-0.05083212,-0.009249914,-0.01843993,0.041323766,0.01870038,0.062418602,-0.029711647,0.042370297,0.035813518,0.0024000548,-0.041451793,-0.07666901,-0.0029742015,0.049466956,-0.0075175334,0.056725137,-0.0035728158,-0.026740352,0.009583714,-0.060534436,-0.03631283,0.029329106,0.014210251,-0.058845695,-0.026320172,-0.0018203238,0.0044793244,0.020796318,0.03653516,0.0071019433,0.06181338,-0.02183167,0.007655663,-0.03630494,-0.034414053,-0.021303726,-0.004328722,0.04880644,-0.0025647755,-0.023908379,-0.0024650174,0.029408079,-0.0058604856,0.0032640742,-0.059343334,-0.005907831,0.029332092,-0.0002825565,-0.0044126953,0.025418447,0.018834297,-0.07199337,0.036291894,-0.00250741,-0.02365977,-0.005517937,0.026929205,-0.029825019,0.002880151,-0.009718552,-0.01563345,0.026406555,0.013518689,-0.006789295,0.035885323,-0.037321523,0.040745694,-0.060733203,-0.03196443,0.040806655,-0.029228624,-0.022358159,0.03293167,0.006658536,0.006770151,-0.02377368,-0.046990182,0.024546256,0.013953669,-0.0015268527,0.021624563,-0.0026299455,0.03293333,0.008501766,-0.011938388,0.0398177,-0.015548049,0.011152954,-0.046559315,0.043141227,-0.01573584,-0.017415194,-0.011000551,-0.018700521,-0.030423487,0.068405285,-0.047248058,-0.0026121316,0.0012146452,-0.0028590725,-0.03289269,-0.0012483817,-0.01802836,-0.07217949,0.008174792,0.009751664,-0.06506839,-0.015395259,-0.027442249,-0.011989705,0.043772846,-0.033342768,0.011480533,0.008993624,-0.04310333]	Keywords: Nesterov accelerated gradient, momentum term, parameter position, gradient descent\nKey Objects: Nesterov accelerated gradient, momentum term, parameters\nRefers to Images: None\nHypothetical Questions:\n- How does NAG improve upon standard gradient descent?\n- What is the purpose of computing the gradient with respect to the approximate future position of parameters?\n- Why is it beneficial for the momentum term to have a sense of direction?\n---\nSummary:\nNesterov accelerated gradient (NAG) provides a method to enhance momentum by giving it a sense of direction, allowing it to anticipate the next parameter position and adjust accordingly.\nOriginal Text:\n### 4.2 Nesterov accelerated gradient  \nHowever, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We would like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.  \nNesterov accelerated gradient (NAG) [14] is a way to give our momentum term this kind of prescience. Weknow that we will use our momentum term v t -1 to move the parameters  . Computing  -v t -1 thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be. We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters  but w.r.t. the approximate future position of our parameters:  \n<!-- formula-not-decoded -->  \n7 https://en.wikipedia.org/wiki/Newton%27s\\_method\\_in\\_optimization  \n8 Some implementations exchange the signs in the equations.\nContextualized Text:\nTo improve upon standard gradient descent, Nesterov accelerated gradient (NAG) was developed. NAG provides a method to give the momentum term a sense of direction, allowing it to anticipate the next parameter position and adjust accordingly. This technique computes an approximation of the next position of the parameters, enabling the calculation of the gradient with respect to this future position.	{"tags": ["optimization", "gradient descent", "machine learning"], "doc_id": "b30ab075-c0ad-4927-a6fb-a95adc981d42", "summary": "Nesterov accelerated gradient (NAG) provides a method to enhance momentum by giving it a sense of direction, allowing it to anticipate the next parameter position and adjust accordingly.", "doc_type": "text", "entities": [], "keywords": ["Nesterov accelerated gradient", "momentum term", "parameter position", "gradient descent"], "key_objects": ["Nesterov accelerated gradient", "momentum term", "parameters"], "contextual_text": "To improve upon standard gradient descent, Nesterov accelerated gradient (NAG) was developed. NAG provides a method to give the momentum term a sense of direction, allowing it to anticipate the next parameter position and adjust accordingly. This technique computes an approximation of the next position of the parameters, enabling the calculation of the gradient with respect to this future position.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.2 Nesterov accelerated gradient"}, "hypothetical_questions": ["How does NAG improve upon standard gradient descent?", "What is the purpose of computing the gradient with respect to the approximate future position of parameters?", "Why is it beneficial for the momentum term to have a sense of direction?"]}
51fdd3b8-dee1-4a09-a32b-8bec407f53b6	abe8c200-bfa1-4355-947e-23ea618c310d	[0.014471787,-0.01199189,-0.007572004,0.039910525,-0.047703926,0.068927914,0.04334717,0.021704601,0.019749193,-0.06610707,0.03069587,-0.0015253826,-0.017028632,0.0077599795,-0.04534292,0.013948245,0.006675109,0.03393006,0.01750556,-0.002491055,0.07816684,0.017752102,0.005472407,0.005636584,0.017678246,0.0578457,0.035130553,0.02822512,0.02333815,0.0051485985,0.017349897,-0.032282535,-0.007276333,0.013923785,0.023547484,-0.033341516,-0.025691852,-0.04227471,-0.03980187,-0.081192374,-0.007633095,0.03054663,-0.049339976,-0.0060341875,0.025384117,-0.051429566,-0.030112911,-0.071462095,0.036860548,-0.037217893,-0.010384129,0.018718025,-0.044124126,0.020827172,-0.066606306,0.052354123,-0.012585953,-0.0051341085,0.0019270821,-0.069270484,-0.04145069,-0.001510144,0.03786512,-0.011800826,0.072957344,-0.0034611255,0.011999352,0.02733429,0.035139326,0.10755483,1.4776558e-05,0.04955955,0.018063847,-0.07033131,0.087867245,0.049926046,-0.018415377,-0.0017644695,0.0006439617,0.007949909,0.05138487,0.03411095,0.0025968275,-0.068893164,0.06401004,-0.017601253,-0.043257535,0.004875514,0.054519717,-0.040532548,0.033361472,-0.030650849,0.0050061145,0.02443927,-0.04501356,-0.027182586,-0.014987383,-0.04009654,-0.004086262,-0.03573769,0.022445317,-0.004096978,0.058125276,0.13931713,0.0058071557,0.02753344,-0.050132193,-0.043784972,0.023682347,0.018039532,-0.027416801,0.048385013,-0.066760056,-0.028077263,-0.036967024,-0.015846206,-0.043254144,0.013104255,-0.0074377963,-0.022385664,0.059207387,-0.039511554,-0.012086323,0.03618644,0.026511407,0.07950835,0.03287139,-0.031516798,-0.043260884,-0.0075316154,0.05756461,-0.013108047,-0.058107883,0.01641868,0.007943889,-0.01326019,0.074973576,-0.013025559,-0.014833112,0.02768174,-0.06473542,-0.009357858,-0.007034409,-0.007414751,-0.02712445,0.047316067,-0.064076915,0.02639613,-0.0057425047,0.07028465,0.037891794,-0.023429232,0.070252456,-0.018668834,-0.043242004,-0.046213396,-0.007368718,-0.008892761,0.016203009,0.003783649,-0.013828827,-0.0107475845,0.015198427,0.08098829,0.017752865,0.14332889,0.009962697,-0.02027826,0.025285253,0.0046711294,-0.024006095,0.00041112996,0.036411427,-0.0557409,0.032245453,-0.012423571,0.02429238,-0.0413087,-0.021035168,0.019046737,-0.015668577,0.03767188,0.01450852,-0.0005786957,-0.035931885,0.012628815,0.0060909633,0.038379773,0.00068156084,-0.054609064,0.009074518,-0.005478577,-0.016352767,0.0065763383,0.019504052,0.056389946,0.01246244,-0.020385075,0.021190707,-0.07029038,0.04704117,0.041636482,-0.0134316515,-0.029731464,0.022776535,-0.0011597866,0.010598517,-0.035663694,-0.012789692,0.04285253,0.009091698,0.048731316,0.005099207,0.035528522,0.029243644,0.025538795,-0.004014615,-0.027763246,0.043968067,0.024700116,-0.014446247,-0.011996789,0.0127160745,0.030383602,-0.023913145,0.011780707,0.010602752,-0.0179743,0.023644812,-0.004310474,-0.0032409257,0.0400033,-0.08217381,0.02734935,-0.03854436,0.022408802,0.010426607,0.015552173,-0.0030616035,0.05771878,0.06458379,0.075168796,0.00078600337,0.069791615,0.022418134,-0.010713442,0.017412592,0.018141376,0.020541089,0.016422505,-0.021360878,-0.0058206497,0.020430058,-0.013673353,-0.010698806,-0.039631836,0.047557134,0.01596858,0.045377072,-0.043316174,-0.03390563,-0.03708267,0.005122521,0.024592623,-0.0037938985,0.03142739,-0.0013918785,0.03960857,-0.0004913054,0.00924527,0.02592001,0.010290256,-0.006284269,-0.0068608527,-0.002182389,0.031606443,0.013852241,-0.006571094,0.03689729,-0.041831195,0.011495597,0.009725453,0.02703194,-0.02195449,-0.0018549148,0.014112167,-0.03187402,0.022547549,-0.0048324624,0.0075171543,-0.014114454,-0.043389577,0.039816525,0.029576084,-0.04106994,0.004462805,0.013979161,-0.015069187,0.03276543,0.053147767,0.096180774,-0.010799556,0.020854976,0.04779903,0.045547143,0.009644227,-0.065695494,0.042225093,0.024581688,0.036908194,0.04061148,-0.033517808,-0.028894171,-0.025289966,-0.035419174,0.05885018,0.06683068,-0.005570521,0.021235516,0.024376024,0.03726942,-0.018650906,-0.026620984,0.067718945,0.024790578,-0.032884635,0.01632206,-0.061271414,-0.059256412,0.058929406,-0.071823806,0.05866232,0.025508523,-0.012814731,-0.0013596432,0.033749,-0.017625099,0.02638005,0.03923213,0.019746138,-0.008635449,-0.08628932,0.024750626,0.0036918027,-0.035688,0.010397762,0.015672699,0.00097170915,0.025535997,0.055262618,-0.026847519,0.03380545,0.022991346,0.030231556,0.058166467,-0.019806549,0.004498199,0.013591525,0.012880813,-0.021768851,0.06027979,-0.05644502,0.024578627,-0.043386336,0.041251987,0.0017542347,-0.05006947,-0.007445601,0.028334035,-0.015835779,-0.0070666466,0.021779407,0.045354366,-0.026069071,0.0037666399,0.022506643,0.024759283,0.020937132,-0.022801863,-0.07360745,-0.0101972595,0.01236159,-0.037722357,-0.007307049,0.034710504,0.0031040993,0.00027659503,0.11071625,-0.014757944,-0.016742287,-0.004062407,0.017038161,0.0146468785,0.018929493,-0.047057208,-0.03025205,0.041123454,0.054093186,0.039617788,0.006929698,0.045796137,0.03360962,-0.0656686,0.00402781,0.055531517,-0.0031477115,-0.07798696,0.0034226966,0.040058047,0.005275947,0.0046751862,-0.060858544,-0.041252002,-0.028705595,0.0335471,-0.034964245,-0.015061318,-0.0095524145,-0.013138509,0.0039956593,-0.010704367,0.039403673,0.022123337,0.028709846,0.019767739,-0.029414702,-0.046649978,0.021216216,0.0046792105,-0.021995293,-0.00907797,-0.020809477,-0.02328949,0.05591023,0.007691017,-0.015729006,-0.0761213,0.05722376,-0.041596457,0.04906362,0.025301237,-0.04014155,0.015733844,0.0027312795,-0.0100535555,-0.0031465946,0.01768453,-0.0024929277,0.007383224,0.01143007,0.006462195,0.02017095,0.004938667,0.027281744,0.04036691,-0.00059502653,0.048189618,-0.025127167,-0.015234456,0.017201047,0.011509888,0.038835842,-0.033850126,0.032719683,-0.03556948,-0.039810397,-0.012500547,-0.025063148,-0.0030093172,-0.06656682,-0.024810394,0.0057036234,-0.031990726,0.024175694,-0.03248949,0.017818393,0.037492916,0.0029006954,0.008901029,-0.0031071668,-0.031574406,-0.014498321,0.02885421,0.036992747,0.041163772,0.0069484287,-0.00560931,0.011753423,-0.05470283,0.047621775,0.022548025,0.00080448773,-0.002440994,0.009518955,0.039268885,-0.06468499,0.0308795,-0.033858236,0.023366418,-0.043405466,-0.005911208,-0.033349793,0.0447025,0.055647448,-0.012091479,0.0070858896,0.016822863,0.07071103,0.007086799,0.04564713,0.0020079846,-0.014615059,0.029610092,-0.06721399,-0.044564635,0.03920527,0.03904524,0.003405844,0.015381446,-0.04290105,-0.009388327,0.024568094,-0.020758966,0.020148287,0.020061009,-0.0065094787,0.0152587,-0.0009674868,0.06455594,-0.096939,0.01909384,-0.017223042,-0.0036113318,0.025073072,-0.08581075,-0.04193066,0.019808512,-0.052682515,0.024337076,-0.010587625,-0.004561898,-0.049809836,0.0037742548,-0.04879107,0.056873504,-0.028460838,0.05654407,0.034811743,0.009529715,0.051519234,0.026948776,-0.03323708,0.018376838,-0.02685897,-0.00818074,0.009687817,0.04251939,-0.018586973,0.015902879,0.015665265,0.044503305,0.0040608402,-0.0053592627,-0.051408052,0.003341045,0.0075014136,-0.01276267,-0.07387766,-0.014542898,0.0053011444,0.0030502386,0.013552153,-0.045440573,0.029830558,-0.03196876,0.066215955,0.08754133,0.03676648,-0.003601116,0.026191644,-0.0022293616,0.035963837,0.03691278,0.021819053,0.007000861,0.007725654,-0.011341696,-0.00416077,6.0212526e-05,0.023763796,0.01629116,-0.039783172,-0.015220466,-0.018970903,-0.0134770535,-0.05151338,0.08499639,-0.018056316,0.023408547,-0.019254113,-0.065630905,-0.0011564685,-0.02475546,-0.041917857,0.031392664,0.0013178603,-0.0034286084,0.024565252,-0.035657305,0.069575556,-0.07531044,-0.034863696,-0.046824735,-0.0020055796,-0.09829693,-0.035002287,-0.03209105,-0.053056527,-0.013310465,-0.033031687,0.027080951,0.017473962,-0.023811137,0.002551407,-0.052953575,0.020195164,-0.0011261551,-0.019972341,-0.005506452,0.0040931217,0.0017695269,-0.031982623,0.085354954,-0.053254794,-0.03286001,-0.004603713,0.014635016,-0.07241848,-0.014557263,0.04485369,-0.012237365,0.014822374,-0.013304491,0.04588327,0.03469416,-0.05781289,-0.0049350164,-0.084393084,0.018408166,0.057553623,0.019131484,0.0793277,0.031021826,-0.041887674,-0.029469172,-0.029441508,-0.0034794807,0.01680775,0.044856843,0.0008892854,0.05551422,0.034733657,-0.019069104,-0.040815298,-0.027501805,-0.0031766908,0.021747349,-0.020042159,0.066604294,0.034973886,-0.047449566,-0.009124642,-0.048297327,-0.023345308,0.008694959,0.01680576,-0.04069438,-0.022298628,0.004112656,-0.012820056,0.06458672,0.038898062,0.0042588143,0.09648618,0.006885369,-0.018805597,-0.051897343,-0.027140211,-0.03692211,-0.002773328,0.057782434,-0.009883904,-0.008589014,-0.0013271993,0.004227596,0.01844267,-0.01924103,-0.05198589,-0.01679146,0.045727354,0.039152917,0.021001,0.023883881,-0.022035392,-0.104074314,0.049568076,-0.024198342,-0.036981784,0.021529792,0.007960611,0.0019229426,-0.0272386,-0.026502699,-0.007960073,0.018246157,-0.009293769,0.0064985445,0.021411892,-0.004846955,0.016146258,-0.09430127,-0.040113132,-0.011791997,-0.012775895,-0.015715314,0.0392021,0.014800979,0.026376395,-0.061341204,-0.02868982,0.029466687,0.03747197,-0.009434654,0.06404793,0.00012782917,0.05590553,-0.03732372,0.0041766264,0.043509036,-0.038097158,0.00080491183,-0.01459194,0.06549794,0.0067699025,0.0003575264,-0.00882139,-0.043431234,-0.020879377,0.04483842,0.0059749023,0.0010110311,0.022365639,0.008767928,-0.06544202,-0.03019337,-0.013569245,-0.09866861,0.045606274,0.025275137,-0.0150365345,-0.03420494,-0.018882504,-0.052541804,0.07339991,-0.0015631241,0.00834538,0.034033902,-0.011879663]	Keywords: Nesterov accelerated gradient, momentum term, RNNs, gradient descent\nKey Objects: Nesterov Accelerated Gradient, Momentum Term, Gradient\nRefers to Images: ./images/an-overview-of-gradient-descent-optimization-algorithms/image_3.png\nHypothetical Questions:\n- How does NAG's approach of 'looking ahead' differ from standard momentum?\n- Why is the anticipatory nature of NAG beneficial for training neural networks?\n- In what specific types of tasks or architectures has NAG proven to be particularly effective?\n---\nSummary:\nNesterov Accelerated Gradient (NAG) improves upon standard momentum by anticipating future parameter positions, which prevents overshooting and increases responsiveness, ultimately boosting performance in tasks like RNNs.\nOriginal Text:\n<!-- formula-not-decoded -->  \n7 https://en.wikipedia.org/wiki/Newton%27s\\_method\\_in\\_optimization  \n8 Some implementations exchange the signs in the equations.  \nFigure 3: Nesterov update (Source: G. Hinton's lecture 6c)  \n  \nAgain, we set the momentum term  to a value of around 0 . 9 . While Momentum first computes the current gradient (small blue vector in Figure 3) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks [2]. 9\nContextualized Text:\nTo improve upon standard momentum, Nesterov Accelerated Gradient (NAG) approximates the next position of the parameters by computing a rough idea of where the parameters are going.  This anticipatory update, which involves first making a 'big jump' in the direction of the previous accumulated gradient, prevents overshooting and results in increased responsiveness, leading to significant performance improvements, particularly in Recurrent Neural Networks (RNNs).	{"tags": ["optimization", "deep-learning", "RNN"], "doc_id": "51fdd3b8-dee1-4a09-a32b-8bec407f53b6", "summary": "Nesterov Accelerated Gradient (NAG) improves upon standard momentum by anticipating future parameter positions, which prevents overshooting and increases responsiveness, ultimately boosting performance in tasks like RNNs.", "doc_type": "text", "entities": ["RNNs"], "keywords": ["Nesterov accelerated gradient", "momentum term", "RNNs", "gradient descent"], "key_objects": ["Nesterov Accelerated Gradient", "Momentum Term", "Gradient"], "contextual_text": "To improve upon standard momentum, Nesterov Accelerated Gradient (NAG) approximates the next position of the parameters by computing a rough idea of where the parameters are going.  This anticipatory update, which involves first making a 'big jump' in the direction of the previous accumulated gradient, prevents overshooting and results in increased responsiveness, leading to significant performance improvements, particularly in Recurrent Neural Networks (RNNs).", "mentioned_images": ["./images/an-overview-of-gradient-descent-optimization-algorithms/image_3.png"], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.2 Nesterov accelerated gradient"}, "hypothetical_questions": ["How does NAG's approach of 'looking ahead' differ from standard momentum?", "Why is the anticipatory nature of NAG beneficial for training neural networks?", "In what specific types of tasks or architectures has NAG proven to be particularly effective?"]}
66db127d-7238-4a16-98a1-44d30ca9ba7d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01947916,0.038792294,0.02443271,0.043382965,-0.015635643,0.07028133,0.010465963,0.016931148,0.031291906,-0.024521058,-0.004730691,-0.024509173,-0.015624014,0.021562854,-0.046746567,0.010170785,-0.009912726,0.08231461,0.0031821998,-0.0070660766,0.050718512,0.006788066,0.018253587,0.020245178,-0.025682412,0.027773796,0.036766734,0.012061583,0.0027657752,0.00073840516,-0.01300889,-0.018767437,0.002888306,0.036130443,0.033906203,-0.0060140165,-0.040193968,-0.016690377,-0.021077555,-0.047947366,-0.05193011,0.035048578,-0.030270671,-0.01872938,0.008084693,-0.066307865,-0.049443994,-0.032755077,-0.020082282,-0.031022776,-0.045166038,0.021514136,-0.0415654,0.039707825,-0.09193116,0.02126221,0.0092749465,-0.0018038177,0.0008139855,-0.046528704,-0.032483228,0.0075198254,0.03716332,-0.037071284,0.065860584,-0.014868792,-0.011527582,0.038160786,0.056946173,0.1299458,-0.0011605598,0.057636313,0.0025047755,-0.14456145,0.11784702,0.05244013,-0.03401938,-0.04883469,-0.0060628266,-0.005209537,0.027867699,0.049773812,-0.0620665,-0.036861435,0.07730143,-0.0040616626,-0.034054533,-0.00373685,0.05379809,-0.060593534,-0.04409974,-0.050221454,-0.002744391,0.046813983,-0.031324863,-0.029134033,-0.011508083,-0.005518353,0.011265404,-0.005943206,0.050645307,-0.02913899,0.050124697,0.11142711,0.017501095,0.012268938,0.008121195,-0.01169329,0.00026451572,0.028241763,0.0059935576,0.02137123,-0.09313496,-0.022700386,-0.08004155,-0.0076495637,-0.018931992,0.022039969,0.018183116,-0.018814715,0.028911391,-0.04198876,-0.0046959147,0.049202994,0.00589067,0.057301156,-0.006408295,-0.04025606,-0.051773686,0.015469042,0.022464324,-0.023633808,-0.019233165,0.019345853,-0.015555188,-0.020461576,0.013258639,-0.0060804016,0.007421051,0.024768708,-0.05559757,0.014420366,-0.0009156138,0.0061460636,0.005541401,0.09368909,-0.06290845,0.023583919,-0.012778388,0.061552394,0.039311327,-0.035394963,0.09565466,-0.026368214,-0.031144623,-0.04458193,-0.024141723,0.04956181,-0.0046833213,0.0010573583,-0.04378752,0.007098778,0.019541306,0.12272737,0.045001533,0.12350857,-0.013720163,0.037119314,-0.003485744,0.025819022,-0.016751308,-0.044340037,-0.0044037523,-0.029907027,-0.0012050697,-0.032248728,0.009637611,-0.03796935,-0.018993529,-0.024779163,0.009697015,0.032552537,-0.041940425,0.022935567,-0.017830716,0.04422812,0.06914876,0.014529937,0.022919772,-0.05006025,0.01410968,0.017582754,-0.027095158,0.0045482405,0.06294245,0.026363144,0.046692394,-0.03886906,0.0053601284,-0.060263243,-0.01291239,0.053664215,-0.038206324,-0.056713194,0.056175265,-0.031250242,0.046935167,-0.02838456,-0.057831433,0.014637248,-0.050519776,-0.013975167,0.0031390758,0.049214903,0.024658082,0.010835452,-0.006188884,-0.009203701,0.058416437,0.00705002,-0.038265977,0.010603957,0.00794342,0.05845279,-0.04051195,0.007137766,0.014287558,0.0012587946,0.029672706,-0.01910885,-0.0039970484,0.029503932,-0.048874315,0.039898682,-0.004694204,0.044149544,-0.0054679397,0.024680402,-0.0036050007,0.069506906,0.031880125,0.016849902,-0.0090895025,0.07173083,-0.014236958,-0.002521619,0.030195892,0.03879965,-0.010831877,0.021049801,-0.022596741,0.0024651247,0.021544358,-0.04896479,0.011032831,0.014915356,0.031228492,0.016380614,0.035902947,-0.052227486,-0.03140853,-0.011421459,-0.03802607,0.0049460437,-0.019437844,0.039025553,0.03161764,0.02043385,-0.050171144,0.0440729,0.054187544,0.033574488,0.0037990084,-0.018235933,-0.021361822,0.020150656,-0.03953134,-0.01431707,0.049949832,-0.011411337,0.042592634,-0.0150737,0.021554176,0.020680616,-0.009906316,0.010582546,0.023218747,0.0601258,-0.006766433,0.010797167,0.018182151,-0.073381804,0.061552934,0.0021337757,-0.027962474,0.0020034157,0.012246613,-0.056825794,0.0060756835,0.04565657,0.047038224,-0.017121464,0.04532191,0.005226165,0.013956003,0.017699406,-0.049033076,0.050894253,0.004890315,0.045591652,0.037140783,-0.019922784,-0.04794076,-0.0018222175,-0.0018589076,0.030409172,0.062374584,-0.013230664,0.022776872,0.02312395,0.0057947407,-0.0068583097,0.010778369,0.034668203,0.008369046,-0.018697724,-0.0076038768,-0.026615018,-0.07888851,0.0652719,-0.059920657,0.006126153,0.047408044,-0.0175049,0.0076405667,0.031828176,-0.030548532,0.03843434,0.007966786,-0.030662945,-0.026905477,-0.056093693,0.017054908,0.0012917663,0.0045047556,0.019525997,-0.008087893,0.0048797526,-0.008337851,0.032570146,-0.007954338,0.037486933,0.00033773045,0.012201813,0.04164121,0.012383533,-0.05113666,0.043121327,0.05055138,-0.064342946,0.019084342,-0.0480321,0.037573554,-0.08573019,0.08298486,-0.02632177,-0.034531485,-0.013678851,0.012146167,-0.011338026,-0.0023568675,-0.014932889,0.028208824,-0.069426216,0.036274176,0.05856893,0.06355747,0.03158991,-0.012289492,-0.06338481,0.021207744,0.0662771,-0.0034154663,-0.02918426,0.044070527,0.02210717,0.005523466,0.08384503,0.025549537,0.03488178,-0.02294936,0.002677936,0.028095277,0.023717413,-0.036477428,-0.03957703,0.03094064,0.06263872,0.006927828,0.016589634,0.0030826947,0.037866786,-0.056942347,0.024423616,0.009967304,0.015980287,-0.02761066,0.006621214,0.031793945,-0.029111674,0.030637758,-0.05009969,-0.04971023,-0.0020831667,0.029603818,-0.02541047,-0.0047563217,-0.018557869,-0.0195997,-0.017313102,-0.016222993,0.058295757,0.019864073,0.046450946,0.011476267,0.014656429,-0.010456515,0.052562162,0.006903691,0.007253307,0.0046109324,-0.009352431,-0.019433595,0.025596274,0.035001606,-0.0146789765,-0.031376105,0.076638505,0.005122258,0.04996765,0.008335645,-0.043540224,0.01646832,0.0076814787,-0.021439012,-0.00720898,0.025104295,-0.040265005,0.014213349,-0.0038787303,0.04196825,-0.004746909,-0.028386561,0.014256528,0.07363201,0.016142506,-0.0064386465,-0.031797208,-0.048643053,0.03138052,0.0042333137,0.063919105,0.019395625,0.012169965,-0.049704954,-0.049946606,-0.055672824,-0.06641041,0.0047139307,-0.02997685,-0.047822073,0.037537284,-0.04835854,-0.00047725247,-0.0042795944,0.017702652,0.009986003,0.031645313,0.050641056,-0.03398695,-0.030843845,0.01765578,0.044471074,0.0458559,0.029878262,-0.022837488,-0.008623399,-0.0047768205,-0.01173391,0.003660398,-0.011733591,0.019210702,-0.01623325,0.027652089,0.046146277,-0.05733189,0.03702555,-0.02365435,0.02051631,-0.030380813,-0.0037057765,-0.01896947,0.07567523,0.081536904,0.026963865,0.038514387,0.05560283,0.063670106,-0.0021325254,0.0071276245,0.007644253,0.020494934,0.034566864,-0.04163796,0.0076085813,-0.0018391113,0.021469014,-0.0076477607,-0.0002552601,-0.04390975,0.044275157,0.043715924,-0.016831528,0.022244414,0.013296128,0.0005216661,-0.029276762,0.0016544858,0.08408711,-0.062017825,-0.03467168,0.009946235,-0.031910166,0.043761756,-0.032195345,-0.060199663,0.0066369274,-0.038588177,0.031205872,-0.029500479,-0.0040354645,-0.029427072,-0.014479421,0.0147400405,0.048185118,-0.051052906,0.048646227,0.039194107,-0.0036204164,0.08407906,0.024698632,-0.006663556,0.0051712454,-0.043244243,0.010919853,0.04527458,-0.004945877,0.0050978158,0.054250687,0.038703907,-0.00027827796,-0.017141659,-0.01693694,-0.04040364,9.0268964e-05,0.028066436,-0.0029584023,-0.031845227,-0.052227613,0.0128027955,0.0021773244,0.02140731,0.0029924456,0.017615907,-0.015206064,0.035789676,0.033411637,0.005143753,0.031040741,0.03449964,-0.02849288,0.015377095,0.06738309,0.017802475,-0.0010832986,0.048862606,-0.009226022,0.004679877,0.014440628,-0.025560211,0.026652548,-0.031886436,-0.017447779,-0.028617332,0.014050281,-0.016718768,0.10226763,-0.0060737333,0.018999644,-0.030681914,-0.056765113,-0.0018906858,-0.05783832,-0.031212648,0.048171643,0.020218257,-0.008394507,0.008847557,-0.03538758,0.0421777,-0.06877222,-0.009067493,-0.043362744,-0.02821322,-0.072464265,-0.041689143,0.025276706,-0.052798133,-0.016139107,-0.024269653,0.043675475,-0.003041938,-0.055556715,-0.024772866,-0.042647384,-0.0032813747,-0.02978676,-0.037250783,-0.025170738,0.005565229,-0.0047554467,-0.0007519539,0.048970643,-0.019772828,-0.035328668,0.030132478,-0.029038662,-0.057518896,-0.049007215,0.0060215606,-0.028522566,0.021215482,-0.014924882,0.021504844,0.0099480115,-0.038196348,-0.020615825,-0.06395419,0.04015157,-0.0007058031,0.0468878,0.07249398,-0.018888393,0.0022637914,-0.016800268,-0.038216624,0.02197355,0.0543939,0.031520586,-0.010887072,0.0720258,0.046519894,0.023563055,-0.07655567,-0.06504368,-0.0012967688,-0.00078648253,-0.00014145643,0.09348905,0.054908693,-0.031242073,-0.011787411,-0.032942943,-0.014868889,-0.01011063,0.033502206,-0.057058863,0.022678195,-0.000843601,-0.025099467,0.049582962,-0.02773047,-0.0002545792,0.06414752,-0.015354957,-0.020411458,-0.037923966,-0.011740751,-0.012644493,-0.016526656,-0.0027770745,-0.021921413,0.0015892364,0.014511041,0.019508444,0.023369983,-0.016982844,-0.049459543,-0.037827063,-0.019087065,0.03038537,0.026637906,0.019984152,-0.059962187,-0.09031668,0.07094665,0.027624184,-0.014406982,-0.018930376,0.005792449,-0.011107015,-0.021613155,0.003375543,-0.0107943425,0.030770335,-0.016345352,0.01826749,-0.013367526,-0.025743105,0.05375077,-0.047845192,-0.03495754,0.005066127,-0.00807584,0.01479135,0.009109781,0.030395344,-0.0070115305,-0.036428723,-0.007239599,0.038155816,0.01924043,-0.021142114,0.04567977,0.0060728025,0.015045157,-0.0095915375,0.008392038,0.03537478,-0.04414769,-0.016774025,-0.020818487,0.029737791,-0.009904041,-0.028497735,0.012365641,-0.04390095,-0.014432177,0.024233961,-0.0034717377,0.018267533,0.024639077,0.008338748,-0.033997275,-0.009459653,-0.030817388,-0.04266881,0.020101488,0.035485607,-0.026421165,-0.059642732,-0.044814244,-0.04900629,0.035766374,-0.014935122,0.012976567,-0.018387303,-0.021667128]	Keywords: Adadelta, Adagrad, learning rate, optimization, squared gradients\nKey Objects: Adadelta, Adagrad, learning rate, past gradients\nRefers to Images: None\nHypothetical Questions:\n- How does Adadelta differ from Adagrad in terms of how it handles past gradients?\n- Why is it problematic for an optimization algorithm to have a monotonically decreasing learning rate?\n- What is the purpose of limiting the window of accumulated past gradients in Adadelta?\n---\nSummary:\nAdadelta is an optimization algorithm that builds upon Adagrad by addressing its issue of a rapidly decreasing learning rate by restricting the window of accumulated past gradients.\nOriginal Text:\n### 4.4 Adadelta  \nAdadelta [22] is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size w .  \nInstead of inefficiently storing w previous squared gradients, the sum of gradients is recursively defined as a decaying average of all past squared gradients. The running average E [ g 2 ] t at time step t then depends (as a fraction  similarly to the Momentum term) only on the previous average and the current gradient:  \n<!-- formula-not-decoded -->  \nWe set  to a similar value as the momentum term, around 0 . 9 . For clarity, we now rewrite our vanilla SGD update in terms of the parameter update vector   t :  \n<!-- formula-not-decoded -->  \nThe parameter update vector of Adagrad that we derived previously thus takes the form:  \n<!-- formula-not-decoded -->\nContextualized Text:\nAs an extension of the Adagrad optimization algorithm, Adadelta aims to mitigate Adagrad's tendency to have a monotonically decreasing learning rate. This is achieved by limiting the window of accumulated past gradients to a fixed size, rather than accumulating all past squared gradients.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "66db127d-7238-4a16-98a1-44d30ca9ba7d", "summary": "Adadelta is an optimization algorithm that builds upon Adagrad by addressing its issue of a rapidly decreasing learning rate by restricting the window of accumulated past gradients.", "doc_type": "text", "entities": [], "keywords": ["Adadelta", "Adagrad", "learning rate", "optimization", "squared gradients"], "key_objects": ["Adadelta", "Adagrad", "learning rate", "past gradients"], "contextual_text": "As an extension of the Adagrad optimization algorithm, Adadelta aims to mitigate Adagrad's tendency to have a monotonically decreasing learning rate. This is achieved by limiting the window of accumulated past gradients to a fixed size, rather than accumulating all past squared gradients.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.4 Adadelta"}, "hypothetical_questions": ["How does Adadelta differ from Adagrad in terms of how it handles past gradients?", "Why is it problematic for an optimization algorithm to have a monotonically decreasing learning rate?", "What is the purpose of limiting the window of accumulated past gradients in Adadelta?"]}
3c23d925-3491-441d-8205-d0662512a989	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.063221656,0.011866265,-0.0006671899,0.06713486,-0.054831836,0.013635975,-0.011100954,0.0060712504,0.06412579,-0.00993145,0.013729393,-0.022539455,-0.0033056443,0.012049949,0.021461563,0.08865317,0.065394066,0.036374025,-0.029166644,-0.043865282,0.03811591,0.029645963,0.0046425336,-0.03590531,0.04133781,0.026101917,-0.00068338285,0.088151954,-0.033266302,0.047483772,0.042309724,-0.05009294,-0.010205882,0.01929805,0.022108557,0.025624277,-0.035170052,-0.06560846,-0.0078081624,-0.03746493,-0.05077472,0.024022292,0.0016380828,0.067406036,-0.017113518,0.014628233,-0.06330601,0.01761112,-0.047263563,0.013655381,-0.031594682,0.039107453,-0.06272445,0.030200921,-0.061245956,-0.031480644,-0.034999385,-0.025842289,-0.068566635,-0.058803428,0.014418328,-0.037390724,-0.02229187,-0.02016594,0.025024066,-0.036063146,-0.027006,0.0651821,0.033584118,0.18168184,-0.04535574,0.030949108,-0.001124721,-0.0739695,0.074688755,-0.0656374,0.035323255,-0.0067048236,0.002530283,-0.018667966,0.09295196,0.009066642,-0.038296025,0.03604705,0.0620593,-0.017534394,-0.023973122,-0.018792802,0.040852603,-0.07841645,-0.009364582,0.004528316,0.009219434,0.054561637,-0.038474154,-0.017559033,-0.008643027,-0.016198672,-0.010487642,-0.014025127,0.024149476,-0.03888842,0.02526922,0.09440072,0.043776814,0.042734694,-0.030167915,-0.011053243,0.0040580984,-0.006012695,-0.019448165,-0.015280385,-0.024103487,0.055584773,0.00846631,0.048950065,-0.044053357,-0.021441352,0.02768048,-0.038562648,0.050318226,0.004898317,0.022004541,-0.018738762,0.044500727,0.017300386,-0.02928199,-0.052036945,-0.009991997,-0.011961443,0.07850488,0.014043754,-0.066850424,-0.0019305251,-0.019953696,0.040887922,0.025449745,0.012292022,-0.009049706,-0.023879196,-0.0033235822,-0.08135725,-0.064545475,0.022977185,0.002640363,0.01281299,-0.040314175,-0.032181147,0.011045679,0.07794174,0.0201358,0.02648175,0.036915295,-0.01972512,-0.0021452666,0.051270317,-0.022250751,0.015630303,0.008192067,-0.02469425,-0.0960968,-0.037571926,0.014634677,0.04374395,0.079680994,0.03418473,-0.010891856,0.07312262,-0.05078769,0.007331909,-0.017165283,-0.04427816,-0.013312555,-0.0192774,-0.020288495,-0.003485325,-0.046289153,-0.020616872,0.013023947,0.03790589,0.010265652,0.040274028,0.0132398,0.030184599,-0.0327081,0.030391304,0.00043020674,0.0052817753,-0.006553318,0.0007415597,0.012636472,-0.016914226,-0.0668829,0.000587193,0.006304314,0.040812027,0.044596296,0.0030752637,0.0099486355,0.03689118,-0.05512152,-0.022763716,-0.08769761,0.02361881,-0.03121495,-0.017146038,0.0039000346,0.030790826,0.024421988,-0.0010954977,-0.006272883,0.038041886,-0.017702442,0.029467827,0.05551988,-0.0036020349,-0.041345343,0.029408023,0.02116912,-0.03270119,-0.008426315,0.014913884,-0.016609764,-0.023085391,-0.0062669264,-0.002767376,-0.06519653,0.01262875,0.0066924053,-0.015439011,0.047018506,-0.003277007,0.017352546,0.0035261086,-0.017031873,0.0017396875,0.024162829,0.03589626,0.016830368,0.047687702,-0.005797051,0.058452673,0.011174856,0.06645236,0.035078507,-0.011452295,0.05823289,0.009541144,0.024136892,-0.009102587,0.002541655,0.0395499,0.037335068,-0.03475499,0.028775698,0.05656298,-0.010998537,0.025188027,0.05359816,-0.057627082,0.030906666,-0.015078207,-0.033367366,-0.009614435,0.010406629,-0.0068286425,0.045579284,0.01611352,-0.0510682,0.016191028,0.03582533,0.012352018,-0.010961414,0.025782304,-0.0035642076,-0.03510287,-0.041490156,0.031855483,-0.018241836,-0.011250587,-0.023442034,-0.014157139,0.016469806,-0.005555745,-0.0131619405,0.027346497,0.012764012,-0.027418291,-0.0651251,-0.06848147,-0.0018302915,-0.029069316,-0.012560876,0.029118458,0.006771057,0.00118947,-0.002709481,-0.02116393,-0.015680801,0.0344375,0.0427494,-0.06113499,0.010023032,0.031703033,-0.0010252601,-0.0055927597,-0.015024447,-0.009260742,0.007223672,-0.002901544,-0.012035975,0.020608334,-0.0019816724,-0.007956266,0.07710356,0.12040422,0.03516819,0.03159315,-0.029746024,0.055041123,0.03044772,-0.029633798,-0.06683214,0.030685792,0.057042073,-0.0043202452,0.05176746,-0.076047614,-0.044044986,0.060418714,0.017055746,0.04600255,0.048664328,-0.019689517,0.020133613,0.013211064,0.0037843871,0.003229403,-0.018716156,-0.029704547,-0.0812186,-0.041425224,-0.008387094,-0.013517504,0.033651937,0.037284765,-0.011600242,0.010133585,-0.002970878,0.01659083,0.012992423,0.027897986,-0.012070634,-0.0030972045,-0.01171406,-0.05294244,0.0170204,0.05051034,0.063561626,-0.0644765,-0.020152837,-0.02990338,0.020029772,0.015145318,-0.010984901,-0.060959753,-0.005914687,-0.01897558,0.007686636,0.01684825,0.015059399,0.018819425,0.011151927,-0.008505585,-0.0117586665,0.07130195,0.021540577,0.0023894485,0.025972143,-0.010075415,-0.032412294,-0.00034051118,0.00030405656,0.015427822,0.008939815,-0.010905832,-0.025983347,0.026442736,-0.056556717,0.057746157,-0.020536816,0.002509166,0.004026594,0.012132745,-0.014454176,-0.055243008,-0.009797007,0.030598747,0.026897825,-0.04951349,0.014040684,0.055277225,-0.036044434,-0.004915202,0.028406715,-0.01055531,0.009635815,-0.01874141,0.021715583,-0.00507902,0.038595796,-0.033037327,0.019770358,-0.0049481746,0.018844541,-0.021197869,-0.022852508,0.014967258,-0.042257044,-0.053816486,-0.026789045,0.012249297,0.053476144,-0.014386522,-0.0020097464,0.028687023,0.017659009,0.06544752,0.012539522,-0.10016707,0.044748146,-0.03583768,-0.031220127,0.020671656,-0.03003866,-0.048859194,-0.030051796,0.04663661,0.012207985,0.0622411,0.017751336,-0.02748157,0.056570802,-0.042735834,-0.011120357,-0.011647308,-0.02118921,0.0055845585,0.020642532,-0.043930594,0.040648162,0.014172219,0.03601922,-0.03369836,0.0028437783,0.044453945,-0.0054888134,-0.035704646,0.0054997434,0.00672573,0.033388395,-0.011311435,-0.032855015,-0.0025894318,0.010897356,-0.033711363,-0.012793142,-0.043915775,-0.0037991414,0.04868393,-0.054009166,0.035060175,0.0078064394,-0.008094681,-0.06295562,0.025051123,0.037158888,-0.020087706,0.035515405,0.0046737217,-0.011910491,-0.015475146,0.05312866,-0.02651113,0.006039356,-0.043574147,0.0075837364,-0.0019646133,-0.0010556448,0.010050964,-0.0046153073,-0.037694722,-0.008638539,0.070623145,0.00455553,-0.042868126,-0.03896741,0.014521507,-0.0043630153,0.041497692,0.019162562,-0.092108965,-0.010517835,0.031244403,-0.017663313,0.066268384,-0.020223334,0.002270762,-0.03880217,0.04160998,-0.04871146,-0.029378789,-0.011954253,0.011424273,-0.08020988,-0.021334162,0.020273078,-0.028302878,0.038067967,0.028191354,0.07789908,-0.0364245,0.015613613,0.054199044,0.03230791,0.012584154,-0.05143294,-0.015267741,0.010591003,-0.048577953,0.041761298,0.014060872,-0.0515091,-0.01765478,-0.02433251,-0.060241424,0.011944719,-0.039274603,-0.014054726,0.025028523,-0.029488502,-0.025400853,-0.059046313,-0.0020437536,0.068837605,-0.0049876273,0.02136972,-0.018835818,0.008951534,-0.044207722,0.018177388,0.025516491,0.017610442,-0.031894263,-0.0066503924,0.0013084218,0.020261444,0.04738261,0.035100177,-0.017525459,0.045773614,0.0014583773,0.046568464,-0.058660857,-0.01391429,-0.03649395,-0.07612352,-0.071664214,-0.039339572,0.044987597,-0.014566934,-0.0010907276,-0.012944468,-0.020099439,-0.035905372,0.09348695,0.069104105,-0.018683597,-0.07333036,0.010859614,0.024466602,0.008180263,0.0039811386,0.05829529,0.008424915,0.04696072,-0.00083338947,-0.0027324616,-0.006482596,0.053636968,0.0374061,-0.07921594,-0.021093663,-0.016258264,0.017525481,-0.03527526,0.0065716254,-0.019225286,0.0061734566,-0.06461362,-0.047920696,-0.03306378,-0.0031973026,-0.029996226,-0.0076853354,-0.008027066,-0.010777709,0.03249946,-0.06471681,0.05348874,-0.007716968,-0.01211314,0.015670504,0.057665363,-0.00032111056,-0.04730834,-0.0009963004,-0.011625939,-0.038516473,-0.024619928,0.035726078,0.0029232781,0.05190907,-0.028084753,0.0010270379,-0.011262448,0.035989273,-0.043763556,-0.01970233,0.0369877,0.013709028,-0.036867123,0.04642728,0.062388085,-0.029533133,0.0050616483,-0.0013455522,-0.06176931,0.016808586,-0.0120726,0.033455804,0.05537607,0.009438708,-0.0048866263,0.013210817,-0.09272646,0.029987037,0.009022938,0.04589619,0.0558822,0.036163487,0.0841698,-0.056541122,-0.06910219,-0.010482816,-0.041949797,0.00071155844,0.016663464,0.05430923,-0.019640943,0.07127853,0.033722598,-0.0067054057,-0.022271281,-0.038393233,0.0046771434,-0.009335813,-0.01046635,0.037419878,-0.0078022447,-0.07790696,0.0021449917,0.000546612,0.024343247,0.0047717006,0.038038004,-0.0027660402,-0.042787485,0.024827924,0.026008556,0.061963383,-0.057871092,0.026464451,0.022215664,-0.0005055763,0.023294922,-0.043735042,-0.0025548819,-0.015367419,0.039180286,0.014328269,-0.06174372,0.03672472,-0.030915204,-0.007717398,-0.017448846,0.04486425,-0.01808034,-0.052601166,0.05163805,-0.029356362,0.040863585,0.0149172805,-0.038088232,0.025815722,0.029285701,-0.009985026,-0.028565606,0.0048261834,0.021753483,-0.017745165,-0.016591223,-0.025141012,0.014157584,0.018627143,0.016405255,0.027161065,-0.0059117093,-0.00037716876,0.009840023,-0.0632541,-0.031632934,0.05210803,0.0006383227,0.035861,0.0035117797,0.036801912,-0.039610557,0.066496745,-0.07039121,0.043201473,-0.035527084,0.042476535,-0.0135830175,0.017590184,0.010161012,-0.010992485,-0.011352639,0.012184568,-0.039233066,-0.023807172,-0.042623505,0.0017194551,-0.015237991,-0.00069701066,-0.011815513,-0.058082122,-0.03297577,0.0027608606,-0.046929386,-0.006911176,-0.006707993,-0.00025746992,-0.009878804,-0.02100691,-0.08363957,0.021468608,-0.013599458,0.0437012,-0.041570045,-0.009380513,0.024360675,0.017040607,0.016777342,-0.04337145,0.01057781,0.0084727025,0.072994344]	Image title: Three Intersecting Vectors\nTags: vector, direction, geometry, line, intersection\nKey objects: Blue Vector, Brown Vector, Green Vector, Intersection Point\n---\nSummary:\nThis diagram depicts three vectors represented by lines, each with a distinct color: blue, brown, and green. The vectors intersect at a common point, illustrating their relative directions and how they cross paths.\nFull description:\nThe image shows three vectors meeting at a common intersection point. The first vector is colored blue and extends upwards and to the right. The second vector is brown and points downwards and to the right. The third vector is green and extends horizontally to the right. All three vectors meet at a single point.\nText found in image:\n- 	{"tags": ["vector", "direction", "geometry", "line", "intersection"], "title": "Three Intersecting Vectors", "doc_id": "3c23d925-3491-441d-8205-d0662512a989", "source": "./images/an-overview-of-gradient-descent-optimization-algorithms/image_3.png", "summary": "This diagram depicts three vectors represented by lines, each with a distinct color: blue, brown, and green. The vectors intersect at a common point, illustrating their relative directions and how they cross paths.", "doc_type": "image", "key_objects": ["Blue Vector", "Brown Vector", "Green Vector", "Intersection Point"], "parent_doc_id": "51fdd3b8-dee1-4a09-a32b-8bec407f53b6", "text_in_image": [], "contextual_description": "The image shows three vectors meeting at a common intersection point. The first vector is colored blue and extends upwards and to the right. The second vector is brown and points downwards and to the right. The third vector is green and extends horizontally to the right. All three vectors meet at a single point."}
9cccbd90-e2c4-4551-9162-7805cb432f43	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.024865149,0.011110941,0.00054462417,0.052714605,-0.056533396,0.06616408,-0.021609444,0.015918031,0.051615506,-0.03300053,-0.009507777,-0.01693837,-0.00019890626,-0.0034807872,-0.0036596602,-0.04459623,-0.012764958,0.10958097,0.035185747,-0.015166824,0.05539473,0.03018949,0.04394205,0.017375097,-0.013791318,0.020523507,0.02607918,-0.012072773,0.018087454,-0.02720304,-0.017993582,-0.0025696633,-0.009557094,0.030902365,-0.0010021344,-0.019414267,-0.016095942,-0.06002344,-0.055478122,-0.039018463,-0.0602188,0.042221732,-0.024929201,-0.00087075774,0.010620734,-0.04571677,-0.03342134,-0.03293269,0.0545783,-0.033746913,-0.04657454,0.051438477,-0.024871208,0.037924223,-0.0641273,0.038823873,-0.009264529,-0.0070464285,0.0141552575,-0.030680615,-0.058673616,-0.0093424665,0.031050894,0.0012421015,0.05589427,-0.0142990565,-0.002484289,-0.00042892233,0.038685977,0.13647473,-0.015073464,0.017017039,-0.023448937,-0.10019481,0.13377209,0.017528243,-0.06593845,-0.04336324,-0.041111764,-0.0042587887,-0.025978047,0.020879328,-0.078045174,-0.043942783,0.08665052,0.024303854,-0.05863259,-0.027367916,0.057150643,-0.06812145,-0.035024315,-0.029356169,0.0016133367,0.03689628,-0.020509375,0.0059270724,0.0037497284,-0.027147327,0.033941478,0.018595235,0.032580875,-0.0029849166,0.0316345,0.09537995,-0.028280977,-0.025385771,-0.014295472,-0.02915204,0.006470759,-0.023556342,0.042290233,0.026191223,-0.033524904,-0.045000535,-0.037114717,-0.037206765,0.0028643766,0.029149935,0.010020744,-0.011070815,0.021544756,0.0053862426,-0.014638793,0.007273228,-0.009697332,0.070075996,-0.01836607,-0.0271035,-0.057235997,-0.020832727,0.037752185,-0.01789526,0.012391566,0.015612805,-0.026302474,-0.015288077,0.018928276,0.00676666,0.005545875,0.018272663,-0.056213643,-0.036889695,-0.022454122,0.019591907,0.007129983,0.018438587,-0.035210323,0.015319501,-0.005308394,0.041753184,0.03060866,-0.051379647,0.055265512,-0.035735972,-0.05880412,-0.04838008,-0.027526343,-0.007003156,-0.013344404,-0.044706095,-0.009294492,0.0224576,-0.019689443,0.058077395,0.08756246,0.13373113,0.0007215217,0.036214333,-0.04160896,0.019106328,-0.04752217,-0.03540122,0.0074786097,-0.038896855,-0.013239454,-0.034912553,-0.0040843682,-0.0340853,-0.0029887843,0.007414839,9.7753866e-05,0.06296792,-0.023797022,-0.00079854956,-0.0010379088,0.064391024,0.016995246,0.032518245,-0.0069739665,-0.03086377,0.037661456,0.018110074,0.011394752,-0.041493483,0.041142657,0.043765083,0.058288436,-0.00911271,-0.040499236,-0.05802847,-0.032440692,0.0166286,-0.05349148,-0.037934776,0.037247103,-0.03973964,0.022088245,-0.04463698,-0.021819327,0.0021436766,-0.06179164,-0.0055631273,0.0039670835,0.047127627,0.008606304,-0.022095524,-0.05173656,0.007450095,0.050507177,-0.061117273,-0.056049816,0.047116887,-0.0015777209,0.03628547,-0.016256439,0.011378993,-0.003174143,-0.032427568,0.06476418,-0.011724178,-0.011862667,0.051810328,-0.023351874,0.06056561,-0.0022738038,-0.012396716,-0.013130203,0.02209265,0.027841901,0.029790992,0.031574283,0.044937912,-0.0013387392,0.08913779,0.000985376,-0.02109893,-0.02396176,0.053857643,0.026045227,0.03481351,-0.058687665,0.014164169,0.0071471278,-0.012386505,0.015651505,-0.0040138615,0.035675537,-0.020177778,0.026380936,-0.07253323,-0.004033622,0.005099002,-0.019598845,0.024836391,0.011409796,0.002408419,-0.019782726,0.005426295,-0.02710748,0.011131584,0.0336164,0.015371171,-0.00031237875,-0.032080818,-0.034083784,0.03382548,-0.0074048857,0.01661227,0.014762518,-0.031958234,-0.0081661,0.027171966,0.023913514,0.03573729,0.034163512,0.023206118,0.030091302,-0.01892101,-0.03967485,-0.007544437,-0.008380659,-0.051190745,0.08344544,0.009432162,-0.023400623,0.024054512,-0.016916797,-0.00044803624,-0.027527168,0.002019794,0.083555706,-0.011774751,0.049903918,0.0003162786,0.002946802,0.03779973,-0.07772748,0.0042602136,-0.030478278,0.044876836,0.02844897,-0.010614395,0.011856497,0.005201337,0.00848461,-0.0018755571,0.06724045,0.021526465,0.029122831,0.030681862,0.0430928,-0.0013734903,-0.008829696,0.040799346,0.04600462,0.0014983516,0.07556878,0.003222521,-0.060100727,0.05174968,-0.033051044,0.0127101485,0.08342257,0.00155492,-0.023436885,-0.016778124,-0.0006662252,0.014554438,0.056719758,-0.0070853047,-0.023893382,-0.051823378,0.00784847,0.025552692,-0.014731013,0.01699158,0.029034903,0.00019268799,-0.025113909,0.024026318,0.015090767,0.021554697,0.003920007,0.037847493,0.026529662,-0.018148193,-0.0027117908,-0.014978466,0.05202735,-0.046908643,0.03361891,-0.05120687,0.004538191,-0.05627917,0.044034373,0.019502642,-0.084019125,0.040749136,0.022226647,-0.02009387,0.019794334,-0.0020646797,0.050850347,-0.046702296,0.02069396,0.045253895,0.044272467,0.025543444,-0.0011475557,-0.052547067,-0.013746886,0.011873508,-0.013116409,-0.018161401,0.050201815,0.055949263,0.015769722,0.07611629,-0.00045900815,0.02115471,-0.018674005,0.016771417,0.036695305,0.04577478,-0.023612436,-0.024653826,0.059829216,0.015722064,0.01629978,0.0077022812,-0.004710307,0.026465962,-0.07515752,0.008977465,-0.005283122,-0.0028296355,-0.04593899,-0.023604553,0.037149243,0.03174596,0.027463011,-0.090926595,-0.03693251,-0.01866856,0.0053748903,0.013569038,-0.04050489,-0.0017701408,-0.022142394,-0.010615205,0.005635674,0.05531758,0.05261027,0.050458767,-0.01793373,0.040751103,-0.014220792,0.037985176,-0.017032247,-0.03748017,-0.014616975,0.005806667,-0.045208536,0.042322244,0.04968574,0.0126800295,-0.01828702,0.084343195,0.006525163,0.00945756,-0.00084945135,-0.012065101,-0.005773346,-0.008021804,0.012948712,0.013548492,-0.004899308,-0.05442407,-0.024989124,0.0032494252,0.024559652,0.01833381,0.004604067,-0.03388395,0.0363954,-0.013025879,-0.048761733,-0.032403804,-0.023908895,-0.00030541985,0.007382578,0.027702218,0.012301516,0.012643058,-0.009297627,-0.00097333966,0.021078859,-0.048926752,-0.021695156,0.009764434,-0.043105464,0.00054283586,-0.047450006,0.024792302,-0.022301381,0.0056161606,0.014182082,0.052763663,0.05369691,0.01630729,-0.052120313,-0.017157609,0.041772056,0.00066467945,0.009956308,-0.089264445,-0.03774103,-0.0037772858,-0.04603942,-0.027069,-0.017870026,0.004849947,-0.008726306,0.028183155,0.0053534796,-0.07838728,0.023452075,-0.006761847,0.015580189,-0.016913973,-0.034045324,-0.005100491,0.04508184,0.07967271,0.007598923,0.027391033,0.008532268,0.029887466,-0.059561998,0.030057834,-0.021367649,0.04080914,-0.0047105844,-0.0141178705,-0.0049300655,-0.00734957,-0.011873011,0.010061373,-0.019438682,-0.059374314,0.026161345,0.0009701443,-0.023960948,-0.027417352,-0.0008219877,0.017876009,0.03525187,0.0102854045,0.0246766,-0.076848984,-0.014092143,-0.03183787,-0.0071902843,-0.0018923531,-0.056000974,-0.06531182,0.032352816,-0.012196068,0.032314885,-0.049530264,-0.005734968,-0.05081095,0.00026517743,0.0062640724,0.12219591,-0.046897754,-0.021882033,0.018335972,-0.0036333904,0.08386954,0.050078582,0.012487976,0.046984475,0.0009788595,-0.038023755,0.047107928,-0.012875062,-0.0067600873,0.04385848,0.04278085,-0.005130889,-0.02928634,-0.023418993,-0.039496366,-0.00048468861,0.011245447,-9.3391856e-05,-0.027751284,-0.03782927,-0.022992155,0.05383983,0.0019416576,0.012468489,0.003528745,-0.016775442,0.10993964,0.036617212,-0.009769805,0.0404059,0.06353216,0.013138945,-0.0029330575,0.09126106,-0.0021101867,0.010311614,0.050845336,0.00477004,-0.0068639196,-0.036658738,-0.019822886,-0.0040856274,-0.0012955787,-0.030043831,-0.028421292,-0.018853106,-0.05096255,0.052360207,-0.01753604,0.06149274,-0.056195002,-0.06775895,0.008412976,-0.02218179,-0.07360848,0.059698306,-0.040678594,0.035749953,0.04870098,-0.017299049,0.08385476,-0.04505315,-0.033759728,-0.0027577768,-0.018184045,-0.041693762,-0.035964925,0.016809242,-0.029718515,0.0022350436,-0.051588856,0.07238264,-0.016169837,-0.018371984,-0.027325561,-0.008703695,-0.0031708316,-0.034632795,-0.039045796,0.004824516,0.029135354,-0.013653798,0.015717758,0.048846368,0.00397639,-0.035065368,0.013105505,-0.003575905,-0.057046875,0.019357251,0.0018338019,0.035810992,0.031071493,-0.01716127,0.032120034,-0.015961641,0.03122157,-0.040469278,-0.020704204,-0.0025411348,-0.07050397,-0.012500474,0.06357296,0.029222084,0.0035799064,-0.01299234,-0.025642617,0.018375408,0.0037293448,0.026757516,0.0071388166,0.088571034,0.03218556,0.023563057,-0.05326058,-0.06617709,0.0103620645,0.034458105,0.026591746,0.054601297,0.018908458,0.005258855,0.002136978,-0.034583032,-0.019263724,0.022973498,-0.016341485,-0.03293677,0.004538106,-0.0046250103,-0.029568445,0.02352001,-0.0038016478,0.013921576,0.067456685,0.019848626,-0.041622747,-0.06527019,-0.013447947,-0.012819186,0.0020360353,0.0058598625,-0.011505879,0.011172961,0.02751619,0.01603172,0.07105052,0.025615783,-0.07512665,-0.021338845,0.065047726,0.03281878,0.017930599,0.042957686,-0.011804808,-0.036656495,0.035887502,0.025867574,0.0016727552,-0.05345357,-0.02272728,-0.04082157,-0.02538472,0.025741799,-0.009371949,0.01124854,0.015710197,0.038183782,0.034159824,-0.05386408,0.05664978,-0.023459867,0.017606452,0.038157612,-0.005222686,-0.037257545,0.032178886,0.01865707,-0.0019918415,-0.026788035,0.037959326,0.07414793,0.02943818,0.015188946,0.06903476,0.06052947,-0.015275745,-0.012295371,-0.024949888,0.08816071,-0.042794947,0.021814702,-0.004223841,0.048550915,-0.0069095353,-0.026890207,-0.050536763,-0.02339961,-0.0025330114,0.028850775,0.018922653,0.03013509,-0.0008805584,0.0016802887,-0.024852997,-0.011594786,-0.021735959,-0.026264574,0.06186812,0.038621876,-0.015191083,0.001965453,-0.03552229,-0.005117636,0.045522556,-0.0420778,0.023095217,-0.01764995,0.013867893]	Keywords: parameter importance, update sizes, optimization\nKey Objects: parameters, updates\nRefers to Images: None\nHypothetical Questions:\n- How would one determine the 'importance' of a parameter?\n- What are the potential benefits of performing larger updates for some parameters and smaller updates for others?\n- How would this adaptation of update sizes impact the overall convergence behavior of the optimization process?\n---\nSummary:\nThe next step in optimization is to adapt update sizes for each individual parameter based on their importance.\nOriginal Text:\nNow that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance.\nContextualized Text:\nHaving adapted our updates to the slope of the error function and accelerated SGD, the next step is to further refine the process by adapting our updates to each individual parameter to perform larger or smaller updates depending on their importance.	{"tags": ["optimization", "SGD", "parameters"], "doc_id": "9cccbd90-e2c4-4551-9162-7805cb432f43", "summary": "The next step in optimization is to adapt update sizes for each individual parameter based on their importance.", "doc_type": "text", "entities": [], "keywords": ["parameter importance", "update sizes", "optimization"], "key_objects": ["parameters", "updates"], "contextual_text": "Having adapted our updates to the slope of the error function and accelerated SGD, the next step is to further refine the process by adapting our updates to each individual parameter to perform larger or smaller updates depending on their importance.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.2 Nesterov accelerated gradient"}, "hypothetical_questions": ["How would one determine the 'importance' of a parameter?", "What are the potential benefits of performing larger updates for some parameters and smaller updates for others?", "How would this adaptation of update sizes impact the overall convergence behavior of the optimization process?"]}
57baff05-8626-4e77-96b5-68752d2a1a4d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017249307,0.004543822,0.012760845,0.043429893,-0.0025110515,0.06418191,0.017516633,-0.0020367596,0.014395631,-0.025416035,0.0050710402,-0.0059873336,0.006054049,0.014235194,-0.0020936064,0.011056538,-0.0039388193,0.093781345,0.023512162,-0.009647111,0.07317498,0.002036387,0.010326355,-0.0037559767,-0.008668852,0.05211618,0.052174635,0.012813235,0.044249307,-0.002384438,-0.009787703,-0.013075954,-0.031964235,-0.025380684,0.026838634,-0.0022045095,-0.02922554,-0.029715959,-0.041379556,-0.070314944,-0.037892945,0.02663542,-0.03228772,-0.023351436,0.02582677,-0.05654878,-0.059950497,-0.04970702,-0.07102456,0.025091996,-0.022387426,0.020598559,-0.03581032,0.030897483,-0.08499313,0.024600945,-0.0056983544,-0.0020480072,0.026677318,-0.041957326,-0.047309995,0.013712306,0.036137685,-0.008169856,0.045349754,-0.0066111395,-0.011309909,0.03804654,0.043606035,0.14501524,0.002610504,0.043113213,-0.0050451853,-0.10098456,0.10416239,0.030026706,-0.040463254,-0.062443383,0.005463183,-0.012064237,0.026382355,0.039770212,-0.030857453,-0.02716095,0.08219975,0.003810161,-0.03151584,0.0099967485,0.08102055,-0.030690322,-0.051778283,-0.023956463,-0.014344347,0.08424734,-0.03551094,-0.030360488,-0.00015303468,0.00628356,0.009304583,0.00068073027,0.028988726,0.0075537805,0.048493166,0.1016558,0.00228356,-0.01566597,0.018281445,0.012375329,0.025425296,0.006789279,0.065798216,0.051411383,-0.04757637,-0.015681589,-0.068293706,-0.023056675,-0.024866078,0.034859255,0.0074693174,-0.0099189645,-0.0100277085,-0.0257121,-0.019969191,0.055880528,0.0035876601,0.069077104,0.011468997,-0.020269657,-0.034832686,-0.006390097,0.04655574,-0.07515711,0.032732442,0.036574982,-0.012457453,-0.026209418,0.012380058,-0.036449593,0.013550526,0.02317319,-0.044695973,-0.0007750054,-0.024657438,-0.022601232,-0.012412436,0.08653899,-0.04286657,-0.0045383037,-0.009230315,0.06265522,0.010065083,-0.056529682,0.046939272,-0.032482587,-0.026310848,-0.032465745,-0.03270429,0.026812136,-0.0017462542,-0.0284933,-0.057161044,0.0136958845,-0.008923363,0.08329519,0.038260482,0.10705418,-0.012487471,-0.008587465,-0.0073185912,0.054083683,-0.0115593,-0.040219575,0.02357419,-0.028534973,0.013113413,-0.016806247,-0.006123574,-0.0402064,-0.04506597,-0.010882318,-0.018113539,0.045563206,-0.06606933,-0.026261544,0.0028312032,0.024486862,0.043843392,0.051774587,-0.021752113,-0.027485369,0.017137367,0.010884266,-0.023697808,0.0062306304,0.05209925,0.015095796,0.030583,-0.0054186485,-0.00590119,-0.05656102,0.016472034,0.054353546,-0.028356088,-0.03381514,0.023199685,-0.040463172,0.020659521,-0.046311047,-0.04049291,0.008096859,-0.032632638,0.016656347,-0.0039010511,0.012375655,0.004621836,-0.006757856,-0.024916654,0.0039586104,0.047922898,0.013042621,-0.06644016,-0.009687787,0.01458894,0.02097032,-0.03556468,-0.006092035,0.015753804,-0.03610778,0.031390045,0.02370599,0.035039686,0.045624804,-0.05105914,0.053401798,0.015441113,0.028337607,0.010318192,0.049991842,0.0064062895,-0.008334277,0.05521974,0.012082148,0.020461997,0.07064296,0.00020062704,-0.010288367,0.015707975,0.0019836526,0.034028724,-0.01682805,-0.013421475,0.008936196,0.07483471,-0.014192707,0.023335438,0.019949108,0.04925544,0.007143748,0.021767156,-0.050866038,-0.025595106,0.023169268,-0.042746942,0.022079237,0.03151707,0.015559545,0.024823707,0.013304134,-0.040959887,0.00929134,0.03266176,0.026427997,0.0169498,-0.017472174,-0.010474989,0.07059591,-0.029547071,-0.03512537,0.033807196,0.0025788294,0.03597238,-0.014320003,0.03373187,0.008636655,-0.01780376,0.015535408,0.0112773115,0.06088603,0.011992153,0.0074850735,0.003365351,-0.10257233,0.06584391,0.017634261,-0.03857998,0.04083726,-0.017471923,-0.05230814,0.030443594,0.019160787,0.06078841,-0.011445516,0.023984643,-0.0034108202,0.006289438,0.0034230596,-0.058492444,0.07283241,0.058553275,0.047495477,0.041482475,0.001460797,-0.013077866,0.022562224,0.020087717,0.03370153,0.08014211,0.027044613,0.026830204,0.04094828,0.038399443,0.026637191,0.020297656,0.07073913,0.028618695,-0.03330116,0.0141616985,-0.03162068,-0.049098995,0.0369147,-0.056058727,0.0147233885,0.0449396,-0.0057597067,0.009163783,0.0138845835,-0.0053022164,0.018636514,0.0177364,0.011266681,0.00015187783,-0.040272746,0.06203362,-0.024951514,-0.027581695,-0.00813243,-0.0007330476,-0.024471,-0.0012098621,0.008966993,-0.004015114,0.022481913,0.013913095,0.0030201883,0.02350987,-0.00698907,-0.006350142,-0.008839591,0.04332191,-0.054154523,0.015853206,-0.023664368,0.060891014,-0.06985664,0.07610712,-0.023282371,-0.037883416,-0.02754471,0.040150467,0.0032798122,-0.0035850701,0.0040133274,0.032751597,-0.072359115,0.01772263,0.063794315,0.061927196,0.060567588,0.008678728,-0.03999073,-0.024210362,0.044652525,-0.0016484525,-0.013152313,0.01674648,-0.004225243,-0.025228996,0.10296177,0.0004694463,0.04936904,-0.04432444,-0.015954427,0.019951887,0.027873166,-0.048673682,-0.029565852,0.03308854,0.048280846,-0.01760296,0.003957012,-0.0018343828,0.021885103,-0.06439744,0.02725647,-0.016931541,0.038444113,-0.018051162,0.02092103,0.040733077,0.008307269,0.04307715,-0.060957134,-0.06208942,0.014297718,-0.01833484,-0.034291483,-0.016956182,-0.02963828,-0.028752297,-0.01787021,-0.034055088,0.071875155,0.0396517,0.04346558,0.019126326,0.012045442,-0.027204944,0.06407385,-0.03522567,-0.014548248,-0.009347158,-0.032569773,-0.022333639,0.025940772,0.026542641,-0.036221243,-0.011472176,0.06546451,-0.02416257,0.054251857,0.048604265,-0.024302835,0.0062684133,0.008437781,-0.02094349,0.020071324,0.022568606,-0.06364422,-0.022907417,0.022415152,0.033009656,-0.013812164,-0.015400768,-0.0057400325,0.037075914,0.024376046,-0.041207127,-0.019182999,-0.0053586527,-0.0042276625,0.022156755,0.047013883,0.035318986,-0.012884496,-0.044802424,0.0051502855,-0.029902576,-0.032649573,-0.009874968,-0.040317096,-0.008673086,-0.011141469,-0.004717381,0.056482404,0.011062319,0.007683994,0.015203491,0.020143496,0.010989741,-0.021002373,-0.017877104,-0.014871506,0.020105561,0.06578139,0.010001602,-0.027339932,-0.032894596,-0.03745769,-0.024693629,-0.020429183,-0.032597233,0.034601312,-0.015633296,0.008419831,0.019025853,-0.057016168,0.021722436,-0.014197782,0.02268376,-0.039054636,-0.026914323,-0.010073494,0.06347018,0.1275282,0.02428973,0.019039242,0.090944186,0.06271044,-0.010940502,0.0028231652,0.028894063,0.017739987,0.031039832,-0.027179025,0.009805742,0.022067364,0.020008164,-0.024962418,0.008942346,-0.02735635,0.0044847773,0.027781153,-0.014397725,-0.009650474,0.048358344,-0.0077283657,-0.0034431526,0.003402483,0.04876607,-0.054078903,-0.03681216,0.008282825,-0.022594053,0.019169915,-0.063033335,-0.040381286,-0.00272214,-0.017950458,0.046540253,-0.017868528,-0.012954248,-0.054349247,-0.025886077,0.00092138676,0.066694096,-0.045767456,0.016814783,0.06321571,0.0020948846,0.072764724,0.015121693,-0.050212063,-0.0009304543,-0.05004199,-0.010576846,0.0123394085,-0.014705067,-0.0031150263,-0.0015541308,0.03197721,0.032164052,-0.001433101,-0.020777658,-0.024693614,-0.01815817,-0.0020319093,-0.049690004,-0.04418503,-0.040479377,0.0058560646,0.019786928,0.012030085,0.008791201,0.006581921,-0.009317728,0.04666658,0.021722114,0.026376212,0.0429358,0.011073201,0.0028969806,0.010024014,0.059045706,0.010654941,-0.010641583,0.036860853,0.014892871,0.0021343464,0.026127625,-0.027139807,-0.017086558,-0.018921692,-0.02667357,-0.038450837,-0.0052603376,-0.016713167,0.055591427,-0.021273104,0.040136404,-0.007883382,-0.09451953,0.03557808,-0.036743097,-0.02850963,0.089762874,0.019070482,-0.004746396,0.056935724,-0.032682877,0.03554069,-0.020304494,0.012998166,-0.022046758,-0.027024385,-0.07842194,-0.065220594,0.016072923,-0.0338185,-0.0043255473,-0.051993906,0.026647758,0.019665292,-0.065785885,0.023525964,-0.027035862,0.017186908,-0.03985869,-0.0071824547,-0.035675813,-0.0049337028,-0.0029128466,-0.01696783,0.053877436,-0.08483231,-0.045820598,0.027051358,-0.015091806,-0.103493,-0.026858645,0.0052721747,-0.0029020756,0.012207115,0.02661378,0.014448366,0.013303451,0.0005782606,-0.061276965,-0.08540744,0.0018157419,-0.0036747486,0.04411584,0.07439214,0.028635047,-0.026037734,0.012072841,-0.02942224,-0.030299822,0.026873576,0.0003014925,-0.008670364,0.05636199,0.040856805,0.011974126,-0.122381955,-0.03464059,-0.011884657,0.022401467,-0.027107764,0.07066356,0.046180338,-0.04356701,-0.0035145474,-0.02843264,-0.010057987,-0.014409916,-0.0060021617,-0.034255873,0.011320044,-0.022214811,-0.026301045,0.07160629,-0.015235179,0.047287118,0.0853584,0.04510873,-0.018679807,-0.08941121,-0.0077856765,0.01363899,-0.0148820765,0.020560075,-0.025065834,0.020668464,0.0038086746,0.031278197,0.012766729,-0.023897644,-0.0516195,-0.03798118,-0.011440222,0.03266496,0.007878407,0.040362336,-0.04407964,-0.040171016,0.08187411,0.007223176,-0.013524768,-0.025601618,0.01056958,-0.0036291284,-0.034953192,0.042017635,-0.0073430683,0.0073399725,0.031328097,0.018937547,0.018145772,0.0077379807,0.01847125,-0.03817976,-0.008899406,0.00435045,-0.02096139,-0.0021106405,0.02446606,0.019404698,0.0018588594,-0.03793228,0.03481128,0.035175923,0.033092204,-0.016801976,0.019819427,0.034276724,0.054803137,-0.03309289,0.023298413,0.06772852,-0.0054939273,-0.003476341,-0.016802127,0.041260336,0.0029368673,-0.010653465,0.0016050049,-0.01626087,-0.016050182,-0.014587996,0.031208584,0.033908073,-0.0009844928,0.028647311,-0.023995016,0.004365896,0.01069189,-0.01357171,0.08791056,0.039169066,-0.0054047285,-0.03715607,-0.021220354,-0.053402588,0.06452959,-0.024078868,-0.030390557,-0.003347299,-0.017074157]	Keywords: Adagrad, gradient-based optimization, learning rate, sparse data, optimization algorithm\nKey Objects: learning rate, parameters, gradients\nRefers to Images: None\nHypothetical Questions:\n- How does Adagrad's adaptive learning rate benefit training with sparse data?\n- What specific tasks were Adagrad successfully applied to, as mentioned in the text?\n- Why is it advantageous for infrequent parameters to receive larger updates during optimization?\n---\nSummary:\nAdagrad is a gradient-based optimization algorithm that adapts the learning rate for each parameter, providing larger updates for infrequent parameters and smaller updates for frequent ones, making it particularly effective for sparse data.\nOriginal Text:\n### 4.3 Adagrad  \nAdagrad [8] is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data. Dean et al. [6] have found that Adagrad greatly improved the robustness of SGD and used it for training large-scale neural nets at Google, which - among other things - learned to recognize cats in Youtube videos 10 . Moreover, Pennington et al. [16] used Adagrad to train GloVe word embeddings, as infrequent words require much larger updates than frequent ones.\nContextualized Text:\nAdagrad is an optimization algorithm designed to adapt the learning rate for each parameter in a model. It is particularly useful when dealing with sparse datasets, as it provides larger updates for infrequent parameters and smaller updates for frequent ones. This was demonstrated by Dean et al. who used Adagrad to train large-scale neural networks at Google, and by Pennington et al. who utilized it to train GloVe word embeddings.	{"tags": ["optimization", "deep-learning", "algorithm", "NLP"], "doc_id": "57baff05-8626-4e77-96b5-68752d2a1a4d", "summary": "Adagrad is a gradient-based optimization algorithm that adapts the learning rate for each parameter, providing larger updates for infrequent parameters and smaller updates for frequent ones, making it particularly effective for sparse data.", "doc_type": "text", "entities": ["Adagrad", "SGD", "GloVe"], "keywords": ["Adagrad", "gradient-based optimization", "learning rate", "sparse data", "optimization algorithm"], "key_objects": ["learning rate", "parameters", "gradients"], "contextual_text": "Adagrad is an optimization algorithm designed to adapt the learning rate for each parameter in a model. It is particularly useful when dealing with sparse datasets, as it provides larger updates for infrequent parameters and smaller updates for frequent ones. This was demonstrated by Dean et al. who used Adagrad to train large-scale neural networks at Google, and by Pennington et al. who utilized it to train GloVe word embeddings.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.3 Adagrad"}, "hypothetical_questions": ["How does Adagrad's adaptive learning rate benefit training with sparse data?", "What specific tasks were Adagrad successfully applied to, as mentioned in the text?", "Why is it advantageous for infrequent parameters to receive larger updates during optimization?"]}
405d391b-19bd-4499-9341-3eafae3caacc	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01144893,0.009647656,0.0031556722,0.08011345,-0.015320621,0.07360988,0.027308356,0.012979392,0.023983484,-0.0072065373,-0.026151784,-0.020503784,-0.0055747144,0.011799142,-0.036576867,-0.012765853,-0.001625381,0.080982685,0.002396413,-0.035438105,0.049827088,0.017403081,0.022626635,0.018371895,-0.0048119565,0.053113516,0.024661908,0.010593288,0.0065235407,-0.018054081,-0.008347584,-0.028937027,-0.016612237,-0.013744529,0.01947118,-0.0043418976,-0.04013031,-0.03070579,-0.05778753,-0.0365946,-0.03287665,0.06070322,-0.037914664,-0.0032539624,-0.0041683693,-0.05310283,-0.04440099,-0.03926912,-0.0032203323,0.0041716225,-0.05228511,0.03902416,-0.049107645,0.022547571,-0.09605314,0.04147255,0.008184902,-0.012217808,0.004070482,-0.059262153,-0.006190641,0.011909774,0.04175422,-0.0014482255,0.037665505,-0.021430325,6.432093e-05,0.025456412,0.03913881,0.123993635,-0.022499861,0.037557527,-0.021085138,-0.115422435,0.08812477,0.051164836,-0.04111042,-0.05569232,-0.013153413,-0.03037402,0.0010120856,0.024068322,-0.08166131,-0.008724808,0.06328347,0.0050875004,-0.054957572,-0.015054999,0.0467124,-0.055530347,-0.049984206,-0.04017115,-0.009956157,0.032933988,0.00015492529,-0.027449558,0.009776951,-0.0047743577,0.038727768,-0.024225008,0.042036626,0.0021152163,0.04337925,0.093413636,-0.0006273019,-0.01628707,-0.016250996,-0.0010217552,0.009834679,0.0012336674,0.041745268,0.056215197,-0.058365017,-0.026498593,-0.053512152,-0.016317986,-0.0015408668,0.0060699186,-0.0037554102,0.009074976,0.010479124,-0.015345761,-0.025845667,0.061996374,0.024117488,0.05989538,0.018245852,-0.034943063,-0.05824006,-0.005931094,0.06944901,-0.04158555,0.0054438156,0.036497805,-0.019953318,-0.0017934659,0.028544573,-0.019359292,0.02515839,0.022826083,-0.05731174,-0.008217268,-0.02008401,-0.0024128575,-0.008788582,0.057449125,-0.05186614,0.035806242,-0.0025447276,0.05337581,0.032533377,-0.025267918,0.07594602,-0.05030538,-0.04043512,-0.06988519,-0.029472044,0.05434767,0.010983954,-0.029786183,0.0013623225,-0.006060162,-0.03784146,0.061803006,0.04043737,0.114171684,-0.0049806344,0.031579547,-0.036632948,0.05212638,-0.047004994,-0.03241363,0.036655746,-0.03390038,0.004674752,-0.05122356,0.0010525791,-0.04036596,-0.027647411,-0.003672017,0.011994123,0.05149042,-0.023296664,0.011085894,-0.0068816505,0.04668504,0.06239267,0.012309593,-0.015179876,-0.059308536,0.031962804,-0.007101547,-0.02286513,-0.0039294856,0.048051186,0.04629486,0.029735547,-0.00847089,-0.0045684995,-0.051206265,-0.008897284,0.070033655,-0.03409448,-0.047372777,0.04030219,-0.04020746,0.030782245,-0.040352296,-0.033053752,0.0023313644,-0.038556624,-0.00093068887,0.032357823,0.03798347,0.016484246,-0.018407581,-0.021992419,0.017401028,0.063272886,-0.04549515,-0.013408376,0.01653356,-0.0034758958,0.057638288,-0.018033221,-0.0023229374,0.0017245353,0.006306319,0.038106944,-0.0113245975,-0.012333849,0.055900365,-0.036349375,0.03528669,-0.017691901,0.023260083,-0.007720736,0.032405708,0.0070985705,0.012773389,0.043158475,0.025944807,0.021040222,0.075683,-0.0076426715,0.020656738,0.041063268,0.02695022,0.011705724,0.045280743,-0.005044476,-0.00587966,-0.0007498306,-0.015420554,-0.004751182,-0.02408791,0.029174987,0.0358745,0.03731732,-0.059724335,-0.011444721,0.010925053,-0.012500653,0.013045049,0.02538922,0.007847893,0.018098488,0.017393947,-0.03797167,0.030662976,0.04199821,0.04487697,-0.018190203,-0.0052713407,-0.020156262,0.07172808,-0.03886322,0.005473468,0.031300243,-0.02841034,0.032980863,0.006526033,0.051725812,0.0150477365,-0.0075083766,0.008821231,0.0037850495,0.03343957,-0.0018714671,0.030827729,-0.019886902,-0.070388645,0.036278408,0.017642189,-0.024696391,0.02204106,-0.0013235912,-0.04206274,-0.02134617,0.036341455,0.06593982,-0.02732823,0.03388114,-0.0396639,0.01680061,0.033568725,-0.04274324,0.020753585,0.013639013,0.04126611,0.025009593,-0.0074880347,-0.00013579693,0.026440829,-0.00496402,0.044481434,0.07971086,0.02514106,0.03695371,0.024584187,0.040608782,-0.000603349,-0.014710478,0.059829183,0.036240377,0.0012056229,0.013058399,-0.037006494,-0.070909746,0.06758403,-0.026920108,0.030726785,0.03413768,-0.0079114,-0.0058911284,-0.0118646575,-0.024327494,0.041482463,0.031124776,0.0017234186,-0.035702225,-0.050085746,0.029507682,0.028791653,0.0062521324,0.009228662,0.015670542,-0.03062869,-0.005380777,0.05230518,-0.009231003,0.03410675,-0.0061968984,0.01393006,0.03967302,-0.011012464,-0.03538922,0.007385808,0.036267474,-0.047364928,0.023486685,-0.023031238,0.03619592,-0.07281812,0.052115303,-0.0074966936,-0.053374812,0.00082315924,0.010048199,-0.010865467,0.018051112,-0.005745067,0.033866268,-0.05824989,0.027396835,0.059793204,0.07689588,0.023276392,-0.021970049,-0.07441413,-0.014227666,0.020695249,-0.04417877,-0.013412683,0.06450838,0.02628192,0.004593139,0.094746,0.005479163,0.0010930691,-0.009006252,0.0062948223,0.030817095,0.032325655,-0.035357837,-0.041944858,0.02770067,0.036178056,0.0052002696,0.019220585,0.033435803,0.029967323,-0.07446081,-0.006899733,-0.025950367,0.022918936,-0.041928798,0.0015841947,0.065502636,-0.018366236,0.026783945,-0.060724046,-0.035823,-0.009782144,-0.0009672712,-0.013188281,-0.04921911,0.008127532,-0.011255476,-0.033167344,-0.018542761,0.07174276,0.045210775,0.067621864,-0.0070980936,0.007523327,-0.033828232,0.004626226,-0.017519897,-0.027280824,-0.0013837059,0.00023677974,-0.021404698,0.052024137,0.05788826,-0.019224431,-0.05159121,0.10202151,-0.032816116,0.019503115,0.030745482,-0.035225533,-0.0054284707,0.009289409,-0.010342846,-0.012370677,0.029272687,-0.03713865,-0.019349081,0.008456963,0.047607105,0.019073354,0.0007013039,0.012300894,0.060895614,-0.003081553,-0.012092533,-0.04638069,-0.032136094,0.023319094,0.0041409023,0.040033866,0.026145088,0.005839504,-0.05420784,-0.029634692,-0.036812145,-0.05950328,-0.003160184,-0.016479248,-0.037899036,0.050478682,-0.04116888,0.051025666,-0.024590075,0.03168548,-0.0061945077,0.06548393,0.03241883,-0.01348439,-0.052254513,-0.0062157977,0.046987463,0.014598052,-0.0012267424,-0.010472266,-0.022767436,-0.04078109,-0.007735579,-0.0032072184,-0.0392459,-0.0024304513,-0.013252718,0.027359199,0.013404105,-0.07302936,0.019791428,-0.02098773,0.025597978,-0.026185578,-0.047993533,-0.0054098614,0.041751783,0.09010186,0.032452494,0.039724134,0.010556787,0.07115295,-0.03458925,0.04761164,0.0063468707,0.013357824,0.0016969183,-0.024648227,2.5463129e-05,-0.0032083218,0.02225242,-0.013044222,0.019294824,-0.04163198,0.0055191,0.00939921,-0.018200858,-0.011131284,0.031605642,0.011588327,-0.022760026,0.0014564225,0.053188182,-0.0625334,-0.03806593,0.00037506528,-0.024542267,0.011636034,-0.048454564,-0.074588016,0.017862218,-0.041051127,0.021284154,-0.0004945332,-0.025356935,-0.071876824,-0.006758253,-0.02225229,0.061860684,-0.060708415,0.030847892,0.030471815,-0.015244429,0.10944323,0.04515267,-0.023526365,0.022642829,0.0026132718,0.01594272,-0.011802357,-0.0038193222,0.016066352,0.034631908,0.031927362,-0.028241983,0.005927551,-0.026508294,-0.04685492,0.012505161,0.0067995996,-0.0069515924,-0.03760836,-0.0638422,-0.0036394242,0.008760637,0.02359844,-0.008013967,0.009136959,-0.026616868,0.08469078,0.03801019,0.027124748,-0.0046883943,0.052339394,0.019495888,0.02713557,0.10790866,0.05475697,0.003551085,0.06284195,-0.02696815,0.009125406,0.012374616,-0.007285614,0.0019012871,0.0060676946,-0.03340425,-0.024016008,-0.0013783275,-0.047308158,0.03817187,-0.036518976,0.016817525,-0.030926649,-0.05414917,-0.001108149,-0.04377796,-0.026452398,0.06083376,-0.017330356,-0.011613292,0.029276615,-0.004070514,0.05136346,-0.0677216,-0.027327493,-0.06296419,-0.006298316,-0.06694627,-0.05989741,0.010198524,-0.033318132,-0.00033817693,0.004184269,0.028361104,-0.015522602,-0.069302246,-0.021687372,-0.028057976,-0.015963973,-0.0046776677,-0.021721313,0.00057117076,0.022481192,0.00068947993,-0.021914193,0.06572696,-0.0314596,-0.038646102,0.05862969,-0.015874092,-0.07971933,-0.013804852,0.017827632,-0.0233439,0.013757873,-0.001040501,0.022715604,-0.0053956476,-0.015167205,-0.01251865,-0.028480116,0.007900261,-0.019800412,0.058645666,0.08889866,0.001716243,-0.025652202,-0.039176706,-0.048271175,0.04028327,0.02951786,0.02343526,0.0065329783,0.04617492,0.054824337,0.019280197,-0.06990734,-0.04017694,-0.0021800234,0.04248847,0.01771404,0.09013622,0.059163343,-0.016659021,-0.008110125,-0.037199635,-0.015584019,0.010399159,0.020587323,-0.04954619,-0.02394361,-0.026018856,-0.027409215,0.06927,0.021204531,0.032884344,0.122717276,0.0027985247,-0.03742113,-0.07478744,0.00024584765,0.009908938,-0.016403748,0.014427052,-0.041833725,0.010583521,0.018150143,0.03158229,0.06141682,0.0033602149,-0.055321768,-0.030094463,0.0135076735,0.012028246,0.041046523,0.019235369,-0.06565632,-0.059146076,0.050718572,0.007726792,0.0052987062,-0.032055415,-0.008502502,-0.008524088,-0.036925573,0.007218606,-0.026200166,-0.0012560204,0.016822932,0.033104442,0.012509986,-0.03273903,0.06148739,-0.05509143,0.0055993753,0.014398955,0.02084532,-0.02090377,0.010703253,0.0434271,0.011772482,-0.016101308,0.021185959,0.05094855,0.0017058206,-0.00071902893,0.049649358,0.007333653,0.011177621,-0.03199886,0.0048734085,0.050219163,-0.055976916,0.0019199167,-0.0106708575,0.0646816,-0.02536827,0.007360327,-0.004989176,-0.020282755,-0.0035679818,0.02012832,0.026048027,-0.010756674,0.028113402,0.013481841,-0.055040695,-0.010607187,0.0023819355,-0.057862666,0.047025967,0.043803286,-0.008170281,-0.040194985,-0.05808887,-0.031949893,0.016658919,-0.0054608914,-0.019132,0.010655526,0.0025647124]	Keywords: Adagrad, learning rate, parameter update, gradient descent, SGD\nKey Objects: parameters, learning rate, gradients\nRefers to Images: None\nHypothetical Questions:\n- What is the main difference between the parameter update rule in SGD and the update rule used by Adagrad?\n- Why would adapting the learning rate for each parameter be advantageous?\n- What does g t,i represent in the context of the update rule?\n---\nSummary:\nPrior to Adagrad, all parameters were updated simultaneously using a single learning rate. Adagrad introduces a per-parameter learning rate, which is adapted based on past gradients, requiring a modified update rule.\nOriginal Text:\nPreviously, we performed an update for all parameters  at once as every parameter  i used the same learning rate  . As Adagrad uses a different learning rate for every parameter  i at every time step t , we first show Adagrad's per-parameter update, which we then vectorize. For brevity, we set g t,i to be the gradient of the objective function w.r.t. to the parameter  i at time step t :  \n<!-- formula-not-decoded -->  \nThe SGD update for every parameter  i at each time step t then becomes:  \n<!-- formula-not-decoded -->  \nIn its update rule, Adagrad modifies the general learning rate  at each time step t for every parameter  i based on the past gradients that have been computed for  i :  \n<!-- formula-not-decoded -->\nContextualized Text:\nIn traditional stochastic gradient descent (SGD), all parameters () are updated at once, using a single learning rate (). Adagrad deviates from this approach by adapting the learning rate for each parameter ( i) at every time step (t), based on the accumulated past gradients. To illustrate this per-parameter adaptation, we're using g t,i to represent the gradient of the objective function with respect to parameter  i at time step t.	{"tags": ["optimization", "algorithm", "machine learning"], "doc_id": "405d391b-19bd-4499-9341-3eafae3caacc", "summary": "Prior to Adagrad, all parameters were updated simultaneously using a single learning rate. Adagrad introduces a per-parameter learning rate, which is adapted based on past gradients, requiring a modified update rule.", "doc_type": "text", "entities": [], "keywords": ["Adagrad", "learning rate", "parameter update", "gradient descent", "SGD"], "key_objects": ["parameters", "learning rate", "gradients"], "contextual_text": "In traditional stochastic gradient descent (SGD), all parameters () are updated at once, using a single learning rate (). Adagrad deviates from this approach by adapting the learning rate for each parameter ( i) at every time step (t), based on the accumulated past gradients. To illustrate this per-parameter adaptation, we're using g t,i to represent the gradient of the objective function with respect to parameter  i at time step t.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.3 Adagrad"}, "hypothetical_questions": ["What is the main difference between the parameter update rule in SGD and the update rule used by Adagrad?", "Why would adapting the learning rate for each parameter be advantageous?", "What does g t,i represent in the context of the update rule?"]}
2b90aa20-ef1f-4a5e-a259-0781cde1bb4b	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.010967375,-0.015424298,0.011263495,0.05298216,-0.02221112,0.080950424,0.042202868,0.016927065,0.07091263,-0.006186734,0.014181067,-0.03635443,-0.014869491,0.029074201,-0.045018073,-0.015480153,-0.0046631126,0.07432117,0.005592156,0.002428852,0.06538389,0.017254008,-0.0058728885,-0.0034816985,-0.0032187195,0.057007995,0.027791262,0.008155037,0.021799602,-0.020896386,0.01681323,-0.028919019,-0.03503317,-0.020464657,0.039939303,-0.0026578675,-0.012440115,-0.037705414,-0.024470255,-0.024219401,-0.01102678,0.016066033,-0.046180338,-0.0048825988,0.024417097,-0.042539455,-0.04851039,-0.06562476,-0.03423699,-0.017291103,-0.030048585,0.053744756,-0.05577083,-0.01063839,-0.052142497,0.040583048,-0.015995767,-0.008079345,0.013158497,-0.04449063,-0.02962562,-0.0083713755,0.021509612,-0.010087227,0.041636925,0.024630273,0.00458768,0.037831202,0.05555374,0.11950378,-0.006325588,0.0043448242,0.036566578,-0.08585324,0.12671502,0.032140624,-0.010147429,-0.072422594,-0.008483709,-0.0120516345,0.00034505708,0.041925866,-0.03680214,-0.046088208,0.07068293,0.015399614,-0.015716817,-0.010477632,0.071627386,-0.05383546,0.0034039654,-0.022946471,0.021146385,0.026693227,-0.0052174907,-0.0626579,-0.00060318114,-0.009202494,-0.018974515,-0.0021028668,0.03038493,0.018678483,0.04102809,0.12599134,0.009790013,0.00046085118,-0.014488386,-0.0009747413,0.026878987,0.041452307,0.012145764,0.072395176,-0.057923462,-0.007610885,-0.06479904,-0.024308642,-0.015741661,0.060782753,0.013224155,-0.0009273625,0.025289416,-0.05658491,-0.02220922,0.059889138,0.008960408,0.054216426,-0.0050732372,-0.04394675,-0.05353566,0.025928333,0.081358045,-0.03613251,-0.005382117,0.025892675,0.0015541539,-0.02983683,0.0556475,-0.012845538,-0.0072381655,0.024861854,-0.055213377,-0.0011533323,0.021710154,-0.01916371,-0.030725563,0.055183493,-0.03706468,-0.018802779,0.009182661,0.048278406,0.005152027,-0.028643316,0.050658632,-0.04028194,-0.02053061,-0.019535486,-0.0056211483,-0.0039012393,-0.012647897,-0.008395485,-0.028950343,-0.0059935595,-0.007863585,0.085077174,0.009155016,0.13303427,-0.020108357,0.018933779,0.011152627,0.019242954,-0.023708919,-0.040394604,0.03432484,-0.01639576,0.008833074,-0.04161473,0.031868327,-0.021606449,-0.02292333,-0.017268628,0.0036119488,0.033963937,-0.020654833,0.014068399,-0.017735,0.007044473,0.09406768,0.010321852,-0.021865245,-0.07244936,0.0071124933,0.018794546,-0.040691078,0.014449251,0.03055922,0.0311323,0.040614888,0.012504806,-0.0077526765,-0.06345927,-0.0031496824,0.081892826,-0.05780539,-0.04362024,0.058359936,-0.03588286,-0.004831715,-0.019080102,-0.029798677,0.047074903,0.010499727,0.02834628,0.010222621,0.049830113,0.028315341,0.021260608,-0.03318602,-0.017062968,0.027550053,0.018380217,-0.03377677,-0.0034865951,0.006637651,0.06498986,-0.053033844,-0.01092461,-0.0036195768,-0.016227007,-0.002921899,0.0035656183,0.049452215,0.02092001,-0.0449043,0.04761329,-0.020998098,0.021066917,-0.009527472,0.043206196,-0.0065791355,0.033327512,0.03887989,0.035647877,-0.027585011,0.07315505,0.030516218,0.02174693,0.017780907,0.024380613,-0.0010444822,0.0064927624,0.00025965404,0.012700719,0.05594081,-0.021057768,0.012123497,-0.0013973142,0.04452613,0.023645423,0.032966513,-0.03279506,-0.031417012,-0.012453788,-0.020748707,0.023884363,-0.005978846,-0.010068806,0.015173824,0.035958525,-0.038805466,-0.0085452525,0.048473723,0.05228939,-0.0018151593,-0.004089571,-0.005023283,0.01390625,-0.033565044,0.007964797,0.05102583,-0.02562548,0.038128503,-0.014838935,0.049878124,0.01238683,-0.03134054,-0.0029942307,-0.011614937,0.025610056,0.011356932,0.007195683,0.0077688484,-0.067617446,0.0123646995,0.022374423,-0.027457371,0.034351733,0.0065529766,-0.041300237,0.0035969352,0.05131614,0.07565032,-0.01208398,0.011971719,0.0033002242,-0.0030633882,0.019449033,-0.07050063,0.061986636,0.041232456,0.04303962,0.018429805,-0.015936071,-0.0027256308,-0.0063024084,0.0035055345,0.059842195,0.04965575,0.008062744,0.024319744,0.008971557,0.01910498,-0.017070942,-0.001398876,0.051817335,0.042491842,0.017846456,0.03273722,-0.07087026,-0.04641527,0.050116908,-0.04816279,0.030359985,0.047865786,-0.004704946,-0.009160657,0.02654021,0.01769367,0.022757594,0.03145479,0.02449873,0.00076436764,-0.07343806,0.034610596,0.006887801,0.024224984,0.034132067,0.024538692,-0.015437674,-0.012222853,0.015087754,-0.02637428,0.027084578,0.0015237126,-0.017362379,0.050820578,0.0035585437,-0.009580485,0.025475416,0.05399581,0.0061906236,0.04123779,-0.01686874,-0.00071785256,-0.042752825,0.08460887,-0.01007119,-0.040947486,-0.041926216,0.037407853,0.010605341,0.015177288,-0.02827521,0.022165118,-0.052007027,0.030002715,0.04846689,0.045325387,0.02194602,-0.026938185,-0.09597445,0.014250542,0.048752714,-0.007399778,-0.020943027,0.036779366,-0.009794254,-0.018294355,0.08421752,0.022596957,0.023306625,-0.008857832,0.053181514,0.05350171,0.014379477,-0.07925296,-0.029074553,0.012839681,0.06862756,-0.038523395,-0.026905416,0.027364293,-0.010068422,-0.05615033,-0.026320416,0.028428275,0.017270451,-0.0021675448,0.018630696,0.05122261,-0.045524664,0.040775117,-0.06716342,-0.01040107,-0.0030271325,0.031979743,-0.044397064,-0.02385489,-0.02179204,-0.011946422,-0.0100973975,0.005870483,0.050211444,0.0261854,0.020993298,-0.0020237698,-0.033354633,-0.024454106,0.0359809,-0.017856082,0.0002830892,-0.007406891,-0.019941168,-0.000112314905,0.06210028,0.022512052,-0.031144504,-0.06997616,0.10029003,-0.02200141,0.030396463,0.046628945,-0.0025994685,0.010468522,0.025267823,0.0003724988,-0.01735702,0.030300114,-0.013467216,-0.03762895,-0.0043605543,0.039420243,0.00095000333,-0.007677821,0.032625012,0.06512805,0.04117317,-0.027641231,-0.0198581,0.0060301223,0.0020554732,0.021962544,0.030544482,0.02405986,-0.01813562,-0.032834172,-0.0042969887,-0.03997374,-0.016003687,-0.008984347,-0.018870572,0.00010293648,-0.020673666,-0.06150003,0.028390877,-0.014947499,0.04043349,0.039979756,0.022956586,0.03825762,-0.01221315,0.01096493,0.033097222,-0.0003521822,0.050271112,0.026600217,-0.0045213704,-0.020484898,-0.0015930567,-0.01674659,0.046562158,-0.046437353,0.045906413,0.007668196,-0.004185398,0.03760457,-0.029565074,0.016779598,-0.022420269,0.046766225,-0.012702721,-0.020828195,0.006059393,0.0577311,0.07080853,0.0046396474,0.06840308,0.031248387,0.082846515,-0.019978814,0.026723003,0.040058356,0.009567822,0.044657826,-0.028295381,0.023947507,-0.0076360432,0.024625203,-0.014449766,0.0077895736,-0.019038122,0.018182697,0.0351567,-0.0019493615,0.0082986895,0.07572997,-0.017525658,-0.03920447,-0.02125163,0.066551626,-0.06586893,-0.03577323,0.005175495,-0.030264553,0.016257133,-0.05602505,-0.07042699,-0.011477963,-0.0271306,0.033521026,-0.029810548,-0.011896877,-0.0726879,0.0075954497,0.006548525,0.07173173,-0.061583407,0.055713218,0.009789717,0.030278197,0.074345455,0.04009557,-0.024781737,0.018428348,-0.02276009,0.013713717,-0.007639406,0.017829169,0.026171032,-0.0029862414,0.012486013,0.044491023,0.014312756,0.043837916,-0.050769977,-0.033818997,-0.024315786,-0.02229621,-0.069333196,-0.0840451,0.028161338,0.012217988,0.02152612,-0.022160208,-3.3251148e-05,-0.0029874926,0.05357597,0.034651358,0.007204244,-0.011949818,0.038351547,0.004900315,0.016163256,0.054337822,0.03266931,0.03154287,0.04616824,0.033963304,0.0131132975,0.013191974,0.01610773,0.0007138797,-0.041113194,0.011414206,-0.04217313,-0.009638623,-0.049098983,0.0510131,-0.032973267,0.051501926,-0.032100458,-0.051524524,0.023380177,-0.016930252,0.014906448,0.04499334,0.022685785,-0.034479987,-0.011212234,-0.06534202,0.07157412,-0.07447125,-0.030183287,-0.014957201,-0.0096287355,-0.08702804,-0.06720368,-0.013274439,-0.056951985,-0.012292331,-0.025028303,0.013662794,0.0086215995,-0.04044968,-0.012173917,-0.064301856,0.00085076934,-0.009550178,-0.018762201,0.006558406,0.010754437,0.0020205798,-0.0019474664,0.046775308,-0.035095006,-0.05686752,0.0044754576,-0.014332784,-0.025167905,-0.0353538,0.004691637,-0.038336843,0.01782359,-0.015599797,0.026634159,0.026969235,-0.028111303,-0.014690362,-0.06028483,0.056803443,0.0012457572,0.0564096,0.06265656,0.040524848,-0.033741947,0.006871123,-0.039456133,0.019040754,0.017055154,0.0079049645,0.0058307853,0.021814778,0.046403937,0.028623957,-0.08324461,-0.039610934,-0.024904475,0.06502784,0.016513327,0.11615334,0.03713598,-0.007865501,-0.003205529,-0.020497277,-0.0055597625,0.040123343,0.0004713404,-0.017692897,-0.017049631,-0.009932375,0.005542105,0.00818422,0.00064964266,0.020313928,0.12831023,-0.004313451,-0.051739477,-0.051139887,-0.015760852,-0.019503033,-0.026402202,0.061194442,-0.013297274,0.021222655,0.026433973,0.03483055,0.046837907,-0.010171648,-0.03731437,-0.08237093,0.003598535,0.0031492864,-0.009624896,0.0034910413,-0.009905632,-0.0952824,0.07264495,0.017512966,-0.024469826,-0.020362616,0.047243427,-0.011038237,-0.03684974,-0.010464595,-0.0030489662,0.010856651,0.018218864,0.03150401,0.04651711,-0.0065241945,0.049392324,-0.060756907,-0.006439507,0.0054346803,-0.00082048244,0.009984632,0.04835156,0.027521204,-0.00482864,-0.01914007,0.0039231526,0.026858246,0.048352905,-0.008034336,0.030408313,0.010634464,0.036190223,-0.016656635,-0.0031033815,0.056121606,-0.005506142,-0.01971628,-0.012515899,0.05272344,0.0063644596,-0.02869143,0.0132560395,0.0054016043,-0.015766792,0.022296669,0.009399178,0.039653815,0.022524765,0.02633561,-0.03773332,0.021407217,-0.026210112,-0.044466816,0.0074795024,0.05775294,-0.020398181,-0.055599306,-0.020774534,-0.06134845,0.04368753,-0.005631936,-0.0070487317,-0.0012662504,0.00018478786]	Keywords: Adagrad, diagonal matrix, gradients, smoothing term, parameter update\nKey Objects: Gt, diagonal matrix, gradients, smoothing term\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of including a smoothing term (epsilon1) in the update rule?\n- Why is Gt represented as a diagonal matrix, and what implications does this have for computation?\n- How does the accumulation of squared gradients affect the learning process in Adagrad?\n---\nSummary:\nIn Adagrad, Gt is a diagonal matrix where each diagonal element represents the sum of the squares of past gradients with respect to a parameter i, and a smoothing term (epsilon1) prevents division by zero.\nOriginal Text:\n<!-- formula-not-decoded -->  \nG t  R d  d here is a diagonal matrix where each diagonal element i, i is the sum of the squares of the gradients w.r.t.  i up to time step t 11 , while glyph[epsilon1] is a smoothing term that avoids division by zero (usually on the order of 1 e -8 ). Interestingly, without the square root operation, the algorithm performs much worse.  \n9 Refer to http://cs231n.github.io/neural-networks-3/ for another explanation of the intuitions behind NAG, while Ilya Sutskever gives a more detailed overview in his PhD thesis [19].  \n10 http://www.wired.com/2012/06/google-x-neural-network/  \n11 Duchi et al. [8] give this matrix as an alternative to the full matrix containing the outer products of all previous gradients, as the computation of the matrix square root is infeasible even for a moderate number of parameters d .\nContextualized Text:\nAdagrad adapts the learning rate for each parameter based on historical gradients. A key component of this process is Gt, a diagonal matrix. Each diagonal element (i, i) in Gt sums the squares of the gradients for parameter i up to a given time step. A small smoothing term, often denoted as epsilon1, is also included to prevent division by zero during updates.	{"tags": ["optimization", "algorithm", "neural networks", "Adagrad"], "doc_id": "2b90aa20-ef1f-4a5e-a259-0781cde1bb4b", "summary": "In Adagrad, Gt is a diagonal matrix where each diagonal element represents the sum of the squares of past gradients with respect to a parameter i, and a smoothing term (epsilon1) prevents division by zero.", "doc_type": "text", "entities": ["Adagrad"], "keywords": ["Adagrad", "diagonal matrix", "gradients", "smoothing term", "parameter update"], "key_objects": ["Gt", "diagonal matrix", "gradients", "smoothing term"], "contextual_text": "Adagrad adapts the learning rate for each parameter based on historical gradients. A key component of this process is Gt, a diagonal matrix. Each diagonal element (i, i) in Gt sums the squares of the gradients for parameter i up to a given time step. A small smoothing term, often denoted as epsilon1, is also included to prevent division by zero during updates.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.3 Adagrad"}, "hypothetical_questions": ["What is the purpose of including a smoothing term (epsilon1) in the update rule?", "Why is Gt represented as a diagonal matrix, and what implications does this have for computation?", "How does the accumulation of squared gradients affect the learning process in Adagrad?"]}
0eaff04e-a986-43ee-8a4d-780be47270e9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0019040841,0.0066194893,0.01923503,0.034689035,-0.045299828,0.05681288,0.016646126,0.0029054857,0.058147207,-0.025022192,-0.01581322,-0.013818993,-0.005406883,0.0007468179,-0.029252723,0.005658715,0.013614595,0.0933817,-0.008523044,-0.022972574,0.044954512,-0.011407733,-0.01046686,0.0065459614,-0.00059618324,0.0088288905,0.03347498,0.039251946,0.021368224,0.005566968,0.0065196054,-0.010312987,-0.000103151324,0.016293306,0.019399473,0.024372675,-0.020102512,-0.038587052,-0.026126908,-0.045502316,-0.047884483,0.031118706,-0.03486451,-0.042545084,0.0019493513,-0.040906142,-0.056751568,-0.06089127,-0.043434948,0.023130968,-0.0026941954,0.02244086,-0.05863865,0.0030336624,-0.067179196,0.020406371,-0.020709686,-0.007552846,0.036928177,-0.05541124,-0.040290214,0.011560581,0.030177651,-0.019132541,0.05987372,-0.003590327,-0.0037370685,0.05205649,0.03339649,0.13230984,0.0049001775,0.035482835,0.013519671,-0.10849399,0.12002879,0.0507898,-0.019168342,-0.093578696,-0.010320773,-0.021972077,0.011433685,0.028346049,-0.03206257,-0.015541817,0.079008885,0.033039805,-0.008210445,-0.0025571454,0.073417634,-0.05905571,-0.019753022,-0.03189673,0.012212118,0.032119174,-0.02434887,-0.048603814,-0.0066752876,-0.032211494,0.009606178,-0.02138684,0.056675702,-0.014620744,0.04929362,0.107935265,0.0033296407,-0.0106846085,0.023287619,-0.025656315,-0.0016747871,0.017699717,0.049064532,0.047829747,-0.11041946,0.005170155,-0.046503447,-0.019831788,-0.024140418,0.057120092,-0.004638012,-0.027389254,0.04186948,-0.033507768,-0.0054597855,0.026527442,0.020013466,0.015890455,-0.0010940778,-0.0498435,-0.058158677,0.027780019,0.06589649,-0.053027626,0.0041997274,0.001801921,-0.0040384573,-0.022476418,0.044212807,-0.006625671,-0.009166934,0.026958179,-0.08199533,-0.0071296263,-0.016220715,-0.0015925858,-0.015521079,0.076926336,-0.030651761,-0.016572507,-0.0069454233,0.057795115,0.0008380432,-0.011367492,0.042768303,-0.002142229,-0.00488996,-0.018511236,-0.0067376485,0.025651908,0.007475629,-0.012648932,-0.062971406,0.00012545421,0.024843937,0.07784737,0.02238125,0.10584709,-0.017637778,0.02867385,-0.004266299,0.042113647,-0.013669499,-0.060106196,0.03270524,-0.031229382,-0.0128642805,-0.046047986,-0.017954309,-0.012634257,-0.0024595547,-0.041476615,0.017957613,0.00031795894,-0.043571755,0.018986922,-0.0008931386,0.011603792,0.047221094,0.032334067,-0.024986539,-0.045868408,0.0061919196,0.008283267,-0.0006882742,-0.0028031892,0.034863174,0.028528014,0.030035483,0.0003763307,-0.02326944,-0.043712266,0.015296317,0.06625059,-0.016788797,-0.027844597,0.04993734,-0.03916169,0.026287716,-0.05876554,-0.03004594,0.03536563,-0.014474172,0.013642761,0.013936791,0.055452198,-0.0005822087,-0.0018818037,-0.025822354,0.008891222,0.033630695,0.017445853,-0.05088892,0.0012989553,-0.008940833,0.06606806,-0.04622664,-0.01119914,0.005028112,0.018625919,0.0426154,-0.018462889,0.018391185,0.0029021332,-0.04085159,0.04813048,-0.036687005,0.00880682,-0.010177635,0.02801802,0.0053009596,0.044083584,0.0388256,0.029631622,-0.025944093,0.09306201,-0.0097924555,0.022284955,0.030286178,0.040315434,0.012879397,0.006733067,-0.01630656,0.016705338,0.032109693,-0.049669918,-0.00086435996,0.0027716411,0.029766135,0.03489698,0.024543546,-0.059815332,-0.022195004,-0.018554814,-0.031421464,0.030962525,0.002293524,0.012376797,0.053914644,0.018688371,0.0017634294,0.006065013,0.056093756,0.059057195,-0.018596001,-0.015856937,-0.0063787103,0.025439076,-0.028855972,-0.022648005,0.050653048,0.0069265706,0.032730363,0.004974569,0.032602776,0.015420945,-0.035100497,-0.0072747255,0.010405206,0.038123738,0.017204316,0.010055513,0.01744424,-0.054916944,0.049950656,0.019230925,-0.04737699,0.011439386,0.010747542,-0.035611518,-0.005075867,0.028504722,0.08444689,0.003676318,0.02501905,0.012325642,0.0105051,0.01899153,-0.07049781,0.07802719,0.023027783,0.052881543,0.038106438,-0.010364488,-0.019517254,-0.018488498,-0.019986294,0.036400225,0.04960755,0.0064775925,0.028971463,-0.0078524165,-0.0075287605,-0.023510527,0.001565426,0.023409726,0.0026571401,-0.00039460775,0.011462123,-0.022756867,-0.06135824,0.04217999,-0.030172646,0.0470689,0.05003208,-0.008841507,-0.047081646,0.009142323,-0.060900394,0.022047993,0.032678492,0.026861485,0.007902647,-0.05027157,0.016005054,0.0050650197,0.023562333,0.027583871,0.020602148,-0.006024938,0.024789732,-0.0016460696,-0.008881575,0.055831518,0.027707808,-0.019248508,0.04550137,-0.011772966,-0.015810553,0.021690398,0.046673417,-0.009471757,0.008147914,-0.03833595,0.0029777717,-0.022083167,0.07447027,-0.016421013,-0.051772904,-0.060534794,0.010509225,-0.0139552085,-0.022482919,-0.025997793,0.02988502,-0.04249624,0.017305063,0.06720668,0.059093338,0.03114506,-0.019335099,-0.0818221,-0.0059126643,0.06348136,0.016047634,-0.027504792,0.011513166,-0.016685648,-0.020811293,0.090036154,-0.011362147,0.06162458,-0.036990616,0.016604485,0.050663322,-0.031293046,-0.06671776,-0.02107596,0.02348022,0.039008304,-0.027363606,-0.034393895,0.009012203,0.02474227,-0.06910519,-0.010365496,-0.039799396,0.05362677,-0.015115729,-0.019722547,0.044551406,-0.032386255,0.016085083,-0.06520317,-0.04409554,0.00498971,0.0502355,-0.03278492,-0.005540866,-0.0099383015,-0.017322462,-0.052951496,-0.01601834,0.036056023,0.020746054,0.025422037,-0.010699586,-0.0055543557,0.0006130457,0.055946592,-0.0468338,-0.00199109,-0.013266534,-0.034678277,-0.015942978,0.030613722,0.030008188,-0.017849823,-0.030997386,0.07166603,-0.006308417,0.029072288,0.028402459,-0.034197118,0.0037022417,0.009440329,0.021839617,0.0015485907,0.023265319,-0.027637629,-0.01525456,0.008632629,0.029689811,0.0037695337,0.011176568,0.013795419,0.08275677,0.029149506,-0.009955294,-0.028842153,0.005408531,0.0044907643,0.010813647,0.030288298,0.03659175,0.0027344776,-0.050896227,-0.034787003,-0.04768202,-0.051662464,0.020914264,-0.013924882,-0.028071525,0.023625933,-0.08553112,0.028198287,-0.004206674,0.028204296,0.047053028,0.05777078,0.024840472,0.004353596,0.0073757395,0.02917748,0.014475249,0.025346447,0.02341595,-0.027723646,-0.033997387,-0.012207417,-0.012638467,0.038452093,-0.073047355,0.013292983,0.009553033,0.0030663258,0.027276646,-0.043426346,0.013140679,-0.02548931,0.023344476,-0.04590862,-0.021470834,0.03528324,0.06602062,0.094633974,-0.006437324,0.03894618,0.03746577,0.071582496,-0.0015783604,0.019001829,-0.0038716432,0.03468224,0.050101377,-0.043626364,-0.029813921,0.008356349,0.024433248,-0.00966191,0.03287615,-0.024467153,0.035265293,0.0324861,0.012264181,0.010660942,0.07677122,0.019033125,-0.032817222,-0.032040335,0.05462425,-0.051703047,-0.018860081,0.031487532,-0.0065208534,0.02595375,-0.04738465,-0.03812227,-0.00840692,0.007311063,0.016071556,-0.01575794,-0.019847015,-0.06470063,0.008601415,0.019286415,0.049088288,-0.06112072,0.051701978,0.05751661,0.012604345,0.08212838,0.022575708,-0.017369008,0.029779833,-0.043387845,0.012291413,-0.01247216,-0.01244401,0.040023282,0.0136784725,0.021796359,0.022590598,0.013669536,0.0009613315,-0.050420452,-0.03559735,0.0036117197,-0.036191307,-0.07176362,-0.032031987,0.020023093,0.02519196,0.0052563986,-0.024689153,0.014677357,-0.036271907,0.035990607,0.050066143,0.019568967,0.016032265,0.050814036,0.011109169,0.021526363,0.06891527,0.01637132,-0.004587839,0.04081814,0.058914285,0.024349447,0.016207553,0.0011241331,0.010849402,-0.068387955,0.023265274,-0.017740851,0.0020776126,-0.0692033,0.07933212,-0.045064345,0.038310908,-0.035746984,-0.04645294,0.03632645,-0.031434827,-0.029075047,0.024740677,-0.010558271,-0.0022834234,0.038897637,-0.039091755,0.07042462,-0.07964714,-0.026484402,-0.010978102,-0.025129775,-0.059446115,-0.025810828,0.0089725,-0.06013633,-0.0071676313,-0.056889974,0.006326502,-0.030876389,-0.049673073,-0.00054507353,-0.02917162,-0.020410407,-0.01346165,-0.029215274,0.0035088458,-0.006037848,-0.010217742,-0.0057334728,0.029816713,-0.02680179,-0.05640651,0.032851644,-0.016178388,-0.07279574,-0.055201877,0.0018761924,-0.02122256,0.0152970655,0.026433177,0.021005707,0.032191563,-0.028763333,-0.024250017,-0.04459506,0.067607306,0.037718862,0.051340368,0.07448198,0.03875329,-0.013603841,-0.019727312,-0.03935298,-0.002746018,0.06424583,0.022219272,-0.0046392805,0.05621213,0.07675272,-0.018060626,-0.079810925,-0.03521975,0.00465192,0.034991596,0.0009024421,0.04912145,0.00289533,-0.017829817,0.024483204,-0.017627,0.021476695,0.030658256,-0.011773788,-0.009815532,-0.03458537,-0.034543652,-0.008059329,0.062317763,-0.017351871,0.008905842,0.094042964,-0.014286762,-0.06589287,-0.03615221,-0.03115531,-0.05417095,-0.064186916,0.030825024,-0.031265926,0.0018139463,0.04213411,0.0074570356,0.030767603,-0.014134056,-0.050794963,-0.042003363,-0.0052773873,-0.014253328,0.022431714,0.036490154,-0.04699556,-0.07331244,0.0566431,0.024589622,-0.016488798,-0.03169393,0.022954354,-0.021287024,-0.023790315,0.010469986,-0.03316188,0.013400993,-0.0030887169,0.046934016,0.02673831,-0.0178613,0.057721242,-0.08122107,-0.03995871,0.018239258,-0.023602676,0.007748631,0.06203529,0.022470867,0.036034975,-0.032962777,0.009589236,0.025654525,0.03517989,-0.019141505,0.023158265,-0.017035091,0.04725997,-0.013543632,-0.016626846,0.06017545,-0.022393472,0.025998332,0.01595967,0.030876242,-0.017453251,-0.025729025,0.039143484,-0.01857267,-0.0026175303,0.023034723,0.011534859,0.08341522,0.024072666,0.048195515,-0.017681766,-0.0137111535,-0.022051524,-0.03948349,0.011559964,0.06455873,-0.024325738,-0.058571268,-0.010368821,-0.04093328,0.02003009,-0.0030281488,-0.011104945,-0.0025981022,-0.028755648]	Keywords: Adagrad, vectorization, learning rate, optimization, gradients\nKey Objects: Adagrad, Gt, gt, learning rate\nRefers to Images: None\nHypothetical Questions:\n- How does vectorizing the implementation of Adagrad improve efficiency?\n- Why is it advantageous to eliminate the need for manual learning rate tuning?\n- What is the underlying problem with accumulating squared gradients in Adagrad?\n---\nSummary:\nAdagrad's implementation can be vectorized by performing an element-wise matrix-vector multiplication between Gt and gt, and it simplifies training by eliminating the need for manual learning rate tuning, although it has a key weakness related to accumulating squared gradients.\nOriginal Text:\nAs G t contains the sum of the squares of the past gradients w.r.t. to all parameters  along its diagonal, we can now vectorize our implementation by performing an element-wise matrix-vector multiplication glyph[circledot] between G t and g t :  \n<!-- formula-not-decoded -->  \nOne of Adagrad's main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0 . 01 and leave it at that.  \nAdagrad's main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. The following algorithms aim to resolve this flaw.\nContextualized Text:\nIn the Adagrad optimization algorithm, the diagonal matrix Gt, which contains the sum of the squares of past gradients, can be used to vectorize the implementation. This involves an element-wise matrix-vector multiplication between Gt and gt. A significant benefit of Adagrad is that it removes the need to manually adjust the learning rate; however, the accumulation of squared gradients presents a challenge.	{"tags": ["optimization", "algorithm", "deep-learning"], "doc_id": "0eaff04e-a986-43ee-8a4d-780be47270e9", "summary": "Adagrad's implementation can be vectorized by performing an element-wise matrix-vector multiplication between Gt and gt, and it simplifies training by eliminating the need for manual learning rate tuning, although it has a key weakness related to accumulating squared gradients.", "doc_type": "text", "entities": [], "keywords": ["Adagrad", "vectorization", "learning rate", "optimization", "gradients"], "key_objects": ["Adagrad", "Gt", "gt", "learning rate"], "contextual_text": "In the Adagrad optimization algorithm, the diagonal matrix Gt, which contains the sum of the squares of past gradients, can be used to vectorize the implementation. This involves an element-wise matrix-vector multiplication between Gt and gt. A significant benefit of Adagrad is that it removes the need to manually adjust the learning rate; however, the accumulation of squared gradients presents a challenge.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.3 Adagrad"}, "hypothetical_questions": ["How does vectorizing the implementation of Adagrad improve efficiency?", "Why is it advantageous to eliminate the need for manual learning rate tuning?", "What is the underlying problem with accumulating squared gradients in Adagrad?"]}
af81f2c0-d5f6-4045-bfb1-c935cb940555	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.018777274,0.02184109,-0.00031809212,0.08699316,-0.040455643,0.09290435,0.026488103,0.004617513,0.041058898,-0.0013949707,-0.004678958,-0.012788532,0.0016626944,0.031306177,-0.037375208,0.016024457,-0.017935138,0.07823122,-0.031053333,-0.008891481,0.040680382,0.012717278,0.04175497,0.020889057,-0.024943018,0.034746468,0.020697312,0.03446721,-0.015738314,-0.0035463122,-0.019428713,-0.0469627,-0.013983098,0.017277705,0.0154506415,0.013377371,-0.050755333,-0.016248057,-0.023985328,-0.049210608,-0.04164248,0.03875434,-0.04822485,-0.005565018,0.02490187,-0.06968874,-0.05271689,-0.0234613,-0.03455758,-0.019581899,-0.024614645,0.03169392,-0.05643863,0.029391304,-0.081335254,0.027509341,0.006910491,0.011064283,-0.008991864,-0.032415062,-0.01894083,-0.025490547,0.042117517,-0.033480357,0.0500743,-0.0014615845,0.0054913773,0.04410731,0.04977878,0.12946303,-0.011895768,0.036830463,0.017235626,-0.11442142,0.12639113,0.02839631,-0.03316323,-0.047922883,-0.012224208,-0.017548747,0.031881332,0.045432463,-0.046105735,-0.020386778,0.06397476,0.013460112,-0.0470336,-0.008204636,0.07001192,-0.062232297,-0.039506696,-0.043886397,-0.012943254,0.036235154,-0.010845626,-0.015038076,-0.019866018,-0.0399629,0.0034032536,-0.0034698874,0.044086624,-0.021635255,0.032871153,0.1200958,0.00597136,-0.005779575,-0.012269969,-0.015379026,-0.00409972,0.009208021,0.019254992,0.06415875,-0.1083278,-0.026717287,-0.063883975,-0.030032286,-0.013320405,0.020922726,-0.0015598537,-0.010424579,0.0065289596,-0.056224052,-0.0119275525,0.063773476,-0.009116489,0.05136742,0.027443726,-0.07007349,-0.06040964,0.022714991,0.06327046,-0.031153709,-0.013613326,0.040857658,-0.012413041,0.00021519941,0.016331602,0.013473148,0.004025946,0.009364482,-0.07417765,0.032647766,-0.013238188,0.013003353,-0.010644786,0.060447596,-0.033535037,0.018125072,-0.006089434,0.059119366,0.030714588,-0.04121813,0.09941908,-0.032734454,-0.024406927,-0.049946092,-0.012368557,0.040043414,0.006566048,-0.008279536,-0.023861537,-0.009157089,-0.033022143,0.104311444,0.06779818,0.13493642,-0.020784704,0.037613176,-0.0041054357,0.049116734,-0.03226571,-0.06275451,0.017039668,-0.023328772,-0.015130256,-0.0754058,0.02541267,-0.019004235,-0.013056057,0.0061857975,0.011412839,0.025575671,-0.041921712,0.003392967,-0.020682026,0.037972152,0.066850655,0.04016281,-0.03324445,-0.057101488,0.022502586,0.020171456,-0.009456658,0.016615205,0.020203078,0.045864515,0.050404586,-0.035662115,0.0036408473,-0.03476005,-0.006104708,0.065070316,-0.060755968,-0.035894535,0.072573796,-0.041337594,0.03915947,-0.007853219,-0.016078934,0.020197736,-0.026061896,-0.00068437477,0.007878274,0.057732433,0.03246141,-0.010647403,-0.022427067,0.008940668,0.058166433,0.007012098,-0.034692537,0.01896044,0.023296539,0.08263114,-0.043358773,-0.0070225927,0.028410224,-0.0031705657,-0.0059907227,-0.013741714,0.023833184,0.040445887,-0.047431372,0.051433317,-0.033615384,0.013318529,0.002618219,0.021278227,-0.0045097144,0.040178366,0.04810747,0.01789797,-0.034032777,0.06526601,-0.009197869,0.007993527,0.04848035,0.014176911,-0.010647518,0.014563679,-0.0149021195,0.011279255,0.035145763,-0.023427622,0.01932478,-0.011661815,0.031838354,0.0050289654,0.0340807,-0.06010819,-0.022498587,0.0025257485,-0.03715216,-0.0058117127,-0.023793418,0.03246651,0.018073265,0.028876148,-0.043488305,0.02310094,0.073994316,0.02854212,-0.02502736,0.0034869853,-0.0325442,0.015257307,-0.027434738,-0.0020858406,0.055127837,-0.011894492,0.03400292,-0.0071485764,0.035722125,0.031768113,-0.03071685,0.013057396,0.024303215,0.0375164,-0.021545628,0.027178638,0.0067155263,-0.05504432,0.03745413,0.010702964,-0.02828582,0.011300897,0.009525154,-0.054269012,0.0074607665,0.055170324,0.066843025,-0.005461527,0.02935066,-0.007302511,-0.013097988,0.03509991,-0.04505309,0.032968238,0.0072527234,0.052703887,0.024375768,-0.013110208,-0.020382509,0.0145334,-0.025957007,0.04052594,0.07769012,0.0032085378,-0.024893355,0.011549555,0.014680392,0.00039970732,-0.015375095,0.041155647,0.028432028,0.0029101125,-0.0061801765,-0.064511925,-0.06313266,0.050852016,-0.06890628,0.035740633,0.05516263,-0.008661473,0.0012084292,0.02386386,-0.03808417,0.039412674,0.010121397,-0.02563572,-0.02347317,-0.051887978,0.018609077,0.04079941,0.032876164,0.018690463,0.026708942,-0.014255358,0.0022032352,0.0127876345,-0.014572231,0.023870973,0.014165176,-0.0014126307,0.055011913,0.0077941082,-0.0462016,0.042637553,0.047065903,-0.03766023,0.016472438,-0.020241136,-0.011118212,-0.07511871,0.08472013,-0.019418245,-0.036964037,-0.0011778749,-0.0037717,0.0021445656,0.0044337977,-0.011346239,0.029986594,-0.05498785,0.030733863,0.0773076,0.07174267,0.041064475,-0.03270732,-0.050353747,-0.0148047535,0.057890683,-0.022321353,-0.035899755,0.04603933,0.027813515,-0.015369332,0.074280016,0.0036022875,0.01846681,-0.04442538,0.024008563,0.028375119,0.011897712,-0.0477509,-0.04661386,0.024538174,0.064159475,0.0023267113,0.024916794,0.0047733616,0.020813081,-0.06600244,0.010083675,-0.007634852,0.023893297,-0.028536893,-0.0015920821,0.05142593,-0.03292886,0.013988394,-0.049157895,-0.056425173,-0.020295989,0.051137824,-0.0058967937,-0.024133923,-0.0121025,-0.010441332,-0.022888018,-0.017929867,0.057159808,0.01998059,0.03750757,-0.000348324,0.011544483,-0.01839084,0.04952934,-0.012788066,0.01400048,0.00028402757,-0.006121429,-0.020399073,0.04183895,0.032965116,-0.019853128,-0.026580289,0.100794785,0.011106967,0.046032596,0.03273173,-0.03151397,0.014669065,-0.0060490826,-0.03847732,0.006001144,0.039014187,-0.061598275,-0.02282943,-0.00282851,0.04056181,0.007375213,-0.0041528423,0.019668153,0.093438886,0.01096684,-0.01748611,-0.034873053,-0.009628251,0.008600359,-0.0008242941,0.064049765,0.027499594,0.010342526,-0.02793973,-0.029401183,-0.045141213,-0.04888415,-0.007111455,-0.035228617,-0.027657894,-0.004253705,-0.053040877,0.008786491,-0.028398683,0.028635986,0.0034997067,0.041897625,-0.0012339293,-0.02628039,-0.003823233,0.018572474,0.028704265,0.016818043,0.005984483,-0.010481945,-0.027127344,-0.026124742,-0.01064199,0.05074397,-0.012820155,0.04048865,-0.0021575831,-0.0009878675,0.045347914,-0.06458557,0.031015033,-0.007285231,0.04926311,-0.02919371,-0.017778378,0.003989526,0.04742348,0.06300774,0.008466397,0.0467577,0.022694211,0.08630645,-0.01811665,0.030609485,-0.016129551,-0.010217442,0.03687329,-0.044383276,0.0021397627,-0.0071043624,0.024179049,-0.0112601,0.007327071,-0.04660554,0.043932423,0.021368645,-0.022149602,0.023546439,0.037050743,-0.0009875345,-0.008382139,-0.00875863,0.049518652,-0.059384465,-0.036228478,-0.01483906,-0.007393292,0.044496298,-0.027113905,-0.06863063,2.0969435e-05,-0.041840654,0.027093608,-0.031524405,-0.0046384563,-0.046558917,0.008586024,0.009089074,0.0713824,-0.04955088,0.04806381,-0.004609411,-0.010600965,0.0883959,0.029651199,-0.013787481,0.0045299665,-0.04586657,0.008078858,0.017134447,0.02601728,0.014546143,0.040439647,0.0507641,-0.0074872808,-0.0018647976,0.009226018,-0.048429556,-0.02310323,0.004654362,-0.009977014,-0.02880279,-0.060511835,0.005416274,-0.00032307065,0.034243446,-0.022283293,0.022192081,-0.0075368034,0.042261258,0.033382393,0.0036675273,0.0076766284,0.037709232,0.014836113,0.010668677,0.062311552,0.012127886,-0.013014841,0.06467709,0.009530704,0.013314605,0.0074692625,0.004538228,0.018570853,-0.04179145,-0.024824845,-0.05293236,0.0011109033,-0.01946652,0.08763368,-0.04229416,0.05053338,-0.050127983,-0.03424514,0.02712748,-0.04759071,-0.024881825,0.031535562,-0.003899417,0.003783879,0.019616945,-0.004074549,0.04398506,-0.057871655,-0.0323428,-0.023374047,0.004668429,-0.07113107,-0.038147785,0.032140773,-0.042704593,-0.011105149,-0.017483175,0.01493939,-0.03403362,-0.06991588,-0.007148301,-0.050015014,0.006877698,-0.011655957,-0.03206784,-0.009928433,0.0065828296,-0.028485041,-0.0024621622,0.041579783,-0.012647339,-0.06072209,0.020486504,-0.009271844,-0.059479095,-0.029875107,0.0059877615,-0.021118768,0.00432729,-0.00067995035,0.016338274,-0.0012817908,0.0016811828,-0.037014745,-0.042253785,0.012165027,0.02266045,0.04028119,0.06253165,-0.025374884,-0.018836748,0.0003574383,-0.029649727,0.009393001,0.037474103,0.015654745,0.002212898,0.054459248,0.022399126,0.02189589,-0.09300017,-0.053574722,-0.006094395,0.023733981,-0.005082753,0.09356847,0.058369532,-0.009017693,-0.012952147,-0.032574337,-0.012624407,0.001653903,0.017670158,-0.05062762,-0.014325708,-0.0022613304,-0.009105739,0.046334494,0.012162531,-0.009054835,0.06961829,-0.021342427,-0.049872756,-0.05164052,-0.009902934,0.0022826323,-0.038645398,0.012941046,-0.014594288,-0.0157477,0.03358333,0.037847,0.011248423,0.004209298,-0.06834524,-0.048601642,-0.003575021,0.021898696,0.0073580737,0.007383491,-0.05216987,-0.080761865,0.066031285,0.020950193,-0.011716749,-0.014555158,0.024022022,-0.01953787,-0.030448627,-0.0005053393,-0.004838263,0.0067136097,0.019801572,0.028375747,0.0181724,-0.0310243,0.06104954,-0.06529288,-0.004911374,0.0120466985,-0.0054669776,0.0048369057,0.050212353,0.04621117,-0.009527831,-0.031654023,0.01882666,0.0142017,0.009022802,-0.014415696,0.036398,0.01919161,0.017035278,-0.007252992,-0.0053704595,0.03024361,-0.04281939,-0.01888597,0.011193442,0.057643533,-0.017793493,-0.010689129,-0.013405354,-0.02723802,-0.023761922,0.017201671,0.01731704,0.03312163,0.021960597,0.030666431,-0.035696596,0.014129204,-0.018533574,-0.050314896,0.018261017,0.041679613,0.005647185,-0.051180217,-0.031829152,-0.06769008,0.03922986,0.00096060097,-0.00044459672,-0.009375475,-0.028916458]	Keywords: Adadelta, Adagrad, parameter updates, squared gradients, root mean squared error, RMS\nKey Objects: Adagrad, Parameter Updates, Squared Gradients, RMS Error\nRefers to Images: None\nHypothetical Questions:\n- How does Adadelta's decaying average address the limitations of Adagrad's accumulated gradients?\n- Why is it important to consider the units of the parameter update?\n- What is the purpose of calculating the exponentially decaying average of squared parameter updates?\n---\nSummary:\nAdadelta modifies the Adagrad update rule by replacing the diagonal matrix of accumulated squared gradients with a decaying average of past squared gradients, and addresses unit inconsistencies by introducing a decaying average of squared parameter updates.\nOriginal Text:\n<!-- formula-not-decoded -->  \nThe parameter update vector of Adagrad that we derived previously thus takes the form:  \n<!-- formula-not-decoded -->  \nWe now simply replace the diagonal matrix G t with the decaying average over past squared gradients E [ g 2 ] t :  \n<!-- formula-not-decoded -->  \nAs the denominator is just the root mean squared (RMS) error criterion of the gradient, we can replace it with the criterion short-hand:  \n<!-- formula-not-decoded -->  \nThe authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates:  \n<!-- formula-not-decoded -->  \nThe root mean squared error of parameter updates is thus:  \n<!-- formula-not-decoded -->\nContextualized Text:\nBuilding upon Adagrad, Adadelta aims to mitigate its aggressive learning rate by replacing the diagonal matrix of accumulated squared gradients with a decaying average of past squared gradients. To address discrepancies in units between the update and the parameter, an exponentially decaying average of squared parameter updates is introduced. This allows for a more refined adjustment to the model's parameters.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "af81f2c0-d5f6-4045-bfb1-c935cb940555", "summary": "Adadelta modifies the Adagrad update rule by replacing the diagonal matrix of accumulated squared gradients with a decaying average of past squared gradients, and addresses unit inconsistencies by introducing a decaying average of squared parameter updates.", "doc_type": "text", "entities": [], "keywords": ["Adadelta", "Adagrad", "parameter updates", "squared gradients", "root mean squared error", "RMS"], "key_objects": ["Adagrad", "Parameter Updates", "Squared Gradients", "RMS Error"], "contextual_text": "Building upon Adagrad, Adadelta aims to mitigate its aggressive learning rate by replacing the diagonal matrix of accumulated squared gradients with a decaying average of past squared gradients. To address discrepancies in units between the update and the parameter, an exponentially decaying average of squared parameter updates is introduced. This allows for a more refined adjustment to the model's parameters.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.4 Adadelta"}, "hypothetical_questions": ["How does Adadelta's decaying average address the limitations of Adagrad's accumulated gradients?", "Why is it important to consider the units of the parameter update?", "What is the purpose of calculating the exponentially decaying average of squared parameter updates?"]}
9a71a53b-01f1-444c-bb37-d17f7b8dd9e1	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.023782384,0.06461278,0.027021214,0.05146631,-0.036560547,0.071889006,0.041038413,0.01184525,0.032718543,-0.010705943,0.0026696012,-0.01277909,0.022559825,0.0114900805,-0.029665153,-0.0029388366,-0.010304367,0.10027222,-0.041937638,-0.022348884,0.06303161,-0.011127802,0.01616908,0.025169061,-0.008078065,0.051279448,0.024384419,-0.001785509,0.011002427,-0.039517656,0.007830034,-0.052124377,0.017295226,0.07335838,0.0167436,0.020801932,-0.03054325,-0.06340086,-0.053942293,-0.0202845,-0.032690305,0.011646196,-0.036897942,0.024845177,0.001861283,-0.041841228,-0.020499786,-0.00094503176,-0.010484623,-0.02641114,-0.015148584,0.043051913,-0.043622892,0.008009454,-0.054781888,-0.0050046146,0.02851141,0.0075566075,0.006585097,-0.038091373,-0.021876218,-0.045056485,0.0037722236,-0.018479291,0.04083236,-0.008424268,0.0015954326,0.017202294,0.076885715,0.13294095,-0.012948093,0.022920208,0.008790146,-0.10402277,0.14864518,0.04085715,-0.022692002,-0.041192047,-0.02183465,-0.012421475,0.050323587,0.008740455,-0.04650883,-0.026371917,0.08296132,-0.004592506,-0.065272905,-0.02291992,0.033965636,-0.05266538,-0.05765045,-0.07992302,-0.005392568,0.052935608,0.023327516,0.002456309,-0.0026457831,-0.017466003,0.016224306,-0.017355537,0.03743415,-0.03369274,0.05386753,0.0870478,0.006328825,-0.010432978,-0.035270963,-0.017790597,0.01024135,0.017791089,0.028081542,0.017828288,-0.09746578,-0.030220097,-0.06247406,-0.028299598,-0.037991725,-0.0002678193,-0.0018486719,0.011902028,0.04093675,-0.00984879,0.0126469,0.027158992,-0.0042236224,0.027896095,0.030802205,-0.036663976,-0.0320383,0.020622743,0.08064349,-0.011403927,-0.010205139,0.06289333,0.009764535,-0.0023299637,0.043694526,0.0023893523,0.03379187,0.017413827,-0.06834287,-0.001009677,-0.015713952,0.015598907,0.0059484052,0.06456753,-0.04978162,-0.008501597,-0.021168975,0.05951159,0.038565192,-0.02276104,0.073058106,-0.022047482,-0.0040900446,-0.053013537,-0.019455789,0.031688698,0.009628105,-0.024726186,-0.027325131,-0.020180589,-0.00944543,0.05173616,0.0348873,0.079527415,-0.036070097,0.022516156,0.022106854,0.06254058,0.017283991,-0.054484077,0.0071577216,-0.011587788,-0.032150727,-0.0206028,0.023615014,-0.037924044,-0.025241567,0.036386702,0.014595771,0.04117613,-0.018252071,0.012351974,-0.05428918,0.07089068,0.06556933,0.029551491,-0.0009058792,-0.060239453,0.039180793,0.027557468,0.0033667048,-0.0054026484,0.008061536,0.036491975,0.052069053,-0.07847558,-0.037027936,-0.02980663,0.030604942,0.06623444,-0.011631144,-0.0021090172,0.03555026,-0.024900988,0.030780744,-0.0041081375,-0.019732635,0.05660582,-0.0035097844,0.00051313406,-0.04701577,0.055329446,0.031562626,-0.008252954,-0.0037975349,0.019694416,0.04222059,-0.010664983,-0.050656803,0.0019739938,0.035060737,0.06873397,-0.026172698,-0.017349824,-0.00040698354,-0.0048836377,0.03001311,0.012553625,0.038607113,0.06325839,-0.058542803,0.07597207,0.0051102056,0.024814438,0.015245693,-0.017983431,-0.0052761827,0.06249536,0.035797156,0.0715856,-0.01981908,0.08063922,0.04368569,0.008984328,0.03996051,0.056840695,0.014654035,0.050038766,-0.041303344,-0.0096844705,0.0027704854,-0.021788718,0.016558465,0.012987427,0.019522792,-0.006213818,0.048335355,-0.020555522,-0.025475223,0.020003831,0.0018651161,-0.0048463163,-0.014016326,0.01575971,0.045565013,0.036786776,-0.02256122,0.024485953,0.061404556,0.060097765,-0.017303415,-0.023262942,-0.012676278,0.0396878,-0.0020079177,-0.018658668,0.042751387,-0.035882406,-0.010937547,0.029169776,0.023582576,0.030244732,0.0044890703,0.010542464,0.011620278,-0.004369745,-0.051689927,-0.0039321743,0.0024483332,-0.053565223,0.025537975,0.014703661,-0.023745762,0.005915424,-0.009194031,-0.031296905,-0.014602452,0.07583753,0.06605111,0.017807115,0.03845964,-0.0037647495,0.023787899,0.006053253,-0.051045414,0.048198376,0.0012329009,0.05144746,0.00011932424,0.03255686,-0.06376969,0.050647456,-0.0016017572,0.047036048,0.07814315,0.00086694624,-0.0062141973,0.073035896,0.0118568875,0.0047415146,-0.005761199,0.03671354,0.01192838,-0.01985454,0.0100694895,-0.012225519,-0.08696703,0.046598777,-0.05935918,0.0101505015,0.059166763,-0.0046268087,0.011120557,0.009046009,-0.024541669,0.02544886,0.0038547334,-0.032054998,-0.0017683101,-0.0713118,0.03179992,0.032936964,0.0015993065,0.03767659,-0.0122593725,-0.006030306,-0.0045083985,0.058132175,-0.05702432,0.02311332,0.00422453,0.005225322,0.03973103,-0.014508363,-0.01862582,0.030624762,0.05737015,-0.033534113,0.01339559,-0.036146805,0.017972197,-0.058348678,0.06799301,-0.031228984,0.0005164064,-0.020382676,0.011825253,0.0031657687,-0.016701821,-0.016110778,0.029934468,-0.035988186,0.007674767,0.062549606,0.04798392,0.0032342058,-0.014341982,-0.030268772,-0.03490423,0.0458324,0.015482562,-0.015582477,0.0678702,0.032669418,0.0344658,0.08015494,-0.010516709,0.050959725,-0.016644334,-0.015169903,0.02722419,0.010896773,-0.007992738,-0.041008174,0.02300603,0.06193703,0.049613647,-0.003979862,0.019596295,0.03448169,-0.056758396,0.014155033,-0.015774518,0.014498306,-0.038282443,-0.01500172,0.025955204,0.012071595,0.051971506,-0.069120936,-0.061345607,-0.040505055,0.02421506,-0.022622695,0.0033744983,-0.0067266356,-0.036461364,0.0016826122,-0.009286884,0.08740357,0.038932357,0.020435963,0.01674716,0.026068732,-0.039719377,0.022636393,-0.022254094,-0.02510674,-0.0017970329,-0.00911683,-0.016052898,0.046113137,0.064870894,-0.04557003,-0.047694884,0.08117327,0.010811448,0.037289005,0.0012231191,-0.043629788,0.0024336693,-0.0014277961,-0.02038286,0.0023941982,0.03626433,-0.024680762,-0.016846808,-0.026299454,0.050467536,-0.013697102,-0.007962706,0.028121047,0.065703675,0.012990874,-0.032199003,-0.017317738,-0.036942445,0.010969439,0.042719737,0.031467076,-0.0014525437,-0.0036537107,-0.02846078,-0.007689329,0.0012212251,-0.02365198,-0.0112338355,-0.027271343,-0.021732576,-0.026851963,-0.020713497,-0.021007787,-0.04935494,0.02037701,0.012435848,0.02537193,0.022271251,-0.031371932,-0.018956847,-0.030692056,-0.0051162974,0.047285363,-0.012695036,0.0034681654,-0.00196826,-0.02524435,-0.0455279,0.044709604,0.0149921905,0.09570542,0.0082388865,0.0045683933,0.04923986,-0.04906929,0.035198685,0.009479637,0.01592655,-0.014723069,-0.023242462,-0.020030776,0.07252873,0.10113897,-0.014555137,0.018223032,0.048792787,0.06032085,-0.05186797,-0.020517807,0.018821675,-0.0029487112,0.045328688,-0.035135824,-0.03347932,-0.012275811,0.0062782173,-0.042499438,-0.0070052794,-0.066054605,0.003492313,0.01939097,-0.020196147,0.011209909,0.043312095,0.0059420723,-0.04416402,0.02046895,0.046521682,-0.05427326,-0.031269312,-0.010363271,0.0022784383,0.04859785,-0.049286537,-0.05036288,0.00949877,-0.021751653,0.02053794,0.00043743858,-0.009550066,-0.007654592,0.024844939,0.012832775,0.08037828,-0.028587641,0.010567851,0.009569256,0.010445431,0.0754405,0.077180706,0.0074561075,0.003916172,-0.028083205,-0.004097481,0.05952777,0.00127003,-0.015245948,0.025953231,0.02927586,0.0132195335,-0.051664125,-0.02287912,-0.030792866,-0.039813157,-0.015025497,-0.03311882,-0.059740752,-0.015076029,0.0089378925,0.00558657,-0.005247462,-0.01954229,-0.0015819521,-0.018904468,0.047403328,0.035381123,-0.017468816,0.0035683573,0.038251307,0.0034571763,0.024695974,0.06606131,0.030848555,-0.060623385,0.029190471,-0.0043069515,-0.009905016,0.0051905173,-0.021462945,-0.008675901,-0.01170327,-0.03610888,-0.045042194,-0.017486583,0.020132374,0.08627448,-0.028256524,0.036883954,-0.02923701,-0.06011045,0.01340859,-0.012426997,-0.02332376,0.024867741,0.041372504,0.009596331,0.0010552624,-0.055652235,0.017762175,-0.06069826,-0.040154893,-0.036172085,0.030072952,-0.043072857,-0.014635786,0.053404845,-0.05632065,0.007777412,-0.060241435,0.010382525,-0.008161789,-0.04048373,-0.017548874,-0.012285863,0.041439094,-0.026788795,-0.009835201,0.006719416,0.02143834,0.012639684,-0.0012250778,0.06795238,0.011306458,-0.014372764,0.024421267,-0.007751924,-0.04628978,-0.07843297,-0.006427314,-0.0076419553,0.014485758,0.01981534,0.026958143,-0.0206372,0.011603604,-0.017088616,-0.049128328,0.0035052106,0.008061432,0.013544707,0.029745666,-0.008832763,-0.0101800915,-0.0073770736,0.009009894,0.015956422,0.04350606,0.046652373,0.0218749,0.07172579,0.05279265,-0.034946278,-0.08280567,-0.031985022,-0.03820818,0.014833702,0.005585133,0.050502837,0.035406772,-0.063692436,-0.00031350675,-0.020715108,0.00297234,0.024041558,0.0016525214,-0.0369806,0.006499029,-0.028127534,-0.046930954,0.047014218,-0.0056066173,-0.0423151,0.09269554,0.020442545,0.009605624,-0.04434107,0.0062692147,-0.0033916188,-0.03113952,0.03373232,-0.033410296,0.009117173,0.008929439,0.02713835,-0.00096356496,0.03950368,-0.054416087,-0.032988507,-0.01608952,0.030953253,0.047368314,0.028710129,-0.038261272,-0.063785076,0.0459974,0.023608072,-0.015231471,-0.03817369,-0.01362637,-0.036236156,0.0095073115,-0.018201856,-0.022356555,0.016658202,0.024108693,0.007321444,0.005973132,-0.02694839,0.049682442,-0.0016055737,-0.030550992,0.028945249,-0.017204793,0.031743754,0.02952158,0.03341207,0.0139853135,-0.06899927,0.0005847004,0.039291322,-0.003340053,-0.04419586,0.061139185,-0.009550343,-0.007470444,-0.044378716,-0.005801487,0.05975052,-0.031645108,-0.0080906395,0.008572809,0.019873103,0.02161751,0.020882519,-0.047075722,-0.020599198,-0.031284712,-0.05377206,-0.008328125,0.035971075,0.029959893,0.011174874,-0.026884045,0.0064242496,-0.040020842,-0.09193343,-0.00143752,0.054620083,-0.021315001,-0.055071775,-0.060161512,0.001137585,0.043649253,0.029556183,0.021969732,0.002848169,0.0027683934]	Keywords: Adadelta, root mean squared error, parameter updates, learning rate\nKey Objects: parameter updates, learning rate, Adadelta update rule\nRefers to Images: None\nHypothetical Questions:\n- Why is it beneficial to eliminate the need for a default learning rate?\n- How does approximating RMS[  ] t with RMS[  ] t -1 affect the training process?\n- What are the potential advantages of using Adadelta compared to algorithms that require a manually tuned learning rate?\n---\nSummary:\nAdadelta eliminates the need for a default learning rate by approximating the root mean squared error of parameter updates with values from the previous time step.\nOriginal Text:\n<!-- formula-not-decoded -->  \nThe root mean squared error of parameter updates is thus:  \n<!-- formula-not-decoded -->  \nSince RMS [  ] t is unknown, we approximate it with the RMS of parameter updates until the previous time step. Replacing the learning rate  in the previous update rule with RMS [  ] t -1 finally yields the Adadelta update rule:  \n<!-- formula-not-decoded -->  \nWith Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule.\nContextualized Text:\nTo address limitations in previous optimization algorithms, Adadelta approximates the root mean squared error of parameter updates using the root mean squared error from the previous time step. This approach eliminates the need to manually set a default learning rate in the update rule.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "9a71a53b-01f1-444c-bb37-d17f7b8dd9e1", "summary": "Adadelta eliminates the need for a default learning rate by approximating the root mean squared error of parameter updates with values from the previous time step.", "doc_type": "text", "entities": [], "keywords": ["Adadelta", "root mean squared error", "parameter updates", "learning rate"], "key_objects": ["parameter updates", "learning rate", "Adadelta update rule"], "contextual_text": "To address limitations in previous optimization algorithms, Adadelta approximates the root mean squared error of parameter updates using the root mean squared error from the previous time step. This approach eliminates the need to manually set a default learning rate in the update rule.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.4 Adadelta"}, "hypothetical_questions": ["Why is it beneficial to eliminate the need for a default learning rate?", "How does approximating RMS[  ] t with RMS[  ] t -1 affect the training process?", "What are the potential advantages of using Adadelta compared to algorithms that require a manually tuned learning rate?"]}
cd2597c0-fbc1-4e5b-a16a-8eb89e0edc3c	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.036333118,0.051935047,0.009434348,0.05748555,-0.030609263,0.06668041,0.03830081,0.012477577,0.0029261424,-0.0040502786,0.015157611,-0.042534962,-0.00449433,-0.007176679,-0.046581093,-0.029321417,-0.006558241,0.08123906,-0.03724402,0.00034348218,0.041080847,-0.014702932,0.027651202,0.03174736,-0.016010517,0.03597057,0.03350534,0.009928854,0.013736369,-0.030490639,0.022710104,-0.05860626,0.0042777457,0.05220733,-0.027178342,0.0026219168,-0.027100487,-0.042741988,-0.03196199,-0.03749493,0.0036415698,0.014504112,0.007089599,-0.013438609,-0.028042343,-0.026163794,-0.02244784,0.013271096,-0.04276774,-0.016630232,-0.025340455,0.053174764,-0.07715107,0.002233747,-0.06904264,0.0347735,-0.008778818,-0.0038997915,0.00888673,-0.0033458255,0.022248616,0.0058259456,0.0069989767,-0.008230704,0.060460716,-0.02909669,0.0019642485,0.0063611967,0.08321723,0.12565003,-0.048415553,0.018819999,0.021502078,-0.07409319,0.11703873,-0.0018910983,0.03420927,-0.09664403,-0.009200596,-0.0027440803,0.013270347,0.021213664,-0.016477397,-0.014547911,0.064816155,0.008560114,-0.042922847,0.00064819277,0.07662581,-0.060579848,-0.020708801,-0.050892286,0.020013735,0.004574794,0.029664075,-0.014562602,-0.024473993,-0.0010372109,0.025163375,0.019122785,0.020687042,-0.017012702,0.037519984,0.085294746,-0.017994467,-0.035090193,-0.033850145,-0.019159757,0.01165926,0.053007152,0.08566542,0.03750379,-0.06663175,-0.0033662457,-0.05105915,-0.022079762,-0.021945614,4.46724e-05,-0.0022495731,-0.03901922,0.058420807,-0.025498046,-0.026762374,0.08290772,0.034570515,0.052476343,0.01911683,-0.017911047,-0.013986752,0.0044884738,0.037080694,-0.028972842,0.007128646,0.04583235,-0.007829134,-0.026127279,0.015741825,0.01811916,0.021347517,0.0041180025,-0.047488213,-0.008407437,-0.04859119,-0.038467642,-0.0029797596,0.06912727,-0.033491153,-0.054383744,0.002561855,0.089242764,0.015078887,-0.023517467,0.076462075,-0.018486574,-0.036527213,-0.038300056,-0.007832316,0.00024572594,-0.01937697,-0.015036246,0.007556691,-0.007407116,-0.004451038,0.103365235,0.07119123,0.072633944,-0.019820426,0.022472838,0.0313877,0.019343436,-0.011398319,-0.049037695,0.009810968,0.0051434273,0.017435307,-0.026960477,-0.0016090923,-0.056481212,-0.011486513,0.02096569,0.02267124,0.08213303,-0.04166723,-0.0041091265,-0.042683113,0.038443245,0.06834805,0.06191087,0.020508664,-0.040563148,-0.0055464096,0.020901214,-0.03783352,-0.0023444819,-0.014561688,0.021653216,0.044938795,-0.079341225,-0.020683324,0.0031877484,0.02901112,0.036635965,-0.054519814,4.0324107e-05,0.0376723,-0.061998233,-0.00791616,0.013735803,-0.04308336,0.019449597,-0.04774945,-0.006258117,-0.020316124,0.005071139,0.039042126,-0.024960488,0.008232301,0.004571642,0.046357267,0.0020873467,-0.022324469,-0.017419497,-0.007713848,0.09130975,-0.009662217,-0.0027456833,-0.011703417,0.012722004,0.043732584,0.012108855,0.036634743,0.053168785,-0.02122115,0.06090676,-0.004104673,0.015367772,0.021612566,-0.053185437,0.020092884,-0.013409169,0.008881495,0.041184027,-0.011730801,0.0602482,-0.004251207,0.019883445,0.039445166,0.01494594,0.019812005,-0.025496522,-0.041253895,0.0029921345,-0.0010677445,-0.040880352,0.010982969,-0.0077425977,0.0080011515,0.022177631,0.07183254,-0.025819113,0.021932391,0.04563866,-0.013380629,-0.003811301,0.0066461344,0.030367125,0.016019491,-0.0033109726,0.014361562,0.02774848,0.05664509,0.0982166,0.0039200336,0.006873927,0.026936792,0.041764285,-0.01565541,0.006661047,0.08772004,-0.0092556365,0.039021675,0.008882541,0.011547712,0.016913911,-0.0040271874,0.015800418,-0.007273516,0.0017985923,0.005533698,-0.029150438,0.0045094574,-0.05567358,0.063867286,0.037624,-0.04182459,0.03798816,-0.017349152,-0.05460875,-0.020451041,0.052647766,0.061201517,0.014180038,0.0006718575,0.0019682297,-0.030029505,0.018504217,-0.06908929,0.026974829,0.028054744,0.068288974,0.009812062,0.014176752,-0.0136858225,-9.420624e-05,0.0046471125,0.02150718,0.034771405,0.015483283,0.0414075,0.050361138,0.020489646,-0.010478397,-0.012357358,0.06936712,0.011977416,-0.027390849,0.004991715,-0.01271098,-0.08014769,0.019014567,-0.012567425,0.026635116,0.05041368,-0.03403623,-0.020598805,0.020956961,-0.02092552,0.010414325,-0.0031845437,-0.01078941,-0.024440603,-0.059747513,0.049805447,0.0112042045,-0.02051964,-0.01936937,-0.0043617585,-0.0039421683,-0.022615878,0.016091393,-0.08695282,0.055415273,-0.0036934125,-0.01234216,0.040804904,-0.01545107,-0.023222657,-0.02545925,0.014152738,-0.02443763,0.023601081,-0.027413089,0.013466009,-0.07097989,0.10290201,-0.012534275,-0.015791187,-0.029627929,-0.014956538,-0.022019591,-0.017712792,-0.033232015,0.03998411,-0.054525122,0.017503908,0.042535596,0.047371846,0.027676558,0.0067091174,-0.054153528,-0.016182665,0.039809003,0.046921447,-0.0123503385,0.05292962,0.034324612,0.009330663,0.065875605,-0.024899889,0.007449548,-0.023596551,0.028783867,0.027919054,0.044686705,-0.021373175,-0.024089774,-0.005579154,0.090211496,0.034492332,-0.061320797,-0.026112482,0.014018192,-0.031790156,-0.016303563,-0.02584582,0.043914005,0.016843267,0.004062113,0.03545553,0.014882876,0.020221632,-0.04187911,-0.05096168,0.0024446484,0.025399707,-0.008066625,-0.03801796,-0.031241825,-0.029132059,0.011442384,-0.03077424,0.015448495,0.04788947,0.038040474,0.010545023,-0.016309049,-0.056233667,0.032251753,-0.002611504,0.025375864,0.019773442,-0.013866204,-0.030896984,-0.003115328,0.056180358,-0.060928557,-0.04122011,0.0518949,0.0094377445,0.047649283,0.019547105,-0.027388444,0.048239194,0.030700656,-0.009973105,-0.0060512256,0.0040300796,-0.05318209,-0.01906591,0.0061317533,0.036977347,-0.020782318,-0.0493686,0.022237906,0.083253026,-0.0018187992,-0.032725487,-0.011164911,-0.030016772,0.024371384,-0.012494804,0.025383582,-0.015114763,0.023008093,-0.050086346,-0.038250826,-0.013205053,0.0023801273,-0.017635718,-0.051674064,0.012745872,-0.018889792,-0.013707927,-0.0036417362,-0.029099762,0.03226776,-0.0008498998,0.050970342,0.03776593,0.02836784,-0.018727591,-0.03509094,0.010860063,0.052376125,-0.02064105,0.015931372,-0.012507683,-0.015473518,-0.02025286,0.045646686,-0.035576057,0.042502582,0.034102798,0.053551164,0.046754666,-0.0118663255,-0.015420705,-0.012616454,0.043882895,-0.05070325,0.003060654,0.022472464,0.09080256,0.07650153,0.06840762,0.063559815,0.04796855,0.027983043,-0.020686043,0.0443958,-0.0029859769,-0.03125228,0.016330898,-0.0062666424,-0.028831663,0.004619731,-0.019127004,0.0029797938,-0.006825448,-0.028654944,0.0014401093,0.013944077,-0.033723645,0.004758113,-0.0040167803,-0.0021589631,-0.037230283,-0.015712466,0.035187002,-0.08828877,0.03382543,0.00509608,0.033345126,0.04416822,-0.02160692,-0.031757824,0.033763975,-0.041168675,0.035423134,-0.023724992,0.020323165,-0.031756558,-0.0053696046,0.015055917,0.069116235,-0.039220717,0.014725317,-0.012184814,-0.025597475,0.06920418,0.062855035,-0.043383803,0.018282386,-0.062212016,-0.015492132,0.029655261,-0.047640406,-0.0019954606,0.03931931,0.01266323,0.026897622,0.013362272,-0.005783533,-0.04241685,0.0026059628,0.043587234,-0.09126366,-0.08183948,-0.025242422,-0.0058208164,-0.017971965,0.018498037,-0.019016122,0.017818425,-0.045422714,0.05640051,0.014938327,0.008318899,0.017793206,0.0557422,0.029508356,0.0034145121,0.04843849,0.008630165,-0.009778449,0.020876246,0.0038722227,0.04845509,0.012366868,0.014145177,-0.01493367,-0.03686695,-0.04432576,0.017132722,-0.021252872,0.008711067,0.089012854,-0.0433664,0.014633232,-0.064091735,-0.045027535,0.033814363,-0.008877141,-0.0065710847,0.064150594,0.0054651024,0.040675413,0.02509509,-0.06328507,0.05268826,-0.07148408,0.017330483,-0.044591513,0.018485006,-0.08057428,-0.00976125,0.04664374,-0.054851316,-0.0067501203,-0.06429632,0.047946136,0.006508319,-0.06476549,-0.009358643,-0.028094841,0.03725928,-0.014880779,-0.04183306,-0.013683659,-0.0043631303,0.020579798,0.0007924917,0.039598174,0.0011890406,-0.0026766728,0.014398802,0.0052523813,-0.03957684,-0.067892134,0.035976607,-0.030959789,-0.012377341,-0.024178145,0.015451391,-0.0072489325,-0.017750937,-0.01753244,-0.026022675,-0.02034121,0.025391238,0.03691125,0.051527698,0.016608993,0.012131784,0.00782515,-0.01662463,0.006957596,0.02118305,0.064677365,0.01983652,0.055501934,0.035178747,0.013985568,-0.056938745,-0.025000636,0.016484212,0.014289234,0.039953474,0.092642136,0.051409222,-0.054668613,0.018479578,-0.022133984,-0.016541848,-0.0044882186,-0.008612666,-0.061464183,-0.003726404,-0.01599864,-0.0023806135,0.040580977,-0.045732412,-0.017335037,0.06720807,0.041471623,0.002851201,-0.03812792,-0.0061278013,-0.0012847879,-0.058992453,0.041043375,-0.036274813,0.058916125,0.0231765,0.0049430574,-0.039763093,0.00459245,-0.021771502,-0.00468984,-0.021478513,0.00036282558,0.033306632,0.061901357,-0.012675837,-0.028880166,0.06634846,0.013954209,-0.01101472,-0.022178948,0.03603577,-0.025218483,-0.027711289,-0.01254689,-0.020430084,0.02951113,0.028495302,0.03977298,0.009159781,0.009891921,0.03886142,-0.04771187,-0.027455373,0.020570531,-0.002906345,0.039060768,0.02019635,0.021661362,0.031889405,-0.07624587,-0.024675135,0.05979535,0.007374522,-0.060345937,0.050252333,-0.030420875,-0.026753958,-0.00817184,-0.014068593,0.033975907,0.009050375,0.03347809,0.03718938,0.003512092,0.019738203,-0.0029883403,0.012533193,-0.019817926,-0.020189147,0.027127733,-0.016210929,0.05158991,0.032846473,0.064859726,-0.016485777,-0.006761015,-0.024912884,-0.04126432,-0.026283376,0.08557918,-0.017418036,-0.08698641,-0.011656587,0.0064668744,0.04090268,0.020226795,-0.016179055,-0.023177441,-0.0052244673]	Keywords: adaptive learning rate, RMSprop, Adagrad, learning rates\nKey Objects: RMSprop, learning rates, Adagrad\nRefers to Images: None\nHypothetical Questions:\n- What problem does RMSprop aim to solve?\n- How does RMSprop relate to Adagrad and Adadelta?\n- Why is an adaptive learning rate beneficial?\n---\nSummary:\nRMSprop is an adaptive learning rate method proposed by Geoff Hinton to address the issues with Adagrad's decreasing learning rates.\nOriginal Text:\n### 4.5 RMSprop  \nRMSprop is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class 12 .  \nRMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad's radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta that we derived above:  \n<!-- formula-not-decoded -->  \nRMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests  to be set to 0 . 9 , while a good default value for the learning rate  is 0 . 001 .\nContextualized Text:\nRMSprop is an adaptive learning rate method, proposed by Geoff Hinton in a Coursera lecture. It was developed alongside Adadelta to resolve the problem of Adagrads tendency to have drastically diminishing learning rates.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "cd2597c0-fbc1-4e5b-a16a-8eb89e0edc3c", "summary": "RMSprop is an adaptive learning rate method proposed by Geoff Hinton to address the issues with Adagrad's decreasing learning rates.", "doc_type": "text", "entities": ["Geoff Hinton", "Coursera"], "keywords": ["adaptive learning rate", "RMSprop", "Adagrad", "learning rates"], "key_objects": ["RMSprop", "learning rates", "Adagrad"], "contextual_text": "RMSprop is an adaptive learning rate method, proposed by Geoff Hinton in a Coursera lecture. It was developed alongside Adadelta to resolve the problem of Adagrads tendency to have drastically diminishing learning rates.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.5 RMSprop"}, "hypothetical_questions": ["What problem does RMSprop aim to solve?", "How does RMSprop relate to Adagrad and Adadelta?", "Why is an adaptive learning rate beneficial?"]}
7c01e6d6-3fa8-4328-add8-9e3840bb8a71	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01839817,0.0039765136,0.0002708411,0.015298214,-0.042863663,0.09282115,0.039796416,0.03982123,0.026462952,-0.052651223,-0.019296536,-0.039497145,-0.011857928,0.011782642,-0.06658967,0.0043640975,-0.01936151,0.08540526,-0.03526209,-0.018062394,0.01889761,-0.016140902,0.03936931,0.010378791,0.02885834,0.054049928,0.03802006,-0.0039364006,-0.0024613815,0.02236441,-0.01554826,-0.09341806,0.031181866,0.049090594,-0.02154168,0.022496868,-0.0147942575,-0.04887366,-0.032580744,-0.06698866,-0.05284439,0.028260095,0.0037172448,-0.012617048,0.012340003,-0.022731649,-0.035287313,-0.019010982,-0.0067538586,-0.0138794985,-0.01669581,0.02513265,-0.05128826,0.03555733,-0.09435169,0.045741808,-0.017652752,-0.020341534,-0.0177983,-0.016325604,-0.02087139,-0.017126568,0.018184109,-0.00074823585,0.05658076,-0.0428882,-0.019567672,0.010211111,0.035673242,0.10678042,-0.028093586,0.021217883,0.054280274,-0.11631791,0.11683954,0.037607778,-0.0018332339,-0.047579855,-0.027281428,-0.0120471185,-0.0030906936,0.04347134,-0.039768454,-0.04215725,0.07901174,-0.021064157,-0.04796574,-0.01896929,0.117479675,-0.035845496,-0.0011431036,-0.046216298,-0.02383934,0.03244534,0.023965161,-0.031204022,-4.0316696e-05,-0.0193407,0.10296605,-0.0095496,-0.0069554877,0.0032236187,0.0487941,0.096079595,0.02051591,-0.051607568,-0.030495847,-0.00045680342,0.010135873,0.024595762,0.05059294,0.027271392,-0.06926708,0.0064177834,-0.07062586,-0.046252932,0.011701974,0.051088806,0.010953235,-0.01389277,0.05319049,-0.04293195,-0.014199026,0.041711375,0.022696866,0.03164261,0.06872645,-0.05605953,-0.048548505,0.015821218,0.05817816,-0.0018235536,-0.018845668,0.028198725,0.010180056,-0.046059117,0.005856975,-0.018138517,0.0016320697,0.032751247,-0.022090195,-0.028820869,-0.0379195,0.0054676775,-0.04430656,0.08412151,-0.0547473,-0.028902436,-0.0251732,0.103941515,-0.016451543,-0.004396119,0.048738528,-0.03513783,-0.047241617,-0.032489654,-0.01101438,-0.009295408,-0.00749035,-0.01720963,-0.0048240377,-0.026139263,0.00069054635,0.07735957,0.043951366,0.08199277,0.028378854,0.009683732,-0.016463708,-0.006479386,-0.015180138,-0.03267632,0.026166048,-0.03245852,-0.02514521,-0.07479885,-0.010839922,-0.0014863745,-0.02354071,-0.01588823,0.0239456,0.025082802,-0.03647371,0.021076689,-0.04260665,0.03305542,0.04586625,0.030435191,0.056689795,-0.05105913,0.00622864,-0.005438037,-0.016041731,-0.006393861,0.01238339,0.021048421,0.04405827,-0.057481308,-0.019620074,-0.06507741,-0.008847426,0.037295878,-0.045508165,-0.024319747,0.09085998,-0.031654894,0.0021348328,-0.017385544,-0.045673784,0.04431814,-0.033548262,-0.027697463,-0.014887654,0.024314566,0.041967012,-0.012950574,0.00018833885,0.014826411,0.054905813,-0.021852998,-0.008128399,0.010093458,-0.037480116,0.075824134,-0.04743974,-0.0008680344,0.004520756,0.0319222,0.040501945,-0.009699286,0.035689596,0.010939971,-0.037188873,0.045229726,-0.005573034,0.05895795,0.034388527,0.00026350433,0.0044669244,0.021182563,0.028123986,0.025503589,-0.0043140557,0.06838499,-0.030290969,0.0056887735,0.022145374,0.048031483,0.0061522434,-0.014666666,-0.009264012,0.0046343617,-0.007903967,-0.038542252,-0.02486825,-0.025913045,0.009679753,0.010029796,0.07911913,-0.022875609,0.021260103,0.005476208,-0.0037962107,0.015747271,-0.029269049,0.036136363,0.022393575,0.05561454,0.030898692,0.022924507,0.022962352,0.073804386,-0.047242597,-0.012821124,-0.030819913,0.0357032,-0.017931208,-0.013976895,0.084576786,0.0121640675,0.043017313,-0.021977985,-0.004521694,0.021531304,0.021479454,0.00015642376,-0.017630855,-0.0007027682,-0.0066634016,0.009328122,0.016107788,-0.044681847,0.038307257,0.024859298,-0.051234517,0.020056535,-0.009513549,-0.07638603,-0.0053158407,0.0669981,0.070154555,-0.006538259,0.02871999,-0.038791317,0.00024209995,0.012059937,-0.08504668,0.054419104,0.007556992,0.044742733,0.025021242,0.0029053413,-0.0066265757,0.024097731,0.021753225,0.013938183,0.052710086,0.029370967,0.020037029,0.048347116,-0.00021028021,-0.044651065,-0.03620512,0.05904754,0.0010906153,0.015222202,-0.03169103,-0.033549894,-0.07406196,0.04986779,-0.05459752,0.02762387,0.060762867,0.024648719,0.0013035944,0.020050967,-0.02528632,0.028604355,0.008418163,0.0051152552,-0.003052179,-0.078703955,0.018287443,0.0007058369,-0.009704442,-0.0014289715,0.008526017,-0.0077487603,-0.017170478,0.06409272,-0.023102747,0.059664745,0.01579612,-0.013642498,0.014940352,0.021192746,-0.027848266,-0.009539085,0.03933337,-0.045699134,-0.013282467,-0.030116888,0.044697754,-0.07110803,0.07980902,0.0212774,-0.05799046,-0.034077358,0.017112838,0.013611282,0.009736671,-0.01996752,-0.007348905,-0.060260862,0.03177421,0.05612655,0.043898847,0.022208061,-0.013026656,-0.042203408,-0.040818892,0.0324931,0.037683845,-0.019758973,0.050863396,0.036332507,-0.019429632,0.079626,-0.0112118805,0.0075087124,0.02025196,0.038366158,0.019715386,-0.01425139,-0.007277912,-0.023582572,-0.014099931,0.059453424,0.014039671,-0.0007526703,0.019152027,0.011367745,-0.047881957,0.011667508,-0.023705479,0.03726982,-0.056570552,-0.010997084,0.02432317,0.01639834,0.041468162,-0.041303173,-0.051684115,-0.018896997,0.0054528406,-0.036697146,-0.017364506,-0.004602741,-0.041315556,-0.0052422225,-0.010011688,0.013965883,0.046735268,0.008291203,0.025156971,-0.00849866,-0.053342972,0.035757307,0.0070443717,0.032785594,0.0113571845,-0.0082592815,-0.04536627,0.01747717,0.022803476,-0.014705021,-0.021663938,0.04125022,0.011954589,0.04503228,0.043366525,-0.016420074,0.028201947,0.018167797,-0.034982305,0.0068981214,0.060677268,-0.053271774,-0.027343323,0.020219482,0.0288082,-4.3459255e-05,-0.040167842,0.0046178517,0.05546993,-0.0019210113,-0.022835497,-0.030181604,-0.0425412,0.027326435,-0.02982379,0.07071001,0.014299831,-0.0009747039,-0.021742286,-0.008705225,-0.01241461,0.00801114,-0.011764793,-0.008130332,-0.005149654,0.014738296,-0.030211682,-0.021675384,0.014020817,0.043534007,0.006029057,0.01382674,0.089849286,0.012471922,-0.00903263,-0.009961926,0.0044687944,0.06804359,0.007401416,-0.028948328,0.02985429,0.0075808465,-0.044450976,0.020112475,-0.017966429,0.025005175,0.001194493,0.022974271,0.029926958,-0.006790233,-0.0211039,-0.0046075424,0.017947882,-0.03395336,-0.032245908,-0.0069745253,0.031007849,0.025458325,0.06471526,0.04790714,0.048278607,0.051177207,-0.022344252,0.06309985,0.001548657,-0.0034058904,0.02385915,-0.042723987,-0.025636718,0.005446164,0.040250298,-0.038467836,-0.028416978,-0.037535895,0.024998112,0.01265049,-0.018297644,0.005653999,0.009827741,-0.0067801615,-0.026736105,-0.01996297,0.04398427,-0.06765758,0.03492502,-0.006089328,-0.0088415295,0.03793923,0.008809916,-0.0043550567,-0.01430347,-0.033929214,0.004595547,-0.028369669,0.016611153,-0.04378028,0.025578836,-0.007748771,0.07799622,-0.08177699,0.045801546,-0.011838672,-0.027168505,0.058773395,0.04789183,0.0026297257,0.025868783,-0.03271924,-0.036823757,0.03749024,0.00558184,0.034159753,0.029461479,0.052882634,0.045156717,0.02389172,0.020162867,-0.067342885,-0.0053132554,0.02843671,-0.040525034,-0.05451684,-0.049852878,-0.027239036,0.010654382,0.023720672,0.0026838277,0.023034519,-0.034797203,0.07199052,0.030249065,-0.017041638,-0.0013945098,0.049049452,0.054175645,0.022135075,0.049316004,0.0028262252,-0.009483234,0.0116101615,-0.02586664,0.06400112,0.021497944,-0.027619671,0.019844685,-0.03147997,-0.012179549,-0.056728244,-0.035673868,-0.019756978,0.096873015,-0.02622417,0.023758585,-0.052553706,-0.07796986,0.010769746,-0.02836497,-0.023774378,0.050788958,0.006545642,0.031720106,-0.014332524,-0.03173209,0.04440071,-0.045596305,-0.006120101,-0.04224795,0.042113114,-0.079442374,-0.03235432,0.0016829524,-0.042999547,-0.010292728,-0.059062872,0.008229292,-0.025704723,-0.033178847,-0.023698041,-0.063552305,-0.011465679,-0.018764103,0.0046606916,-0.04836536,0.0055323746,0.0047886954,0.009130891,0.052721992,-0.0061918446,0.03681249,0.02220958,-0.014619991,-0.033297934,-0.07015344,0.038399346,-0.023685195,-0.035997555,-0.029623158,-0.010295874,0.015431856,-0.006104373,0.01560065,-0.052938346,-0.005386572,-0.020630494,0.00030572375,0.04703451,0.02865848,-0.016839493,0.007391413,-0.02956907,-0.0084675,0.025592655,0.04425235,0.04595935,0.06638247,0.102977835,0.024133263,-0.024268841,-0.030796958,0.027415438,0.02624318,0.019148536,0.07451984,0.06066237,-0.05181808,-0.0042843423,-0.03717721,-0.003687849,0.004609547,-0.00119544,-0.03704812,0.029462242,-0.009367012,0.009425682,0.05820698,-0.0033557473,-0.008901223,0.07237332,-0.007268189,-0.02065265,-0.06490174,-0.019748861,-0.016696464,-0.02557143,0.017763602,-0.029666685,0.029778743,-0.0076755644,0.010638645,0.013001037,-0.02067141,-0.038808633,-0.030411111,0.022297727,0.028560493,-0.0011918177,0.009459722,-0.055207882,-0.066000775,0.07408083,0.02894878,-0.010838567,0.0048022526,0.0177796,-0.024099674,0.019873552,0.016568456,-0.029788084,0.045132287,0.009591431,0.008125866,0.021528337,0.026420351,0.025328757,-0.047637243,-0.0072738156,0.014151198,0.021074114,0.0016027475,0.030181298,-0.0026754946,0.02197429,-0.038994785,-0.01200124,0.043421518,0.033926718,0.0024265517,0.020855062,-0.008551796,0.046905756,-0.018519249,0.004229434,0.04392008,-0.01450745,0.0050469777,-0.0012089973,0.009823491,0.036642976,0.018455599,-0.038915467,-0.050784588,-0.033230454,0.011920712,-0.015873453,0.033080056,0.0027695172,-0.029349104,-0.022533324,-0.038691062,-0.032643322,-0.042716216,0.030855695,0.033363,-0.008935479,-0.09287076,-0.024532793,-0.011548914,0.071706325,0.0213862,-0.008465947,-0.006496571,0.006148135]	Keywords: adaptive learning rates, Adam, exponentially decaying average, gradients, momentum\nKey Objects: Adaptive Moment Estimation, gradients, decay rates\nRefers to Images: None\nHypothetical Questions:\n- How does Adam's approach to adaptive learning rates differ from Adadelta and RMSprop?\n- Why are the initial values of m t and v t often biased towards zero?\n- What are the roles of  1 and  2 in the Adam optimization process?\n---\nSummary:\nAdaptive Moment Estimation (Adam) calculates adaptive learning rates for each parameter by storing exponentially decaying averages of past squared gradients and past gradients, similar to momentum.\nOriginal Text:\n### 4.6 Adam  \nAdaptive Moment Estimation (Adam) [10] is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients v t like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients m t , similar to momentum:  \n<!-- formula-not-decoded -->  \nm t and v t are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As m t and v t are initialized as vectors of 0 's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e.  1 and  2 are close to 1 ).  \nThey counteract these biases by computing bias-corrected first and second moment estimates:  \n<!-- formula-not-decoded -->  \n12 http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture\\_slides\\_lec6.pdf\nContextualized Text:\nAdam is an optimization method that computes adaptive learning rates for each parameter. Like Adadelta and RMSprop, it stores an exponentially decaying average of past squared gradients (v t ). Additionally, similar to momentum, Adam keeps an exponentially decaying average of past gradients (m t ). These averages (m t and v t ) are estimates of the first and second moments of the gradients, respectively.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "7c01e6d6-3fa8-4328-add8-9e3840bb8a71", "summary": "Adaptive Moment Estimation (Adam) calculates adaptive learning rates for each parameter by storing exponentially decaying averages of past squared gradients and past gradients, similar to momentum.", "doc_type": "text", "entities": ["Adam"], "keywords": ["adaptive learning rates", "Adam", "exponentially decaying average", "gradients", "momentum"], "key_objects": ["Adaptive Moment Estimation", "gradients", "decay rates"], "contextual_text": "Adam is an optimization method that computes adaptive learning rates for each parameter. Like Adadelta and RMSprop, it stores an exponentially decaying average of past squared gradients (v t ). Additionally, similar to momentum, Adam keeps an exponentially decaying average of past gradients (m t ). These averages (m t and v t ) are estimates of the first and second moments of the gradients, respectively.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.6 Adam"}, "hypothetical_questions": ["How does Adam's approach to adaptive learning rates differ from Adadelta and RMSprop?", "Why are the initial values of m t and v t often biased towards zero?", "What are the roles of  1 and  2 in the Adam optimization process?"]}
b7e3d153-6f60-4e60-abb7-3b86827a2d47	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.032470882,-0.011483527,0.004276333,0.023522906,-0.039380908,0.10234055,0.06103371,0.023445439,0.022989608,-0.057966866,-0.02149276,-0.038328957,-0.005127469,0.02780569,-0.06668153,-0.008452723,-0.021095488,0.08358668,-0.052623287,-0.015116435,0.016590204,-0.024122717,0.056587376,-0.0013144242,0.0014215657,0.05073033,0.044984773,-0.000703022,-0.01073813,-0.011159867,-0.0074341064,-0.10867912,0.02302295,0.08252563,-0.011195105,0.013569234,-0.010585583,-0.05856583,-0.045847703,-0.044813175,-0.06985207,0.026400562,0.008509665,-0.021430468,-0.004145736,-0.01666841,-0.038523328,-0.03212845,-0.02026746,-0.008132271,-0.018565748,0.017528944,-0.047402672,0.037231505,-0.08097466,0.040234942,0.017121376,-0.012954338,-0.01177787,-0.042815167,-0.009336819,-0.013724071,0.027619893,0.0023498468,0.04757466,-0.035212945,-0.005778025,0.0021967036,0.037923627,0.11234016,-0.04782692,0.044354253,0.046758614,-0.0658212,0.10665069,0.037745375,0.0015771997,-0.04293402,-0.021210456,-0.0241364,-0.0014919819,0.028706606,-0.018278947,-0.05019926,0.06429894,-0.01585021,-0.04917949,-0.04481541,0.108169064,-0.04511806,-0.0108841825,-0.048887625,-0.021025494,0.0063814097,0.015447712,-0.0048506856,-0.02913103,-0.014291735,0.09433609,-0.011259368,0.00023565527,0.026440904,0.03479352,0.09616066,0.013397606,-0.06085417,-0.028328052,0.0020907451,0.01683137,0.008339151,0.046418633,0.043709636,-0.08061183,-0.024634924,-0.06583257,-0.053178374,0.0153494,0.07802599,0.0073059318,-0.010931841,0.037550412,-0.022045774,-0.00955143,0.07235923,0.004892564,0.035613008,0.05803347,-0.056160923,-0.042311322,0.018088708,0.06938376,-0.0032248285,0.0025567117,0.0375507,0.0013084136,-0.05036349,0.011742432,-0.032636758,0.0048431186,0.02995593,-0.030846914,-0.036287434,-0.045285013,0.0073125977,-0.053942505,0.07628083,-0.059675474,-0.016906232,-0.023238365,0.07875937,0.011788739,-0.032995734,0.028329879,-0.070915736,-0.04313616,-0.022716263,-0.015188107,-0.0010393343,-0.0047022374,-0.020299694,-0.021523401,-0.018449636,-0.008440652,0.046594214,0.019542424,0.06623168,0.047383502,-0.0108374255,-0.016675694,-0.0068173907,-0.02352381,-0.027245663,0.026087802,-0.032869954,0.006896711,-0.06439514,-0.014226836,0.012549491,0.0010922493,0.004102799,0.039066765,0.025542658,-0.0361032,0.01640532,-0.04259837,0.015842063,0.048767854,0.04900816,0.03467783,-0.06106607,0.006624731,-0.0063536996,-0.02106158,-0.015228881,0.02789381,0.044050854,0.048009984,-0.06583122,-0.005635172,-0.038235392,-0.0152939875,0.030840833,-0.06634012,-0.011596511,0.07787487,-0.0328208,-0.0023921744,0.014769698,-0.053794395,0.04022062,-0.037124947,0.0060294564,-0.012937103,0.011354176,0.029541692,-0.009729517,-0.017380917,0.034055706,0.062655695,-0.013531306,0.024260068,0.004891802,-0.02124476,0.0627222,-0.0501995,0.0042368276,-0.0039800443,0.032953914,0.029355641,-0.004033087,0.008711572,0.015181133,-0.02558016,0.087512285,-0.021717317,0.017991625,0.021054363,0.020811126,0.0015408068,0.0042771627,0.03719452,0.030227888,0.012548832,0.07028712,-0.032190435,0.013405321,0.02162402,0.030651568,0.01255556,-0.009960423,-0.035835125,0.00044676347,-0.016848713,-0.02470605,-0.008502909,-0.04232691,0.007980052,0.027489265,0.08946491,-0.022005694,0.013302817,0.021645986,-0.014769394,0.018105272,-0.025253335,0.023737688,0.009035072,0.017490588,0.002446036,0.028212981,0.036565714,0.040005565,-0.038074315,-0.032553878,-0.010460813,0.05385535,-0.05151106,-0.013601669,0.058790702,-0.0055771684,0.016552106,0.024276342,-0.008841574,0.023623407,0.03880554,0.028594324,-0.025682813,0.0035841023,-0.0138477385,0.016588109,0.012350552,-0.03757738,0.024918122,0.021874761,-0.04527904,0.026015468,-0.007985628,-0.032396547,-0.013889787,0.069543615,0.07765669,0.00706121,0.007048043,-0.031770352,0.008851227,0.03392537,-0.055935066,0.050292395,-0.0161954,0.056628928,0.021762894,-0.010911635,0.0023843057,0.029713418,0.034721423,0.006479195,0.06511242,0.03567384,0.025583386,0.04973963,0.014120397,-0.031070927,-0.039763812,0.07113825,-0.008781898,-0.0041201646,-0.02270625,-0.019012576,-0.09895756,0.034788713,-0.06287792,0.017800298,0.05658706,0.019804971,-0.018608563,0.008345679,-0.013052211,0.024376387,0.026016265,0.01359833,-0.02143449,-0.07414991,0.032019727,0.03009283,-0.041989833,0.0030674355,-0.014859442,-0.0016301975,-0.01887282,0.06692211,-0.03624759,0.039922204,0.008472404,-0.00096930313,0.055640507,0.008589751,-0.03389937,-0.007394677,0.03480593,-0.0757741,-0.00517867,-0.040829755,0.028513195,-0.06949007,0.06698202,0.01906304,-0.046383712,-0.017590525,0.01729808,0.009717131,0.019174213,-0.02618732,0.01621693,-0.043765217,0.029562652,0.04720577,0.05005541,0.0014181428,-0.047348693,-0.049101472,-0.03545056,0.018367475,0.033417836,-0.0027181536,0.049810242,0.056391757,-0.016366104,0.084090024,-0.0075367545,0.008817825,0.009842492,0.033259172,0.009616129,-0.015185776,4.729558e-05,-0.043225117,0.014944524,0.0672285,0.021844128,0.02192435,0.018092303,0.019742839,-0.045413334,0.034462597,-0.025803348,0.033356175,-0.051871873,0.007543992,0.03340232,0.010774728,-0.007583275,-0.047351208,-0.054450616,-0.032366905,0.01721797,-0.027627375,-0.03436263,-0.025740663,-0.027856525,-0.008550268,-0.013096792,0.0310061,0.046594515,0.0059731654,0.0185867,-0.011423078,-0.0790953,0.048304364,0.002916383,-0.0061653815,0.004971917,-0.025070023,-0.027497368,0.023298834,0.019060997,-0.020515366,-0.043794084,0.036299236,0.008542628,0.04037102,0.019063186,-0.037058443,0.028175151,0.007683053,-0.006492361,0.0046197167,0.057882097,-0.031479593,-0.025140343,0.024668582,0.026419075,0.01259718,-0.033002008,0.0023541553,0.05564561,-0.005574521,-0.0153839905,-0.022480322,-0.06279668,0.031032685,0.0023060094,0.06233826,0.017557867,0.012379051,-0.037363954,-0.02589384,-0.014978111,-0.0047321087,-0.015647136,-0.011646609,0.0026351463,-0.036622647,-0.000604249,-0.0112824645,-0.00039584047,0.029072046,0.010751263,0.016220748,0.06535414,0.023914661,-0.019421222,-0.003941321,-0.0054073287,0.069606096,-0.0047601815,-0.012761123,0.019281808,-0.0054127816,-0.052087504,0.015218368,-0.014027696,0.03110308,0.021025587,0.031769328,0.02906825,-0.016149392,-0.00028784163,0.0020190838,0.00033519158,-0.019636804,-0.03701909,0.0063071423,-0.001990146,0.029022254,0.057309825,0.07278476,0.04341656,0.03156862,-0.02866258,0.044629302,0.0037339223,-0.01999815,0.008245595,-0.03858251,-0.018677462,-0.011899037,0.012544886,-0.029995305,-0.019092232,-0.01286886,0.0147676505,0.039009485,0.001966443,-0.0095885815,0.011388404,0.0015337695,-0.03742249,-0.017132018,0.023649719,-0.09145866,-0.0013557547,-0.014317019,-0.0006839455,0.042174406,0.0075060152,-0.000357363,0.008245696,-0.035322227,-0.03135868,-0.0140570635,-0.009651747,-0.054966383,0.028765364,0.00030364055,0.07310431,-0.10644559,0.014219751,-0.0081047565,-0.0021126256,0.06153437,0.045417305,-0.018053578,0.0043248995,0.015647925,-0.03661514,0.034061342,-0.012729924,0.048014868,0.03865589,0.037804116,0.034448102,0.009525588,-0.026819583,-0.050106604,0.0020145408,0.0126952855,-0.05758779,-0.053315416,-0.061979316,-0.030120376,0.027522806,0.005095756,0.02042249,0.0052085025,0.0046282494,0.056160823,0.01581009,0.0019661526,0.030240593,0.08346972,0.056302484,0.02819775,0.059086252,0.012298559,-0.02562775,-0.0054157316,-0.037137944,0.055526868,0.0128035275,-0.03591223,0.011791846,-0.015448975,-0.021771336,-0.033091318,-0.03201833,0.001959641,0.081337646,-0.030686859,0.044051025,-0.06589693,-0.0795819,-0.004812686,-0.048616167,-0.05144422,0.03992802,-0.008937301,0.030165888,0.006584733,-0.02632856,0.044224955,-0.053982392,0.0050667766,-0.020751972,0.028189437,-0.07102512,-0.04523535,0.007899053,-0.03154333,-0.01553751,-0.0782238,0.0062708096,-0.0328232,-0.079213545,-0.03246235,-0.05360033,-0.01560587,-0.045742333,0.008785309,-0.03164516,0.0019063044,0.00744053,-0.00036363033,0.07055051,0.015691936,0.013245081,0.03049604,0.0009173619,-0.04609227,-0.08481339,0.025993517,0.0027261598,-0.030701268,-0.029431904,0.033898085,0.015263665,0.007884727,-0.011793818,-0.03596897,-0.020348586,-0.011338911,0.013253942,0.03637125,0.0063996003,-0.005228314,0.0056839697,-0.011629071,0.0022495615,0.03003558,0.04128428,0.019842066,0.050272565,0.073652804,0.035364073,-0.033508852,-0.02476329,0.010694173,0.02700678,0.025350364,0.07815996,0.061920088,-0.05396312,-0.020127252,-0.029037546,-0.017556926,6.238639e-05,0.0057788957,0.0042601707,0.0040095076,-0.03162519,0.009423257,0.035343133,0.0072134864,0.00050185755,0.05638302,-0.003489456,-0.041009586,-0.06801443,-0.024268068,0.0027843055,-0.018074678,0.023830919,-0.046256028,0.039832473,-0.012183688,0.0039271982,-0.008805463,-0.0049327724,-0.056265038,-0.028436674,0.03072479,0.022514908,0.009967278,0.00952079,-0.051275097,-0.062319893,0.06804581,0.023768295,-0.006429935,-0.04854428,0.0027595516,-0.034353655,-0.015054583,-0.025888503,-0.056957047,0.04602654,0.023887428,0.0026134439,-0.0037133943,0.0093942145,0.05485804,-0.03389575,-0.016924903,0.034776356,0.0018944297,-0.012997113,0.039898645,0.010440154,0.012947984,-0.04853389,-0.006882117,0.0513291,0.051196925,0.018761592,0.052065346,-0.02287513,0.012805483,-0.012873462,-0.03306987,0.026478088,-0.020218303,0.016681397,0.0010575856,0.033330746,0.031031245,0.027609507,-0.060803954,-0.04292291,-0.066192314,0.011907171,-0.0055632787,0.051894967,-0.01656511,0.00044588113,-0.02815654,-0.026363231,-0.03963618,-0.05421088,0.020771496,0.025358187,-0.026677174,-0.067431375,-0.039752893,-0.010873067,0.056100268,0.0041298377,9.012506e-05,-0.0090873,-0.009182528]	Keywords: adaptive learning rates, parameter updates, bias correction, empirical performance\nKey Objects: parameters, gradients, estimates\nRefers to Images: None\nHypothetical Questions:\n- How does Adam's use of bias correction improve upon previous adaptive learning methods?\n- What is the significance of the default values proposed for  1 ,  2, and epsilon?\n- In what scenarios might Adam be preferred over other adaptive learning algorithms?\n---\nSummary:\nAdam utilizes bias-corrected estimates of past gradients and squared gradients to update parameters, similar to Adadelta and RMSprop, and demonstrates strong empirical performance compared to other adaptive learning methods.\nOriginal Text:\n<!-- formula-not-decoded -->  \n12 http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture\\_slides\\_lec6.pdf  \nThey then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule:  \n<!-- formula-not-decoded -->  \nThe authors propose default values of 0 . 9 for  1 , 0 . 999 for  2 , and 10 -8 for glyph[epsilon1] . They show empirically that Adam works well in practice and compares favorably to other adaptive learning-method algorithms.\nContextualized Text:\nAdaptive Moment Estimation (Adam) computes adaptive learning rates for each parameter by storing exponentially decaying averages of past gradients. Adam uses bias-corrected estimates of past gradients and squared gradients to update parameters, similar to Adadelta and RMSprop. The authors propose default values for parameters like  1 and  2, and demonstrate empirically that Adam performs well compared to other adaptive learning methods.	{"tags": ["optimization", "deep-learning", "algorithms"], "doc_id": "b7e3d153-6f60-4e60-abb7-3b86827a2d47", "summary": "Adam utilizes bias-corrected estimates of past gradients and squared gradients to update parameters, similar to Adadelta and RMSprop, and demonstrates strong empirical performance compared to other adaptive learning methods.", "doc_type": "text", "entities": ["Adam", "Adadelta", "RMSprop"], "keywords": ["adaptive learning rates", "parameter updates", "bias correction", "empirical performance"], "key_objects": ["parameters", "gradients", "estimates"], "contextual_text": "Adaptive Moment Estimation (Adam) computes adaptive learning rates for each parameter by storing exponentially decaying averages of past gradients. Adam uses bias-corrected estimates of past gradients and squared gradients to update parameters, similar to Adadelta and RMSprop. The authors propose default values for parameters like  1 and  2, and demonstrate empirically that Adam performs well compared to other adaptive learning methods.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.6 Adam"}, "hypothetical_questions": ["How does Adam's use of bias correction improve upon previous adaptive learning methods?", "What is the significance of the default values proposed for  1 ,  2, and epsilon?", "In what scenarios might Adam be preferred over other adaptive learning algorithms?"]}
741c1f63-2f48-4b1b-85be-2948173649bd	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03680706,0.020645281,0.027387736,0.028201398,-0.02898352,0.07919659,0.020612273,0.043455504,-0.014609979,-0.058897667,0.0020722048,-0.04383649,-0.00646696,0.024822053,-0.04894956,0.0010223516,-0.041521594,0.08008948,-0.009463445,-0.01905344,0.047623016,-0.018931797,0.040299214,0.01583193,-0.009565951,0.069420464,0.036264226,0.041215885,-0.026881037,0.017792478,-0.021659259,-0.044507865,0.016200686,0.059823737,-0.0023717557,-0.0048899176,-0.056553025,-0.047323283,-0.014534843,-0.07270788,-0.051237416,0.013808895,-0.034422077,-0.005289619,-0.0034223571,-0.027750306,-0.0788464,-0.059143554,-0.016029371,-0.022056384,-0.04802455,0.018592894,-0.018114716,-0.002554822,-0.028218683,0.022851078,0.006703744,0.011506729,-0.00013274328,-0.059389513,-0.03275958,0.0015910727,-0.012890789,0.014242883,0.049644552,-0.00634816,-0.0031996253,0.062599264,0.060870346,0.13405442,0.0039445665,0.03539418,0.028708538,-0.103654176,0.12732351,0.040542655,-0.023514476,-0.034320988,-0.021289768,-0.045126006,0.06351166,-0.020258358,-0.037953466,-0.03210925,0.09158616,0.04243481,-0.032771606,-0.07666163,0.064682536,-0.038958967,0.0080461865,-0.051623985,-0.010337443,0.033120535,0.038785275,0.021310817,-0.0057510673,-0.017561661,0.018622728,-0.023608332,0.029547146,0.006570143,0.042926177,0.09510857,0.0028732037,-0.04411795,-0.044383213,-0.0055731046,-0.0022338582,0.0013610347,0.008577969,0.06466224,-0.048852824,-0.0087457765,0.0029466194,-0.038157117,-0.010958465,0.015765848,0.005384313,-0.033056192,0.031804107,-0.027515186,0.022209305,-0.005192802,0.037493736,0.022794457,-0.01890013,-0.023436878,-0.06769089,0.019874314,0.06616972,-0.01285674,0.007764341,-0.017339695,-0.0018281456,0.020083215,0.024785584,-0.02263022,-0.019356402,0.032880593,-0.06726699,0.0022057917,-0.016741898,0.018241128,-0.00011421452,0.07556081,-0.044449925,-0.018691786,-0.020391613,0.080850355,0.044902377,-0.028124854,0.054707017,-0.010602961,-0.04568273,0.031576812,-0.022956612,-0.015769541,0.0133632645,-0.0448816,0.007914007,0.022116544,-0.04263202,0.038436677,0.07253383,0.06250123,-0.0034632995,0.0007427559,0.0062715565,0.0038678367,-0.044417378,-0.051905967,0.0022137547,0.04616384,-0.024296662,-0.06027944,-0.028173741,-0.017450504,-0.001305726,0.048362162,0.014123177,0.030138316,0.027189953,-0.009995619,-0.028009187,0.049478553,0.017055396,0.022693677,0.027253956,-0.05321642,0.049974233,-0.022761973,-0.016663022,-0.0046780915,0.030801235,0.045332763,0.053350084,-0.00521909,-0.056231897,-0.02748577,0.025693014,0.03783939,-0.026764698,-0.033422746,0.045638982,-0.0027714365,-0.012882949,0.0020987473,-0.05970842,0.027762368,0.025696632,-0.0067754732,-0.022337459,0.069734156,0.040448494,0.022332923,-0.022057675,-0.008820259,0.049253725,-0.009715967,-0.04047435,0.08040305,-0.002770996,0.09174465,-0.015239325,-0.0033436196,0.0068801986,-0.019767513,0.090412356,-0.022848979,0.04383908,0.026284946,-0.079525575,0.07903637,-0.018322168,0.01517404,-0.015543573,0.03239944,0.01888002,0.032030977,0.027857497,0.04530223,-0.017976614,0.07032489,0.018502753,0.03126555,0.0028663764,0.059385337,0.007711542,0.028026337,-0.035865225,0.037392184,0.009981977,-0.016904809,0.04273937,0.008841078,0.053379882,0.021835929,0.03120039,-0.01066646,-0.0033542146,-0.0057654767,-0.030879425,-0.010360016,0.00695504,0.032727335,0.03183353,-0.017855708,-0.007678141,0.0332625,0.048196834,0.017594036,0.017769722,-0.0414392,-0.0577648,0.013706912,0.00764433,-0.052610938,0.04018034,-0.0052531417,0.008770303,-0.04467367,0.018258166,0.035975598,0.017044997,0.013045248,-0.016796479,-0.023704274,0.030421937,0.017594587,0.025541702,-0.032930892,0.008149056,0.017847879,-0.059352677,0.008163101,-0.015680147,-0.015010165,-0.01061395,0.04737623,0.094980575,-0.006348252,0.0376652,0.016530104,-0.002323307,0.018894348,-0.08729537,0.06005414,0.03237053,0.046621777,0.0414922,0.010871341,-0.010853863,0.012397751,0.032579903,0.060795862,0.0361098,0.025681809,-0.01472741,0.043537952,0.027481334,-0.022835556,0.0013439455,0.034864087,0.05943679,-0.0012584327,-0.007731753,-0.005826867,-0.047934778,0.06258852,-0.063656926,0.012988655,0.08658964,-0.02842963,-0.021355938,-0.00068789104,0.023782916,0.029512977,0.021319333,-0.02826399,0.007770792,-0.013749591,0.0055743936,0.004357513,-0.0318829,0.034891464,0.05204992,0.0005999975,-0.026416717,0.025150534,-0.0016034914,0.043274462,-0.003583505,-0.014221148,0.049257807,0.0311749,-0.003880535,0.03996537,0.01741617,-0.034217175,0.01923301,-0.022693839,0.017247172,-0.04670987,0.05196866,0.0069561573,-0.019029936,-0.022189347,0.008887363,-0.019165762,-0.027923414,-0.009805627,-0.005952189,-0.048152905,0.00029528435,0.028406844,0.019461127,0.009844465,-0.00298734,-0.07155758,-0.03147266,0.037375435,-0.036488134,-0.023612985,0.060792234,0.03435792,0.015916148,0.053761266,-0.026054092,-0.00589914,0.017448412,0.019624557,0.044686917,-0.06709694,-0.020735377,0.027743377,-0.012609146,0.10038176,-0.010672869,-0.0040843105,0.04342006,-0.013931579,-0.06266045,0.026090737,0.047261126,0.038374107,-0.02903905,0.0008844623,0.03866275,0.011670768,-0.007732621,-0.051057614,-0.062646784,-0.013971431,0.015265318,0.018831933,-0.016143093,0.007062908,-0.024420347,-0.0008291496,-0.0047954675,0.032577984,0.04931828,0.05071171,0.017220523,0.06337439,-0.019599166,0.044358447,-0.0041306913,-0.02420105,-0.018118126,-0.03052631,-0.028955856,0.06057841,0.022700533,-0.033866636,-0.04151998,0.06558695,0.02107486,0.033176206,0.027495794,-0.027481187,0.06465857,-0.011932189,-0.01772233,0.0083387215,0.036537364,-0.026621556,0.022133673,0.011455326,0.018621575,0.004287689,-0.016052675,-0.035172172,0.049646333,0.033649188,0.0044689947,-0.05269583,0.005747295,0.0114717,0.019640144,0.030625569,0.030688994,0.034908537,-0.054817364,-0.017038444,0.005808468,-0.043222878,-0.011477698,-0.015285012,-0.011606568,-0.0059442297,-0.019206611,0.016989896,0.015287406,0.022680806,0.0008744557,-0.0037465445,0.027652493,-0.038215313,-0.047140997,-0.019605063,0.01810991,0.039392754,0.023771089,-0.015649369,-0.008774502,0.0032425707,-0.034293465,-0.00409622,-0.015248482,0.058749426,0.028938008,0.051738836,0.0060740267,-0.047787063,-0.0072118565,-0.018228892,0.055187806,0.0087831365,-0.039883655,0.020468744,0.045360636,0.06946235,0.062661305,0.03684983,0.016699614,0.019158157,-0.019539103,0.023300674,0.0077010887,-0.0013660446,0.044504423,-0.039099757,-0.0039304635,0.034658827,-0.05390216,-0.016737254,0.01946983,-0.044607244,0.03783766,0.02627536,-0.0034053568,0.03652285,0.036794364,0.010278246,0.019926237,0.022850059,0.024265973,-0.06791373,0.034742784,0.010866364,-0.007796133,0.03387527,-0.052511185,-0.0019181977,0.053743817,-0.08992572,0.026215192,-0.022022525,-0.01773039,-0.053214703,0.0016191651,-0.0038050148,0.08046311,-0.048468757,-0.005021954,0.044839263,-0.004717487,0.040770434,0.0636676,0.002648702,-0.007999814,-0.028049693,-0.029323123,0.068348855,0.036356512,0.022312766,0.0072901975,0.047432814,0.035091456,-0.024447836,0.021371234,-0.07513342,0.007297926,-0.05861709,0.010675145,-0.013812626,-0.044443734,-0.022322092,0.04741521,-0.012865699,-0.00012613005,-9.936774e-05,-0.052298374,0.049203873,0.062361553,0.010261168,0.014843745,0.06901741,0.0055593858,-0.002967226,0.009468725,0.052106112,-0.057527356,0.04963192,-0.017780783,0.035969406,0.049004667,0.00010143427,-0.014180172,-0.028298458,0.01581667,-0.033150073,-0.02753633,-0.013782463,0.08222574,-0.011316277,-0.004701927,0.0036479535,-0.06433097,-0.015812943,-0.07151508,-0.010987832,0.074925855,0.0042103804,-0.02222217,-0.011398297,-0.040829647,0.034335922,-0.05058463,0.027560204,-0.043462448,0.0073988102,-0.036414284,-0.02098413,0.029831141,-0.06694194,-0.003169142,-0.06862332,-0.025467103,-0.0031493625,-0.057284668,-0.04110958,-0.043246333,0.036151543,-0.003009278,-0.00061212067,-0.009108835,0.011075299,-0.011262941,0.0026916447,0.03442839,0.015900986,-0.005619876,0.023561638,0.017891081,-0.05821293,-0.030576857,0.020336369,0.027177583,0.004516424,0.036455974,0.033718634,-0.008693664,0.002266523,-0.034491155,-0.06569567,0.029450169,-0.025225181,-0.021062274,0.1177401,0.0030949244,-0.028186733,-0.018868325,-0.055690542,-0.015534165,0.020910667,0.004834644,0.009676824,0.05836444,0.032379452,-0.017576305,-0.069428965,-0.01366423,-0.015497881,0.0093249865,-0.018612564,0.032875553,0.057646453,0.009492836,-0.0072170775,-0.041528612,0.00861193,0.008056567,-0.027172104,-0.007963379,0.013790132,0.0042181937,-0.0072104633,0.02595637,-0.049881876,-0.012516549,0.061187945,0.0021823742,-0.012663704,0.0031970073,0.013784073,-0.007880799,-0.04774739,-0.014631239,0.04232293,0.0026638478,0.0018206359,-0.0052673602,-0.008170768,-0.013578541,-0.08037731,-0.041515168,0.0219784,0.04489303,-0.030258866,0.02292972,0.006921373,-0.048049744,0.041803323,0.035263445,0.031273507,-0.0070997803,-0.049815316,-0.048781455,-0.0087287845,-0.006009372,-0.030702498,0.044672716,0.010163709,0.06875973,-0.021333897,-0.007755843,0.05090517,-0.01789063,-0.03492214,0.0010510485,0.026562976,-0.009766511,-0.0046248822,0.056115814,-0.009286561,-0.056203842,0.029120822,0.039883886,0.01854947,0.03217556,0.0762913,0.02271726,-0.005570902,-0.016741656,-0.039856892,0.03739851,0.055908326,0.008096744,-0.0125764115,0.024523217,0.02077022,-0.032467287,-0.023680594,-0.027768563,-0.049695842,0.037387922,0.0031111513,0.04841928,0.01700375,-0.025752893,-0.014690441,0.040768083,-0.055150483,-0.02927815,-0.00038514912,0.021647036,-0.059952963,-0.018104076,0.0011053222,-0.013103821,0.08347422,-0.018944759,0.02664369,-0.011791516,-0.020882295]	Keywords: Adam, AdaMax, optimization, infinity norm, update rule\nKey Objects: AdaMax, infinity norm, update rule, Adam\nRefers to Images: None\nHypothetical Questions:\n- Why are large p values in the glyph[lscript] p norm problematic for optimization?\n- How does using the infinity norm in AdaMax contribute to greater numerical stability?\n- What is the purpose of introducing 'u t' and how does it differ from the traditional 'v t' in Adam?\n---\nSummary:\nAdaMax is a modification of the Adam optimization algorithm that utilizes the infinity norm (glyph[lscript] ) to create a more stable update rule by replacing   v t + glyph[epsilon1] with u t.\nOriginal Text:\n### 4.7 AdaMax  \nThe v t factor in the Adam update rule scales the gradient inversely proportionally to the glyph[lscript] 2 norm of the past gradients (via the v t -1 term) and current gradient | g t | 2 :  \n<!-- formula-not-decoded -->  \nWe can generalize this update to the glyph[lscript] p norm. Note that Kingma and Ba also parameterize  2 as  p 2 :  \n<!-- formula-not-decoded -->  \nNorms for large p values generally become numerically unstable, which is why glyph[lscript] 1 and glyph[lscript] 2 norms are most common in practice. However, glyph[lscript]  also generally exhibits stable behavior. For this reason, the authors propose AdaMax [10] and show that v t with glyph[lscript]  converges to the following more stable value. To avoid confusion with Adam, we use u t to denote the infinity norm-constrained v t :  \n<!-- formula-not-decoded -->  \nWe can now plug this into the Adam update equation by replacing   v t + glyph[epsilon1] with u t to obtain the AdaMax update rule:\nContextualized Text:\nIn the Adam optimization algorithm, the 'v t' factor initially scales gradients based on the glyph[lscript] 2 norm. AdaMax is a modification that generalizes this to the glyph[lscript] p norm, recognizing that large p values can lead to numerical instability. To create a more stable update rule, AdaMax utilizes the infinity norm (glyph[lscript] ) and replaces   v t + glyph[epsilon1] with u t.	{"tags": ["optimization", "deep-learning", "algorithm", "stability"], "doc_id": "741c1f63-2f48-4b1b-85be-2948173649bd", "summary": "AdaMax is a modification of the Adam optimization algorithm that utilizes the infinity norm (glyph[lscript] ) to create a more stable update rule by replacing   v t + glyph[epsilon1] with u t.", "doc_type": "text", "entities": ["Kingma", "Ba"], "keywords": ["Adam", "AdaMax", "optimization", "infinity norm", "update rule"], "key_objects": ["AdaMax", "infinity norm", "update rule", "Adam"], "contextual_text": "In the Adam optimization algorithm, the 'v t' factor initially scales gradients based on the glyph[lscript] 2 norm. AdaMax is a modification that generalizes this to the glyph[lscript] p norm, recognizing that large p values can lead to numerical instability. To create a more stable update rule, AdaMax utilizes the infinity norm (glyph[lscript] ) and replaces   v t + glyph[epsilon1] with u t.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.7 AdaMax"}, "hypothetical_questions": ["Why are large p values in the glyph[lscript] p norm problematic for optimization?", "How does using the infinity norm in AdaMax contribute to greater numerical stability?", "What is the purpose of introducing 'u t' and how does it differ from the traditional 'v t' in Adam?"]}
9ab23d96-0b9b-4295-bc0e-3b1297efc62a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03870108,0.015925165,0.057837106,0.04572996,-0.00013977216,0.064461306,0.02123222,0.034286108,-0.01789519,-0.04609825,-0.014662486,-0.036487658,-0.0062097963,0.04306127,-0.06261138,-0.006851327,-0.0151780695,0.040340386,-0.018484162,0.006473518,0.019112676,-0.0283023,0.0424344,0.018534394,0.028501866,0.07224108,0.030964198,0.036224797,-0.016976714,0.0070210695,-0.012591854,-0.06712592,-0.0127339335,0.07236644,-0.0041238493,0.02244285,-0.039795645,-0.057181347,-0.014343014,-0.0725895,-0.04034469,0.030948695,-0.020090355,0.014327649,0.00015158679,-0.037368026,-0.0798099,-0.059898175,0.0053181257,-0.0021898933,0.008007601,0.009701127,-0.042064622,-0.023233382,-0.05330397,0.001461047,0.04453472,-0.009166456,-0.005217777,-0.057924192,-0.0166057,-0.016943974,0.0030946569,0.031897146,0.041310716,0.00660469,0.045008663,0.032500874,0.066044725,0.11413554,-0.013650187,0.034896735,0.025742961,-0.08343019,0.10418216,0.05105132,-0.025481047,-0.011341589,-0.044660736,-0.05195747,0.041698597,0.00029783856,-0.033472646,0.0012858653,0.057926837,-0.0012890963,-0.06291656,-0.0738892,0.026760299,-0.025161972,0.0008312119,-0.06509182,-0.017502397,0.017005501,0.043658894,0.024074124,0.019314606,-0.033320155,0.029031994,-0.045522436,0.037366517,0.00804659,0.014870372,0.09513335,0.02176499,-0.04885779,-0.04226607,-0.0067351116,0.0158204,-0.008713204,0.024237217,0.042156123,-0.07324477,-0.05248719,-0.006111318,-0.057360638,-0.02046862,0.020230483,0.0020582979,0.00456851,0.06950354,-0.045599878,0.04608223,0.008785965,0.014684118,0.017927809,0.018125033,-0.023372093,-0.04385374,0.02212604,0.09350887,0.019471169,0.010298071,0.0046727518,0.016294327,0.004283174,0.023492485,-0.031880476,-0.00022511868,0.028573962,-0.04894641,0.005955887,0.0034645384,0.015368376,0.008558004,0.035063818,-0.029490404,0.020985698,-0.039425712,0.052728906,0.028863257,-0.050899792,0.06287954,-0.029182665,-0.025257405,-0.008115748,-0.02277121,-0.018105786,0.012887025,-0.026081864,0.011620731,-0.008579725,-0.0441813,0.043002047,0.04219102,0.048671063,0.040746965,-0.001630113,0.017451784,0.010763799,-0.043960318,-0.040738776,0.010737004,0.0385038,-0.010559839,-0.10927463,-0.006651001,-0.008096432,0.010485095,0.039695304,0.015383778,0.03606154,-0.0030924315,-0.0047433977,-0.027180426,0.024959745,-0.004498359,0.024967616,-0.012146876,-0.04118783,0.071016595,-0.04053824,-0.04343912,-0.009499208,0.025176981,0.06603271,0.03429483,-0.07307052,-0.024922587,-0.030177385,0.033634014,0.015029475,-0.06337393,-0.017208073,0.015510072,-0.015542021,-0.019889725,0.033617906,-0.024291867,0.05602323,0.0247932,0.018338183,-0.04760529,0.0618792,0.06305273,-0.0354843,-0.074293576,-0.0065686586,0.039120886,-0.03574323,-0.04087528,0.047005866,0.021198256,0.102357686,-0.008459279,0.0015954622,-0.01701375,-0.00066374004,0.04080671,-0.07713244,0.0340814,0.02430435,-0.06622803,0.06259485,-0.04481676,0.008180945,-0.008421619,-0.000350682,0.012392175,0.036027815,0.05998662,0.08151661,0.0026458255,0.060121663,-0.0031850417,0.03243523,0.030796355,0.044863243,-0.016040877,0.052476287,-0.048630796,0.0071666175,-0.044377852,0.004226744,0.021219552,0.011780335,0.03432513,0.01743716,0.061601404,-0.018884454,0.01412089,0.011574433,-0.029447692,-0.015123585,0.01950934,-0.022132652,0.00010942484,-0.00822185,-0.008880198,0.041509565,0.06711755,-0.00038522927,0.0042627184,-0.041091505,-0.05002955,0.042817716,0.0027272818,-0.0043484005,0.042346258,-0.001155883,-0.0023820712,-0.005766576,0.037597943,0.016131045,0.0045044306,-0.004679923,-0.009639931,-0.02215424,-0.018180536,0.017217914,-0.008594093,-0.022952072,0.0038999952,0.017444659,-0.032561984,0.00042895,-0.0077170157,-0.026553873,-0.019057116,0.08932578,0.10039184,-0.025583407,-0.0044134026,0.0019004883,0.023026293,0.025681265,-0.04865084,0.07005082,0.027957652,0.034046546,0.014693193,0.023371277,-0.02718853,0.0133004105,0.045325886,0.059291456,0.045705143,0.026524568,0.005309103,0.050854165,0.05848706,-0.013479292,0.012713123,0.04463018,0.06565586,-0.0064602536,-0.00554018,-0.013644706,-0.0837654,0.068573505,-0.08441318,0.026423799,0.06591024,-0.035678674,-0.026384676,-0.0058076633,0.005779528,0.031008838,0.045163922,-0.044520047,-0.00393003,-0.023375597,0.026846185,0.03571994,-0.019906988,-0.0007658205,0.012299831,0.0068594543,0.0015680885,0.022807969,0.003064477,-0.015430128,-0.012024139,0.0077296374,0.06277825,0.024051754,0.0010126437,0.020442499,0.006891998,-0.078415275,0.055026576,-0.027754132,0.015240485,-0.07446655,0.040946353,-0.0019298185,-0.04130868,0.024846395,-0.014269299,-0.02202377,0.0016961038,-0.0024139937,0.028769583,-0.038924705,-0.001983125,0.0424826,0.013716011,0.014452012,-0.030965256,-0.047214486,-0.07600478,-0.002988041,-0.026295932,0.008707305,0.05439363,0.02228311,0.029668361,0.057168175,-0.022845818,-0.009189547,-0.0123406835,0.012412848,0.035974164,-0.046191726,-0.013270742,0.0029749405,0.019029181,0.08948004,0.027398217,0.036751598,0.028138055,-0.00071348686,-0.05349887,0.008473499,0.048493415,0.0057561067,-0.056568306,-0.0030589805,0.044161655,0.016045026,0.003154131,-0.07267144,-0.053291395,-0.020010026,0.027940867,-0.015848286,0.0010858182,0.005564919,-0.004975401,-0.00914354,-0.026656637,0.03207287,0.03557152,0.03672258,0.030174796,0.039328143,-0.051984135,0.017118953,0.01890233,-0.008708152,-0.020106416,-0.036748536,-0.050078597,0.063318126,0.0383224,-0.062455878,-0.049184274,0.06741551,0.006235189,0.030754045,0.004193567,-0.029589266,0.024247041,-0.0048528803,0.011234495,-0.014476541,0.006651633,-0.038922712,0.03041601,0.04075125,0.039942376,0.021494703,-0.022991449,-0.028808093,0.07302451,0.019768218,0.0080832625,-0.035480045,-0.009480126,0.045589253,0.009344366,0.0010246151,0.033881415,0.024325391,-0.05865711,-0.013440848,0.0040206653,0.0020000422,-0.020397456,-0.020899255,0.0058403164,-0.019393172,-0.029642275,0.04593299,-0.0029649693,0.032126848,0.014821956,-0.010885854,0.045331754,-0.036880862,-0.060809042,-0.001283147,0.0138053065,0.012187237,0.01122865,0.010818828,0.0036150587,-0.032511372,-0.025699578,0.0056886813,-0.018602079,0.054718845,0.010518747,0.026526956,0.009712673,-0.01716295,-0.014181852,-0.038161214,0.05836854,0.005000643,-0.055176187,0.027574688,0.011262274,0.032396656,0.05106794,0.06406339,-0.010936456,0.006649431,-0.030072113,0.034480035,-0.007900553,-0.03009813,0.039908145,-0.0092943525,-0.02079657,0.01949447,-0.03803037,-0.0021310719,0.010821242,-0.028354289,0.03342901,0.013260994,-0.033304237,0.04626465,0.02881905,0.016552493,0.056417108,0.039694816,0.0020888369,-0.11168153,0.031681664,0.0038785595,-0.01703021,0.042568784,-0.045123294,0.012734652,0.06094838,-0.063529365,0.013151494,-0.0032287098,-0.037696667,-0.055335727,0.005326984,0.006241269,0.06965211,-0.077708155,0.024845496,0.022000186,-0.030456541,0.059106637,0.08544356,0.004363085,-0.0058104154,-0.0066595967,-0.0089814095,0.05025614,0.032955494,0.020911211,-0.012636491,0.025846047,-0.002845402,-0.015303336,0.02469037,-0.07144757,0.028703868,-0.01614862,-0.008618504,0.009343267,-0.02811323,-0.028535316,0.013382733,-0.027987933,0.0030676085,-0.0046087014,-0.041207887,0.060358692,0.055415604,-0.008593778,0.021417076,0.06742526,0.015296568,0.0010571321,0.048688173,0.03659113,-0.010272949,0.049826946,-0.029162452,0.042016845,0.07536595,0.008164237,-0.029742124,0.001991939,0.015832696,-0.060923163,-0.027758915,0.018688424,0.03095132,-0.0301383,0.02710806,-0.024097243,-0.0724682,-0.0571336,-0.06845132,-0.013327834,0.06404655,0.019485245,-0.024728537,-0.02875736,-0.04046777,0.01928354,-0.063265875,-0.0029760287,-0.030444818,0.011384436,-0.025133938,-0.014703575,0.04499039,-0.049330723,-0.021713043,-0.06647085,0.011928347,0.009696035,-0.06858898,-0.041618876,-0.03750396,0.02400162,0.008875901,-0.008220221,-0.003467876,0.009255157,-0.0050748806,-0.01085095,0.06799095,0.03284805,-0.014690789,0.048406664,0.03749187,-0.04884814,-0.025546202,0.007899595,0.042501833,-0.0032373765,0.017764218,0.019044366,-0.009139542,0.021127626,-0.047719903,-0.036159225,0.018519923,0.019523405,-0.01357519,0.086043306,-0.012981275,-0.03220828,-0.017876433,-0.051509537,0.013636526,-0.010931275,0.031283256,0.017910909,0.05517679,0.019924728,-0.039865553,-0.06289891,-0.032766607,0.010719811,0.025145227,0.0014385061,0.056288663,0.08214526,-0.011654357,0.008285175,-0.04915376,0.009312123,-0.0032554432,-0.005802406,-0.0165077,-0.027351499,-0.035185095,-0.006466289,0.038611673,-0.011433419,-0.002391897,0.09571486,0.013020456,-0.0017249123,0.011096059,-0.024338659,-0.016133837,-0.0053229225,-0.0095262835,-0.016222361,0.013225681,-0.0099265715,0.009414613,-0.021684239,-0.0070587727,-0.10838907,-0.020858165,0.013794398,0.03196554,0.014560115,0.023985349,0.027763523,-0.018902421,0.027180284,0.011496549,-0.0077063856,-0.012798231,-0.039671876,-0.029480964,-0.03113109,0.0033169915,-0.043359596,0.03424782,0.0149872275,0.010408519,0.023968197,-0.020649586,0.032591034,-0.027449392,-0.020980394,0.007740865,-0.016801698,-0.005803736,0.00031570165,0.0013526855,0.004120008,-0.053529713,0.047437597,0.0353698,0.0072748456,0.030747985,0.0852365,0.023124188,0.005028482,-0.045794576,-0.039237045,0.04865831,-0.0064096153,0.013476906,-0.017872503,0.0338425,0.0012189912,-0.004398049,-0.053149246,-0.00554526,-0.05745586,0.028050926,0.0072904,0.04700737,-0.00898286,0.016080046,-0.037796926,0.007354763,-0.048497904,-0.029794203,-0.0011359283,0.006724505,-0.046722077,-0.023286546,-0.008486407,-0.017554881,0.08867148,0.0011484661,-0.0116499625,0.01789038,-0.01478723]	Keywords: AdaMax, infinity norm, bias correction, Adam update\nKey Objects: AdaMax update rule, infinity norm, bias correction\nRefers to Images: None\nHypothetical Questions:\n- How does the infinity norm constrain the update differently than Adam's approach?\n- Why is the bias towards zero problematic in the Adam update?\n- What are the advantages of using AdaMax over standard Adam?\n---\nSummary:\nThe AdaMax update rule is derived by substituting u_t, representing the infinity norm-constrained v_t, into the Adam update equation, and it avoids the need for a bias correction due to its reliance on a max operation.\nOriginal Text:\n<!-- formula-not-decoded -->  \nWe can now plug this into the Adam update equation by replacing   v t + glyph[epsilon1] with u t to obtain the AdaMax update rule:  \n<!-- formula-not-decoded -->  \nNote that as u t relies on the max operation, it is not as suggestible to bias towards zero as m t and v t in Adam, which is why we do not need to compute a bias correction for u t . Good default values are again  = 0 . 002 ,  1 = 0 . 9 , and  2 = 0 . 999 .\nContextualized Text:\nTo derive the AdaMax update rule, researchers substituted u_t, which is a value representing the infinity norm-constrained v_t, into the Adam update equation. This modification is significant because u_t's reliance on a max operation prevents it from exhibiting the same bias towards zero as m_t and v_t in Adam, thereby eliminating the need for a bias correction.	{"tags": ["optimization", "algorithm", "deep-learning"], "doc_id": "9ab23d96-0b9b-4295-bc0e-3b1297efc62a", "summary": "The AdaMax update rule is derived by substituting u_t, representing the infinity norm-constrained v_t, into the Adam update equation, and it avoids the need for a bias correction due to its reliance on a max operation.", "doc_type": "text", "entities": ["Adam"], "keywords": ["AdaMax", "infinity norm", "bias correction", "Adam update"], "key_objects": ["AdaMax update rule", "infinity norm", "bias correction"], "contextual_text": "To derive the AdaMax update rule, researchers substituted u_t, which is a value representing the infinity norm-constrained v_t, into the Adam update equation. This modification is significant because u_t's reliance on a max operation prevents it from exhibiting the same bias towards zero as m_t and v_t in Adam, thereby eliminating the need for a bias correction.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.7 AdaMax"}, "hypothetical_questions": ["How does the infinity norm constrain the update differently than Adam's approach?", "Why is the bias towards zero problematic in the Adam update?", "What are the advantages of using AdaMax over standard Adam?"]}
7c901422-29b7-4eed-9b97-79e73df1370c	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0055457773,0.009104449,0.019548705,0.028917499,-0.075692154,0.074462086,0.02974407,0.043142203,0.027138913,-0.01454915,0.007310117,-0.03338277,0.0100144,0.026060214,-0.06915403,-0.0049816486,-0.009282119,0.09720004,-0.031120926,-0.022757772,0.048039667,0.0027070364,0.072569504,0.037869304,0.009246218,0.05311298,0.03211286,0.03005783,0.01607579,-0.023246434,0.020357935,-0.056712676,0.02750134,0.04379582,5.6032128e-05,0.004077313,-0.018536607,-0.044834245,-0.041143265,-0.06880164,-0.040415447,0.032136213,-0.0018974064,-0.010135989,-0.024398798,-0.018583832,-0.0312617,-0.028594857,0.015908848,-0.008598766,-0.047471236,0.06658184,-0.052151766,-0.0032380954,-0.044048637,0.019056747,-0.04458497,-0.021054983,-0.023575366,-0.0749703,-0.027531058,-0.009448783,0.01684458,-0.010819819,0.041230142,-0.030579166,-0.030954763,0.056225576,0.069845945,0.11874416,0.008141448,0.07141509,0.051927947,-0.08511768,0.10104103,0.045536384,-0.0035810757,-0.05326664,-0.0038371177,-0.0019293149,0.010387896,0.019483443,-0.02986949,-0.06213523,0.08018995,-0.0045331917,-0.037917856,-0.0122451335,0.076627396,-0.07661131,-0.00028086663,-0.03059362,-0.024756726,-0.008495834,-0.0011044478,-0.007435852,0.01407918,-0.04033597,0.057663098,-0.020708505,0.009009623,0.0043529505,0.033468347,0.10113536,0.0073483586,-0.032341827,-0.028692283,-0.032290127,0.00645682,0.001026982,0.013757431,0.04545231,-0.07239116,-0.009650612,-0.011019714,-0.016761703,-0.01839978,0.045038722,-0.026248736,-0.03381675,0.039831582,-0.051838975,-0.004158136,0.024720468,0.04709008,0.06066354,0.022899877,-0.02948528,-0.06355741,-0.0016441651,0.046170276,0.0040786075,-0.023306033,-0.012511895,-0.017091466,-0.018808186,0.05148242,0.0063158735,-0.01796112,0.052598715,-0.06039005,-0.039598178,-0.026083566,0.0064152493,-0.018395966,0.048250884,-0.06935712,0.009630759,0.009744332,0.056311164,0.031220252,-0.033603523,0.08689688,-0.019852037,-0.05390753,-0.045118902,-0.004884797,0.032657903,0.011314538,-0.021553995,-0.011973242,0.03371461,-0.009462842,0.045069873,0.035987116,0.09322098,-0.010577542,0.03176956,-0.016392633,-0.01045341,-0.030854242,-0.020002728,0.0055236663,-0.012226489,0.0014481506,-0.040786523,-0.022664677,-0.0058540134,-0.01969584,0.039569475,0.005196337,0.010685221,0.014691472,-0.0008218996,-0.02655829,0.03205508,0.006449465,0.008765687,0.0040501556,-0.070416786,0.04605291,0.0031707701,0.0056997263,-0.02988454,0.03646123,0.06863218,0.03461545,-0.0575402,-0.013139048,-0.028682906,0.02026862,0.04732923,-0.00066616957,-0.033223573,0.028745148,0.020603279,0.010470686,-0.018158263,-0.042350046,0.022527365,-0.035063546,0.033162516,8.078846e-05,0.051147874,0.027861556,0.03568449,-0.027683381,-0.019816069,0.032109886,0.0043195225,-0.026346337,0.039897047,-0.006918882,0.06572337,-0.014643762,-0.003456857,-0.0010826106,-0.0048796777,0.050519057,7.864041e-05,-0.007665092,0.038523927,-0.075305045,0.048222557,-0.014946037,0.031958595,-0.013536722,0.034592733,-0.004732667,0.08375761,0.047281872,0.038925253,0.0021642633,0.07783836,0.02193345,-0.018810559,0.030314876,0.056548152,0.033394933,-0.0047300323,-0.02139336,0.009876323,-0.0152858775,-0.01851671,0.008252708,-0.018318618,0.05605808,0.009724861,0.058969487,-0.028106868,-0.005921746,-0.0040228264,-0.03159493,-0.013243055,-0.0129429195,0.028911237,0.024609217,0.028873267,0.00074953114,0.047588177,0.060789354,0.026700674,0.009416151,-0.03010848,-0.006144035,0.03740212,-0.0034503883,-0.008927744,0.03691767,-0.014898174,-0.01264883,0.04264348,0.031844147,0.0003845324,0.005363028,0.021234473,-0.024645062,0.018906094,0.0021357448,-0.006595661,0.008489829,-0.026646027,0.047764868,0.014155865,-0.008844927,0.010931729,0.0020763506,-0.035162292,0.010127421,0.06123476,0.11330572,-0.02064156,0.039719507,0.0037107505,0.0032705583,0.027538538,-0.061101314,0.03227765,-0.0283675,0.053668372,0.043783728,-0.023038281,-0.010176806,-0.0061446065,0.012126336,0.03298803,0.037998244,0.0028364535,0.0342312,0.06318447,0.021761421,-0.032745272,-0.063861914,0.040679343,0.0092602195,-0.019575497,0.012805226,-0.032637224,-0.06473932,0.04151798,-0.05903312,0.012152603,0.054418184,-0.029599512,-0.04478779,0.012670011,-0.013573744,0.022177031,0.039763927,-0.0015750575,-0.055445623,-0.028953228,0.0044255527,0.004585502,-0.03995062,0.020639826,0.012515538,-0.00073558715,-0.023503514,0.05593547,0.00014400299,0.04874311,0.012459892,0.040861625,0.047250394,-0.042202685,0.0028742752,0.035144128,0.007823728,-0.048737958,0.019735122,-0.06188069,-0.0034005712,-0.06826266,0.06421276,-0.009313122,-0.03624926,0.0075724456,0.004824365,0.01629541,0.0062394626,-0.008935871,0.023330783,-0.018257672,0.003163212,0.042591624,0.022650417,0.02407299,-0.0037644021,-0.052600477,-0.0076607214,0.038808957,-0.018902844,-0.024158519,0.035565972,0.033452995,0.043374024,0.10317622,0.003822083,0.00019627818,0.0015350959,0.00047335666,0.06589463,-0.020177294,0.008339324,-0.010759338,0.014960085,0.08107594,0.053334426,0.008362426,0.046450026,0.04328363,-0.064404555,0.026316073,0.05193548,0.014856762,-0.05122767,0.007403249,0.021068413,-0.004010486,0.0031732796,-0.05244423,-0.04468974,-0.00052324706,0.025644455,-0.010589707,-0.0034018084,-0.0038977817,-0.012823209,-0.014227557,0.0022625804,0.023103455,0.028288148,0.029681409,0.016727673,0.008660424,-0.048837617,0.06566702,0.036109112,-0.017657755,0.012309781,0.0024514406,-0.02744412,0.030034147,0.04069467,0.013236238,-0.06697012,0.072589,-0.0034499126,0.082680166,0.019965934,-0.001590485,0.034701586,0.009425287,-0.025623724,-0.0089957705,0.014709169,-0.001551203,0.014883156,0.014458142,0.021837162,-0.0037098918,-0.011925235,-0.007306756,0.04851223,-0.00073676906,-0.0017909748,-0.039734256,-0.021248704,0.040738657,0.0135737695,0.046897303,0.00335483,0.017462268,-0.03513282,-0.022033332,-0.041419715,-0.017886471,-0.00049652887,-0.012174764,0.016957415,0.011643422,-0.0278682,0.0019300818,-0.024794448,0.03028144,0.014068862,-0.01122945,0.06553851,-0.005922808,-0.035132956,-0.011821805,0.016337737,0.037494782,0.044939693,-0.014414013,0.015348734,0.012783554,-0.08625852,0.015884446,0.00046828465,0.033275075,0.021127092,0.0061263917,0.057083175,-0.054322217,0.04421472,-0.031519845,0.025507564,-0.02064245,-1.8437031e-05,-0.004209596,0.03731972,0.037023067,0.024878595,0.020206517,0.027191069,0.068164684,-0.0252251,0.04953957,-0.015467635,0.01983596,0.009819391,-0.06592865,-0.03606672,0.040875137,0.0239731,-0.0109772505,0.053542472,-0.03713336,0.025566986,0.027824014,-0.019073663,-0.011820498,0.038503863,-0.025860045,0.007914919,0.00029143252,0.058656804,-0.06921717,0.007366218,0.008821273,0.03379492,0.039489754,-0.06792171,-0.025545847,0.019938812,-0.03401718,-0.0051199957,-0.009075783,0.0043869056,-0.045556348,-0.002531296,-0.036221534,0.06505294,-0.053546302,0.00050289015,-0.0017796814,-0.00014456475,0.046142418,0.040482294,0.032398593,0.07051454,-0.029731775,-0.016968735,0.06314081,0.015276959,0.01354522,0.0372301,-0.006684914,0.034660444,-0.019349182,-0.043667886,-0.08631357,0.0014670182,0.0032400768,-0.01517648,-0.06593577,-0.009313294,-0.012024167,0.034011245,0.043604016,-0.04272074,0.005941385,-0.005964554,0.10170645,0.07920813,-0.004734279,0.026435219,0.058386404,0.004119995,0.021557085,0.042296976,0.041637305,-0.033924464,0.043701194,-0.029593324,0.02911632,0.00492705,0.011558835,-0.011788098,-0.021670487,1.7175753e-05,-0.028237645,-0.0014345655,-0.061312545,0.10606716,-0.0109476745,0.022877071,-0.050048564,-0.075777374,-0.017216053,-0.03715099,-0.0490526,0.019195747,-0.011475244,-0.009196667,0.016068593,-0.041398864,0.06750805,-0.09874453,-0.040378734,-0.020434657,-0.035324015,-0.09667497,-0.0437933,0.0001510191,-0.06178833,0.0033567254,-0.0401828,0.00010020051,-0.024385424,-0.047590654,-0.0021497996,-0.059338864,-0.0004055275,-0.049850073,-0.011237939,-0.032803994,0.02106582,0.0068349433,-0.016513554,0.07355277,-0.011969802,-0.02061297,0.024156272,0.01908577,-0.06234406,-0.028568732,0.04977914,0.021792416,0.0066695064,-0.0073079863,0.019665154,0.016662927,-0.005476971,0.014500645,-0.06144661,0.053383972,0.01851085,-0.009253413,0.06955543,-0.008309178,-0.025345998,-0.037720624,0.0034944715,0.027484,0.016139062,0.04696396,-0.015440536,0.064065896,0.060193013,-0.008494136,-0.048327647,-0.049919747,0.006775854,0.04087825,0.006082042,0.06594098,0.017599063,-0.01566914,-0.031378705,-0.030467926,-0.02224122,0.018682035,0.041203827,-0.006322566,-0.040899925,0.014296231,-0.009134925,0.031073872,-0.022435684,0.03820752,0.06893609,0.030678498,-0.0022301355,-0.04174756,-0.029943462,-0.035439186,-0.01979557,0.037602622,-0.009342977,-0.0037057328,-0.0037992445,-0.00017127057,-0.0027304552,-0.010340934,-0.036885966,-0.015168928,0.0623885,0.004422414,0.017823717,0.03415953,-0.021484142,-0.08753679,0.052566994,-0.0032447085,-2.0770178e-05,-0.012127791,-0.0033674086,-0.023987262,-0.03181104,-0.017815944,-0.03653521,0.021562105,-0.016985705,0.026316166,0.012610706,-0.018282121,0.059494313,-0.057897378,-0.041766506,0.04003532,-0.008482081,-0.0028216783,0.037468906,0.017778454,0.029391835,-0.032239072,-0.029094864,0.07023227,0.017436238,0.0041156774,0.045000423,0.0074142152,0.012510759,-0.039139565,-0.024145007,0.040029537,-0.018976247,-0.0031135865,-0.017740035,1.6949757e-06,0.0018432145,0.0024935019,-0.009256614,-0.058527227,-0.035144385,0.04756913,-0.021728309,0.012608441,0.01672282,0.0022993449,-0.046607602,-0.012353117,-0.039288096,-0.067565545,0.0044890186,0.052801445,-0.0507348,-0.052859843,-0.035837438,-0.02640642,0.04006685,-0.024840297,0.050796323,0.0049601262,-0.008640295]	Keywords: Nadam, Adam, NAG, momentum, optimization, gradient descent\nKey Objects: Nadam, momentum term, gradient\nRefers to Images: None\nHypothetical Questions:\n- What are the core advantages of combining Adam and NAG?\n- How does Nesterov Accelerated Gradient (NAG) improve upon vanilla momentum?\n- What is the significance of modifying the momentum term, m t , in the Nadam update rule?\n---\nSummary:\nNadam is an optimization algorithm that combines Adam and Nesterov Accelerated Gradient (NAG) to leverage the benefits of both approaches.\nOriginal Text:\n### 4.8 Nadam  \nAs we have seen before, Adam can be viewed as a combination of RMSprop and momentum: RMSprop contributes the exponentially decaying average of past squared gradients v t , while momentum accounts for the exponentially decaying average of past gradients m t . We have also seen that Nesterov accelerated gradient (NAG) is superior to vanilla momentum.  \nNadam (Nesterov-accelerated Adaptive Moment Estimation) [7] thus combines Adam and NAG. In order to incorporate NAG into Adam, we need to modify its momentum term m t .  \nFirst, let us recall the momentum update rule using our current notation :  \n<!-- formula-not-decoded -->  \nwhere J is our objective function,  is the momentum decay term, and  is our step size. Expanding the third equation above yields:  \n<!-- formula-not-decoded -->  \nThis demonstrates again that momentum involves taking a step in the direction of the previous momentum vector and a step in the direction of the current gradient.\nContextualized Text:\nNadam is an optimization algorithm designed to improve upon Adam by incorporating aspects of Nesterov Accelerated Gradient (NAG). NAG offers a more accurate step direction by updating parameters before computing the gradient. To implement Nadam, the momentum term, m t , within Adam is modified.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "7c901422-29b7-4eed-9b97-79e73df1370c", "summary": "Nadam is an optimization algorithm that combines Adam and Nesterov Accelerated Gradient (NAG) to leverage the benefits of both approaches.", "doc_type": "text", "entities": ["Adam", "NAG", "RMSprop"], "keywords": ["Nadam", "Adam", "NAG", "momentum", "optimization", "gradient descent"], "key_objects": ["Nadam", "momentum term", "gradient"], "contextual_text": "Nadam is an optimization algorithm designed to improve upon Adam by incorporating aspects of Nesterov Accelerated Gradient (NAG). NAG offers a more accurate step direction by updating parameters before computing the gradient. To implement Nadam, the momentum term, m t , within Adam is modified.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.8 Nadam"}, "hypothetical_questions": ["What are the core advantages of combining Adam and NAG?", "How does Nesterov Accelerated Gradient (NAG) improve upon vanilla momentum?", "What is the significance of modifying the momentum term, m t , in the Nadam update rule?"]}
bb3c95ce-7b62-409b-b2b8-b33c8ea7486d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.018060656,-0.014140376,-0.022743385,0.036805566,-0.048242174,0.061075345,0.015437164,0.025075078,0.015365308,-0.047489792,-0.0016755188,-0.04922718,0.000924913,0.017574435,-0.04550405,0.013234104,0.008229418,0.07492189,-0.0027099934,-0.01862528,0.043839965,0.035595827,0.044886336,0.029222287,0.012553542,0.034239464,0.0048315823,0.045606643,0.002458985,0.004945476,0.0028206124,-0.06914272,0.029177774,0.039342336,0.037850995,-0.0063385027,-0.053351186,-0.06156401,-0.022379074,-0.031518716,-0.023807427,0.058694795,-0.0134832645,0.020235104,0.01654508,-0.037094366,-0.026943928,-0.02360244,0.015648736,-0.014772087,-0.04842694,0.04152835,-0.07410276,0.012504755,-0.043842405,0.02657152,-0.024471099,-0.0073757004,-0.022634499,-0.050987314,-0.015353996,-0.050243054,0.016036699,-0.009321791,0.06673882,-0.024033085,-0.0052296235,0.011463086,0.04503734,0.11804686,-0.00780117,0.04514783,0.046358116,-0.0785899,0.11679421,0.045496993,-0.0049907467,-0.04327842,0.018499568,-0.0196782,0.043158907,0.0452588,-0.028211502,-0.035364386,0.056061283,-0.012708187,-0.049575597,-0.031975366,0.066366285,-0.070248485,0.0014551021,-0.03332968,-0.012314954,0.019490566,-0.075010896,-0.02319014,0.0005842738,-0.03425221,0.012468119,-0.028364109,0.043668065,-0.0014216935,0.03361047,0.08368773,0.024795001,-5.1172865e-05,0.001407941,-0.045512978,0.006506028,-0.0025525654,-0.026721118,0.036053378,-0.09575511,-0.032149006,-0.015973033,-0.023023658,-0.0070409826,0.052636653,-0.01010249,-0.0015207918,0.03163334,-0.090299904,0.014135741,0.03886527,0.06535001,0.060195904,0.022054099,-0.032428868,-0.035667196,0.0021151195,0.06543348,0.023723451,-0.02616373,0.0015435289,-0.0072499197,-0.010563919,0.06527426,-0.022898652,-0.023742154,0.02665891,-0.07405377,-0.02953781,0.0017233775,0.03388177,0.019271487,0.048017826,-0.057396576,0.021718789,-0.008201613,0.0562784,0.042650722,-0.041742086,0.09721775,-0.03257989,-0.053660277,-0.034406025,-0.027856745,0.03685229,0.035426058,-0.0023821723,-0.010207399,0.031646557,-0.016471045,0.07953281,0.032152515,0.12737738,0.0001575517,0.059226178,0.0065281177,-0.039843243,-0.007542454,-0.030847743,0.031741854,-0.044888757,0.016117023,-0.03694972,-0.007935091,-0.004333516,-0.022442421,0.040350895,0.017766237,0.015221435,0.020283276,0.010095194,0.016278343,0.069111936,0.023632156,0.009696358,-0.0016718787,-0.05616926,0.04369837,0.0011504008,0.03363254,-0.026696488,0.03475462,0.07382635,0.03818757,-0.036776997,0.012885318,-0.04269961,0.021015555,0.0397095,-0.039978202,-0.039434437,0.015095552,0.008748568,0.028446333,-0.01545565,-0.02927404,0.04215211,-0.015594875,0.041097134,0.017103672,0.052322425,0.03366804,0.035015814,-0.023141604,0.020585,0.0551402,-0.011714646,-0.051070195,0.015813615,0.009752429,0.063402124,0.004175562,0.03423258,0.0007776296,-0.010215901,0.041580524,0.0035567768,0.00921497,0.022640938,-0.088559635,0.0153034665,-0.047588192,0.011755284,-0.027486963,0.049297262,0.015960451,0.08939368,0.052365806,0.034921918,-0.00014298598,0.077763565,0.021547362,-0.049798615,0.049474366,0.011823968,0.030329755,0.018544745,-0.031079486,0.019394312,0.008275674,-0.03572917,0.0040902616,-0.036166552,0.04641742,-0.019882582,0.025409756,-0.045533706,0.020665897,0.00039938086,-0.027536323,-0.0022185259,-0.014539512,0.038122002,0.021594444,0.062468056,-0.015510293,0.025873464,0.06016664,0.008355411,-0.036925696,-0.015719038,-0.018064292,0.017615288,7.16676e-05,0.023162324,-0.006268908,-0.0028803486,-0.006773316,0.015713736,0.0071015814,-0.00065855763,-0.012425585,-0.003226367,-0.015177035,0.0012047814,0.005516102,0.016743353,-0.021259584,-0.030603139,0.048243392,0.008638659,0.010616451,-0.0131297335,-0.0068804584,0.012164523,0.040053494,0.058644284,0.060670592,-0.022887928,0.031566586,-0.008783903,0.017582187,0.021478033,-0.04798101,0.038444042,-0.028396493,0.020958824,0.017116442,-0.010514522,-0.0600286,-0.030145518,-0.008864098,0.07325058,0.05409283,-0.019524228,0.019604336,0.008856311,0.03472388,-0.028769722,-0.08311178,0.04342879,0.013016351,-0.0019295504,0.030899415,-0.06998751,-0.052734125,0.04236291,-0.0633217,0.0004982904,0.041561633,-0.009250784,-0.045499206,0.058298863,-0.012816688,0.005802609,0.030019047,0.017941333,-0.046490442,-0.04564508,0.033323813,0.025518576,-0.0021077597,0.022154117,0.0020466114,0.01635192,0.009240363,0.051344298,-0.0016295903,0.054143697,0.01584225,0.045890443,0.032687362,-0.03436267,-0.017717263,-0.0023674704,0.013522768,-0.040425237,-0.03249573,-0.042985253,-0.024143146,-0.039994873,0.044293195,-0.01665002,-0.038959265,-0.02867572,0.02670548,0.015222688,0.051107317,-0.009700468,0.03540703,-0.004784647,0.004890846,0.037222866,0.0010939364,0.0018240275,0.017461142,-0.029613232,0.0029793005,0.0036164094,-0.038066268,-0.054188073,0.047471877,0.011902885,0.0073333383,0.09887645,-0.02096085,-0.008505552,0.025065003,-0.015576711,0.029729672,0.0068709287,0.0010690585,-0.029880404,0.048459105,0.03507745,0.02694944,0.011438039,0.050989658,0.034852274,-0.07654878,0.0010418976,0.035284728,0.015877249,-0.08244533,-0.025649572,0.018158944,0.00059129175,-0.016007878,-0.038638517,-0.04349623,-0.0052214013,0.026492575,-0.011481439,-0.019939566,-0.036514726,-0.020603357,-0.0266507,0.007306132,0.04948745,0.048841957,0.054455526,0.025740083,0.0036905522,-0.009880661,0.048731558,0.029917447,-0.0051972535,-0.015043831,0.014379261,0.006647722,0.03175545,0.023093313,0.002479504,-0.06326209,0.06953627,-0.025202233,0.0371434,0.002356813,-0.018249502,0.02602898,-0.019387871,-0.027582357,-0.029921087,0.051604293,0.031695947,0.005438774,0.011308421,0.0041653477,0.011280542,0.017701901,0.027767016,0.06871972,-0.019898083,0.025869945,-0.03889648,-0.024214,0.029150115,-0.03233245,0.0027862962,-0.0073398026,-0.004051086,-0.0040339446,-0.03664738,-0.024447627,-0.02591292,-0.0049223863,-0.021597214,-0.043698758,0.0023738586,-0.030210707,0.029737974,-0.033825584,0.018318981,0.024184305,0.008400028,0.056115907,0.011498131,-0.08187338,-0.0106323045,0.025355391,0.029979467,0.041830838,-0.021940064,0.0077028237,-0.009923591,-0.06354539,0.05683524,-0.0029976794,0.017283417,-0.013832559,0.014241757,0.058949396,-0.05509674,0.017425798,-0.025971621,0.02083213,-0.013672137,0.030516986,-0.017782213,0.015742013,0.0029978896,-0.012336179,0.029346107,-0.008557666,0.05990903,0.018273769,0.012166535,-0.030188367,0.0033319676,0.0043188594,-0.060458116,-0.00079668954,0.03115375,0.0144115295,-0.00054332084,0.015712848,-0.052874107,0.031547066,0.020842347,-0.034872767,-0.007439716,0.029678794,0.009326617,-0.040421065,-0.004515617,0.04817663,-0.045590546,0.015106059,-0.0037791308,0.010112486,0.012041751,-0.039147187,-0.044361703,0.053855676,-0.045659162,-0.017111618,-0.0064599644,0.0019395399,-0.031313322,0.017511107,-0.039544757,0.061366595,-0.034403626,0.016190842,-0.020022027,0.0047724554,0.019631801,0.05558469,0.04323373,0.049493782,-0.016882,-0.013080284,0.015421841,0.014163097,-0.007917066,0.023590866,-0.015494506,0.039384026,-0.009069392,-0.004572371,-0.101918586,0.031648,0.021087706,0.0043212166,-0.071802825,-0.03940738,0.005726484,0.004762156,0.0473969,-0.073618315,0.003280329,-0.03321752,0.10728746,0.084406376,0.018242864,-0.016943125,0.040076848,-0.005993888,0.046744477,0.075312175,0.03000779,0.005974209,0.016435284,-0.02023074,0.0071888524,-0.020084469,0.003273958,0.035454202,-0.021260679,-0.021249555,-0.042514298,0.015874922,-0.05459161,0.07215666,-0.03905561,-0.0077196267,-0.048357274,-0.07372612,0.00078497827,-0.030132905,-0.071225576,0.01895922,-0.009282227,-0.016644362,0.005729132,-0.0059102816,0.02757642,-0.042916074,-0.011463812,-0.03881104,-0.04449342,-0.0838408,-0.06338536,-0.009240452,-0.055080988,0.029865919,-0.040989015,-0.0067963875,0.008452324,-0.026158102,-0.008418384,-0.010459162,0.023111727,-0.034793485,0.013315524,0.030124843,0.027540777,-0.016482435,-0.019531375,0.04967776,-0.04791336,0.0020211865,-0.0014020116,0.032189805,-0.03146509,0.011472459,0.05562643,0.055552766,0.013529851,-0.009759574,0.027539304,0.01859804,-0.028977059,0.038464505,-0.058137495,0.06341085,0.032370426,0.0109423455,0.11143882,-0.010486198,-0.050177936,-0.041181643,-0.037619494,0.044003036,0.015925774,0.08399361,-0.032061145,0.061439674,0.020556677,-0.030210178,-0.036599163,-0.043860797,-0.0065741516,0.029166648,0.008179304,0.06039162,0.003464981,-0.023913648,-0.0012943676,-0.029719913,-0.022898179,0.040158603,0.046153598,-0.026174292,-0.0390638,-0.020443965,0.016594548,0.049189743,-0.019753145,0.035372872,0.045804568,-0.017977512,-0.0008639588,-0.05872346,-0.024911873,-0.049525976,0.0042481,4.8470825e-05,-0.014482485,-0.008627946,-0.014997544,0.028251652,0.0347596,0.01339857,-0.048037898,0.008814252,0.014708995,0.028219586,0.026841918,0.023874294,0.005423818,-0.073554486,0.039643336,-0.012318784,-0.005405168,0.0049090884,0.021084068,-0.016816117,-0.03155076,-0.0207944,0.0012697618,0.010270167,-0.0008664027,0.014851442,0.03331035,-0.02640181,0.0010189676,-0.0651801,-0.017191282,0.04060275,-0.01073835,-0.0002766675,0.076259024,0.054524537,0.017444277,-0.022600695,-0.03295248,0.051183026,0.01233483,-0.0189467,0.07763948,-0.0082626,0.04727346,-0.037861522,0.016856523,0.036895066,-0.039427083,-0.007782297,-0.03025956,0.024661826,0.012512346,0.006888563,-0.0042601465,-0.03776837,-0.030323626,0.06334376,-0.0373287,-0.008687367,0.011618985,-0.017810525,-0.04259603,0.019887526,-0.045144826,-0.06020144,0.00035676823,0.024377441,-0.038696222,-0.045978505,-0.044888172,0.008702152,0.084491305,-0.02339388,0.05168857,0.019977184,-0.02939488]	Keywords: momentum, Nesterov Accelerated Gradient, NAG, gradient descent, parameter update\nKey Objects: momentum, gradient, parameters\nRefers to Images: None\nHypothetical Questions:\n- How does NAG improve upon standard momentum?\n- What is the advantage of applying the look-ahead momentum vector directly to update parameters?\n- Could this look-ahead approach be applied to other optimization algorithms?\n---\nSummary:\nNesterov Accelerated Gradient (NAG) enhances momentum by updating parameters with a momentum step before calculating the gradient, and Dozat proposes applying the look-ahead momentum vector directly to update parameters.\nOriginal Text:\n<!-- formula-not-decoded -->  \nThis demonstrates again that momentum involves taking a step in the direction of the previous momentum vector and a step in the direction of the current gradient.  \nNAG then allows us to perform a more accurate step in the gradient direction by updating the parameters with the momentum step before computing the gradient. We thus only need to modify the gradient g t to arrive at NAG:  \n<!-- formula-not-decoded -->  \nDozat proposes to modify NAG the following way: Rather than applying the momentum step twice one time for updating the gradient g t and a second time for updating the parameters  t +1 - we now apply the look-ahead momentum vector directly to update the current parameters:  \n<!-- formula-not-decoded -->\nContextualized Text:\nTo improve upon standard momentum, Nesterov Accelerated Gradient (NAG) calculates a more accurate step by first updating the parameters with a momentum step and *then* computing the gradient. Dozat further suggests a refinement: instead of applying the momentum step twice  once for the gradient and again for the parameters  the look-ahead momentum vector directly updates the current parameters.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "bb3c95ce-7b62-409b-b2b8-b33c8ea7486d", "summary": "Nesterov Accelerated Gradient (NAG) enhances momentum by updating parameters with a momentum step before calculating the gradient, and Dozat proposes applying the look-ahead momentum vector directly to update parameters.", "doc_type": "text", "entities": [], "keywords": ["momentum", "Nesterov Accelerated Gradient", "NAG", "gradient descent", "parameter update"], "key_objects": ["momentum", "gradient", "parameters"], "contextual_text": "To improve upon standard momentum, Nesterov Accelerated Gradient (NAG) calculates a more accurate step by first updating the parameters with a momentum step and *then* computing the gradient. Dozat further suggests a refinement: instead of applying the momentum step twice  once for the gradient and again for the parameters  the look-ahead momentum vector directly updates the current parameters.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.8 Nadam"}, "hypothetical_questions": ["How does NAG improve upon standard momentum?", "What is the advantage of applying the look-ahead momentum vector directly to update parameters?", "Could this look-ahead approach be applied to other optimization algorithms?"]}
54914a76-9157-43b2-a890-998bd969e049	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.012200674,-0.0038393766,0.015702488,0.04027251,-0.037954915,0.050729945,0.023988182,0.0051729083,0.04344619,-0.043728404,-0.004239562,-0.029217921,-0.0023312832,0.025644783,-0.08034213,0.007917889,-0.00042445687,0.07503614,-0.015707092,-0.02421761,0.012234221,0.0077970256,0.06092311,0.032073554,0.028121699,0.04744698,0.020204699,0.047786776,0.0044964785,-0.02905844,0.0038072476,-0.077578776,0.036006276,0.0504933,0.0023612988,0.005468313,-0.03939171,-0.03686166,-0.04108082,-0.04638691,-0.024771178,0.03984261,-0.03541568,0.012730678,0.0043025473,-0.028965017,-0.011678231,-0.041442525,0.012093152,0.005962902,-0.01780899,0.053532396,-0.056196127,0.0072379652,-0.049353436,0.020815555,-0.03412607,-0.007039855,-0.02671577,-0.07208928,-0.021075813,-0.047315653,0.037248127,-0.008460799,0.031772938,-0.018080035,-0.02248569,0.028114805,0.058380447,0.112979636,0.026112646,0.07447595,0.052414898,-0.083052665,0.10073266,0.06507219,-0.014078869,-0.030705145,-0.01790989,-0.024873871,0.025308596,0.014039132,-0.025261207,-0.05850254,0.068363085,-0.023396144,-0.056337893,-0.013857517,0.04420251,-0.068585046,0.005093962,-0.041912396,-0.03976045,-0.017818956,-0.011282114,-0.006955164,0.020580355,-0.0502067,0.04332558,-0.02655431,0.013474196,-0.007725093,0.04147133,0.099227525,0.025577439,-0.009744768,-0.024067104,-0.048988737,0.03046381,-0.0196147,0.017558875,0.051114667,-0.08035663,-0.03110114,0.0034816563,-0.049130905,0.005675193,0.047226164,-0.026892709,-0.025056476,0.038805176,-0.06624802,0.021735603,0.021124648,0.040008526,0.0257202,0.025618846,-0.045561273,-0.061641514,-0.003052815,0.052334554,0.04290705,-0.01944449,-0.008360736,-0.027901975,-0.018495755,0.06344922,-0.019557264,-0.017918121,0.0578283,-0.083516166,-0.018827548,-0.026447,0.02799965,-0.0010345536,0.05036525,-0.0529846,0.0010976869,0.021329667,0.04631862,0.036421333,-0.026544264,0.09605919,-0.0169117,-0.022207009,-0.033387158,0.008311063,0.048471175,0.0126755,-0.013065775,0.0036249997,0.02361227,-0.015691293,0.036022082,0.02143798,0.100212164,-0.03063666,0.05509407,-0.010473404,-0.0036286053,-0.042300813,-0.015387151,0.021965114,-0.02578949,-0.010859624,-0.074686415,-0.045305323,0.0033070182,-0.023183262,0.048682302,0.033954185,0.0050931,0.045950774,-0.021999972,-0.0044228113,0.036750033,-0.00025297573,-0.003293653,0.009813334,-0.045309298,0.054842953,-0.02788946,0.0012107954,-0.025570177,0.026020773,0.08964244,0.033783615,-0.03201716,-0.03145628,-0.06559382,0.030597376,0.04170481,-0.020722978,-0.03494964,0.02283936,0.03734393,0.012352578,-0.04267883,-0.034981463,0.033913527,0.011214258,0.05207291,0.011059228,0.047526482,0.034402028,0.023012068,-0.020715844,-0.015692294,0.03533541,-0.018067367,-0.021789463,0.051210072,0.0148507655,0.0531862,0.005290498,0.0062167323,-0.012835591,-0.0027899237,0.035184525,-0.013140233,-0.025841715,0.02429759,-0.088163644,0.028949982,-0.034617558,0.027570138,-0.027128445,0.06414778,-0.01467345,0.08076584,0.050486505,0.04534016,-0.014039563,0.07390418,0.023757271,-0.018578943,0.04709835,0.04482365,0.018130139,0.005964714,0.0067053675,0.0073554823,-0.035713762,-0.029112361,-0.010615542,-0.0065369317,0.052323446,0.006076331,0.033234112,-0.029158445,0.012579352,-0.027199257,-0.044728655,-0.006706594,-0.035948772,0.040959515,0.01317144,0.02581279,-0.015946826,0.017127885,0.070123665,0.008562517,-0.037928745,-0.031137044,-0.029185714,0.010390465,0.0097538475,0.017650766,0.01422455,0.00034787747,-0.026998986,0.023066817,0.008402609,-0.0016162426,0.010957578,0.0076730307,-0.02735027,0.020991798,-0.00091763213,0.026764337,-0.015524041,-0.01850623,0.00265173,-0.0011758843,-0.028037354,0.0074586007,0.0071842936,-0.018029245,0.003981099,0.058240958,0.08742525,-0.007918054,0.040966853,-0.008040551,0.023726042,0.04225836,-0.04498894,0.031154415,-0.025509998,0.030361937,0.024477001,-0.00937829,-0.023924835,0.02119934,0.013144576,0.053147927,0.075152054,6.7893656e-05,0.03732779,0.05994395,-0.01518721,-0.025762448,-0.07440907,0.033908304,0.020882409,-0.0032958211,0.008275153,-0.05775948,-0.05795146,0.069791436,-0.058765475,-0.007384649,0.03974613,-0.009657363,-0.036424607,0.0198539,-0.02852206,0.028143033,0.042717148,0.0077743437,-0.053917468,-0.031703644,0.022595257,0.033161942,0.0039452575,0.036275297,0.020710021,0.0032748,-0.002747497,0.05773312,-0.00014806974,0.061437592,0.0020890008,0.034056988,0.07976885,-0.04153382,-0.013599129,0.036197193,0.007215372,-0.037801825,0.00039960645,-0.039506536,-0.0053110165,-0.075188585,0.052230626,-0.011924999,-0.010116155,-0.007830821,0.028093208,-0.005211367,0.02473441,-0.011615978,0.019089255,-0.032186836,0.00746079,0.030538095,0.0120734675,0.020201998,-0.00935416,-0.054676514,-0.013489317,0.015474707,-0.027626952,-0.011789941,0.039240528,0.037227612,0.010522068,0.094834246,-0.0044609695,-0.0159506,-0.012184214,0.0069438214,0.06219237,-0.028006542,0.0029154185,-0.031044269,0.038705688,0.058223847,0.06889354,0.02104062,0.070553936,0.03077679,-0.063985124,-0.0021025296,0.05725186,0.0035654532,-0.07228829,-0.017613862,0.027442988,-0.014418371,-0.014187193,-0.03904111,-0.04090617,-0.03811503,0.03992955,-0.015013815,-0.0064099785,0.0018046544,0.0025167419,-0.028671779,0.0111994,0.05834576,0.029036634,0.030780785,0.01589898,0.0040126964,-0.026711382,0.035067614,0.027170628,-0.009032266,0.009463567,0.0136385495,-0.005998817,0.041745357,0.0016817196,-0.0046415883,-0.07575171,0.0704335,-0.0042158877,0.062004432,-0.014953204,-0.009939147,0.03125914,-0.02498409,-0.024847047,0.002241629,0.02850624,0.0077002547,0.022151547,0.009510361,0.013077282,0.008271907,-0.0038221073,0.01867326,0.06280212,0.0073413816,0.014544398,-0.04037577,-0.03459664,0.013705125,0.02453338,0.019678367,-0.019284539,0.01796989,-0.016855193,-0.006781338,-0.033339947,-0.01755326,0.01016034,-0.018379126,0.0025408994,0.007317622,-0.03909904,-0.011450018,-0.020160666,0.0072801826,0.013720448,-0.02429197,0.04634001,-0.02242071,-0.030280264,0.0013257057,0.02019224,0.013958403,0.030621558,-0.036066953,0.006902263,-0.0035173292,-0.059225023,0.029107591,0.031825066,0.041082606,-0.011462708,0.00095429376,0.039604086,-0.056106016,0.010204058,-0.051692855,-0.0032533514,-0.016150303,0.0044022985,0.011573015,0.012614561,0.025213346,0.016794609,0.015686564,-0.01311853,0.06560541,-0.015432282,0.03162303,-0.029280452,0.015995827,0.005428912,-0.057836045,-0.029176211,0.03908366,0.015718723,-0.005676272,0.03416571,-0.06269231,0.019805085,0.04963751,-0.034965217,-0.007406554,0.024083428,-0.0053944536,-0.0005439957,-0.029024385,0.03968612,-0.0493242,0.0071934196,0.017470965,0.02654681,0.03603085,-0.038311824,-0.026712343,0.03521446,-0.040330812,-0.0006480586,0.0015009135,0.018721623,-0.045251608,0.0024364742,-0.025446434,0.07409679,-0.046956863,0.011422345,-0.005888088,-0.017195314,0.011853476,0.0280074,0.055811048,0.066763476,0.007629933,-0.022780096,0.031853504,0.020156596,0.010440827,0.044251345,-0.008984797,0.025253478,-0.033882223,0.002977726,-0.10414814,0.014454552,0.004015484,-0.0074127084,-0.062754035,-0.018629529,-0.013108873,0.03482825,0.020564549,-0.062241558,0.005438546,-0.023085322,0.087859266,0.076539405,-0.005370831,0.026144078,0.046996173,-0.011284442,0.057343353,0.048526153,0.06612479,-0.016524171,0.042907078,-0.03753442,0.0075734463,0.004510393,-0.0041973013,0.018858047,-0.0019156709,-0.018090198,-0.050420556,0.019183291,-0.0791881,0.09130366,0.0071282326,0.019646421,-0.053110275,-0.07482112,-0.02333714,-0.07455863,-0.055384304,0.02170416,-0.016810924,-0.015407408,0.0003921168,-0.0088137835,0.053634662,-0.06601,-0.041482523,-0.02523268,-0.030804476,-0.08118927,-0.045406617,-0.004308302,-0.05589348,-0.0005983036,-0.05209281,-0.014793502,-0.028971374,-0.05437243,-0.027537413,-0.03461973,0.036429513,-0.023633352,0.0021053548,-0.012293776,0.024259059,-0.005761898,-0.01781241,0.08156333,-0.01876731,-0.022939792,0.031654082,0.010484599,-0.04899694,-0.014458458,0.045406498,0.032318063,0.000409749,-0.016424386,0.013903519,0.019515611,-0.020001242,0.05791628,-0.07087345,0.06921748,0.04251542,-0.028554905,0.07173166,0.007130304,-0.033548363,-0.03367979,0.009187833,0.037423123,0.0049447184,0.027863922,-0.018553538,0.061072644,0.046144556,-0.017263783,-0.047217127,-0.035464432,-0.0006553993,0.050630767,0.0071704066,0.06162824,0.024803814,-0.025143247,-0.023629166,-0.027887344,-0.02226894,0.020757835,0.052259993,0.022077179,-0.043455485,0.013184736,0.018863976,0.026831107,0.01269327,0.039719183,0.053925287,0.008905787,0.0047563044,-0.036764145,-0.036427792,-0.018584592,0.019585593,0.041350953,-0.020180203,-0.012345785,-0.01364995,0.029175017,-0.0012465202,0.007810372,-0.07025337,0.006205084,0.05722386,0.013361971,0.023134733,0.039238606,-0.020075412,-0.06946033,0.045653544,-0.009974861,-0.0059315027,-0.005178494,0.00057665294,-0.012965179,-0.03367191,-0.015419288,-0.015979642,0.024417158,-0.019395877,0.0056028403,0.015352576,-0.0140627185,0.042954825,-0.05837506,-0.0016556344,0.030007377,-0.0063199615,-0.0033946359,0.04717988,0.0138363205,0.024961961,-0.0350118,-0.03493618,0.065309905,0.010214834,-0.003156102,0.08143068,0.014178589,0.025662102,-0.03424315,-0.007176577,0.022820652,-0.02879729,0.0077418103,-0.02703594,0.033922367,-0.025158796,0.000698475,-0.0404099,-0.054815326,-0.058109738,0.03270475,-0.039495934,0.0036532378,-0.02185309,-0.0032464142,-0.053197965,0.036658313,-0.049894452,-0.07087825,0.004177971,0.029633386,-0.06295824,-0.048719503,-0.043457534,-0.03876034,0.07398084,-0.029501269,0.06095326,0.0018207805,-0.016311353]	Keywords: Nadam, Adam, Nesterov momentum, momentum vector, parameter updates\nKey Objects: momentum vector, parameter updates, Adam update rule\nRefers to Images: None\nHypothetical Questions:\n- What is the advantage of using the current momentum vector instead of the previous one in Nadam?\n- How does Nadam build upon the principles of both Adam and Nesterov accelerated gradient?\n- What is the role of the bias-corrected estimate of the momentum vector in the Nadam update rule?\n---\nSummary:\nNadam incorporates Nesterov momentum into the Adam optimization algorithm by replacing the previous momentum vector with the current momentum vector to improve the accuracy of parameter updates.\nOriginal Text:\n<!-- formula-not-decoded -->  \nNotice that rather than utilizing the previous momentum vector m t -1 as in Equation 27, we now use the current momentum vector m t to look ahead. In order to add Nesterov momentum to Adam, we can thus similarly replace the previous momentum vector with the current momentum vector. First, recall that the Adam update rule is the following (note that we do not need to modify  v t ):  \n<!-- formula-not-decoded -->  \nExpanding the second equation with the definitions of  m t and m t in turn gives us:  \n<!-- formula-not-decoded -->  \nNote that  1 m t -1 1 - t 1 is just the bias-corrected estimate of the momentum vector of the previous time step. We can thus replace it with  m t -1 :  \n<!-- formula-not-decoded -->\nContextualized Text:\nTo create Nadam, an optimization algorithm combining Adam and Nesterov accelerated gradient, the previous momentum vector in Adam's update rule is replaced with the current momentum vector. This 'look-ahead' approach utilizes the current momentum vector to estimate future parameter updates, leading to a more accurate step in the gradient direction.	{"tags": ["optimization", "deep-learning", "algorithm"], "doc_id": "54914a76-9157-43b2-a890-998bd969e049", "summary": "Nadam incorporates Nesterov momentum into the Adam optimization algorithm by replacing the previous momentum vector with the current momentum vector to improve the accuracy of parameter updates.", "doc_type": "text", "entities": ["Adam", "Nesterov"], "keywords": ["Nadam", "Adam", "Nesterov momentum", "momentum vector", "parameter updates"], "key_objects": ["momentum vector", "parameter updates", "Adam update rule"], "contextual_text": "To create Nadam, an optimization algorithm combining Adam and Nesterov accelerated gradient, the previous momentum vector in Adam's update rule is replaced with the current momentum vector. This 'look-ahead' approach utilizes the current momentum vector to estimate future parameter updates, leading to a more accurate step in the gradient direction.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.8 Nadam"}, "hypothetical_questions": ["What is the advantage of using the current momentum vector instead of the previous one in Nadam?", "How does Nadam build upon the principles of both Adam and Nesterov accelerated gradient?", "What is the role of the bias-corrected estimate of the momentum vector in the Nadam update rule?"]}
6492ec39-3c83-4dd3-a092-32adeccc4e4d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017737608,-0.0060918108,0.022587402,0.041672852,-0.031401932,0.065466866,0.03220086,0.0041166507,0.026696725,-0.04605781,-0.014094772,-0.037370626,0.0025558963,0.028941138,-0.08653218,0.0053718113,-0.006853977,0.058979716,-0.015877925,-0.007792783,0.030732065,0.0075564375,0.032128908,0.017313061,0.04541818,0.0613961,0.015431592,0.041379616,-0.0058642384,-0.024236778,-0.017919926,-0.08589254,0.031983443,0.09025295,-0.017381534,0.0009252377,-0.027214462,-0.03408564,-0.041264676,-0.061913762,-0.021079231,0.028516967,-0.0474764,0.026291696,0.027209146,-0.035645522,-0.027262516,-0.049905863,0.00512902,-0.021192761,0.002729116,0.036950633,-0.055248823,0.0026312005,-0.045083556,0.009354753,-0.013582849,-0.011808831,0.0047471263,-0.0602962,-0.023854362,-0.06561172,0.03912563,0.016753731,0.025019906,-0.011208327,-0.0017271735,0.019381577,0.053364027,0.11497994,0.025616901,0.074442305,0.061685663,-0.056316182,0.1078547,0.04187245,0.002941268,-0.024583744,-0.0123475,-0.009820663,0.024422767,0.040105313,-0.0126727205,-0.048933115,0.0747787,-0.046626415,-0.049528513,-0.031463705,0.057874367,-0.056921426,0.00893339,-0.03521729,-0.02735967,-0.018464703,-0.00905492,-0.0053738807,0.037602168,-0.049659126,0.019356908,-0.019942882,0.011689882,0.006629939,0.03954566,0.09742888,0.01590046,-0.030912304,-0.034692355,-0.024849216,0.053084545,-0.012344946,0.021452643,0.044443112,-0.0788954,-0.04477148,-0.0050327517,-0.0493647,0.0076560774,0.045284208,-0.028811269,-0.008070361,0.04226985,-0.058344793,0.035395496,0.02623484,0.035641924,0.031714566,0.028667986,-0.06333863,-0.051862214,0.024327079,0.08464333,0.043347925,-0.031307913,-0.006923996,-0.0047261235,-0.026329825,0.03795384,-0.005512437,-0.016746586,0.057095528,-0.073752835,-0.009910367,-0.024688061,0.0075409897,0.012282583,0.04498316,-0.024587126,0.011580665,0.009829796,0.05088363,0.023019636,-0.038420323,0.08987885,-0.02837306,-0.04655751,-0.01308068,-0.0014967916,0.024036193,0.010731375,-0.011674209,0.009687327,0.024503488,-0.027687391,0.059765205,0.03539714,0.1008912,-0.018105095,0.036070805,-0.015088329,-0.007307692,-0.04584491,-0.034561094,0.021662075,-0.007333542,0.0023350115,-0.09858356,-0.056432102,0.00069872924,-0.03789276,0.054956425,0.032182805,0.0129779065,0.021485018,-0.023416216,-0.01596881,0.026118582,-0.0013423914,-0.019392736,0.014576509,-0.036115184,0.06767326,-0.019368762,-0.018758718,-0.008310295,0.050584164,0.07887275,0.033925094,-0.049488965,-0.0151957935,-0.054490115,0.03193467,0.029332288,-0.017097795,-0.01992074,0.048062474,0.017381344,0.0030311432,-0.019694287,-0.04027501,0.05286795,0.026834914,0.056891635,0.02292264,0.054831177,0.021925522,-0.0041222153,-0.03796328,-0.019006161,0.034648016,-0.023891157,-0.00593672,0.05473333,0.009328734,0.06522829,-0.0117488075,0.021568421,-0.0064658127,-0.01083525,0.015351774,-0.018261382,0.008289403,0.032293957,-0.093753986,0.027312553,-0.041572113,0.03277585,-0.03158566,0.04235298,-0.015762582,0.065549545,0.061440036,0.058064606,0.008785247,0.069195144,0.011734072,-0.011758202,0.023399213,0.0136345355,-0.008568706,-0.007774091,0.002601413,0.000712373,-0.0788728,-0.03022521,-0.02110762,-0.0062466823,0.031033851,0.0041102916,0.049821604,-0.035257544,0.029199686,-0.015153203,-0.048207533,-0.01728452,-0.035015583,0.0029193414,0.0063491864,0.04129214,-0.027308755,0.015891235,0.06022413,0.013689297,-0.033945285,-0.03585325,-0.0260218,-0.0013014518,0.0022366454,0.011603072,0.01941785,-0.010154646,-0.018785112,0.015249437,0.01666392,0.012665868,0.012344032,0.0012505621,-0.04459919,0.0096326675,0.014733938,0.036903445,-0.00059185264,-0.03348286,0.0028857046,0.0004837487,-0.024767805,-0.00065394084,0.008624019,3.267655e-05,-0.0044385586,0.06784261,0.0791916,-0.031448018,0.030231295,0.007108328,0.03179986,0.041644778,-0.048217457,0.036207948,-0.0077553936,0.034937218,0.020306107,-0.010461068,-0.007290557,-0.00014333843,0.027076438,0.034151513,0.07443015,0.008168122,0.040452495,0.032695636,0.029355625,-0.03696973,-0.052598175,0.05157101,0.027340062,-0.0016344177,-0.008424913,-0.034228787,-0.03722234,0.052048706,-0.059283625,-0.012607185,0.052669663,-0.033505686,-0.025732612,0.033385552,-0.03550415,0.015453339,0.06048572,0.014245339,-0.056896757,-0.010774844,0.002004836,0.03400764,0.0007343673,0.014623673,0.011390562,0.010650499,-0.008029846,0.06563048,0.0019430618,0.040082622,-0.0070578093,0.017414635,0.082152516,-0.015416199,-0.007948213,0.027577948,0.02084558,-0.07029538,0.015684545,-0.016176134,-0.012910343,-0.092468195,0.049235962,-0.012249418,-0.023868488,-0.004137594,0.010712076,0.011737179,0.009788525,-0.027794579,0.029022573,-0.009161109,-0.013798526,0.030972287,0.004748175,-0.0028561396,-0.012168446,-0.0506054,-0.03637709,0.013245478,-0.024677193,-0.022444643,0.04872071,0.05273065,0.0065707616,0.09064241,0.0061692484,-0.019809831,-0.024855688,0.0012033961,0.048985824,-0.05097465,-0.00056904496,-0.040973425,0.042761408,0.063060544,0.054623265,0.011709888,0.07403298,0.023686798,-0.054249063,0.0052333786,0.04118467,0.007793792,-0.06916342,-0.0261351,0.016715841,-0.04034217,0.005730922,-0.056682218,-0.042019475,-0.024407975,0.021436216,-0.033679023,-0.0053918753,-0.00810566,0.00073738093,-0.011211498,0.018285433,0.06276143,0.023306422,0.0071414025,0.014097449,0.0054199295,-0.058966715,0.011350438,0.045260232,-0.00026528738,-0.01911682,0.006640996,-0.009875188,0.028789196,0.0135660395,-0.017571319,-0.07880589,0.08826204,-0.010095838,0.03598209,-0.032454923,-0.023971373,0.03306311,-0.010526798,-0.004699816,0.003148947,0.025780255,-0.024290765,0.023376063,0.016720565,-0.012865071,0.0016756179,-0.037279356,0.013666364,0.06998639,0.012637376,0.0070341425,-0.03957021,-0.023624878,0.03055851,0.02362794,0.018861251,0.0019933577,0.024476796,-0.013463973,0.004757482,-0.030741375,0.0113274595,0.02808058,-0.023623101,0.022050254,-0.034454957,-0.022830697,-0.023424238,-0.017030606,0.016244642,0.005237851,-0.02537006,0.038020354,-0.03537776,-0.031558566,0.004512488,0.010601571,0.022117853,0.04171875,-0.021541363,0.031391494,-0.009325288,-0.06682978,0.04401415,0.031125715,0.05008786,0.0109720295,-0.007199961,0.019050684,-0.028600825,0.0038832903,-0.039162766,0.0041991137,-0.00858125,-0.023464091,0.0032484846,0.0073676445,-0.0074456297,0.03421705,0.012441294,-0.02596557,0.058541223,-0.03882157,0.024336623,-0.011541724,0.007579826,0.035855826,-0.054404538,-0.0002428311,0.023263976,0.009394651,-0.026579704,0.020114757,-0.05320528,0.02295647,0.04702494,-0.07239859,0.023021493,0.03244858,0.017236304,0.013468967,-0.020216849,0.010892255,-0.091991,0.004672629,0.028068444,0.021130836,0.04247147,-0.02393229,-0.02412917,0.05079447,-0.051053077,-0.0020842417,-0.005828524,-0.019720623,-0.044022948,-0.005004334,-0.02211661,0.058788158,-0.0512784,0.0016194382,-0.032683525,-0.012045463,0.036879074,0.065382525,0.03653905,0.06891186,-0.01758171,-0.0384792,0.035549033,0.013947746,0.006987558,0.022751689,-0.01226032,0.012657494,-0.014716072,0.0068355305,-0.087702245,0.02125151,-0.005532304,-0.029100096,-0.06182336,-0.01586521,-0.025981229,0.026001584,0.008923308,-0.062808685,-0.0013446563,0.007836953,0.0802403,0.06391865,-3.9135528e-05,0.03132258,0.063643955,-0.011974117,0.05722445,0.03354006,0.04260383,-0.01857191,0.027271578,-0.056155298,0.0074356105,0.018262815,0.013266799,0.042253856,-0.012981889,-0.025081553,-0.041753642,0.001711123,-0.048178095,0.047623355,-0.011895572,0.013630626,-0.057586975,-0.06889081,-0.018192502,-0.053590238,-0.040182903,0.024111958,-0.0030849974,-0.015795574,-0.012122443,-0.010125485,0.0556579,-0.07150024,-0.038633633,-0.016761465,-0.006708605,-0.076116204,-0.02652756,-0.0049495744,-0.059099603,-0.031465113,-0.050626162,-0.0053899228,-0.019863695,-0.056043938,-0.024970898,-0.061833017,0.0077082175,-0.029984333,0.02950596,-0.005411967,0.015983546,-0.0022944817,-0.015389819,0.08001578,0.008905059,-0.026933819,0.033110585,0.024571326,-0.059355073,-0.03151839,0.03462936,0.014138418,-0.001214728,-0.047765907,0.012846668,0.0122447,0.01809475,0.031736787,-0.028690876,0.03856339,0.04540024,-0.015017832,0.05757664,-0.010729283,-0.030637287,-0.030084377,-0.00429812,0.02250216,-0.0044029877,0.02293924,0.0029360275,0.028567584,0.04274763,-0.021133339,-0.044428278,-0.04705993,0.023006704,0.034759156,0.009449905,0.06884279,0.03804468,-0.03586642,-0.019971488,-0.05262623,-0.009973019,0.019722208,0.02520962,0.025375426,-0.04751753,8.401886e-05,0.022019282,0.017155463,0.025045896,0.046129826,0.07566378,0.0038520673,-0.018004073,-0.048610438,-0.051281296,-0.014576665,0.020540971,0.034609593,-0.0193583,0.007037418,-0.009129715,0.018443909,-0.009470154,0.010592123,-0.11815139,0.009213,0.044517472,0.016400684,0.032288164,0.030741438,0.013552603,-0.06387611,0.049574744,0.012094824,0.0001968685,-0.024449106,-0.0007665548,-0.024117138,-0.031468865,-0.0070850523,-0.010280236,0.033071686,-0.029883014,-0.012355552,0.025570035,-0.022559602,0.04387493,-0.03356757,-0.0023854007,0.017155213,0.014713353,-0.013268306,0.022579506,-0.006970389,0.0376832,-0.03349089,-0.049300145,0.046199623,0.015668878,0.003629898,0.09180899,0.021719586,0.03780319,-0.021349536,-0.01611088,0.046113662,-0.018584562,-0.002796845,-0.011106026,0.06304467,-0.012038602,-0.002241399,-0.06081665,-0.029539363,-0.0739334,0.042442773,-0.014755869,0.009489371,-0.0073527494,0.006817995,-0.048898324,0.051159125,-0.029213607,-0.07340793,0.014587267,0.03010605,-0.043114476,-0.05187809,-0.03074636,-0.018964294,0.078241974,-0.041144405,0.069134854,-0.006878639,-0.014784894]	Keywords: Nadam, momentum vector, bias correction, optimization\nKey Objects: momentum vector, bias correction\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of bias correction in the momentum vector?\n- How does replacing  m t -1 with  m t contribute to the Nadam update rule?\n- In what way does Nadam improve upon the Adam optimization algorithm?\n---\nSummary:\nTo derive the Nadam update rule, the bias-corrected estimate of the previous momentum vector ( m t -1 ) can be replaced with the bias-corrected estimate of the current momentum vector ( m t ).\nOriginal Text:\nNote that  1 m t -1 1 - t 1 is just the bias-corrected estimate of the momentum vector of the previous time step. We can thus replace it with  m t -1 :  \n<!-- formula-not-decoded -->  \nThis equation looks very similar to our expanded momentum term in Equation 27. We can now add Nesterov momentum just as we did in Equation 29 by simply replacing this bias-corrected estimate of the momentum vector of the previous time step  m t -1 with the bias-corrected estimate of the current momentum vector  m t , which gives us the Nadam update rule:  \n<!-- formula-not-decoded -->\nContextualized Text:\nAs part of the Nadam optimization algorithm, which combines Adam and Nesterov accelerated gradient (NAG), a step involves incorporating a bias-corrected estimate of the momentum vector. Specifically, the bias-corrected estimate of the previous momentum vector ( m t -1 ) can be replaced with the bias-corrected estimate of the current momentum vector ( m t ) to create the Nadam update rule.	{"tags": ["optimization", "deep-learning", "momentum"], "doc_id": "6492ec39-3c83-4dd3-a092-32adeccc4e4d", "summary": "To derive the Nadam update rule, the bias-corrected estimate of the previous momentum vector ( m t -1 ) can be replaced with the bias-corrected estimate of the current momentum vector ( m t ).", "doc_type": "text", "entities": ["Nadam", "Adam"], "keywords": ["Nadam", "momentum vector", "bias correction", "optimization"], "key_objects": ["momentum vector", "bias correction"], "contextual_text": "As part of the Nadam optimization algorithm, which combines Adam and Nesterov accelerated gradient (NAG), a step involves incorporating a bias-corrected estimate of the momentum vector. Specifically, the bias-corrected estimate of the previous momentum vector ( m t -1 ) can be replaced with the bias-corrected estimate of the current momentum vector ( m t ) to create the Nadam update rule.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.8 Nadam"}, "hypothetical_questions": ["What is the purpose of bias correction in the momentum vector?", "How does replacing  m t -1 with  m t contribute to the Nadam update rule?", "In what way does Nadam improve upon the Adam optimization algorithm?"]}
4e1d7039-6394-40d4-8b19-7e0c750149e4	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0034105827,0.021688392,-0.03451812,0.02513684,-0.04467656,0.066980965,-0.013430285,0.0389469,0.03479288,-0.06261209,0.019699667,-0.009077989,-0.009681762,0.0015954523,-0.03739318,0.024341326,-0.03256796,0.090790495,0.013469531,-0.008860441,0.025573334,0.03230678,-0.010649322,0.036267743,0.022157485,0.042012963,0.00018016965,-0.04930594,0.018229706,0.03798418,-0.0009788512,-0.04119545,0.03364889,0.03547935,-0.022874074,-0.003953655,-0.005772438,-0.043219127,-0.019952431,-0.051749308,-0.066776186,0.03695179,-0.025287338,-0.0009982737,0.0062266607,-0.02217125,-0.040097207,-0.012433156,0.0122275585,-0.014505246,-0.00026634478,0.03195295,-0.073831394,0.0008114998,0.011582912,0.00961975,-0.04123994,0.015526439,-0.006878912,-0.023601092,-0.05263864,0.0072830715,-0.008375011,-0.062473677,0.05534663,-0.052324075,-0.046644725,0.053701747,0.0353375,0.11541698,-0.007608916,0.023073658,-0.002003431,-0.10930898,0.128924,0.01360869,-0.022935841,-0.058903858,-0.023041837,-0.00915764,0.038428977,0.009282939,-0.028176611,-0.0042272843,0.0895516,0.027111448,-0.072468296,-0.041204058,0.05997506,-0.060043257,0.022304561,-0.04684331,-0.002461878,0.013940484,0.007190603,0.0008759854,0.024412485,-0.018747313,-0.011253754,-0.045725808,0.03145342,0.0031190366,0.024833605,0.09098725,0.018483294,0.013721497,-0.030297056,-0.026315363,-0.015136993,0.008155049,0.028504066,0.016438488,-0.053948797,-0.017223913,-0.023529047,0.012282424,-0.03857399,0.021377835,0.022704279,-0.028971653,0.061614912,-0.022815807,-0.013709801,-0.014374841,0.062388472,0.07375288,-0.007593236,-0.02280083,-0.006829466,-0.023829266,0.02030523,0.03822325,-0.03380297,0.01586507,-0.018636804,-0.030514609,0.060397506,0.018456995,-0.021854548,0.004613679,-0.046044365,-0.073922195,-0.044764224,0.042821914,0.024369445,0.039677,-0.058885947,0.042584293,-0.008488947,0.07546217,0.0039019154,-0.030622855,0.026918335,-0.008244342,-0.069779545,-0.0029152127,-0.017970232,0.010441503,0.015771382,-0.06622255,-0.043488692,-0.009750019,-0.0146539975,0.05171543,0.030895367,0.09710168,-0.0037642037,0.029684281,-0.014477039,-0.00578844,-0.0032186464,-0.03941582,0.0015434152,-0.005562146,0.00013704061,-0.021986548,0.017934125,0.0060523283,-0.021327836,-0.02615099,-0.025872422,-0.0009097203,-0.015322368,-0.013682122,0.03563351,0.05979655,0.02303329,0.03285919,0.050152324,-0.04133661,0.01018224,0.008069259,0.033191342,0.019156795,0.032051533,0.045377042,0.023743195,-0.015830461,0.017024383,-0.022704735,0.030280672,0.020594843,-0.035611868,0.0025791842,0.028779393,0.006122178,-0.011074334,-0.029502414,-0.056153145,0.04143451,-0.0057994914,0.040751196,-0.023234291,0.046606388,-0.0025379641,0.03424577,-0.029593525,-0.0027671594,0.037515838,0.016026696,-0.053844616,-0.010604709,-0.02899788,0.04143317,0.010007592,-0.029779524,-0.05801497,-0.0031226755,0.057567447,0.025524791,-0.025138643,0.07484169,-0.029066654,0.074001424,0.011880557,-0.0035983168,0.026842043,0.0051162704,0.019010771,0.063044496,0.007258541,0.005990264,-0.009056741,0.08399953,0.008488847,-0.018967392,0.022864303,0.06757621,0.028169835,0.029858444,-0.05415919,0.014907203,0.013404918,-0.03070719,0.001381151,-0.018537031,0.027111355,-0.0134424735,0.06939385,-0.07588215,-0.0013456278,-0.057222527,-0.010898781,0.0039336807,0.008091681,0.020895345,0.028493935,0.012040899,0.019447105,0.09128727,0.038721614,0.061384067,0.055011507,-0.019185578,-0.046758156,0.044701833,-0.010440484,0.04100007,0.04329975,-0.0075939586,-0.034071572,0.036672402,0.0106195165,-0.0041950727,0.01986128,-0.0016666764,9.601133e-06,-0.06805063,-0.0689841,0.018103808,0.057856873,-0.06810633,0.050625406,0.036388956,-0.041348584,0.022400532,-0.0024498461,-0.03522949,0.034285437,0.06381998,0.10839791,-0.024562744,0.0635966,0.022953494,-0.016711708,-0.054089036,-0.08454599,0.030861517,-0.00859671,0.028738115,0.057713974,-0.036725815,-0.06395672,0.019300804,0.039368723,0.04330166,0.0658579,-0.0047778585,0.014560981,0.020713972,-0.008957463,0.0041273157,-0.040932454,0.04381899,0.0667803,0.028951248,0.018875146,-0.060958136,-0.07556564,0.015956052,-0.034493893,0.0390428,0.064339966,-0.06648972,-0.0048498083,-0.0027324015,-0.041229628,-0.018173523,0.030052252,0.066350326,0.008934248,-0.021626012,0.038641937,0.020869842,-0.036521662,0.0026949362,-0.032449234,0.03314986,-0.004193743,0.042130053,-0.013228669,0.056594957,0.0090302145,0.006754095,0.053242803,-0.002592282,-0.017641036,0.010572663,0.03174858,-0.0163941,0.019850915,-0.03311409,0.019678237,-0.036145765,0.015418362,-0.010854512,-0.03164668,-0.03309258,-0.031940844,-0.027045162,0.026663879,0.0011177639,0.022405228,-0.031662103,0.014905054,0.001572551,0.030451689,-0.027713472,0.028487861,-0.06646208,-0.023046942,0.0036979516,-0.02135565,-0.04229994,0.033375505,0.036481325,0.02776241,0.073233984,0.0060090222,0.015870094,0.03398512,-0.013756947,0.02301626,0.009235418,0.0064227115,-0.010273905,0.010340165,0.056875445,0.04527073,-0.0058251536,0.02352988,0.0057104314,-0.06886316,0.038695183,0.008465392,0.03537109,-0.028268196,-0.03048414,0.03946718,0.016688596,0.049778845,-0.0793486,-0.005685298,-0.022427632,0.009269551,-0.034466367,-0.02425301,-0.0215064,-0.037286587,-0.045583926,-0.035583932,0.05282204,0.038193,0.019934071,-0.008300089,0.019879863,-0.041419722,0.06789345,0.0054579573,-0.03468404,0.04559921,0.023847992,-0.016063519,0.024314698,0.02265834,-0.0010651161,-0.07297787,0.07595766,-0.017717633,-0.022625878,0.006302605,-0.030436553,0.042604554,0.010225551,0.0049967836,-0.019630473,-0.008383837,-0.049330212,-0.014176551,0.001892057,0.002771807,-0.0033859317,0.01981067,0.018354284,-0.022936592,0.0035962428,-0.021370707,-0.006048351,0.0068705203,0.0055337963,0.023877747,0.015657138,-0.057065282,0.028862216,0.019002808,-0.034054074,-0.03320224,-0.060481966,0.00658885,0.009711757,-0.052602876,-0.033320207,-0.03740378,-0.004336285,-0.018493509,0.009771498,0.019383408,0.012287991,0.12638348,0.031789146,-0.049809713,-0.059495293,0.0034065067,0.034263555,0.028862221,-0.0045250775,0.0026496816,-0.015082323,-0.06903898,0.05557167,-0.001375521,-0.004427994,-0.0040134164,0.040923588,-0.011173946,-0.031512123,0.023028834,-0.009383114,0.013107557,-0.015036161,-0.011885317,0.008271226,0.06831786,-0.0075315866,-0.00039870196,0.012072251,0.0045477864,0.0475797,-0.04126155,0.020705756,-0.003952969,-0.008552608,0.0029939485,-0.030251598,-0.002177515,0.018278282,0.0025788534,-0.0302948,0.01021453,-0.0116215795,0.035457354,0.047635384,0.03094603,-0.0182412,0.040270966,-0.014749772,-0.0209272,0.03788533,0.01738673,-0.047992602,0.0055802353,-0.02082734,0.008338844,-0.0014227655,-0.09384103,-0.066783294,0.027050594,-0.0068376507,0.0035080223,0.019314889,-0.00036664252,-0.039350554,-0.009517318,-0.022651399,0.07795473,-0.036851514,-0.00012956918,0.022720305,-0.017195625,0.025001861,-0.0021402135,0.026785277,0.02424472,-0.070699684,-0.008417173,0.045955706,0.025931729,-0.026439194,0.012833724,0.01782106,0.036336675,0.014314411,0.016014224,-0.06086569,-0.023288272,0.0019629733,-0.00811948,-0.079193845,-0.03364342,0.011677899,-0.0057933894,-0.009039037,-0.017280249,-0.02152526,-0.030213179,0.090379424,0.067794204,-0.019260025,0.007900502,-0.021645399,-0.0022897176,-0.011257272,0.04309241,0.049861215,0.0009331735,0.009586519,0.0045698783,0.042663623,-0.016919468,0.02833891,-0.04535322,-0.056980535,-0.0058341366,-0.03134386,-0.02686202,-0.01283119,0.081274815,0.00258382,0.027172547,-0.028049158,-0.083015084,0.0056892717,-0.02183681,-0.05364669,0.013254819,0.019614354,0.012060979,0.015965275,-0.02946438,0.021324856,-0.08189408,-0.021259787,0.024517246,0.002278688,-0.07907249,-0.01879693,0.007771393,-0.053630147,-0.047326695,-0.10063424,0.038308747,0.0258042,0.008911941,-0.032719914,-0.048798468,0.010653933,-0.017244501,-0.024536187,-0.01080291,0.0388362,0.027179824,-0.0011399005,0.004281698,0.009034707,-0.0037093817,0.022939172,0.02354574,-0.04540774,-0.03755866,0.06663627,0.014588769,-0.004698358,-0.044063717,0.031384636,0.008614076,-0.009915338,-0.012921402,-0.022207461,0.05453993,-0.021161405,-0.0041981195,0.10638561,3.852326e-06,-0.005980771,-0.028995456,0.022254232,-0.027757665,0.015727311,0.08320387,-0.0029604733,0.04609925,0.021826604,-0.060083408,-0.010645073,-0.03835532,0.018079951,0.018867297,-0.008965688,0.04295595,-0.001175181,-0.046391867,-0.017650336,0.008463977,-0.037414867,0.059048757,0.026511176,-0.024835994,-0.023026554,-0.017529923,-0.036743063,0.030407665,-0.054113064,0.015170697,0.03498602,0.07286813,0.018269528,0.024828969,-0.025887363,-0.005462172,-0.052524228,0.029407045,-0.03333685,0.02407528,-0.022715213,-0.0022938715,0.016362242,0.013875391,-0.03930479,-0.030568562,0.015158299,0.027283328,0.028796513,0.06297354,-0.039003704,-0.08141014,0.039318386,0.025821546,-0.012597513,0.0046667843,0.021555284,-0.028186196,-0.029956777,-0.009403981,-0.014642172,0.009536368,0.031344578,0.0019770446,0.019621758,-0.03986149,0.014704313,-0.01388405,-0.021335233,0.047591332,0.0047614262,-0.006208864,0.005552039,0.021036353,0.03822396,-0.047720324,-0.03482563,0.06591362,0.040633094,-0.014752104,-0.013157946,-0.03133704,0.059372097,-0.019052926,0.015698042,0.09038224,-0.006829324,0.023915432,-0.015860058,0.02128976,0.016483426,-0.022033567,0.036342595,-0.04914998,-0.035256624,0.036849473,-0.022343375,0.0368837,0.0031631477,-0.037779346,-0.032236002,-0.024756346,-0.028287731,-0.033422284,0.026537849,0.033892624,-0.01408182,-0.039659612,-0.030180605,0.009090099,0.03685714,-0.027453195,0.06266585,-0.030647567,0.012474101]	Keywords: optimization algorithms, loss surface, Beale function, momentum, NAG\nKey Objects: optimization algorithms, loss surface, momentum, NAG\nRefers to Images: None\nHypothetical Questions:\n- Why were Momentum and NAG initially led off-track?\n- What does it mean for an algorithm to 'converge similarly fast'?\n- How does NAG's responsiveness contribute to its ability to correct its course?\n---\nSummary:\nTwo figures illustrate the optimization behavior of different algorithms, showing their paths on a loss surface and at a saddle point, providing intuition for their performance.\nOriginal Text:\n### 4.9 Visualization of algorithms  \nThe following two figures provide some intuitions towards the optimization behaviour of the presented optimization algorithms. 13  \nIn Figure 4a, we see the path they took on the contours of a loss surface (the Beale function). All started at the same point and took different paths to reach the minimum. Note that Adagrad, Adadelta, and RMSprop headed off immediately in the right direction and converged similarly fast, while Momentum and NAG were led off-track, evoking the image of a ball rolling down the hill. NAG, however, was able to correct its course sooner due to its increased responsiveness by looking ahead and headed to the minimum.\nContextualized Text:\nTo provide intuition towards the optimization behavior of algorithms like Adagrad, Adadelta, RMSprop, Momentum, and NAG, Figure 4a visualizes the paths they took on the contours of a loss surface, specifically the Beale function. All algorithms began at the same point, but took different routes to reach the minimum; some algorithms, such as Momentum and NAG, were initially led off-track.	{"tags": ["optimization", "visualization", "deep-learning"], "doc_id": "4e1d7039-6394-40d4-8b19-7e0c750149e4", "summary": "Two figures illustrate the optimization behavior of different algorithms, showing their paths on a loss surface and at a saddle point, providing intuition for their performance.", "doc_type": "text", "entities": [], "keywords": ["optimization algorithms", "loss surface", "Beale function", "momentum", "NAG"], "key_objects": ["optimization algorithms", "loss surface", "momentum", "NAG"], "contextual_text": "To provide intuition towards the optimization behavior of algorithms like Adagrad, Adadelta, RMSprop, Momentum, and NAG, Figure 4a visualizes the paths they took on the contours of a loss surface, specifically the Beale function. All algorithms began at the same point, but took different routes to reach the minimum; some algorithms, such as Momentum and NAG, were initially led off-track.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.9 Visualization of algorithms"}, "hypothetical_questions": ["Why were Momentum and NAG initially led off-track?", "What does it mean for an algorithm to 'converge similarly fast'?", "How does NAG's responsiveness contribute to its ability to correct its course?"]}
376a0686-a372-430e-b6bc-5d94b406987c	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0012238346,0.0023216351,0.0015588403,0.020552678,-0.038188044,0.05788341,-0.013871139,0.027870337,0.02166182,-0.031554524,-0.012149589,-0.04224809,-0.023688179,0.0016281904,-0.028274918,0.013902501,-0.010684872,0.09735987,0.008172792,-0.015546261,0.060518436,0.017788557,0.011379441,0.0038959978,-0.040867437,0.03021167,-0.020047484,-0.013257787,0.044992376,-0.008023781,-0.0048444066,-0.035844855,0.008039612,0.04038234,0.021136303,-0.014764591,-0.004024913,-0.026873803,-0.02767636,-0.071890146,-0.063134335,0.07366566,0.016208626,0.011865301,0.017468397,-0.0483459,-0.028474582,-0.033790108,0.015023351,-0.01943463,-0.009505867,0.034333285,-0.038344916,0.023987178,-0.040127546,0.04612568,-0.06611965,0.009503332,-0.001974208,-0.016626129,-0.03349553,0.0008176877,0.008457329,-0.02279161,0.09802594,-0.05743929,-0.05927249,0.058870517,0.0027169236,0.088807024,-0.023047304,0.06682845,-0.019716356,-0.10589464,0.10831845,0.014659974,-0.033776395,-0.06294967,-0.009731402,0.0022836542,0.048685428,0.05178839,-0.062435534,-0.016309278,0.08128092,0.015229666,-0.03602627,-0.015201359,0.09195376,-0.05538932,-0.027449828,-0.061163705,0.005143003,0.03041307,-0.014862309,0.01385331,-0.002995188,-0.008756257,-0.009343998,-0.046042774,0.055599146,-0.009820037,0.0153460875,0.09143555,0.007496005,0.033660226,0.034573033,0.0035632725,-0.026130421,-0.0053641316,0.021294132,0.033291165,-0.042146713,-0.0035384449,-0.0552301,-0.006240391,-0.025801565,0.031198176,0.018628983,-0.052366626,0.032255363,-0.04740293,-0.052110825,-0.0022808472,0.028434105,0.088209376,-0.023418294,-0.015098076,-0.025916982,0.0031658257,0.022280375,-0.011253572,-0.050434235,0.041675914,-0.01641803,-0.0736336,0.01053163,-0.0212228,-0.007574933,0.0032897843,-0.05768915,-0.023309398,-0.04707233,-0.00522621,0.011246925,0.038911384,-0.079259776,0.03655991,-0.0103159975,0.05428614,0.0019567604,-0.052624635,0.0704153,-0.02522561,-0.053995248,-0.020261796,-0.017904932,0.043116845,-0.0061386917,-0.020107616,-0.043646444,0.0593254,0.024629653,0.0647573,0.046504334,0.09055841,-0.002522513,0.055801384,-0.0144053,-0.02731768,-0.023257883,-0.04116415,-0.012780111,0.014983428,-0.008356919,-0.048382338,0.039555706,-0.02854865,-0.0040022493,0.004886918,0.004982803,0.0060539553,-0.033309806,-0.022971962,0.019795131,0.03950434,0.033490002,-0.0028376856,0.07674891,0.0042338017,0.0022997204,0.013088223,0.02288443,0.002141827,0.04068334,0.053932328,0.038614843,0.016406773,0.020806743,-0.037097435,0.0065528504,0.029230196,-0.06362591,-0.019519277,0.036174934,-0.024779232,-0.0006970158,-0.09505959,-0.05619223,-0.013242602,-0.04672248,0.06135246,0.012153771,0.022172932,0.0152142495,0.05176603,-0.03310575,0.014007752,0.059860602,0.009938091,-0.016710674,-0.010620472,-0.0024455604,0.038688634,-0.0075587155,-0.020545233,0.014755747,-0.028048253,0.062468287,0.019067084,-0.04210423,0.059171684,-0.03167747,0.053024992,0.017423738,-0.013022259,0.011886523,0.01949656,0.020385392,0.058622144,0.017094806,-0.016207768,-0.011774374,0.080654435,-0.028831465,-0.02951118,0.041173037,0.020381933,0.024367914,-0.013260778,-0.07527533,-0.00079044374,-0.011036862,-0.03812577,0.013684646,-0.015793309,0.04773637,0.02229144,0.07270323,-0.075610496,-0.036893703,-0.052997906,-0.01408185,0.010973957,-0.019325113,0.04138753,-0.0040520146,-0.0036015185,-0.013408036,0.055726234,0.04579323,0.0090429485,0.03205853,-0.0068973494,-0.03551356,0.06000493,-0.024706032,0.02004378,0.04682438,-0.037195217,-0.023047015,0.037809744,0.0072730184,8.562765e-06,-0.0061416193,0.07177259,-0.005321689,0.0040247533,-0.032213837,0.019493243,-0.009647288,-0.068633065,0.06035006,0.034993313,-0.033118352,0.014460401,0.010128178,-0.0354104,0.015777906,0.038212303,0.08379059,-0.033864286,0.0636518,-0.017732091,0.016878806,-0.015493923,-0.089568876,0.025362208,-0.008624526,0.01633936,0.069247305,-0.04216145,-0.012894912,-0.015126182,-0.014905104,0.027933069,0.05860083,0.02319134,-0.0016541093,-0.020490428,0.019027533,-0.013962619,-0.017065182,0.055798452,0.021690745,0.024228955,-0.00796379,-0.047444962,-0.055829614,0.012256517,-0.026393842,0.059413727,0.04691143,-0.02564949,-0.020678021,0.008792267,-0.040839165,0.010638867,0.039223235,-0.0071984697,-0.018016234,-0.03363808,0.01382462,0.0009121172,-0.0047248346,-0.020483771,-0.0066485982,-0.01732847,-0.012390215,0.085763045,0.0043696603,0.04714279,0.042465053,-0.010898135,0.017462209,-0.016147155,-0.046435874,0.04664302,0.04205419,-0.0482296,0.00023443962,-0.03186759,0.025353706,-0.086806126,0.038781013,0.028984627,-0.026416818,0.01720774,-0.012033616,-0.010134807,0.010127238,-0.012361157,0.021821346,-0.03206138,0.023757778,0.034188967,0.031632964,0.029985715,0.014805869,-0.07841242,-0.0067875925,0.03799696,-0.02803303,-0.02758571,0.024825724,0.020595219,0.009275267,0.08767616,0.013771774,0.0116529735,-0.0050462484,0.00734777,0.0153867155,0.024678333,-0.029701574,0.0006304727,0.005764491,0.051429454,0.02044295,-0.00015681425,-0.0027868566,0.05036258,-0.047577918,0.01873569,-0.006649956,0.052533038,0.016148947,-0.008961802,-0.011199668,-0.0149551565,0.016483864,-0.07422927,-0.035598777,0.004156181,-0.0010251021,-0.010581825,-0.0230817,-0.007729198,-0.029549746,-0.07148768,-0.05795597,0.031616867,0.023844525,0.023320692,0.014253539,0.019112442,-0.013217416,0.0698514,-0.0008718198,-0.006677769,0.027007865,0.06395828,-0.02878737,0.045341875,0.0068531274,0.0029102464,-0.018437719,0.04583056,0.020311821,0.039645653,0.041994754,-0.046059687,0.06657478,0.04143109,-0.015563099,0.0066117994,-0.0009527256,-0.01589863,-0.0037845962,-0.0025686156,0.0034996537,-0.016037678,-0.004246147,-0.029529681,0.014289001,0.024880504,0.01209702,-0.06477754,0.009219711,0.045033228,0.0052914657,0.030383393,-0.024090253,0.0017316659,0.007833899,-0.019039202,-0.018402008,-0.028756576,0.02371398,0.0068863523,-0.09209294,0.011903858,-0.062229764,0.03243015,0.008911576,0.027240219,0.040725205,0.00376724,0.0963757,0.014352469,-0.026049964,-0.030321267,0.037696935,0.029023327,0.025524972,-0.012942083,-0.04046506,0.0026852568,-0.044287384,-0.008253604,-0.010525891,0.015754534,-0.035253253,0.04381503,0.023895621,-0.0666297,0.023037007,0.0033124157,0.0021751574,-0.067757234,-0.033527404,0.003260442,0.055368125,0.051933985,0.022780722,0.007909753,0.06472474,0.056481853,-0.04830692,0.02346481,-0.010263347,0.0062018074,0.008801062,-0.06988655,0.020768477,-0.009611486,0.0028716952,-0.012913809,0.043949276,-0.01767066,0.026445001,0.04142308,0.008606646,0.009096787,0.027903032,-0.030055592,-0.008959001,0.02596787,0.03887068,-0.037385773,-0.02897925,0.00024052562,-0.006431694,0.012946488,-0.07981587,-0.06781768,0.020108905,-0.022720085,-0.0053679748,0.003846027,-0.01679609,-0.028368985,-0.030331561,-0.011319995,0.06700946,-0.041906815,-0.010222257,0.03693945,-0.016054126,0.025524292,0.015912341,-0.028030714,0.019947752,-0.05473726,0.0043354686,-0.012333946,-0.0073906644,-0.012044926,0.023873582,0.0032292171,0.013460662,-0.010796231,-0.0017013183,-0.06391135,-0.019152159,0.008808641,-0.04094221,-0.04390814,-0.050795604,-0.022601334,0.02680394,0.020030094,-0.0022728015,0.017027507,-0.049183697,0.08066337,0.05224225,-0.0015156481,-0.0058078156,0.01501329,-0.0016780108,0.005535976,0.07494883,0.027747585,0.017045982,0.044485558,0.022079928,-0.024398487,-0.0044079307,-0.0032980805,0.019002674,-0.036585283,-0.007599905,-0.05020277,-0.0038001644,-0.025758727,0.07624089,-0.016040249,0.03963815,-0.02684895,-0.05367738,0.0026059938,-0.042912804,-0.061951015,0.023731034,-0.007923258,-0.028665522,0.05789986,-0.013886639,0.031887,-0.051928647,-0.034675144,0.03412198,-0.007018659,-0.08438087,-0.054860804,-0.0038510808,-0.014359922,-0.015062346,-0.0453107,0.04659635,-0.012657199,-0.019602375,-0.018290004,-0.088803485,-0.01656451,-0.02726904,-0.030387212,-0.027937435,-0.0009078953,0.008979884,0.0023073563,0.008874937,0.0007109026,-0.016961046,-0.010240258,-0.011025942,-0.051578265,-0.00079997355,0.0720883,-0.015772548,0.027491719,-0.018884955,-0.0024430268,0.04521007,0.010063721,-0.029681526,-0.022327164,0.04182761,-0.009502079,0.030487161,0.07879875,-0.0050625443,-0.0037864102,-0.062491555,-0.03496717,0.01819062,-0.015121514,0.059295688,-0.01781752,0.034693193,0.062463112,-0.014807791,-0.011239615,-0.056394372,0.033413783,0.023593104,-0.004268294,0.066631585,0.03967956,-0.021086989,-0.025519196,-0.044698145,-0.048796672,0.023445802,0.03748539,-0.07778299,0.010633541,-0.018427238,-0.07840688,0.05169121,-0.009379722,0.0013633438,0.046050888,0.051434007,-0.043104254,-0.03618588,-0.02499261,-0.020283509,-0.025788797,0.027971173,-0.04201661,0.0060460176,-0.018643627,0.02759408,0.020310823,-0.00059276173,-0.060841158,-0.06478511,0.047477048,0.013102395,0.033349395,0.09391785,-0.050864957,-0.065448195,0.0106177125,0.03646631,-0.0039027089,-0.0038532382,0.015774222,-0.021951215,-0.03641626,0.0032542786,0.02698486,0.0076098456,0.02895446,0.009394452,0.025307784,-0.03686857,0.015087169,-0.041602165,-0.028009675,0.016041834,0.005912886,-0.018458987,0.051978398,0.04788388,0.027145496,-0.03551731,0.026271712,0.06173774,0.040964015,-0.02637217,-0.010249806,0.0040451745,0.047855955,-0.017002566,-0.0031422975,0.05402255,-0.05828277,0.013764246,-0.02744573,0.047172338,0.0032998146,-0.033248644,0.025303658,-0.065444626,0.0015696955,0.031265374,-0.019042777,0.002441507,0.014776836,0.012345707,-0.02566617,-0.021657042,-0.018435704,-0.024307704,-0.010792188,0.029881997,-0.032927293,-0.0045123203,-0.007760571,-0.05578825,0.015822174,-0.0137955565,0.042253327,-0.014777488,0.012447524]	Keywords: saddle point, optimization algorithms, gradient descent, symmetry\nKey Objects: Saddle Point, Optimization Algorithms, Gradient\nRefers to Images: ./images/an-overview-of-gradient-descent-optimization-algorithms/image_4.png, ./images/an-overview-of-gradient-descent-optimization-algorithms/image_5.png\nHypothetical Questions:\n- Why do algorithms like SGD, Momentum, and NAG have difficulty escaping a saddle point?\n- How do adaptive learning-rate methods like Adagrad, RMSprop, and Adadelta outperform others at saddle points?\n- What is the significance of the 'symmetry' issue mentioned in relation to saddle points and optimization algorithms?\n---\nSummary:\nFigure 4b illustrates how different optimization algorithms behave when encountering a saddle point, where gradients have opposing slopes, posing a challenge for standard SGD.\nOriginal Text:\nFigure 4b shows the behaviour of the algorithms at a saddle point, i.e. a point where one dimension has a positive slope, while the other dimension has a negative slope, which pose a difficulty for SGD as we mentioned before. Notice here that SGD, Momentum, and NAG find it difficulty to break symmetry, although the latter two eventually manage to escape the saddle point, while Adagrad, RMSprop, and Adadelta quickly head down the negative slope, with Adadelta leading the charge.  \n  \n(a) SGD optimization on loss surface contours  \n  \n(b) SGD optimization on saddle point  \nFigure 4: Source and full animations: Alec Radford  \nAs we can see, the adaptive learning-rate methods, i.e. Adagrad, Adadelta, RMSprop, and Adam are most suitable and provide the best convergence for these scenarios.\nContextualized Text:\nFigure 4b demonstrates the behavior of optimization algorithms, such as SGD, Momentum, and NAG, when facing a saddle point  a location with opposing gradient slopes. Standard SGD, Momentum, and NAG initially struggle to overcome this symmetry, while Adagrad, RMSprop, and Adadelta quickly descend along the negative slope, with Adadelta exhibiting the fastest response.	{"tags": ["optimization", "deep-learning", "visualization"], "doc_id": "376a0686-a372-430e-b6bc-5d94b406987c", "summary": "Figure 4b illustrates how different optimization algorithms behave when encountering a saddle point, where gradients have opposing slopes, posing a challenge for standard SGD.", "doc_type": "text", "entities": ["SGD", "NAG", "Adagrad", "Adadelta", "RMSprop"], "keywords": ["saddle point", "optimization algorithms", "gradient descent", "symmetry"], "key_objects": ["Saddle Point", "Optimization Algorithms", "Gradient"], "contextual_text": "Figure 4b demonstrates the behavior of optimization algorithms, such as SGD, Momentum, and NAG, when facing a saddle point  a location with opposing gradient slopes. Standard SGD, Momentum, and NAG initially struggle to overcome this symmetry, while Adagrad, RMSprop, and Adadelta quickly descend along the negative slope, with Adadelta exhibiting the fastest response.", "mentioned_images": ["./images/an-overview-of-gradient-descent-optimization-algorithms/image_4.png", "./images/an-overview-of-gradient-descent-optimization-algorithms/image_5.png"], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.9 Visualization of algorithms"}, "hypothetical_questions": ["Why do algorithms like SGD, Momentum, and NAG have difficulty escaping a saddle point?", "How do adaptive learning-rate methods like Adagrad, RMSprop, and Adadelta outperform others at saddle points?", "What is the significance of the 'symmetry' issue mentioned in relation to saddle points and optimization algorithms?"]}
36067d30-cffc-47cd-8fb6-04e70c9b42fb	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.033560693,0.003525361,-0.008151116,0.040275175,-0.062465627,0.068080224,0.011505274,0.0048187366,0.056605637,-0.07059274,-0.014630598,-0.031658225,-0.032533444,-0.020504957,-0.05480404,0.040332537,-0.020328242,0.07929989,-0.0111822905,-0.040344328,0.030609503,-0.0045842025,0.0009908403,-0.0020852806,0.008215437,0.036416315,-0.004619782,-0.030567575,0.06575956,0.032169618,0.003676568,0.021697264,0.03363236,0.029290171,0.022049418,-0.0104622515,-0.008796077,-0.0040450734,-0.008513796,-0.024870697,-0.070480205,0.045153167,0.013021483,0.010378346,0.045082245,-0.014498669,-0.050552007,0.019956019,0.002393137,-0.029894458,-0.019763598,0.0523451,-0.02719556,0.029789167,-0.042110633,0.03824783,-0.03988328,-0.013041516,-0.024744129,-0.032987904,-0.056665603,0.014724127,0.027355164,-0.00888074,0.055569474,-0.071523145,-0.031199705,0.05019477,0.014976784,0.13489334,-0.026498346,0.05727667,0.013397623,-0.108592145,0.10584042,0.036041148,-0.039436802,-0.0397063,-0.03170824,-0.0019239443,0.059417617,0.04352415,-0.054633588,-0.029734952,0.070482925,0.029772185,-0.07381377,-0.04530065,0.0779549,-0.09084955,-0.020962648,-0.032197274,-0.018618738,0.048130352,-0.037789404,-0.0017537298,0.0076232348,-0.07196899,-0.041265443,-0.048251234,0.036633994,-0.00690971,-0.0036221822,0.08779745,0.022047874,0.031466488,0.010717464,0.00550487,-0.0073057455,0.0030919285,0.013366532,-0.023071693,-0.030323176,0.007994029,-0.010919448,0.05724292,-0.05237669,0.04773085,0.009135538,-0.02820966,0.007522742,-0.025843574,-0.026097162,-0.007835432,0.046286087,0.10937252,-0.04716153,-0.020141937,-0.05009032,-0.05013425,0.0076236483,0.023087228,-0.046947766,0.03494643,-0.046475373,-0.029567147,0.016261766,0.03246953,-0.007808832,-0.04759744,-0.040222187,-0.063239634,-0.030739963,0.014011179,0.02994197,0.045884468,-0.10454253,0.02196147,0.00046888972,0.06771127,0.013631367,-0.03170526,0.0191654,-0.015346832,-0.05207156,-0.0055858847,-0.007191825,0.030787135,-0.012267974,-0.04260292,-0.053335387,0.024888968,0.014406285,0.07914053,0.05068137,0.11043052,-0.034461554,0.044024047,-0.028862694,-0.0106752245,-0.02109557,-0.03778958,0.0021699495,-0.015675278,0.011167907,-0.016432084,0.009412794,-0.002995575,0.00065606425,-0.015245969,-0.035220675,0.019189816,-0.020740386,0.027978629,0.007712751,0.032932356,0.032957368,0.021737525,0.047752522,-0.027336134,-0.008580976,0.01723011,0.027721275,-0.006772627,0.05568719,0.016943613,0.02961988,0.030062357,0.004665608,-0.034755148,0.029946858,0.042467203,-0.045999374,-0.032072116,0.039486878,-0.0012508793,0.014640511,-0.055926103,-0.04177619,-0.00044830923,-0.009896438,0.041984323,-0.0050393254,0.009794181,0.0041158693,0.055142824,-0.015070392,0.017925546,0.051213343,-0.018835094,-0.011927143,-0.013038788,-0.011354841,0.04774933,-0.023364728,-0.016483128,-0.011762668,-0.032396108,0.07192477,0.015926126,-0.012761212,0.074981995,-0.0035994477,0.073186345,0.009930096,0.022651184,0.030116884,0.029362362,0.03901112,0.049089044,-0.004674296,0.0036335404,-0.0097849285,0.081933536,0.004556134,-0.03730247,0.06140446,0.04605236,0.028807675,0.036666155,-0.019177193,0.02539425,0.017994521,-0.027951425,-0.00073792035,-0.018167319,0.03315052,0.0033440676,0.06516149,-0.08038411,-0.056294907,-0.0195386,-0.012932104,0.05499988,0.0298644,0.033890978,0.023791349,0.022849536,-0.019930819,0.06975847,0.007128283,0.005675704,0.026624117,-0.07114173,0.016191319,-0.0007317455,-0.05286308,0.052094016,0.030561747,-0.030277716,-0.02280119,0.044713315,0.010540801,0.014316633,0.025107434,0.01870816,0.008424583,-0.040716227,-0.06411293,0.0033060913,0.05156596,-0.05293443,0.06807645,0.020577198,-0.029935265,0.031357042,-0.003600894,-0.022635069,0.0039739534,0.050836563,0.07982292,-0.020537475,0.06464815,0.015982924,-0.019637117,-0.051094297,-0.028747484,0.047033995,-0.029551847,0.06238552,0.029498534,-0.030787403,-0.029326893,0.009077585,0.031140255,0.049738433,0.07037485,0.012859575,-0.008153557,0.03431926,0.019018838,0.013629922,-0.01670191,0.082364924,0.05460606,-0.008663398,0.036559485,-0.06497363,-0.058775306,0.048718475,-0.013144856,0.03984243,0.07371392,0.008306049,-0.02781643,-0.021458346,-0.026068645,0.018559013,-0.012257864,0.01570211,-0.017424634,-0.01480786,-0.015185521,0.017467603,-0.030149164,-0.004130973,-0.046514794,0.018680893,-0.032550015,0.036040936,0.0016862724,0.017527057,0.035069678,0.018768061,0.016560473,-0.011130768,0.006932509,0.0156465,0.051741958,-0.027694507,0.033712406,-0.05056121,0.024169281,-0.021639686,0.03274321,0.008935893,-0.05794967,0.0012768658,-0.01968925,-0.02562444,-0.010702573,-0.015504649,0.006097873,-0.04167679,-7.109103e-05,0.042901866,0.026544534,0.00027438364,0.019058118,-0.033750895,-0.050201654,0.018860174,-0.049704373,-0.022272225,-0.01017698,0.030979825,-0.011007108,0.08821256,0.033492792,0.008001454,-0.007672372,0.010268783,0.026148396,0.013422578,-0.009931475,-0.056212883,-0.0011570036,0.034631163,0.032141566,-0.0011270207,0.00965944,0.08560868,-0.050136138,0.053984903,-0.005344733,0.030680425,-0.028070746,0.031661745,0.021960123,0.014209738,0.034496706,-0.054281194,-0.006343187,-0.027848694,-0.007423623,-0.020050606,-0.02744979,-0.0076636216,-0.033077788,-0.053505037,-0.042577412,0.036208097,0.032051377,0.057211217,-0.00017330566,0.038814954,0.0079258885,0.060277853,0.011569054,-0.0071139107,0.015953273,0.012196474,-0.0036604432,0.03696131,0.024292804,0.00082223624,-0.030270994,0.0669675,-0.018307272,-2.7019596e-05,0.007889535,-0.035115052,0.017413016,0.0219006,-0.018232234,-0.01991988,-0.009342025,-0.015816245,-0.02478609,0.02176097,0.06431507,-0.033411317,0.04761899,-0.027665209,-0.011654923,-0.0048984233,-0.046826914,-0.02713787,0.011271636,0.013347561,0.015365784,0.053063285,-0.0025046421,0.018352509,-0.0078408215,-0.025861396,-0.04148801,-0.04656116,-0.0096198125,-0.0040551107,-0.061505295,0.027418016,-0.05667973,0.022821445,-0.042387985,0.02640708,0.023008522,0.011246092,0.09441229,0.030063674,-0.059203945,-0.047657922,0.010027798,0.040454846,0.032835394,-0.039086048,0.015990727,-0.015318518,-0.049440105,0.07630761,-0.021115737,0.004107071,-0.05145668,0.04809643,-0.0064748595,-0.04387669,0.035383925,-0.017856665,-0.005025914,-0.048627097,-0.005450431,-0.013462244,0.056066003,0.038377758,-0.0049085496,-0.010179557,0.046527673,0.073964655,-0.027122373,0.036310907,-0.010379633,-0.024273796,-0.01374247,-0.03418543,-0.003635644,0.0077114482,0.010157231,-0.01955962,0.022922827,-0.022494424,0.043450948,0.03601595,0.026155373,-0.0686745,0.0130441105,-0.019438783,-0.064398065,-0.0026804984,0.033174556,-0.066636644,-0.025297647,-0.0063335495,-0.016507046,-0.008507789,-0.10445003,-0.039500166,0.026363049,-0.007499033,-0.0023704693,-0.018022511,-0.03149654,-0.044314284,-0.03046905,-0.02403556,0.04712833,-0.027347613,0.041752506,0.048445314,0.016311556,0.06770359,0.02198177,0.04137142,-0.003090205,-0.04666007,0.024833834,0.07185081,-0.02839557,-0.008055508,0.04794288,-0.0074821096,0.0012742014,-0.0014150427,-0.029272577,-0.049266674,-0.02945125,-0.01979342,-0.038895436,-0.055883612,-0.038438804,0.015857171,-0.026841292,0.024514819,-0.013025215,-0.009622252,-0.016921477,0.047080874,0.017311435,-0.0048506516,0.033776842,0.02189961,0.010855508,-0.0019189211,0.017302208,0.046522435,0.01714646,0.041953646,0.022784537,0.04914562,0.026482295,0.009732523,-0.0005468524,-0.02700486,-0.008412595,-0.04746069,0.029172383,-0.02267962,0.0631227,0.016285129,-0.020118738,-0.023820302,-0.10331084,-0.019264841,-0.02358134,-0.031800944,0.024438428,-0.0049653132,-0.014021517,0.060081538,-0.0111756325,0.034956507,-0.050807744,-0.008813152,-0.01976746,0.039482586,-0.03710359,-0.058330495,-0.014336748,-0.030949166,-0.036971886,-0.033245653,0.051035628,-0.013070316,-0.016520541,-0.0003871482,-0.025213242,0.0049381033,-0.02798037,-0.032395642,-0.010436532,0.018758902,0.028808393,-0.009979131,0.06105753,0.013782378,-0.004176646,0.024401253,-0.015162115,-0.038600866,-0.039108876,0.029019797,-0.027279409,0.021600021,-0.03788691,0.043009378,0.011237641,-0.042371295,0.0006997941,-0.017101156,0.05272358,-0.033006217,0.007687161,0.08121606,-0.03567325,-0.013235292,-0.03435451,0.0013018188,-0.02681484,-0.0055514392,0.060112823,-0.02350494,0.060921427,0.04567847,-0.03452296,-0.044803582,-0.04129797,0.013955238,0.028535208,-0.011556835,0.036656905,0.041249078,-0.06243875,-0.029155308,-0.0035078463,-0.010455814,0.01674807,0.0021603967,-0.015041639,-0.0018736849,-0.0077439384,-0.032727472,0.04552424,-0.029758774,0.045725245,0.038517434,0.02825287,-0.00022046862,-0.051027473,-0.038424313,-0.011222177,-0.021637607,0.047860075,-0.037060086,0.051441625,-0.004720318,0.005928688,0.018366938,-0.009531776,-0.0057513327,-0.052526794,0.007275387,-0.0024708495,0.035136644,0.032814577,-0.046191767,-0.08123661,0.045731682,0.022819687,-0.0047227805,-0.011863979,0.0062429663,-0.035388272,-0.025153872,0.029987283,-0.0021433942,0.030318605,-0.00024159488,0.024242647,-0.03398038,-0.021160506,0.022760604,0.005471568,-0.037480824,0.054777592,0.01649737,0.009842745,0.0020973142,0.021442434,0.0353034,0.025197059,0.013521393,0.04355529,0.010557481,-0.03538502,0.0071918,0.0016256131,0.050872188,-0.01635048,0.031169765,0.036511227,-0.054895878,0.02117312,-0.04336068,0.046150595,-0.01672546,-0.0044771414,0.06133413,-0.046455346,0.001668113,0.01829746,-0.025270026,0.024716321,0.0120229935,-0.021729374,-0.025143003,-0.022599969,-0.026060067,-0.04240747,-0.003576382,0.048728928,0.003997774,-0.046095382,0.00683762,0.0053798594,0.03860505,-0.025747806,0.037024982,-0.031364936,0.036387216]	Image title: Optimization Algorithms on a Loss Landscape\nTags: optimization, machine-learning, loss-landscape, sgd, momentum, nag, adagrad, adadelta, rmsprop\nKey objects: Loss Landscape, Contour Lines, SGD Path, Momentum Path, NAG Path, Adagrad Path, Adadelta Path, RMSprop Path, Local Minima, Global Minimum\n---\nSummary:\nThis diagram illustrates the paths taken by different optimization algorithms on a loss landscape, represented by a series of contour lines. The image compares SGD, Momentum, NAG, Adagrad, Adadelta, and RMSprop, showing how each algorithm navigates the landscape to minimize the loss.\nFull description:\nThe image depicts a loss landscape with contour lines representing equal loss values. Several algorithms are visualized by their paths through this landscape. The 'SGD' path (red) demonstrates a relatively straightforward, albeit potentially slow, descent. The 'Momentum' path (purple) shows an accelerated descent, particularly in areas with varying slopes. 'NAG' (dark purple) has a similar accelerated approach. The 'Adagrad' path (blue) is shown as gradually approaching the minimum. Similarly, the 'Adadelta' path (cyan) demonstrates a slow but steady trajectory. Finally, the 'RMSprop' path (green) shows an accelerated approach toward the minimum. The landscape has both local minima and a global minimum at the center.\nText found in image:\n- SGD\n- Momentum\n- NAG\n- Adagrad\n- Adadelta\n- RMSprop	{"tags": ["optimization", "machine-learning", "loss-landscape", "sgd", "momentum", "nag", "adagrad", "adadelta", "rmsprop"], "title": "Optimization Algorithms on a Loss Landscape", "doc_id": "36067d30-cffc-47cd-8fb6-04e70c9b42fb", "source": "./images/an-overview-of-gradient-descent-optimization-algorithms/image_4.png", "summary": "This diagram illustrates the paths taken by different optimization algorithms on a loss landscape, represented by a series of contour lines. The image compares SGD, Momentum, NAG, Adagrad, Adadelta, and RMSprop, showing how each algorithm navigates the landscape to minimize the loss.", "doc_type": "image", "key_objects": ["Loss Landscape", "Contour Lines", "SGD Path", "Momentum Path", "NAG Path", "Adagrad Path", "Adadelta Path", "RMSprop Path", "Local Minima", "Global Minimum"], "parent_doc_id": "376a0686-a372-430e-b6bc-5d94b406987c", "text_in_image": ["SGD", "Momentum", "NAG", "Adagrad", "Adadelta", "RMSprop"], "contextual_description": "The image depicts a loss landscape with contour lines representing equal loss values. Several algorithms are visualized by their paths through this landscape. The 'SGD' path (red) demonstrates a relatively straightforward, albeit potentially slow, descent. The 'Momentum' path (purple) shows an accelerated descent, particularly in areas with varying slopes. 'NAG' (dark purple) has a similar accelerated approach. The 'Adagrad' path (blue) is shown as gradually approaching the minimum. Similarly, the 'Adadelta' path (cyan) demonstrates a slow but steady trajectory. Finally, the 'RMSprop' path (green) shows an accelerated approach toward the minimum. The landscape has both local minima and a global minimum at the center."}
66c454db-890f-451c-a568-54ae9008e9da	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03444964,0.021969272,-0.010028417,0.04589211,-0.061465397,0.06446143,-0.0024321298,0.017116707,0.07094523,-0.035048783,-0.02501236,-0.021788418,-0.025187826,-0.01111897,-0.032147508,0.04797602,-0.00941539,0.08968592,0.0018461918,-0.0127543155,0.042151786,0.019122645,0.0026342298,0.0007347368,-0.004088654,0.033625934,0.026199838,-0.011384052,0.036381844,0.0321411,0.014210299,0.011999286,0.018532982,0.016296774,0.04084818,-0.025267512,0.0021405986,0.0012562686,-9.7156306e-05,-0.046358787,-0.058884073,0.031401534,-0.013770228,0.019087771,0.013702087,-0.029494468,-0.05265222,0.004322843,-0.012627721,-0.0051748864,-0.031847462,0.051297203,-0.02939817,0.027217928,-0.043270506,0.031111766,-0.03893888,-0.007484762,-0.029881502,-0.03287728,-0.05108957,0.018808173,0.04647036,-0.012551867,0.05639185,-0.07141025,-0.04515122,0.053877402,0.016482553,0.13979924,-0.022149669,0.048790608,0.013167121,-0.11119848,0.090968475,0.021595651,-0.01888898,-0.05130629,-0.012755252,0.0010987959,0.06808253,0.03564535,-0.04983931,-0.034055658,0.056904793,0.060775794,-0.074532785,-0.023567846,0.07057297,-0.076709546,-0.0211531,-0.056797642,-0.0029468145,0.04161596,-0.041037705,0.00683588,-0.013025964,-0.062233068,-0.024634456,-0.050385058,0.04344901,0.019102208,-0.012449335,0.08867697,0.030721227,0.040219657,0.0078311805,0.009788791,-0.008246487,-0.0048420057,0.016822912,-0.006231128,-0.013981151,0.017684944,-0.03717517,0.0529358,-0.07098034,0.030955516,0.01840014,-0.016300967,0.023993265,-0.034184184,-0.020424211,0.0004070376,0.03961624,0.11177105,-0.037563797,-0.03325717,-0.03970446,-0.04300762,-0.0019112787,0.0031297414,-0.034812514,0.057560034,-0.023961378,-0.025391083,0.020648545,0.016972898,-0.010421302,-0.04801049,-0.029527538,-0.072231464,-0.039398905,0.012262885,0.000732772,0.041456297,-0.08647875,0.0070425617,-0.004245823,0.05160095,0.010850603,-0.036712065,0.03805445,-0.027139382,-0.046214633,-0.008768231,-0.010420161,0.029306313,-1.911258e-05,-0.032095872,-0.041234612,0.0073860534,0.018331498,0.08000408,0.0513822,0.10102215,-0.011991344,0.05175264,-0.006981321,-0.00926296,-0.014003877,-0.032120258,0.018975675,-0.008776436,0.016299237,0.0056048697,0.01637661,-0.0010643936,-0.0042272788,-0.023600651,-0.04436668,0.026672622,-0.011193265,0.029207809,0.0140260495,0.0273964,0.051231734,0.035052598,0.054641746,-0.038458645,0.018750213,0.030415215,0.013776446,0.02058073,0.06823255,0.0119269,0.03486425,0.019511335,0.028538961,-0.021244882,0.013409384,0.05367317,-0.059062723,-0.036248434,0.009728656,-0.0024960435,0.0023925146,-0.054942783,-0.025692021,0.012579871,-0.026382864,0.02308542,-0.014276471,0.041163456,0.016152013,0.03452454,-0.009714842,0.019474069,0.041577946,-0.017669458,-0.018395746,-0.005682063,-0.024873076,0.032575123,-0.00842464,-0.008466176,-0.010020439,-0.04157075,0.055054672,0.0075979703,-0.0015134686,0.06298887,-0.015947206,0.058919847,0.0081326,0.008708576,0.02825223,0.018800082,0.028003966,0.043735605,0.0088503,0.010643651,0.0067481543,0.07396597,0.01658328,-0.0074413787,0.06096351,0.03533735,0.02373326,0.033956286,-0.025786368,0.00033677468,0.025857843,-0.033075765,0.0063251355,-0.019309625,0.041144796,0.03708769,0.06546857,-0.088279754,-0.043203868,-0.035633758,-0.024570692,0.021134244,0.024323007,0.02128517,0.009695628,0.020612707,-0.035863925,0.08551926,0.00016508148,0.0012632415,0.026058314,-0.03736771,0.010079484,0.029090157,-0.049386326,0.057235055,0.017325522,-0.009698507,-0.0088619385,0.036865182,0.025982544,-0.002120957,-0.0013765601,0.013206734,0.029578937,-0.039834663,-0.06128986,0.00022111628,0.050986413,-0.058952305,0.05498495,0.019161224,-0.014891833,0.021171182,-0.0022952938,-0.010839677,0.015615078,0.062601164,0.08345884,-0.024016067,0.044738527,0.020852657,-0.021113586,-0.02230929,-0.046109755,0.04362019,0.012490687,0.068731345,0.028339956,-0.020961363,-0.034978844,0.015651956,0.035482604,0.06376629,0.078498065,0.018425232,0.021492727,0.033211578,0.041167494,0.024825148,-0.03218864,0.077387385,0.059679374,-0.0053113564,0.031891644,-0.055513564,-0.04276404,0.02711202,-0.041814033,0.04950297,0.05920064,-0.018074585,-0.0106718615,-0.016189026,-0.03041921,0.0313498,-0.0018308616,0.0026193778,-0.007926606,-0.017782733,0.005544756,0.020905739,-0.011357702,0.0122090755,-0.018405719,0.027357563,-0.013718021,0.041823238,-0.021517856,0.033985715,0.045360442,0.019501653,-0.002458172,0.003242365,0.008813215,0.029287627,0.06163374,-0.05720682,0.02821904,-0.047098912,0.031492442,-0.013597067,0.05838187,0.008615661,-0.051741794,-0.0030276417,-0.027959164,-0.011088735,-0.0071956255,-0.0077392464,-0.005460455,-0.0418408,0.0064741666,0.03531755,0.035803314,0.00870905,0.01823524,-0.07223652,-0.033967264,0.016939588,-0.054078538,-0.008829528,-0.0023296694,0.029115817,-0.019940568,0.078576505,0.027535245,0.008247347,-0.003036925,-0.010376033,0.013686516,0.026794633,-0.03064698,-0.050997496,0.008059718,0.046234537,0.040066242,-0.033574812,0.017966198,0.077096045,-0.058519326,0.042913493,-0.0057060802,0.004000267,-0.029405734,0.016516902,0.048952498,0.018123312,0.04863276,-0.06183469,0.009370581,-0.01599857,-0.015812114,-0.037797045,-0.040324572,0.0023673954,-0.024596222,-0.032926496,-0.024318028,0.034670103,0.044405878,0.04885484,-0.02038019,0.027948773,-0.00062271906,0.060357794,0.0055217617,-0.034510616,0.030380439,0.038349744,0.009889246,0.045935776,0.015612077,-9.9105535e-05,-0.04781561,0.07877686,-0.019692281,-0.00096877717,0.029285004,-0.03257198,0.0458041,0.033445142,-0.0021940607,-0.032269098,-0.0013161041,-0.016440812,-0.0054798806,0.02150717,0.03884959,-0.03535536,0.041740995,-0.028708285,-0.009502374,-0.009015053,-0.046018325,-0.030586408,0.0040396675,-0.008629029,0.031939197,0.04217207,-0.016755404,0.028170202,-0.018288525,-0.03657694,-0.029697344,-0.057681188,0.023781374,0.026777538,-0.04452309,0.036060702,-0.067002356,0.025790237,-0.03880535,0.03997632,0.02678326,-0.0051231985,0.07750035,0.026012918,-0.047407333,-0.055157352,0.01711815,0.055293836,0.004727239,-0.040395416,-0.008403894,-0.0142821325,-0.0393888,0.058503404,-0.009226627,-0.010010338,-0.032443676,0.044211075,-0.008984382,-0.04992031,0.01371632,-0.021066507,0.003392155,-0.03694163,0.007815721,-0.015026959,0.05756027,0.05732739,0.0071607465,0.005952998,0.025071455,0.0789489,-0.027614523,0.01616039,-0.01074807,0.006358925,-0.008154182,-0.03141732,-0.005320286,0.005735706,-0.006660147,-0.041416228,0.011560391,-0.022941615,0.024346363,0.044556543,0.01582608,-0.01469893,0.033152074,-0.00645494,-0.052417584,0.009466078,0.024114884,-0.07490508,-0.02111834,-0.007135084,-0.04661501,-0.016908256,-0.11903396,-0.04850697,0.02382786,-0.024962917,-0.009780288,-0.007420363,-0.028723544,-0.037314333,-0.03328684,-0.01340688,0.055270426,-0.037814777,0.050580543,0.030073551,0.016891174,0.04375694,0.032118198,0.015269813,0.0075163892,-0.045523625,0.016108975,0.056961752,-0.011772646,0.0015611012,0.040616065,0.0010648671,0.023805233,0.02004035,-0.016574197,-0.032414902,-0.0298258,-6.333791e-05,-0.03653104,-0.068256006,-0.048713822,0.02043267,-0.055559415,0.024766734,-4.7911122e-05,-0.00760742,-0.032997,0.06526417,0.039803736,-0.0020778426,0.026596649,0.03316311,0.012893134,0.017809369,0.03577124,0.07067528,0.033329044,0.053573575,0.024167659,0.007868437,0.018334854,0.042032957,0.006850333,-0.01879055,-0.016205404,-0.055463754,0.014230527,-0.018907795,0.09018781,0.007286588,0.008427222,-0.019199139,-0.09661091,0.016682966,-0.028663624,-0.02490951,0.01631273,0.004945426,-0.026265929,0.053828962,-0.024936939,0.04863336,-0.07155741,-0.014481941,-0.018667066,0.018724836,-0.06987569,-0.06108292,-0.0077584814,-0.027472608,-0.0446852,-0.026825506,0.024773825,-0.004116421,-0.005622509,0.021767864,-0.057027273,0.011926478,-0.008322303,-0.03271826,-0.018131008,0.033135425,0.04218505,-0.006046065,0.034255378,0.018465007,-0.030964155,0.027022526,-0.016401842,-0.051742822,-0.03341529,0.042272285,-0.027249068,0.017933726,-0.035269078,0.068358555,0.009094434,-0.040995874,-0.007874646,-0.020355767,0.0431751,-0.028373858,0.027969094,0.11775734,-0.012432665,-0.02715535,-0.03320463,-0.025937837,-0.008688111,0.017560046,0.053370096,-0.015191599,0.05715978,0.040965587,-0.008152006,-0.0497277,-0.05378548,0.034122355,0.027073555,-0.01997312,0.05296523,0.050680995,-0.057001233,-0.04004837,-0.013139494,-0.020047296,0.04407059,0.011363545,-0.008204989,0.007335971,0.010764223,-0.0049468824,0.0661286,-0.029154679,0.029606048,0.05925792,0.017272633,-0.015107728,-0.034533367,-0.034147624,-0.007831128,-0.03676154,0.0474912,-0.03346253,0.031723015,-0.0031873577,0.01826212,0.026861437,-0.009550698,0.0033793726,-0.062502764,0.005640344,0.011947663,0.061651465,0.021798687,-0.05851777,-0.08228885,0.059377305,0.017696217,0.009748441,-0.022382934,0.030878969,-0.021311274,-0.035595827,0.036768805,-0.00017925409,0.024309555,0.005199939,0.008927972,-0.0149556305,-0.014065271,0.00039892635,-0.02396225,-0.027234355,0.031536277,-0.0014965112,0.012869513,0.018485604,0.01808058,0.018430794,0.037693013,-0.008896487,0.03398733,-0.004393237,-0.00028903212,0.002154528,-0.002706565,0.04712135,-0.037985146,0.017502043,0.050135132,-0.03803229,0.0152846305,-0.029428832,0.059425756,0.010563248,0.0030464295,0.05930621,-0.03293221,-0.009251682,0.0056410693,-0.03461577,0.021055425,0.01523949,-0.011894862,-0.0346809,-0.035311237,-0.050828703,-0.03811671,0.02806338,0.058520425,0.005817496,-0.038997974,-0.013607541,-0.018284407,0.018648118,-0.037180535,0.027468914,-0.001564914,0.014268927]	Image title: Optimization Algorithms on a Loss Surface\nTags: optimization, gradient-descent, machine-learning, neural-networks, loss-surface, algorithm\nKey objects: Loss Surface, SGD (Stochastic Gradient Descent), Momentum, NAG (Nesterov Accelerated Gradient), Adagrad, Adadelta, RMSprop\n---\nSummary:\nThis diagram illustrates the paths taken by different optimization algorithms as they navigate a 3D loss surface, attempting to reach the global minimum. Each algorithms trajectory is visualized as a line on the surface, demonstrating their varying approaches to gradient descent.\nFull description:\nThe diagram shows a 3D loss surface, which is represented as a colored landscape. Several algorithms are visualized as paths on this surface, demonstrating how they attempt to find the lowest point (minimum loss). 'SGD' follows a zigzag path directly down the surface. 'Momentum' builds on 'SGD' by continuing its movement based on its past movement, resulting in a less erratic path. 'NAG' takes a similar approach to 'Momentum' but corrects its direction slightly by looking ahead at the gradient. 'Adagrad' adjusts the learning rate based on past gradients, and this is reflected in its path. 'Adadelta' is a variant of 'Adagrad' and has a similar trajectory. 'RMSprop' adapts the learning rate by tracking the exponentially decaying average of squared gradients, leading to a different path on the surface.\nText found in image:\n- SGD\n- Momentum\n- NAG (Nesterov Accelerated Gradient)\n- Adagrad\n- Adadelta\n- RMSprop	{"tags": ["optimization", "gradient-descent", "machine-learning", "neural-networks", "loss-surface", "algorithm"], "title": "Optimization Algorithms on a Loss Surface", "doc_id": "66c454db-890f-451c-a568-54ae9008e9da", "source": "./images/an-overview-of-gradient-descent-optimization-algorithms/image_5.png", "summary": "This diagram illustrates the paths taken by different optimization algorithms as they navigate a 3D loss surface, attempting to reach the global minimum. Each algorithms trajectory is visualized as a line on the surface, demonstrating their varying approaches to gradient descent.", "doc_type": "image", "key_objects": ["Loss Surface", "SGD (Stochastic Gradient Descent)", "Momentum", "NAG (Nesterov Accelerated Gradient)", "Adagrad", "Adadelta", "RMSprop"], "parent_doc_id": "376a0686-a372-430e-b6bc-5d94b406987c", "text_in_image": ["SGD", "Momentum", "NAG (Nesterov Accelerated Gradient)", "Adagrad", "Adadelta", "RMSprop"], "contextual_description": "The diagram shows a 3D loss surface, which is represented as a colored landscape. Several algorithms are visualized as paths on this surface, demonstrating how they attempt to find the lowest point (minimum loss). 'SGD' follows a zigzag path directly down the surface. 'Momentum' builds on 'SGD' by continuing its movement based on its past movement, resulting in a less erratic path. 'NAG' takes a similar approach to 'Momentum' but corrects its direction slightly by looking ahead at the gradient. 'Adagrad' adjusts the learning rate based on past gradients, and this is reflected in its path. 'Adadelta' is a variant of 'Adagrad' and has a similar trajectory. 'RMSprop' adapts the learning rate by tracking the exponentially decaying average of squared gradients, leading to a different path on the surface."}
cd33c731-a1fe-4685-87f9-ea8468725c09	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0055759726,0.048113354,0.015706887,0.02603494,-0.056505945,0.049302757,0.036947697,0.021009404,0.0053537018,0.0007632501,0.0046129273,-0.022426788,-0.0057441113,0.00846968,-0.048800167,-0.014715006,0.00810596,0.074514546,-0.027145777,-0.025999868,0.055916302,-0.028020129,0.011784473,0.013763433,0.013809325,0.038359594,0.036644746,-0.011760993,0.03323319,-0.03890171,0.003413693,-0.050626047,0.027956571,0.051418696,-0.015607602,0.037245125,-0.011016973,-0.075758606,-0.070077695,-0.062025398,-0.025520064,0.022993589,-0.0025048822,-0.039567053,-0.0013747716,-0.029124256,-0.023831107,-0.018894553,-0.01939801,0.016922608,-0.020300508,0.03680099,-0.07246465,0.0013287426,-0.0725299,0.040253412,-0.034584925,-0.026664767,0.016193787,-0.038729534,-0.016843481,0.016612155,-0.010109477,-0.013484791,0.04913484,-0.04481591,-0.007991148,0.042892512,0.07913357,0.10470618,-0.036508802,0.030131685,0.012744888,-0.103881866,0.12264244,0.07344081,-0.013542273,-0.06191483,-0.030965362,-0.018049423,0.021908361,-0.00461985,-0.017023258,-0.03072648,0.077685416,-0.011248696,-0.031082388,0.018772224,0.074544035,-0.033180766,-0.019918766,-0.061252855,-0.03028343,0.0118226595,-0.005699511,-0.00091788557,0.015003893,-0.014940428,0.04593733,-0.03892002,-0.0020909896,0.011691636,0.037165612,0.09053153,0.0037842542,-0.039750624,-0.031076573,0.008696209,0.0007028412,0.025249287,0.06569656,0.016799474,-0.06198989,-0.042365756,-0.07215084,-0.017602565,-0.048561264,0.045748234,-0.02523197,-0.018889893,0.05035291,-0.012689767,-0.018740896,0.04556087,0.010966475,0.08153985,0.03040413,-0.036547195,-0.024060506,-0.008125616,0.04610201,-0.04272915,-0.015840726,0.05696038,-0.004536102,-0.028890856,0.018365243,-0.0005444891,0.034356914,0.019831873,-0.045470428,-0.01887667,-0.05980409,-0.034185097,0.0070184204,0.044095434,-0.05912345,-0.018795643,-0.010336244,0.08632732,0.017135222,-0.006704559,0.049589783,0.0062455223,-0.026483426,-0.04918824,-0.019229736,0.025406078,-0.0014691864,-0.038636506,-0.03503124,0.016816858,0.006765349,0.032954935,0.041239697,0.05701244,-0.008304052,-0.0017700333,0.0034378765,0.018630568,0.010756183,-0.017160142,0.019600693,-0.017147377,-0.00554323,-0.030711988,-0.009099839,-0.0189106,-0.022348562,-0.014051678,0.033126622,0.0535949,-0.01576808,-0.03543899,-0.039895665,-0.012338591,0.0011314235,0.0065568276,0.027619764,-0.05224081,0.016224917,-0.011999551,-0.0011062834,-0.015571172,0.015637934,0.059029225,0.026786143,-0.053449113,-0.014074213,-0.035493203,0.05240497,0.0037996774,-0.035407837,0.008096753,0.015234999,-0.046140075,0.017329244,-0.06451535,-0.027974416,0.022248765,-0.02089942,0.025430435,-0.033216834,0.041931923,0.045458764,-0.012570727,-0.00032531173,0.01501563,0.0563313,0.0003506853,-0.022968367,0.0056704003,9.990454e-05,0.0714327,-0.004930866,-0.013154858,-0.021285346,0.010659609,0.015376494,0.0227726,0.029017288,0.032044966,-0.023505963,0.0597268,-0.004251845,0.028845284,0.03221863,-0.020950835,0.014917018,0.04439283,0.01735044,0.06866212,-0.012841878,0.05404131,-0.00014313628,0.012937832,0.02221529,0.036907416,0.044238903,0.00014505807,-0.025075406,-0.0017105395,0.010899032,-0.027677037,-0.010785298,-0.0015742397,0.021649897,0.013541383,0.090652995,-0.051851965,-0.0016194027,0.004846863,0.01271744,0.047292866,0.002972354,0.00062319945,0.037572376,0.0142315915,0.023428043,0.00855116,0.036154028,0.07042565,0.0151191475,-0.03355389,-0.007045216,0.056388397,-0.018380385,-0.022672968,0.0713915,-0.02619424,0.024794148,0.054594,0.01350003,0.02443194,0.029473124,0.03308022,-0.015971582,0.014792301,-0.034297574,-0.008629175,0.0009918051,-0.056235332,0.052432127,0.023511305,-0.013385393,-0.0008532704,-0.00577149,-0.028295362,-0.022006083,0.058135513,0.0694286,0.02158014,0.037425626,-0.008454004,-0.0044225976,0.002549871,-0.07337159,0.027637083,0.011915961,0.05643837,0.033166714,0.012743023,-0.0030862293,0.054278795,-0.017495593,0.03838419,0.08245175,0.03722316,0.049493,0.04149356,0.024337288,-0.02938544,-0.0028566953,0.08104378,0.022171792,-0.026046017,0.0037648617,-0.025118278,-0.08843559,0.034313425,-0.05892608,0.017432338,0.044700287,-0.039562516,-0.01784904,0.023156341,-0.0484716,0.006971582,0.028459407,-0.008930667,-0.01828106,-0.052909866,0.04294624,-0.0029901664,-0.048676625,0.0024184408,-0.015776725,-0.012914055,-0.006247493,0.048340067,-0.03951228,0.041053884,0.025756026,0.0055887895,0.04220622,0.0015375704,-0.009998134,-0.010610929,0.033350267,-0.0580898,0.047351796,-0.06497598,0.061402258,-0.06446526,0.05595519,0.009043547,-0.047820557,0.0014878932,0.0026164788,-0.019820267,0.006219223,-0.041093525,0.025584035,-0.03693246,0.013606102,0.032013807,0.07174276,0.047589935,0.0050388896,-0.06295884,-0.030113319,0.026038187,0.03312685,-0.011093502,0.015324193,0.045958027,0.010776006,0.07447999,-0.021715594,0.029022932,0.0019758493,0.0081726415,0.006306578,0.008311763,-0.014492563,-0.04223823,0.015653618,0.053571574,0.046802856,-0.012401306,0.0014763582,0.01317915,-0.028354753,0.03445135,-0.020707125,0.040086422,-0.016118096,-0.0036293364,0.023936108,0.022626974,-0.0038364795,-0.08453324,-0.059093885,0.000441588,0.034993686,-0.026013564,-0.011722242,-0.03176307,-0.037803803,-0.004652522,-0.031015344,0.058717735,0.0586349,0.033942338,0.004538703,-0.014741859,-0.038300928,-0.002056833,-0.029390525,0.0015082245,0.011493976,0.002037656,-0.062474567,0.020934049,0.07923781,-0.039177302,-0.042682726,0.060603622,0.0012743996,0.03985727,0.01754571,-0.044854097,0.057321995,0.024250038,-0.0028485595,-0.024349146,-0.015547786,-0.048696794,-0.03303366,0.031518303,0.027717765,-0.026031507,-0.04436506,-0.013468272,0.04284102,0.007853533,-0.028281674,-0.013676795,-0.024059134,0.029435525,0.03937784,0.06791067,0.0034311877,0.020511579,-0.043523166,0.0064338488,-0.036936395,-0.001036051,0.023419488,-0.007728975,0.02535638,0.00040887826,0.024957541,-0.0029579136,-0.019089896,0.046939842,0.013633369,0.015580518,0.055288214,0.02231127,-0.00852337,-0.027493633,0.0059848446,0.04188479,0.00042476717,-0.0028956165,0.009432071,-0.007366311,-0.020940285,0.0042468915,-0.016255885,0.0063866684,-8.077816e-05,0.04004646,0.0047685737,-0.054829214,0.001397643,-0.020079335,0.035940807,-0.060937468,-0.056727543,0.014396267,0.053345904,0.07863494,0.033257358,0.044447403,0.047722742,0.06376825,-0.027765436,0.042318348,0.006221975,-0.01614133,0.0012625197,-0.06766966,-0.043125514,-0.007949866,0.014165192,-0.037776496,0.038581394,-0.01955636,0.004775064,0.039275248,-0.019825514,-0.014045263,-0.007701357,-0.016188476,-0.017587898,0.019549655,0.04228033,-0.12806317,0.009709819,-0.022410976,0.024209464,0.03467856,-0.046868637,-0.023501294,0.014865581,-0.018948963,0.011808204,-0.03547876,0.0027310022,-0.03984199,0.0068947044,-0.010240483,0.0698525,-0.09030279,0.008133057,0.015924657,0.022597065,0.06783906,0.069736004,-0.009487969,0.005288859,-0.045893345,-0.0071333153,0.024718208,-0.03167089,-0.007967421,0.0048296824,0.019733457,0.015347709,0.0055104042,-0.069664404,-0.058333497,-0.0023153273,0.02293472,-0.029120903,-0.07803,-0.011324324,-0.004568421,0.018611003,0.015323307,0.0030054273,0.006357954,-0.015434197,0.08365969,0.035691924,0.0011241221,0.01304706,0.07388773,0.01548067,0.0131781995,0.04550285,0.015796665,-0.025624456,0.019133924,0.006665791,0.0417987,0.019920615,-0.011869668,-0.017769268,-0.014612145,0.0052856454,-0.05792434,-0.01974226,-0.01762443,0.07642825,-0.017977811,0.047701254,-0.05973658,-0.07244044,0.0125657655,0.0063961423,-0.054963473,0.053951144,0.004740106,0.049415227,0.06156136,-0.04011196,0.05514587,-0.083684936,-0.054722443,-0.027514663,0.05476017,-0.05918153,-0.008029559,0.01932334,-0.059790444,-0.02056196,-0.054713707,0.016710231,-0.0074891443,-0.060758732,0.011002118,-0.009853671,0.011541049,-0.051698502,-0.020819537,-0.016195517,-0.031066231,0.023028554,-0.0055272803,0.06348937,-0.011109167,-0.01117187,0.025096558,-0.01438634,-0.031590525,-0.051597893,0.009929695,-0.012724469,0.0054506985,0.032037195,0.012946336,0.025530262,-0.016953224,-0.042375244,-0.08621796,-0.008734566,0.015824942,0.05107705,0.056370627,0.015046821,-0.0042826175,-0.0059044133,-0.013594333,-0.030242017,0.040622976,0.041481644,0.038071066,0.0635423,0.06372798,-0.029588483,-0.036217146,-0.0010442516,0.029305939,0.044995066,-0.00041525406,0.04000753,0.04644605,-0.07751832,-0.006481233,-0.0060582296,-0.038292896,0.013327156,0.004278562,-0.025445702,-0.008755436,-0.019499538,-0.008067207,0.03575389,-0.03540491,0.004699784,0.09914663,0.05326832,-0.022880813,-0.019443918,-0.013090749,-0.04678983,-0.02316091,0.0144599285,-0.061722554,0.030462759,-0.020005874,-0.0038339472,-0.027333956,-0.0007167445,-0.019520491,-0.040431857,0.025773924,0.025150144,0.033020604,0.024140755,-0.026562268,-0.04968573,0.063910395,0.0060200114,-0.007899729,-0.03396605,0.02621309,-0.009788804,-0.021769201,-0.014986988,-0.031314824,0.02987586,-0.008220302,0.016608803,-0.023230176,-0.03130582,0.059436537,-0.036568448,-0.007803797,0.051267486,0.0014357121,0.0047021927,0.045646384,-0.003056555,0.028014092,-0.0433044,-0.00053736323,0.07224248,0.063866384,-0.048602644,0.051014237,-0.0035711888,0.017574346,-0.028409094,-0.003352212,0.047919128,-0.01793744,0.008033767,0.0060717305,0.015947146,0.0017874676,0.04908436,-0.018798215,-0.013003702,-0.04037341,0.007368727,-0.00024744842,0.04517893,0.025060622,0.028798532,-0.035168193,-0.054835435,-0.005337935,-0.04454953,0.022163134,0.05995797,-0.021173403,-0.06908231,0.009221409,0.003295492,0.062377837,0.011597102,0.010824018,-0.03149539,0.010198683]	Keywords: adaptive learning rate, sparse data, RMSprop, Adagrad, Adadelta, Adam, optimization\nKey Objects: optimizers, sparse data, learning rate, RMSprop, Adagrad, Adadelta, Adam\nRefers to Images: None\nHypothetical Questions:\n- Why are adaptive learning rate methods often preferred for sparse data?\n- How does Adam improve upon RMSprop, and why might it be the best overall choice?\n- What is the primary difference between RMSprop and Adadelta?\n---\nSummary:\nWhen dealing with sparse input data, adaptive learning-rate methods are often the best choice for optimizers, offering good results without extensive learning rate tuning.\nOriginal Text:\n### 4.10 Which optimizer to use?  \nSo, which optimizer should you use? If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you will not need to tune the learning rate but will likely achieve the best results with the default value.  \nIn summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numerator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [10] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.\nContextualized Text:\nWhen choosing an optimizer, adaptive learning-rate methods like RMSprop, Adadelta, and Adam are often preferred, especially when the input data is sparse. RMSprop is an extension of Adagrad that addresses diminishing learning rates, while Adadelta is similar except for how it calculates parameter updates. Adam incorporates bias correction and momentum, potentially leading to slightly better performance towards the end of optimization compared to RMSprop. These optimizers often provide good results without the need to manually tune the learning rate.	{"tags": ["optimization", "deep-learning", "algorithms"], "doc_id": "cd33c731-a1fe-4685-87f9-ea8468725c09", "summary": "When dealing with sparse input data, adaptive learning-rate methods are often the best choice for optimizers, offering good results without extensive learning rate tuning.", "doc_type": "text", "entities": ["Adam", "RMSprop", "Adagrad", "Adadelta", "Kingma et al."], "keywords": ["adaptive learning rate", "sparse data", "RMSprop", "Adagrad", "Adadelta", "Adam", "optimization"], "key_objects": ["optimizers", "sparse data", "learning rate", "RMSprop", "Adagrad", "Adadelta", "Adam"], "contextual_text": "When choosing an optimizer, adaptive learning-rate methods like RMSprop, Adadelta, and Adam are often preferred, especially when the input data is sparse. RMSprop is an extension of Adagrad that addresses diminishing learning rates, while Adadelta is similar except for how it calculates parameter updates. Adam incorporates bias correction and momentum, potentially leading to slightly better performance towards the end of optimization compared to RMSprop. These optimizers often provide good results without the need to manually tune the learning rate.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.10 Which optimizer to use?"}, "hypothetical_questions": ["Why are adaptive learning rate methods often preferred for sparse data?", "How does Adam improve upon RMSprop, and why might it be the best overall choice?", "What is the primary difference between RMSprop and Adadelta?"]}
0833e719-8d42-4942-aede-738c13a959bb	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.013893136,0.03530491,-0.022015825,0.0075985175,-0.024027942,0.053380493,0.027616384,0.014701395,0.0422505,-0.062923126,-0.022274101,-0.044515572,-0.046405565,-0.0017933595,-0.0446562,-0.008601125,-0.018266289,0.06938838,0.0037770767,-0.013330318,0.08838969,0.016663337,0.047267828,0.007099751,-0.0104597295,0.034816608,0.03175528,-0.007696535,0.029260345,-0.027817247,-0.038901117,0.0033595313,-0.006442428,0.0044093807,0.052099574,-0.004514245,0.012234072,-0.022090264,-0.023174187,-0.05972069,-0.066959225,0.029365573,-0.03972809,-0.013447776,0.035810344,-0.047262374,-0.023776801,-0.032569554,0.0033803962,-0.03459351,-0.041791968,0.051889084,-0.077738315,0.027357839,-0.0745736,0.06348478,-0.006212753,0.021944068,0.014044449,-0.027752485,-0.06410375,0.016186383,0.0237948,-0.030559793,0.06819511,-0.03314551,-0.031915497,0.041752383,0.015954303,0.1018101,-0.03451013,0.0774706,-0.027710598,-0.11081187,0.14012901,0.059470568,-0.030191887,-0.030627338,-0.028032,-0.013779614,0.006218545,0.030182375,-0.058322072,-0.04320927,0.05213657,-0.014947292,-0.043926716,-0.02641784,0.08080095,-0.030912815,-0.033909876,-0.039684806,-0.025904816,0.04389028,-0.04638797,-0.015372382,-0.007724423,-0.026883611,0.029637046,-0.02901296,0.08178396,-0.009955312,0.034310825,0.09786289,0.032318354,0.032007113,0.0034503008,-0.03177354,0.052328072,0.0028730133,0.011081115,0.031980798,-0.023077114,0.003971672,-0.058800038,-0.036990125,-0.023334745,0.03625993,0.039958347,-0.022810189,0.048004165,0.0048325025,-0.013192416,0.043013833,0.02885292,0.069534756,-0.014751049,-0.053202346,-0.044264324,-0.025781248,-0.011936679,-0.03652999,-0.026640361,0.025515832,0.010349644,-0.05703747,0.040842388,-0.015010397,0.0013603925,-0.022863401,-0.061673403,-0.011978872,-0.005429354,-0.039324567,-0.010711166,0.06369907,-0.08867206,0.036623888,0.007823276,0.07841453,-0.014270056,-0.029325467,0.041278027,-0.038331192,-0.047973838,-0.019742033,-0.041402794,0.05087552,0.004361881,-0.02486629,-0.0051876944,0.0074603334,-0.011740045,0.057118066,0.0622935,0.121291235,-2.6857575e-05,0.014944243,-0.03986269,0.0011524304,-0.009435128,-0.028703608,-0.020027233,-0.02161354,0.0043523633,-0.023511773,0.010575533,-0.02487631,-0.0064576147,0.0155197885,-0.030771144,0.030595748,-0.031147886,0.006445347,-0.00880145,0.03285413,0.008919037,0.0018316129,-0.02912129,-0.039908946,-0.0014276522,-0.0055392496,-0.03053062,0.0013047346,0.01766705,0.031919137,0.03116123,-0.02269483,-0.011273385,-0.041571688,0.0018344305,0.0534629,-0.037192266,-0.018423522,0.0059234737,-0.05619444,-0.007774555,-0.04513928,-0.045068566,-0.005530357,-0.037767787,-0.000105487554,-0.007113793,0.008556098,0.016634779,0.03877611,0.0017521009,0.014812849,0.030267825,0.0046510575,-0.036055874,-0.012027297,-0.032236364,0.020904586,-0.023343433,-0.016288351,0.009762871,0.006451934,0.066423036,0.0153534785,-0.01041688,0.03196153,-0.038382113,0.040867645,-0.00021629305,0.039520558,0.039674554,0.047089446,0.022300228,0.032808285,0.024618488,0.027525138,-0.012919953,0.0950923,0.015917895,-0.04906184,0.054421235,0.02768437,-0.01772219,0.007806475,-0.030579895,0.005679547,0.04077977,-0.034731943,0.02579534,-0.011652943,0.043413423,0.020827448,0.019087082,-0.06507986,-0.038683683,-0.015181076,0.011944172,0.055196553,0.051447324,-0.008650508,-0.017325815,0.015627332,-0.027990377,0.05203655,0.0053133657,0.0071794842,-0.0056793923,-0.003726499,-0.05339081,0.0568247,-0.010509027,0.0026619178,0.062597856,-0.04429262,0.03671819,-0.007095302,0.008495191,-0.000850742,0.002031082,0.026790695,-0.026413074,0.031809982,-0.01480013,-0.00068902335,-0.0041437964,-0.05956938,0.026169993,0.012751208,-0.025343716,0.004766653,-0.015769575,-0.016128968,-0.0017024444,0.013810047,0.07783403,0.0019473095,0.04891938,-0.0030810605,0.019693019,0.0239296,-0.07586241,0.021615295,0.033300627,-0.0046017612,0.04137086,-0.026884496,-0.013257672,0.0047320467,-0.008859926,0.031001704,0.07049575,-0.0047463696,0.023742996,0.044291385,0.03925852,0.013995557,0.008150635,0.06452253,0.0106095625,-0.02946781,0.0125408005,-0.027529312,-0.09703419,0.054053355,-0.04193888,0.02703725,0.03988755,-0.025328051,-0.00041327442,0.056554083,-0.024769893,0.022348553,0.01998426,-0.0046369904,-0.028454205,-0.06109826,0.021673135,0.0007525424,-0.011428413,-0.019456241,-0.031489555,-0.027775617,-0.009711172,0.047150444,-0.01154916,0.08627818,0.06388886,0.033218972,0.014165746,-0.01860441,-0.032252252,0.024296321,0.060099572,-0.01314929,0.056830414,-0.06331687,0.050410412,-0.09318046,0.06258936,0.022643259,-0.053680778,0.0037262975,0.0374036,-0.019495886,-0.00588,0.019370882,0.038372137,-0.05423433,0.050512325,0.008606386,0.02539038,-0.0028095506,-0.0070618307,-0.045089707,-0.017599436,0.04254918,-0.03524336,-0.020720985,0.036852922,0.0059518805,0.0038495664,0.07953849,-0.008555854,-0.018963918,0.017679535,0.010198131,0.025109977,0.0008694418,-0.075057805,-0.041574772,0.010105071,0.018291162,0.015583455,-0.011862297,-0.0070750862,0.043089006,-0.059669662,0.026886463,0.02486186,0.027524943,-0.020695122,0.017097002,0.036977045,-0.0053930823,0.060138013,-0.09969336,-0.03176169,0.026705077,-0.016792685,-0.02991643,-0.004365218,-0.0142857935,-0.023881907,0.0031924339,-0.030452123,0.05341413,-0.00605076,0.03792049,0.03675103,0.036789596,-0.027346164,0.022451134,-0.05981773,-0.037218332,0.012758959,-0.0012423246,-0.047191728,0.0655577,0.025852995,0.0052217175,-0.01715165,0.054040957,-0.019263903,0.05513781,0.06963366,-0.04433739,0.037923846,0.004871012,0.005848704,0.011180876,0.009531372,-0.004306605,-0.033195786,-0.014792732,0.050872765,-0.027913654,0.00047722724,0.00059857743,0.0149668185,0.010105583,0.020498237,-0.016842077,0.011428646,0.04570653,0.0050126663,0.056543093,0.023390347,-0.022656972,0.022096368,0.0029032642,-0.028974628,-0.05346072,0.0024660437,-0.048984215,-0.039115287,0.046279673,-0.04445761,0.027896034,-0.031867433,0.006059832,0.019307138,0.0035005605,0.03219237,-0.02019152,-0.04385892,-0.03389989,0.03893953,0.054450948,-0.01711324,-0.012670184,-0.04717024,-0.043151356,-0.05694887,-0.0009664426,-0.008621176,0.019670034,-0.053431492,0.02846254,0.042305037,-0.04347581,0.015024982,-0.0250063,0.017252566,-0.07311199,-0.0593983,0.00036846835,0.093396656,0.09867161,0.016203877,0.01833661,0.03166016,0.059224844,0.01603077,0.047486607,-0.021466466,-0.014445018,0.02669741,-0.041528944,-0.00691979,-0.006270214,0.012752602,-0.0054759053,0.002389349,-0.046263266,0.04435709,0.046747103,-0.0020499837,-0.022377266,8.530929e-06,-0.033198446,-0.00066453044,-0.031546887,0.06709582,-0.037813634,-0.033332232,-0.017344646,0.022078253,-0.004879885,-0.049835127,-0.05162298,-0.027183717,0.017870998,-0.0021363609,0.00826483,0.010853431,-0.032900702,-0.015351319,-0.035431422,0.056975503,-0.035969287,0.047501415,0.027078604,0.01822838,0.05539278,0.014465333,-0.03521503,0.020457465,-0.025402557,-0.0019082491,-0.011654876,-0.009294157,-0.01103217,-0.009152493,0.040240012,0.019363694,-0.020074962,-0.005597356,-0.04665216,-0.017449701,0.007539755,-0.025196644,-0.013424076,-0.02076188,0.028660642,0.010256661,-0.013533662,0.02221735,0.019788338,-0.03398207,0.057590693,0.029271903,-0.015277451,0.0014540924,0.040015172,-0.0013738632,0.011082663,0.067176424,0.0014831347,-0.005333751,0.04945924,-0.006332749,0.016932545,0.0018146877,-0.007172968,0.033424776,-0.033991255,0.0022589513,-0.068296485,-0.04388517,-0.046595298,0.05339182,0.009426537,0.035371993,-0.0659704,-0.05898588,0.024332598,-0.030610662,-0.039657332,0.0794874,-0.0015450423,-0.009027959,0.028716113,-0.02883243,0.040283352,-0.1018747,-0.001474576,0.008387956,-0.02368801,-0.053959936,-0.056439694,-0.022963068,-0.015231517,-0.024127532,-0.051791295,0.030346453,0.006486041,-0.02212844,-0.0043151174,-0.029674275,0.0014877063,-0.03447836,-0.026955402,0.011426654,0.0007543266,0.0051908274,-0.0053450163,0.061317615,-0.060015347,-0.0008013989,-0.007099069,-8.628978e-05,-0.00051017,-0.06868359,0.014925504,-0.02151316,0.02922342,0.013740413,0.033885334,0.018977048,-0.015226199,-0.053676154,-0.0665231,0.03020012,-0.04124464,0.03810319,0.05790966,0.00012564396,0.002813717,-0.022740461,-0.025311051,-0.02071345,0.0036065893,0.025119007,0.020531718,0.03497624,0.049583156,-0.019008467,-0.043745726,0.0002460283,0.020156175,0.032983787,-0.008781769,0.054145556,0.055268206,-0.051097378,0.016657146,-0.026545687,-0.020452965,0.0048717367,0.022920502,-0.052029658,0.0042977687,0.0026907974,-0.04630494,0.04795637,0.026339466,0.005854091,0.07714065,0.049846645,-0.019774662,-0.046913307,-0.0324294,-0.006226106,0.011215712,0.025191497,-0.036585074,0.019972434,0.00025399515,-0.008418743,0.044323947,0.02435299,-0.035477754,-0.04545788,0.030151172,0.037132643,0.0335726,0.042770885,-0.08660019,-0.11290466,0.036824413,0.011738192,-0.017915316,0.02552738,0.013218897,-0.0047033723,-0.006428107,0.009554938,0.0115028955,0.007842325,0.025613025,0.037870605,0.029580547,0.015818935,-0.0056658676,-0.020800827,-0.043212507,-0.005380012,-0.0034568456,-0.024694998,0.039086375,0.02970014,0.001823585,-0.027069533,0.008387103,0.07207276,0.026767295,-0.0018620645,0.037674356,0.018735193,0.017856855,-0.03479404,0.019526277,0.04982293,-0.054646607,-0.0108370865,-0.025864728,0.09187773,-0.0015923369,-0.004803902,0.001741506,-0.035328496,0.0022630373,0.017065676,0.018621394,-0.010359599,0.038237225,0.015822448,-0.032832883,-0.008056188,0.0038877074,-0.06212928,0.042415865,0.0087059,0.023325639,-0.03203272,-0.029582085,-0.031004889,0.048871234,-0.01292648,-0.02684422,0.012988153,-0.0009687854]	Keywords: vanilla SGD, learning rate annealing, adaptive learning rate methods, convergence\nKey Objects: SGD, adaptive learning rate methods, minimum\nRefers to Images: None\nHypothetical Questions:\n- Why might SGD get stuck in saddle points?\n- What does it mean for an SGD schedule to be 'robust'?\n- In what scenarios might using vanilla SGD be a viable option despite its slower convergence?\n---\nSummary:\nWhile SGD can find a minimum, it typically requires more time and careful tuning compared to adaptive learning rate methods.\nOriginal Text:\nInterestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.  \n13 Also have a look at http://cs231n.github.io/neural-networks-3/ for a description of the same images by Karpathy and another concise overview of the algorithms discussed.\nContextualized Text:\nInterestingly, recent papers often utilize vanilla SGD with a simple learning rate annealing schedule. While SGD is capable of finding a minimum, it generally takes significantly longer than adaptive learning rate optimizers, necessitates a robust initialization and learning rate schedule, and is prone to getting stuck in saddle points instead of local minima. Therefore, for faster convergence when training complex neural networks, adaptive learning rate methods are typically preferred.	{"tags": ["optimization", "deep-learning", "SGD", "convergence"], "doc_id": "0833e719-8d42-4942-aede-738c13a959bb", "summary": "While SGD can find a minimum, it typically requires more time and careful tuning compared to adaptive learning rate methods.", "doc_type": "text", "entities": [], "keywords": ["vanilla SGD", "learning rate annealing", "adaptive learning rate methods", "convergence"], "key_objects": ["SGD", "adaptive learning rate methods", "minimum"], "contextual_text": "Interestingly, recent papers often utilize vanilla SGD with a simple learning rate annealing schedule. While SGD is capable of finding a minimum, it generally takes significantly longer than adaptive learning rate optimizers, necessitates a robust initialization and learning rate schedule, and is prone to getting stuck in saddle points instead of local minima. Therefore, for faster convergence when training complex neural networks, adaptive learning rate methods are typically preferred.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "4 Gradient descent optimization algorithms", "h3": "4.10 Which optimizer to use?"}, "hypothetical_questions": ["Why might SGD get stuck in saddle points?", "What does it mean for an SGD schedule to be 'robust'?", "In what scenarios might using vanilla SGD be a viable option despite its slower convergence?"]}
4362f5ec-ca19-4530-bf1c-6b62baf2a67d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.016789187,0.0047847196,-0.00073339493,0.023693942,-0.016879335,0.058040053,-0.00091187115,0.008784847,0.015059157,-0.016235847,-0.01887867,-0.029083602,-0.0063366364,0.011421047,0.0021587298,0.01275087,0.012069954,0.08930756,0.051796988,-0.05773087,0.02492066,0.029041357,-0.0093377335,0.03297404,-0.011283897,0.03322379,0.03451291,-0.014396646,-0.0032186168,-0.03296469,-0.037401628,-0.0010737777,0.011163523,-0.019059101,0.044496495,-0.020917125,-0.00019417609,-0.027161611,0.017092934,-0.06122309,-0.02256724,0.0185207,-0.015031567,-0.021040522,-0.01403245,-0.04820738,-0.070146605,-0.012678267,0.027104693,-0.013572116,-0.036671385,0.022222616,-0.055241767,0.023060316,-0.057113554,0.0130528705,-0.019751307,-0.0027555325,0.010952422,-0.05090919,-0.08556268,0.023402937,0.0103773195,-0.0040598563,0.045361765,0.003897359,-0.03254494,0.011528834,0.01871972,0.12951417,0.001981097,0.07374625,0.014323512,-0.06632584,0.10618412,0.048654918,-0.075558394,-0.008841813,-0.038006037,0.008894529,0.018595537,0.056939673,-0.02750895,-0.029897915,0.07121214,-0.02538649,-0.032000452,-0.04602924,0.063937515,-0.02886777,-0.06255635,-0.02501092,0.0071771485,0.025303451,-0.06121741,-0.033647604,0.010811255,-0.062003423,-0.016107155,-0.037945423,0.044871144,-0.017004529,0.006472182,0.09708469,0.03207733,0.009211001,0.032824703,-0.04540725,0.016241211,-0.023105621,0.0003596953,0.041361343,-0.031899557,-0.02976607,-0.06125275,0.01836456,-0.051593088,0.002835123,0.0048781894,-0.07784139,-0.01446604,0.0075640744,-0.012775319,-0.015341094,0.011792456,0.06331037,0.020582888,-0.04730458,-0.034165896,-0.021751558,0.024054147,-0.013228547,-0.03377537,0.039251786,-0.047570292,-0.02106089,0.020108765,0.0040935986,-0.016218787,-0.03036061,-0.049124345,0.022528032,-0.056772728,-0.017910726,0.0031425783,0.008881768,-0.078825995,0.060417015,0.014552131,0.06642017,0.014807287,0.0011336508,0.045443553,0.0068276785,-0.031687714,-0.059957303,-0.0208522,0.0064066905,-0.01642029,0.015625302,-0.021078257,-0.0033533736,0.03208054,0.06603993,0.08890409,0.12066686,0.025867635,0.0348501,-0.017520826,0.0032450021,-0.053152844,-0.023619935,0.013533019,-0.032173727,-0.009758383,0.0027972916,-0.0021710938,-0.021908918,0.0071119186,0.0016026798,-0.0018176304,0.03443328,-0.023803256,0.009108868,0.055176232,0.043100104,-0.00081527827,0.0390824,-0.0012161182,-0.010557654,0.0010565609,-0.02847669,0.032031003,0.018529724,0.023102565,0.013820348,0.03192738,-0.0017016138,-0.00172421,-0.06095927,0.027768,0.019344872,-0.054483242,-0.02537322,0.04885691,-0.030202253,0.02869051,-0.06715321,-0.016977096,-0.026634268,-0.04817035,0.089927875,0.006975138,0.026324995,0.020991327,0.049734093,0.0129321655,0.041755907,0.03649895,0.011471002,-0.07556253,-0.026060024,0.016254116,0.03666875,-0.019495523,-0.0091098165,0.023866236,-0.006758003,0.05478225,-0.02616333,-0.051970236,0.033823796,-0.024950858,0.070653245,-0.004085359,0.015226491,-0.006718801,0.020816341,0.011158874,0.06297972,0.015439226,0.005394611,0.02378405,0.08311704,-0.026481505,-0.0026681635,0.010189943,0.007750818,0.01728823,0.025903389,-0.052284498,0.010594928,0.03768678,-0.06288037,-0.036792327,-0.02839411,0.03865552,0.008438226,0.014547453,-0.07582175,-0.01518235,-0.008138446,0.0069116144,0.05810792,0.010169969,0.015503127,0.01477478,-0.007256573,-0.021103617,0.026370885,0.014030951,0.041115288,0.041362718,-0.008066161,-0.028987257,0.028013224,0.0104008205,-0.018971246,0.042434193,-0.058103364,0.034920182,0.0024332958,0.039833892,0.016301328,-0.021037087,0.024934975,-0.023878332,-0.004877307,-0.018307596,0.06936713,-0.0152309025,-0.05869675,0.0755815,0.028630406,-0.003778066,0.0136461165,0.0018700506,-0.030751701,-0.0035014674,0.017632192,0.055322412,-0.00853634,0.036497813,-0.011022799,0.029227972,0.04040767,-0.03468158,0.016539738,-0.022365298,0.014304115,0.036793727,-0.008412379,-0.018965928,-0.0075222296,-0.009528423,0.04712025,0.08378154,0.02822114,0.0011717186,-0.0155622475,0.039404698,0.015247403,-0.025771277,0.026695779,-0.015774252,0.012205797,0.05531418,-0.010030085,-0.08156102,0.05661349,-0.06691321,-0.0036853992,0.047436003,-0.006377471,0.01728881,-0.0009667461,-0.019043956,0.005377148,0.020826828,0.0048661185,-0.0015583761,-0.021104505,0.010432046,0.009211149,-0.026951736,0.0102387555,-0.0075066616,-0.028860025,0.0072224215,0.037524417,0.0041518635,0.056452323,0.047861945,0.018753527,0.011897914,-0.016374094,0.0013065552,0.03528378,0.047013793,-0.020777157,0.025910312,-0.063662365,0.06914404,-0.019192977,0.059727844,0.00784438,-0.046792794,0.038157437,0.0060373168,-0.037730966,0.028482849,0.020769477,0.037392512,-0.055469334,0.026135106,0.056037553,0.05260193,0.040376782,-0.0318149,-0.013365529,-0.0060412386,0.015841553,-0.062144954,-0.002068277,0.024053099,0.020319642,-0.0106939925,0.12068854,-0.0017205724,0.015446117,-0.06388001,0.021863993,0.037739925,0.015453695,-0.057419613,-0.024036458,0.039908394,0.040137347,0.017737955,0.029102696,0.0028083408,0.035168517,-0.04722423,0.035861406,-0.03591762,0.005739649,-0.0033948615,-0.010339103,0.06465996,0.0031851935,0.029424593,-0.102102704,-0.0335305,0.01085974,0.011131587,-0.010095668,-0.0064870697,-0.015232832,-0.009905903,-0.032145616,-0.03448421,0.07073221,0.044142738,0.016594743,0.0060299337,0.010926419,-0.019391444,0.049959973,-0.036514807,0.00564312,-0.017260985,0.013095904,-0.017017001,0.05462118,0.0007939247,0.03217923,0.005177308,0.017583596,-0.049199376,0.010885138,0.050199218,-0.022103375,0.015477796,0.041947506,-0.0059544835,0.047176402,0.0035207854,-0.033748277,0.00020062407,0.001619107,0.021008078,0.0048727836,0.029748559,-0.0048235427,0.044850864,-0.038280938,-0.018018266,-0.06068455,0.00100837,0.05385684,-0.050857637,0.020070985,0.014650725,0.0031621524,0.008459114,0.025955053,0.0010402258,-0.025236493,-0.010739883,-0.042580362,-0.050847597,0.061631612,-0.042190183,0.06476035,-0.04279222,0.0016195251,-0.021588648,0.0229578,0.031329677,0.010495307,-0.082789615,-0.008339493,0.028770238,-0.010536132,0.00090029056,-0.03326819,-0.034714278,-0.013605017,-0.030839436,0.00045099555,-0.048423618,0.005188806,-0.05365759,0.02932288,-0.00011885678,-0.010789957,0.024818745,-0.039601456,-0.031864755,-0.061543517,-0.069379,-0.010796132,0.06269635,0.05894249,-0.0006651213,0.024974057,0.045584366,0.073176816,0.0073993453,0.05957421,-0.01211314,0.03715708,-0.018446304,-0.03645211,0.00667185,0.013416409,-0.0011644083,-0.011404718,0.014395654,-0.027560353,0.009911559,-0.002382629,0.006693191,-0.00965637,0.02128838,0.008454487,0.017010672,0.00011623196,0.06746516,-0.054394823,-0.031014223,-0.019543102,0.00891396,-0.027396584,-0.12801987,-0.054366075,0.007883681,0.01933771,0.0104116015,0.0051548136,-0.01571113,-0.061622594,-0.030625204,-0.045718458,0.032299448,-0.042396214,-0.0044013797,0.043029744,-0.0088344375,0.09467623,0.023771789,-0.023701742,0.01643018,0.015134415,-0.021644609,-0.007879667,0.011415556,-0.011291516,0.021449922,0.043760035,-0.021806981,-0.004175764,-0.060889605,-0.044352338,-0.014462426,-0.016178278,-0.019181093,-0.03554452,-0.034924597,-0.009284723,0.00069967046,-0.007210446,-0.0073536583,0.03889085,-0.008604763,0.078167275,0.011237884,-0.0120081995,0.038459364,0.051470004,0.026807018,0.023901643,0.07046034,-0.0039838296,0.007168456,0.034797326,0.033041283,-0.0004955128,0.027339214,-0.040214036,0.03046176,-0.039396316,0.030150631,-0.019927466,0.002174777,-0.029662455,0.055776536,-0.02711371,0.041408613,-0.028156757,-0.03232282,0.010968622,-0.048554048,-0.059218448,0.012182187,-0.033208445,-0.030201433,0.038814187,0.00503223,0.06476882,-0.08147364,-0.009187116,-0.02174676,-0.0024348784,-0.055017173,-0.038696215,0.0046603424,-0.008080874,-0.015265224,-0.030314673,0.005242066,0.008350427,-0.047614075,-0.010519195,-0.053919468,-0.0283389,-0.022279337,-0.0464465,-0.04629796,-0.0011702161,-0.0012228043,-0.0099145435,0.08318945,-0.053434942,-0.028382089,0.013525017,0.016220396,-0.059766725,-0.015098828,0.017055683,-0.0066049355,0.030746177,0.009447067,0.033508852,-0.01557928,-0.05064062,-0.00075381325,-0.06586706,0.018067595,-0.0003035379,0.06738224,0.05756177,-0.017729772,-0.025188707,-0.0407565,-0.05804143,0.026834786,0.024642603,0.014540747,0.006755791,0.047222402,0.057302173,0.009453583,-0.05365959,-0.04417393,0.046183404,0.04620992,-0.020468581,0.007860818,0.033105966,-0.0029671341,0.007446777,-0.013084901,-0.039243203,-0.03572114,0.0021054065,-0.013783101,-0.0024214108,-0.013835943,-0.012599536,0.046168234,0.014038827,0.038532637,0.08013242,-0.026846366,-0.04052515,-0.04613261,-0.03865412,-0.02361436,-0.008240615,0.003125776,-0.02732745,0.008071162,-0.03060471,-0.011814666,0.061561033,-0.004915571,-0.066490136,-0.005461811,0.01640095,0.021214092,0.035461202,0.02398253,-0.04365945,-0.09233776,0.021369532,-0.026578642,0.011931447,-0.013618274,-0.037346773,-0.017827448,-0.059690498,0.029088084,-0.007947806,-0.021129563,-0.005811334,0.038331445,-0.002995932,-0.018401865,0.020849353,-0.04579903,-0.025052052,0.007672518,0.0015178211,-0.042957332,0.046169586,0.06497609,0.014703025,-0.041889694,0.08350264,0.047342755,0.01495809,-0.017454872,0.055454507,0.050760634,0.044416614,-0.06421512,-0.010978646,0.050689533,-0.06979855,0.017138612,-0.028471494,0.09567665,-0.009633477,-0.020566408,-0.015866296,-0.053790376,0.017207235,0.058256544,-0.00785259,-0.026167933,-0.012476749,0.043778468,-0.04485298,-0.028971938,-0.016676899,-0.0036056617,0.027580304,0.056145642,-0.0074495347,0.016347323,-0.02851063,-0.04786658,0.01699401,-0.010219807,0.0009683836,-0.008654433,0.015367927]	Keywords: Stochastic Gradient Descent, SGD, parallelizing, distributing, large datasets\nKey Objects: Stochastic Gradient Descent, SGD, computing clusters\nRefers to Images: None\nHypothetical Questions:\n- Why is standard SGD inherently sequential?\n- What are the trade-offs between synchronous and asynchronous SGD?\n- Under what circumstances would parallelizing SGD on a single machine be preferable to distributing it across a cluster?\n---\nSummary:\nDistributing Stochastic Gradient Descent (SGD) is a common approach to accelerate training, especially with large datasets and readily available computing clusters.\nOriginal Text:\n## 5 Parallelizing and distributing SGD  \nGiven the ubiquity of large-scale data solutions and the availability of low-commodity clusters, distributing SGD to speed it up further is an obvious choice. SGD by itself is inherently sequential: Step-by-step, we progress further towards the minimum. Running it provides good convergence but can be slow particularly on large datasets. In contrast, running SGD asynchronously is faster, but suboptimal communication between workers can lead to poor convergence. Additionally, we can also parallelize SGD on one machine without the need for a large computing cluster. The following are algorithms and architectures that have been proposed to optimize parallelized and distributed SGD.\nContextualized Text:\nTo accelerate training on large datasets, Stochastic Gradient Descent (SGD) can be distributed across multiple machines or parallelized on a single machine.  Because SGD is inherently sequential, standard implementations can be slow. Distributing or parallelizing SGD is an obvious choice given the widespread availability of commodity computing clusters.	{"tags": ["optimization", "parallelism", "distributed computing"], "doc_id": "4362f5ec-ca19-4530-bf1c-6b62baf2a67d", "summary": "Distributing Stochastic Gradient Descent (SGD) is a common approach to accelerate training, especially with large datasets and readily available computing clusters.", "doc_type": "text", "entities": [], "keywords": ["Stochastic Gradient Descent", "SGD", "parallelizing", "distributing", "large datasets"], "key_objects": ["Stochastic Gradient Descent", "SGD", "computing clusters"], "contextual_text": "To accelerate training on large datasets, Stochastic Gradient Descent (SGD) can be distributed across multiple machines or parallelized on a single machine.  Because SGD is inherently sequential, standard implementations can be slow. Distributing or parallelizing SGD is an obvious choice given the widespread availability of commodity computing clusters.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "5 Parallelizing and distributing SGD"}, "hypothetical_questions": ["Why is standard SGD inherently sequential?", "What are the trade-offs between synchronous and asynchronous SGD?", "Under what circumstances would parallelizing SGD on a single machine be preferable to distributing it across a cluster?"]}
354f6e50-1d7a-4db5-a888-0fec221054dc	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.05479529,0.070635,-0.012997714,0.029980583,-0.0025666503,0.04221943,-0.015307892,0.04319854,0.0007025449,-0.031826504,0.0016986859,-0.04198439,-0.004118176,0.005605495,-0.014669608,-0.027304454,0.020572243,0.09442908,0.014682219,-0.049941834,0.028981017,0.008028346,0.014086623,-0.0020619114,0.010278961,0.02591294,0.016238231,-0.015618208,0.04710526,-0.019127512,-0.032454323,-0.03184765,-0.010119642,-0.030097751,0.074193165,-0.0024472892,-0.026561743,-0.0818154,-0.00086251146,-0.048637208,-0.023437014,0.027081896,-0.023693867,-0.015376193,0.01742884,-0.04037677,-0.058245197,-0.025768265,0.045367103,-0.054320175,0.033523113,0.030868277,-0.044379316,0.0653962,-0.05788346,0.016403735,0.0028510226,-0.014260313,0.028770026,-0.06477865,-0.068974994,0.027988216,0.02658981,0.017260706,0.042019207,0.029016245,0.022596626,0.052797016,0.052647017,0.13322347,0.0068128053,0.040083632,0.0056601753,-0.10711582,0.1030936,0.07453847,-0.034088574,-0.02546583,-0.028515045,-0.0026941656,-0.007864258,0.032371596,-0.04417275,0.0029161961,0.060249623,-0.015680006,-0.02414731,-0.028871307,0.040053792,0.012081624,-0.024886318,-0.0022455137,-0.039700367,0.032765128,-0.010740182,-0.008674955,-0.017075045,-0.026804768,-0.03601435,-0.0060183164,0.0026863297,-0.019319827,0.029128566,0.094067626,0.019358749,0.0135082435,-0.025664622,-0.00089000055,-0.016130859,-0.018049024,-0.01731576,0.05067018,-0.025475703,-0.048706815,-0.046297774,-0.025677906,0.0016753002,-0.00089119945,-0.0054198494,-0.046504866,0.013118404,0.011383733,-0.045797385,0.010363225,0.03837705,0.07668062,0.041684538,-0.045533206,-0.042094346,-0.04092819,0.059424028,-0.035147656,-0.023505643,0.024781993,-0.035233624,-0.051498197,0.052917417,-0.0041500344,-0.02342359,-0.0136676915,-0.0764307,-0.002551432,-0.06355875,0.0041029714,-0.008700307,0.006368529,-0.06621745,0.036728684,-0.0151383355,0.061604418,0.038718652,-0.02409995,0.051252775,0.00494537,-0.022083499,-0.02119843,-0.048054814,0.001987319,0.01075689,-0.044487104,0.024928503,0.021510933,-0.0057834387,0.08297694,0.04952082,0.09916625,0.0076750666,0.027979331,-0.045668226,0.008332,-0.055185605,-0.052133072,0.024540644,-0.040178355,-0.02002385,-0.026028614,0.0058333604,-0.022310281,0.019596063,0.025758864,-0.032167338,0.043930948,-0.0009536305,-0.01905137,0.050925814,0.035689514,0.02823688,0.017441973,0.0117152035,-0.038591746,0.013335956,-0.022564227,0.004567032,0.003052675,0.04380734,0.042061828,0.046068918,0.020851213,-0.0075020473,-0.04971456,0.017701605,0.04543404,-0.025327317,-0.019119829,0.057905886,-0.042444058,0.0022350457,-0.031153334,-0.014522138,-0.015063318,-0.002247662,0.059964433,0.06479885,0.077271186,0.036327288,0.061026275,-0.02949342,0.037466012,0.040468022,-0.02279124,-0.076867215,0.031955004,-0.03827608,0.07386105,-0.027832234,0.020091671,0.016208814,-0.0021848434,0.05367617,0.020928113,-0.01848547,0.033921897,-0.03902301,0.049662717,-0.009744035,0.012403014,-0.021186668,0.021480147,0.018973693,0.034082316,0.012724193,0.004777609,0.024309313,0.08389591,0.008477617,-0.0039098887,0.0022201035,-0.0147591075,0.018091757,-0.012707941,-0.0370197,0.008047369,0.053401846,-0.020443346,-0.015948504,-0.03542483,0.0057026315,0.036926232,0.0051535424,-0.06607465,0.025748437,-0.0240775,-0.007458932,0.004843417,0.028348194,-0.00471283,0.008596014,0.010718584,-0.003269073,0.011271486,0.0374679,0.039518613,-0.0052453754,-0.016077982,-0.024787381,0.016558563,0.0053769127,-0.0028142298,0.04119636,-0.072043546,-0.004973917,0.018554732,0.028125903,0.035611704,-0.02092973,0.057255674,-0.01994306,-0.001008198,-0.031150904,0.021015212,-0.017230552,-0.019270554,-0.011203954,0.022125315,-0.036929294,-0.0046104905,-0.009480204,-0.044241913,-0.051243097,0.022748875,0.13325682,-0.0038847718,0.023380747,0.019684678,-0.008090602,-0.009817138,-0.05068239,-0.0019354881,0.008449216,0.027840346,0.035287995,-0.013895935,0.015465907,-0.013624813,0.015571932,0.047445133,0.08240404,0.007971624,-0.003219344,-0.004841234,0.042696297,0.008529754,-0.012857522,0.0068299775,0.01647229,-0.011319644,0.049346942,-0.03130257,-0.09316261,0.10615029,-0.03586358,-0.006958939,0.062253337,-0.04579433,-0.014671924,0.012118517,0.014526499,0.019716937,0.03203491,0.019070802,-0.022475319,-0.058742017,0.04758755,0.02585332,-0.025631351,-0.0055304673,0.0072837886,-0.03596742,0.0051652016,0.036499053,-0.026958644,0.058555003,0.04704513,-0.006178651,0.01358268,0.0061519556,-0.07742747,0.015175548,0.018626742,-0.031212877,0.038507197,-0.056087535,0.036192738,-0.034319416,0.021313203,-0.016382823,-0.054553915,0.037332572,-0.03208794,-0.012049879,0.027064381,0.035163846,0.032095905,-0.053067375,0.009517005,0.0490939,0.059825543,0.03319168,-0.0496516,-0.05242988,-0.036584605,0.038755134,-0.049939282,-0.04669917,0.064412646,0.06361501,0.013630907,0.0815864,0.0017698912,-0.0007528782,-0.036388453,0.04160793,0.02898839,-0.02680864,-0.035328772,-0.04549071,0.014130366,0.009356159,0.039553586,0.015842676,0.035001058,-0.020908996,-0.057964213,0.029367538,0.0330785,0.02658987,-0.019048495,0.0038397608,0.066949606,-0.012133471,0.052122973,-0.08534675,-0.06186852,-0.009529606,0.018243654,-0.0032191996,-0.004369014,0.0106598735,-0.0143281575,-0.008042378,-0.007864389,0.06398418,0.052422382,0.014032956,0.015622373,0.011631775,-0.033772442,0.019901756,-0.0016750988,-0.016955309,-0.028363792,-0.0016839856,-0.058383044,0.043044887,0.038476054,-0.038006227,-0.031918712,0.03474391,-0.045953788,-0.0047738864,0.03324626,-0.03272946,0.0076401005,0.03786501,0.009325949,-0.009881526,0.020920578,-0.003192605,0.0087815225,-0.018306755,0.039610527,0.026620686,0.05137652,0.002891747,0.028048584,0.0065868483,0.011097399,-0.041430898,-0.008206206,0.034703407,-0.040335942,0.048439804,0.035520248,0.009109096,-0.028046638,0.029852455,-0.017575517,-0.031645108,-0.034602497,-0.058909982,0.0029841347,0.06489165,-0.02340591,0.055312186,-0.02802934,-0.017291423,-0.01895302,0.014154898,0.04666751,-0.0065791854,-0.044198528,-0.0032692656,0.009796318,-0.015014326,-0.010944066,-0.016495619,-0.0046349787,0.008235736,-0.010567995,-0.012023179,-0.03725095,0.008041452,-0.03376537,0.012098299,0.00567121,-0.0029543312,0.038399998,-0.019511065,0.026913641,-0.041507225,-0.06940021,-0.00083728106,0.062193323,0.06685204,0.03534317,0.048950125,0.024788534,0.04631126,-0.0153332045,0.05983495,-0.017476933,-0.016071364,0.018496826,-0.04487816,0.00010060244,0.02962247,0.015767269,-0.04152575,0.026787397,-0.009966266,0.0012345798,-0.015688175,0.0042233095,-0.013423719,0.022287391,-0.016209792,0.030018825,0.0041655563,0.044084553,-0.06787307,-0.032085918,-0.038746078,-0.008194338,0.015533315,-0.057640184,-0.05727365,0.04476369,-0.0018232098,-0.0006069269,-0.027627062,-0.03868358,-0.0297486,-0.010535561,-0.05268893,0.051354934,-0.02245922,-0.0103362035,0.036719184,-0.046662696,0.107103296,0.021458933,-0.0059285415,0.0049787015,0.016950883,-0.037917305,-0.0050314395,-0.008662594,0.01009496,0.01772909,0.044602334,-0.027827837,0.005510269,-0.029072396,-0.03821884,0.010232088,-0.03956887,0.019623637,-0.04908761,-0.025222745,-0.038909428,-0.015664736,-0.0034356401,-0.0033588135,0.04398029,-0.0111766,0.08307707,0.024560284,0.02053367,-0.0010797969,0.022715367,0.01589527,0.028684745,0.0520685,0.02646201,-0.016141936,0.07082258,0.0065556816,-0.023797566,0.0026791692,-0.025977768,0.019675983,0.00043951618,-0.016173122,-0.04050381,0.0039767013,-0.023404585,0.020301364,-0.021412423,0.04530431,-0.044584285,-0.051835522,-0.009485074,-0.05725854,-0.01725593,0.04381875,0.0022959167,-0.0073124785,0.013337306,0.040054854,0.037524454,-0.059736736,-0.023405172,0.02697562,-0.0012755672,-0.050727982,-0.026379343,0.014753125,-0.030222291,0.005926411,0.011497238,0.019543221,-0.038793843,-0.043277077,-0.030810153,-0.055397708,-0.0069359,-0.04916942,-0.033355024,-0.027643152,-0.0036688657,0.001945516,-0.005017914,0.064709105,-0.010570816,-0.033319596,0.0016292168,0.00690352,-0.08453368,0.013403792,-0.0014151372,0.04387216,0.01473693,-0.030360416,0.016019814,-0.022829596,0.0031312136,-0.00011477685,-0.03061665,0.01155662,-0.04824316,0.056822095,0.05098317,-0.02275616,-0.026260111,-0.040327754,-0.041310064,0.022493998,0.018136315,0.036415994,-0.021057907,0.032601923,0.038129285,0.03226472,-0.06351422,-0.06324658,0.033378802,0.0105593875,-0.026328389,0.050679483,0.027923377,-0.039454088,-0.008753768,0.01110268,-0.058637183,-0.038458858,-0.018700495,-0.013326415,-0.024720607,-0.005307683,-0.042723373,0.0010313569,0.010467613,0.01745053,0.08781285,0.005157431,-0.021135835,-0.080441624,-0.004000523,0.0020943552,0.0074637337,-0.036564358,-0.034423918,-0.012651555,0.005214908,-0.01576982,0.07623168,0.030959848,-0.07742895,-0.019199857,-0.010953183,0.006478348,0.020551283,0.019449363,-0.01915799,-0.013240962,-0.004306927,-0.020024417,-0.01960388,-0.011113036,-0.015393471,-0.053598385,-0.030586967,0.023441881,0.016383115,-0.01745933,0.005650339,0.013427386,-0.017200742,-0.037046786,0.035632834,-0.046227008,-0.030158797,0.038547,0.045704104,-0.036746964,-0.01520628,0.062891744,0.047384255,-0.049903523,0.024478728,0.08222259,0.019889465,0.007004479,0.09762171,0.035535876,0.015121446,-0.038738016,0.024434721,0.055418592,-0.050389424,-0.0030042669,-0.044747785,0.0888245,-0.01579536,-0.033058863,-0.03372975,-0.06413762,-0.033500403,0.051197696,0.008004809,-0.03769581,-0.012274095,0.048215993,-0.039000567,-0.008271896,-0.0006506517,-0.034706905,0.042894028,0.042376198,-0.01234864,-0.00075686304,-0.04636814,-0.016143844,0.044969294,-0.04760951,-0.0022067423,-0.0027079298,0.017671773]	Keywords: parallel SGD, Hogwild!, shared memory, sparse data, update scheme\nKey Objects: Hogwild!, shared memory, parameters, processors\nRefers to Images: None\nHypothetical Questions:\n- Why is sparse data a requirement for the Hogwild! update scheme?\n- What is the advantage of allowing processors to access shared memory without locking parameters?\n- How does Hogwild! compare to traditional parallel SGD methods that use parameter locking?\n---\nSummary:\nNiu et al. introduced Hogwild!, an update scheme that enables parallel SGD updates on CPUs by allowing processors to access shared memory without parameter locking.\nOriginal Text:\n### 5.1 Hogwild!  \nNiu et al. [15] introduce an update scheme called Hogwild! that allows performing SGD updates in parallel on CPUs. Processors are allowed to access shared memory without locking the parameters. This only works if the input data is sparse, as each update will only modify a fraction of all parameters. They show that in this case, the update scheme achieves almost an optimal rate of convergence, as it is unlikely that processors will overwrite useful information.\nContextualized Text:\nTo enable parallel Stochastic Gradient Descent (SGD) updates on CPUs, Niu et al. introduced Hogwild!, an update scheme that allows processors to access shared memory without parameter locking. This method is effective when the input data is sparse, as it ensures that each update only modifies a small fraction of the overall parameters, preventing processors from overwriting valuable information and achieving a near-optimal convergence rate.	{"tags": ["optimization", "parallelism", "SGD"], "doc_id": "354f6e50-1d7a-4db5-a888-0fec221054dc", "summary": "Niu et al. introduced Hogwild!, an update scheme that enables parallel SGD updates on CPUs by allowing processors to access shared memory without parameter locking.", "doc_type": "text", "entities": ["Niu", "SGD"], "keywords": ["parallel SGD", "Hogwild!", "shared memory", "sparse data", "update scheme"], "key_objects": ["Hogwild!", "shared memory", "parameters", "processors"], "contextual_text": "To enable parallel Stochastic Gradient Descent (SGD) updates on CPUs, Niu et al. introduced Hogwild!, an update scheme that allows processors to access shared memory without parameter locking. This method is effective when the input data is sparse, as it ensures that each update only modifies a small fraction of the overall parameters, preventing processors from overwriting valuable information and achieving a near-optimal convergence rate.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "5 Parallelizing and distributing SGD", "h3": "5.1 Hogwild!"}, "hypothetical_questions": ["Why is sparse data a requirement for the Hogwild! update scheme?", "What is the advantage of allowing processors to access shared memory without locking parameters?", "How does Hogwild! compare to traditional parallel SGD methods that use parameter locking?"]}
0bc35fe9-9dbb-438e-a6d7-5cbb0d985dea	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.031157197,0.06748864,0.037325267,0.025304353,-0.00822101,0.055460766,0.0022798881,0.0032802762,-0.004201998,-0.026551697,-0.019055814,-0.026074912,-0.009017489,0.002713884,-0.024356633,0.016547134,-0.01678018,0.098294504,0.02997738,-0.0211227,0.02603691,-0.008163654,0.0152343875,0.0077603743,-0.046627697,0.02213258,0.051381752,-0.041278426,-0.0057230815,0.008999029,-0.011371263,-0.0011474985,0.038496744,0.027850272,0.045179237,-0.017497895,-0.033313822,0.0047021555,0.028111234,-0.065400325,-0.03347733,0.027074242,-0.008993951,-0.022425415,-0.017768461,-0.019992217,-0.06805172,0.01753167,-0.00037946898,-0.03359934,0.0013542646,0.04763711,-0.062190894,0.06512503,-0.029384926,-0.0039782394,-0.00076206634,-0.028787857,0.025744442,-0.098623574,-0.06338419,0.031118745,0.0071684816,0.007882635,0.016624918,-0.001680196,-0.021206845,0.0075964597,0.036627643,0.13116036,0.00055575476,0.03582242,-0.0070600896,-0.06444516,0.10424696,0.04430285,-0.04661394,-0.020158265,-0.016596904,-0.0025515025,0.010033754,0.071485296,-0.03863824,-0.02398067,0.06983053,0.009619291,-0.05501325,-0.03366906,0.08830476,-0.03540274,-0.04290198,-0.005574373,0.017546013,0.029843833,-0.041693676,-0.021761745,-0.022668375,-0.045331042,-0.0087926835,-0.0021115292,0.05432822,-0.026660262,0.072608806,0.102474995,0.04944381,0.006947857,0.026682565,-0.012289417,0.065200984,0.015328184,-0.009008732,0.071175545,-0.04585788,-0.0012462083,-0.049002565,0.0054792515,-0.032351606,-0.024938865,0.0021660975,-0.07019621,-0.012373783,0.02600065,-0.01756472,0.025428526,0.036719397,0.078854255,-0.010668072,-0.042336408,-0.025452105,0.020775383,0.028789785,-0.013733296,-0.026902817,0.062375113,-0.03049952,0.007586835,-0.0015101708,0.015932558,-0.014861028,-0.04147699,-0.036937565,-0.0044871797,-0.057583064,0.02242679,-0.03244024,0.03110147,-0.07271915,0.03031384,-0.04613561,0.08307094,0.018990517,-0.0065407273,0.06468102,-0.04002428,-0.022448469,-0.032647245,-0.04660631,0.019128239,0.0045768763,0.00057200337,-0.013150867,-0.009969577,0.027026521,0.07847163,0.058638774,0.13877212,-0.0060797823,-0.0019773773,-0.02716068,0.0112979235,0.00533992,-0.024793677,-0.027389131,-0.032069433,0.009475265,-0.010021048,0.024260638,-0.018778412,-0.0036621175,-0.021808682,-0.023839831,0.03892122,-0.0024743285,0.029766206,0.008737499,0.046284284,-0.004860598,0.071792915,-0.004484267,-0.019780643,0.008600483,-0.037773855,-0.0018088989,-0.0021085853,0.018328728,0.0101722395,0.041569736,-0.008575192,-0.06697112,-0.046829354,-0.0024112198,-0.0043307287,-0.01787625,-0.010601967,0.053845067,-0.073716626,0.02838177,-0.05116455,-0.044941116,0.0052384175,-0.053873878,0.047210097,0.05440709,0.06881002,0.025684936,0.025866995,-0.014182474,0.039731525,0.052082844,-0.0010656891,-0.060902525,-0.013443133,-0.021860387,0.08118115,-0.050501376,-0.0237994,-0.012828043,0.0022827343,0.068685964,-0.0016723142,-0.04964161,0.057649747,-0.04113332,0.03475799,0.013598028,0.022432555,-0.013866106,-0.011337245,0.017425312,0.033405498,0.016177548,-0.008889553,-0.029534936,0.061234992,-0.010313441,0.024414828,0.028976252,0.017537069,-0.018480584,0.031727225,-0.0250427,0.0325961,0.05682776,-0.07274416,-0.02452983,-0.06148842,0.018485585,0.036537718,-0.0022392937,-0.02607788,0.0063218833,0.017541848,0.008622185,0.0044965306,0.014948911,0.020825522,-0.049036503,-0.016729053,-0.05772509,0.018262649,0.0077362135,0.04026293,0.020935996,0.018014662,-0.018266141,0.028396403,-0.003240175,0.0063237944,0.048022814,-0.0039383452,0.024272442,-0.00021731351,-0.00442506,0.016878195,-0.027744113,0.018308884,0.018851725,-0.019101223,-0.029791793,0.0034191478,-0.061044052,-0.0449194,-0.019439489,0.042737655,-0.018320475,0.0122369,-0.0023417424,-0.0754161,-0.0024168135,0.020172367,0.08627259,-0.026142955,0.017823068,0.02547105,0.024734559,0.028052192,-0.058218118,0.04740887,0.050789725,-0.018541804,0.07336588,0.02590439,0.0022342752,-0.015741518,0.056739196,0.050719973,0.063088425,-0.024205294,0.0045706662,-0.010248073,0.040963568,0.03957132,0.0071535595,-0.0009553,-0.0025538877,0.020311939,0.021077361,-0.031243814,-0.039228458,0.070858985,-0.059188105,-0.0068351077,0.05517893,-0.023914192,-0.030580029,-0.016520718,0.0050043385,0.058791425,0.0048143324,-0.013056235,0.00057657907,-0.042668708,0.015771331,-0.018155806,-0.035838686,-0.010081335,-0.029388594,-0.043987155,-0.007081826,0.028195396,0.006232191,0.032264493,0.06783846,-0.017667707,0.023662465,-0.0042015733,-0.022222944,0.035932183,0.035500858,0.028452968,0.02416738,-0.040049836,0.046622198,-0.050514404,0.036689173,0.03555801,-0.06808179,0.024295662,-0.019105019,-0.018645188,0.018137682,-0.0029842213,0.019271594,-0.058302894,0.06203872,0.052891213,0.06121371,0.025980473,-0.031663258,-0.058748294,0.008541237,0.026198113,-0.070141055,-0.061585497,0.035191488,0.0020159788,0.00935752,0.061796576,-0.0430088,0.019582883,-0.05094476,0.016794344,0.010490984,0.0045895344,-0.031548016,-0.004976196,0.00035994733,0.019652452,-0.013339911,0.028244834,-0.0312047,0.040949494,-0.04917048,0.06126531,0.020479042,-0.027131077,-0.041969817,0.029236607,0.060251962,0.0235262,0.073628254,-0.03994604,-0.028320305,0.02121607,-0.003511581,0.020069228,-0.018180821,0.04713073,-0.029357767,-0.030873712,-0.009546409,0.03170365,0.01251506,0.019904526,0.054477252,-0.011083576,-0.048557077,0.0350204,0.0023725226,-0.030607808,-0.03186324,-0.0491986,0.020062257,0.0532607,0.013533886,-0.0018318712,-0.035635073,0.021705903,-0.026528953,0.05438145,0.04574049,0.008871037,0.02392321,-0.027420698,-0.010210058,0.050637174,0.039032035,0.03210536,0.00056965864,-0.020583207,0.02900828,-0.008903534,0.0274021,0.041943528,0.018228525,-0.047112055,-0.008461665,-0.04005944,-0.015537152,-0.0033350768,-0.027579462,0.025644993,0.057501987,-0.010772421,-0.02325338,0.0074070785,-0.008253527,-0.028489167,-0.029850204,-0.052188385,-0.040883243,0.05098704,-0.043862976,0.03837657,-0.054745726,0.006833208,-0.024421822,0.047716852,0.020382993,0.009613121,-0.049082283,-0.020274807,0.028043002,0.035786323,0.009830522,-0.013166092,-0.026637737,0.010933763,0.009032885,-0.03936992,0.004467088,-0.0015936793,-0.0147580365,0.0034603216,0.002570019,-0.011200351,0.046603832,-0.036262434,0.0073305233,-0.040317327,-0.082140654,0.021026975,0.08439357,0.056873124,-0.0025974847,0.073401794,0.017307624,0.053478405,0.012005985,0.035151854,-0.042816073,0.031487927,0.0077696946,-0.0019909276,0.014448745,-0.011155084,0.015731465,-0.0053332015,0.015666716,-0.019303009,0.009523113,-0.014978895,-0.008095245,-0.008555303,0.00054830586,-0.014180846,0.057490736,0.008280192,0.035206586,-0.045738183,-0.0684395,-0.037304062,-0.0003827556,0.0029221938,-0.045725957,-0.055366177,0.00820419,-0.019070722,0.028435376,-0.012637022,-0.005191052,-0.047565144,0.0042723627,-0.041557338,0.03579543,-0.050885294,-0.015115132,0.022118345,0.0027847202,0.10748972,0.0030775336,-0.04989947,0.0034564442,0.005200155,-0.038671758,0.017219255,0.018923985,-0.006462729,-0.00063557044,0.057263266,-0.00958068,-0.014350862,-0.0314337,-0.022653665,-0.045116413,-0.008815135,-0.0036338714,-0.019640302,-0.024137914,0.00906296,-0.005943221,-0.011256168,0.027627163,0.06971677,0.0006454485,0.061493456,0.012663537,0.02002011,0.054727644,-0.021275794,0.03658556,0.018778391,0.058932688,0.011367517,-0.028750183,0.037689153,0.009632871,-0.019585986,0.020782596,-0.050897602,0.054737132,-0.049605593,0.02027175,0.005180469,-0.015720911,-0.026375894,0.049282596,0.009531626,0.00699209,-0.056915578,-0.03485379,0.010539319,-0.027573712,-0.04423403,0.030476535,0.0042769527,-0.03236667,0.020687364,-0.0052241352,0.04052773,-0.017820451,-0.044627506,-0.010960797,0.021490805,-0.05642573,-0.032124262,-0.020865055,-0.019971704,-0.043725047,-0.033786736,0.044988144,0.0015487743,-0.06415028,-0.038888894,-0.04489569,0.010413153,-0.017169693,-0.031891637,-0.08634939,0.009802229,0.017034257,-0.009523399,0.083021626,-0.027297834,0.010016567,0.010189179,0.01027823,-0.060835302,0.0035725455,0.011279208,0.020052925,-0.005805761,0.038880218,0.053207334,-0.016251648,-0.06123442,-0.019139158,-0.0022024112,-0.024178157,-0.022274518,0.016496342,0.06718789,-0.030601239,-0.01778409,-0.026537105,-0.04215925,-0.004120554,0.05273816,0.0130299,0.01893726,0.040415235,0.004084478,0.00828545,-0.103474066,-0.046909668,0.020229194,0.050670393,0.038450133,0.04797746,-0.0017051867,0.012497438,-0.008561838,-0.025497396,-0.041367292,-0.046199188,-0.011987053,-0.029550817,0.021189539,0.007251973,-0.0452129,0.0412219,-0.0075785806,0.0148790395,0.08574416,-0.0006688222,-0.03166302,-0.036371797,-0.00671304,0.023059959,-0.002083575,-0.016776295,0.021261781,-0.009818834,-0.00034552216,-0.0027931374,0.0779168,-0.04166604,-0.047363013,0.0050306115,0.04683185,0.016967667,-0.0037802656,0.014263997,-0.066617884,-0.05209423,0.033866372,0.0022620994,0.0035191914,-0.009954584,0.02491133,-0.0241366,-0.08730397,0.011293681,0.027480144,0.0033255375,-0.031548586,0.006240927,0.0042639486,-0.0044470965,-0.02339004,-0.02776924,-0.02000723,-0.020574506,0.025536548,-0.032861207,0.032997437,0.034387548,0.018622348,-0.0746193,0.041836657,0.045818403,0.04689345,0.0062449863,0.0903416,0.0113795055,-0.0038110458,-0.02847781,0.019500263,0.08367528,-0.04414728,-0.02835609,-0.021899223,0.055612996,-0.010221247,-0.042682562,-0.025336485,-0.041600022,-0.01004405,-0.005198134,-0.0035122053,-0.018786533,0.0023856817,-6.32652e-05,-0.04382335,0.015338138,-0.032112386,-0.0069218352,0.04652736,0.051331498,0.0016766862,0.00799795,-0.0014859722,-0.051849723,0.03859839,-0.052079756,-0.03782922,-0.00795703,0.029513814]	Keywords: asynchronous SGD, parallel computing, parameter server, model replicas, training data\nKey Objects: model replicas, parameter server, training data, model parameters\nRefers to Images: None\nHypothetical Questions:\n- What is the main advantage of using an asynchronous variant of SGD like Downpour SGD?\n- Why does the lack of communication between model replicas in Downpour SGD pose a problem?\n- How does Downpour SGD relate to the development of TensorFlow?\n- What does it mean for model parameters to 'diverge' in the context of Downpour SGD?\n---\nSummary:\nDownpour SGD is an asynchronous variant of SGD used in Googles DistBelief framework (a predecessor to TensorFlow) that utilizes multiple model replicas running in parallel on subsets of training data.\nOriginal Text:\n### 5.2 Downpour SGD  \nDownpour SGD is an asynchronous variant of SGD that was used by Dean et al. [6] in their DistBelief framework (the predecessor to TensorFlow) at Google. It runs multiple replicas of a model in parallel on subsets of the training data. These models send their updates to a parameter server, which is split across many machines. Each machine is responsible for storing and updating a fraction of the model's parameters. However, as replicas don't communicate with each other e.g. by sharing weights or updates, their parameters are continuously at risk of diverging, hindering convergence.\nContextualized Text:\nDownpour SGD is an asynchronous variant of SGD that was used in Googles DistBelief framework, which preceded TensorFlow.  It involves running multiple copies (replicas) of a model in parallel on different subsets of the training data. These model replicas send their updates to a parameter server, which is distributed across multiple machines, each responsible for managing a portion of the models parameters.  A key challenge with this approach is that, due to the lack of communication between the replicas, their parameters are at risk of diverging, potentially hindering the overall convergence of the model.	{"tags": ["optimization", "distributed computing", "deep learning", "SGD"], "doc_id": "0bc35fe9-9dbb-438e-a6d7-5cbb0d985dea", "summary": "Downpour SGD is an asynchronous variant of SGD used in Googles DistBelief framework (a predecessor to TensorFlow) that utilizes multiple model replicas running in parallel on subsets of training data.", "doc_type": "text", "entities": ["DistBelief", "TensorFlow", "Google"], "keywords": ["asynchronous SGD", "parallel computing", "parameter server", "model replicas", "training data"], "key_objects": ["model replicas", "parameter server", "training data", "model parameters"], "contextual_text": "Downpour SGD is an asynchronous variant of SGD that was used in Googles DistBelief framework, which preceded TensorFlow.  It involves running multiple copies (replicas) of a model in parallel on different subsets of the training data. These model replicas send their updates to a parameter server, which is distributed across multiple machines, each responsible for managing a portion of the models parameters.  A key challenge with this approach is that, due to the lack of communication between the replicas, their parameters are at risk of diverging, potentially hindering the overall convergence of the model.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "5 Parallelizing and distributing SGD", "h3": "5.2 Downpour SGD"}, "hypothetical_questions": ["What is the main advantage of using an asynchronous variant of SGD like Downpour SGD?", "Why does the lack of communication between model replicas in Downpour SGD pose a problem?", "How does Downpour SGD relate to the development of TensorFlow?", "What does it mean for model parameters to 'diverge' in the context of Downpour SGD?"]}
fcf16df1-a5f5-4e83-a754-b52bb8f91a00	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.025420612,0.03578415,0.022111636,0.025821371,-0.024107723,0.045803487,0.019870909,0.009397856,0.009160705,-0.036016352,-0.020900967,-0.034163028,-0.016626151,-0.013429153,-0.019106228,-0.005414479,-0.016807823,0.11259506,0.04915942,-0.021359175,0.015251978,-0.0120262625,0.0051220437,-0.00523368,-0.017031679,0.043502457,0.022366336,-0.003499084,-0.017740687,-0.026132034,-0.014763636,-0.033089936,0.042135347,-0.0037127815,0.03047708,-0.0010899793,-0.026277535,-0.05676579,0.03796438,-0.03088869,-0.0551429,0.029061517,0.0025023688,-0.019836284,0.0010359494,-0.07453745,-0.044980958,0.005115858,0.0064043854,0.021284988,0.0071217814,0.057029914,-0.037885968,0.011887167,-0.062389795,0.06631248,-0.02402498,0.0014570715,0.021012694,-0.06780507,-0.09528203,-0.025808664,0.0062048486,-0.05057286,-0.009831753,-0.013716935,0.001460717,0.03703952,0.022104353,0.14449288,0.0024139057,0.04791463,0.004837778,-0.08134497,0.10068868,0.01312614,-0.06198153,-0.038965378,-0.043302797,-0.016191006,0.0028463753,0.07911804,-0.05205535,-0.03003911,0.08289286,-0.04877085,-0.080341905,-0.03944712,0.07846841,-0.015975034,-0.048030425,-0.052239317,-0.047040354,0.0704259,-0.0017306942,-0.03411838,-0.0007699649,-0.0141156055,-0.058826074,-0.025889227,0.016752126,0.03491105,0.020672273,0.096223824,0.027748363,-0.004272815,0.014628805,-0.045472015,0.0055398243,-0.0053185597,0.0065534827,0.035862163,-0.03470342,-0.039809827,-0.034470543,-0.021146659,0.0048116394,-0.025893515,0.006212283,-0.004713268,0.032819975,0.000661486,-0.033454627,0.007060338,0.036079753,0.06717821,0.012723522,-0.04388289,-0.047172707,-0.006610205,-0.008025253,0.005612245,-0.03500793,0.016234031,-0.053133428,-0.037149373,0.0101609845,-0.0021394454,-0.0045113494,-0.0130955465,-0.042765833,-0.022675421,-0.081873514,-0.011322134,0.02407905,0.023460772,-0.081172764,0.023952529,-0.020790674,0.0457248,0.026094154,-0.018129447,0.03409999,0.005692794,0.0032881352,-0.013630669,-0.03315187,0.002048138,-0.020425525,-0.009449993,-0.044900097,0.016734706,0.004020734,0.07968816,0.0743478,0.09307789,0.011023588,0.049205475,-0.016200451,0.008909803,-0.052671388,-0.057687167,0.023970978,-0.021630837,-0.0047918307,-0.060172964,-0.0038718686,-0.029429995,-0.014226379,-0.02039865,-0.001974068,0.01971679,-0.011773345,0.00061980495,0.04764061,0.022807574,-0.027937246,0.038462527,0.0167043,-0.005945975,0.012613222,-0.011729196,0.02643241,0.0023869304,0.029707184,0.016513336,0.03398927,0.0058094794,-0.0103052715,-0.06812529,-0.0130143855,0.0031071147,-0.06644568,-0.0042847954,0.046360373,-0.06171543,0.02928595,-0.027786946,-0.016678715,-0.0188579,-0.044233732,0.03277211,0.056369163,0.03480522,-0.022932364,0.059330527,-0.05461139,0.013489084,0.054113366,-0.024274034,-0.057424344,-0.008712839,-0.015526464,0.025500856,-0.012046548,-0.036309965,0.016957486,0.019765325,0.08264073,-0.022998039,-0.040267237,0.032674626,0.0027337368,0.05419162,0.008940991,0.03439153,-0.010846945,0.041238472,0.014706144,0.057002794,0.013906605,0.040261723,0.011407066,0.08805635,-0.030177884,0.009924668,0.016957844,0.022569451,-0.022378698,0.019636633,-0.0537713,0.018205503,0.032479543,-0.0035465062,-0.04719256,0.0009122186,-0.0013266599,0.00306122,0.0113386735,-0.027350903,-0.017245177,-0.0005114931,-0.0087652095,0.033472184,0.017534984,0.021940567,0.012154259,0.024602694,-0.017900286,0.047718644,0.07822899,0.03804582,0.0025860663,-0.013957581,-0.047141615,0.0035726393,-0.0155988205,0.0069391583,-0.009493452,0.008500508,0.0035851614,-0.012728556,0.035196766,0.012072701,0.012484967,0.004929573,-0.008370397,-0.02083325,-0.03318683,0.09012242,-0.00821983,-0.02699405,0.03046591,0.011157506,-0.0014696245,0.036170408,-0.024071304,-0.032529943,-0.0021381017,0.025799096,0.0829751,-0.027352916,0.00076915964,-0.023356803,-0.019705188,0.021865811,-0.057410903,0.02816771,-0.016374463,0.037868924,0.027144136,-0.024055453,-0.033900682,0.024649061,0.0020041596,0.034997117,0.06411468,0.001905913,-0.013668223,-0.010004836,0.019741986,-0.024716409,-0.022057904,0.05019349,-0.005460102,0.04454082,0.040480487,0.00017696944,-0.08295403,0.055973727,-0.05108691,0.007859087,0.07037703,-0.0034654299,0.020418664,0.023380915,-0.0067041274,0.028701536,0.043340947,-0.019704407,0.015722496,-0.026551114,0.029588321,0.0051734746,-0.015986143,-0.0023598126,-0.008711778,0.008352163,-0.015463074,0.01686306,-0.004749485,0.020074558,0.060731906,-0.010852531,0.027821375,-0.009051682,-0.030264981,0.008147498,0.024996324,-0.04260227,0.025653062,-0.042560603,0.05671494,-0.051621217,0.03315437,-0.025881095,-0.01793336,0.0017641055,-0.015772896,-0.037839122,0.024426155,0.03524493,0.042309392,-0.06606664,0.04367477,0.019747313,0.0456012,0.04999122,-0.04369921,-0.055943977,0.0014729283,-0.038270246,-0.010262126,-0.038777705,0.07846561,0.06725278,0.012920906,0.07642672,-0.025424572,0.022718469,-0.021775195,0.04406411,0.03915833,0.017548569,-0.04440847,-0.014270791,0.011227695,0.020754708,0.018299429,0.03694026,0.018542035,0.06836752,-0.05438349,0.06425271,-0.020289585,0.03203497,-0.06162726,0.031083474,0.025053909,0.047230735,0.046457466,-0.06942845,-0.033343997,-0.019204866,0.026626859,0.0026774006,-0.024807695,-0.014706667,-0.046127856,-0.023535525,-0.014671797,0.10952011,0.03964673,0.017525975,0.03185745,-0.026419016,-0.013643509,0.04623161,-0.03789216,-0.045490123,0.012589166,-0.011529558,-0.05048503,0.06404692,0.015071906,-0.039097603,-0.02970623,0.057007167,0.012790391,0.014565111,0.008404714,-0.014441872,0.0055494444,-0.0046294783,0.0015460486,0.01499933,-0.004917759,-0.014632996,7.0617345e-05,0.00035904636,0.020394431,0.04289193,-0.0007718747,0.064115174,0.039557457,0.002281088,-0.026240448,-0.01890774,-0.011026611,0.035939317,-0.02513926,0.04455106,0.02315032,0.015039596,0.019133259,0.013046204,-0.008717887,-0.05657817,-0.009349345,-0.025062684,-0.044399217,0.012530776,-0.052297246,0.035276763,-0.03938974,-0.0023859679,0.005380503,0.030457005,0.0725692,0.010412413,-0.0712515,0.015284348,0.04207887,0.011368319,0.0064795394,-0.06560258,-0.041186444,0.015485201,-0.024021918,-0.017466214,-0.011622906,0.03741024,-0.06504744,0.043379273,0.0019544177,-0.026162053,0.04681679,-0.044763427,0.019472854,-0.023068702,-0.031125305,-0.0071515082,0.04741039,0.05070371,0.033816833,0.009673136,0.033861607,0.0358933,-0.037878882,0.04139155,0.018187147,-0.016655384,0.0012415369,-0.009801049,0.005135156,0.03339987,-0.03617957,-0.024609337,0.014001727,-0.05796651,0.050961874,0.029456677,-0.0014791474,0.009370024,0.016364481,-0.0078599155,-0.01563035,0.055207614,0.06385426,-0.025246013,-0.013478528,0.02006338,0.024209546,-0.0027935787,-0.089926444,-0.06680583,-0.0022984988,-0.01143465,0.010295532,-0.014179263,-0.01599469,-0.023621498,0.002233067,-0.01297645,0.05395553,-0.036560632,0.016490085,0.029347122,-0.045677233,0.082999505,-0.0052241436,-0.01964649,0.025561823,-0.0027626455,-0.06714249,0.03168154,0.0049901274,-0.008630362,0.017030165,0.02675531,-0.023010738,0.009168465,-0.044247545,-0.034221623,0.028112782,-0.011789582,0.0019462898,-0.014058027,-0.029853165,-0.0004154528,0.03474123,0.00966996,-0.0022818504,0.024421873,-0.008099333,0.1117331,-0.014806202,-0.030702613,-0.021063155,0.015910778,0.027457591,0.015506794,0.0709512,0.021484451,0.0003944116,0.041134093,0.0054214713,-0.01509756,0.0077978936,-0.00880236,0.008484936,-0.029224563,-0.011515436,0.0065757716,-0.009104621,0.005929817,0.06979525,-0.014376957,0.03454831,-0.031303413,-0.06820665,0.011811744,-0.012100525,-0.065179504,0.02713584,-0.0229299,0.024681982,-0.005168686,0.009558041,0.04594674,-0.046178143,-0.059089188,-0.0023619013,-0.03908159,-0.05548169,-0.02990551,0.012394651,-0.018631935,0.00038993437,-0.06461228,0.044108886,-0.05548414,-0.064755045,-0.014414399,-0.02401143,-0.019371651,-0.080491245,-0.016700238,-0.018435847,0.031313587,-0.024197526,0.013218429,0.05076245,0.008237389,-0.014763338,0.0008941446,0.033991173,-0.07262897,-0.01395204,0.010519849,0.01958068,-0.0071944525,-0.03325518,0.039489884,-0.02547124,-0.023611384,-0.00030044196,-0.055759083,0.046529107,-0.029750148,0.025116857,0.041156683,-0.01709411,-0.003970131,-0.078564905,-0.014044714,0.039506827,-0.0017858521,0.035907973,-0.013879086,0.027334407,0.03186022,0.019085212,-0.03601551,-0.064346656,0.057408113,0.027344184,0.005354405,0.027633939,0.0015531096,-0.026338072,0.0052028117,-0.0018859449,-0.05237327,0.004482184,0.02640778,-0.019342082,0.0050367354,-0.0023653691,-0.05543681,0.04645651,0.011270486,-0.011898674,0.0671395,-0.030655533,-0.008092176,-0.0655947,-0.061571043,0.005721786,-0.016159719,0.0026612154,-0.041284315,0.010262187,-0.015520401,0.021491919,0.0761425,0.02456251,-0.08683588,-0.015086167,0.009481631,-0.030565983,0.030003062,0.007969052,-0.03097905,-0.08942505,0.004633563,0.018453462,-0.014154167,-0.027433587,-0.023858644,-0.03517323,-0.031007878,-0.017238641,0.017682862,0.0074013676,0.041261274,0.011788196,0.034034528,-0.0002193409,0.04491359,-0.03444095,-0.031802304,0.014204338,0.050866842,-0.04020678,0.0071748532,0.060510818,0.04399393,-0.038782824,0.04962112,0.043878265,0.018367426,0.040709753,0.034684587,0.028200135,0.005632861,-0.048813585,-0.026214324,0.043780867,-0.085362345,0.009107776,-0.023804516,0.07468669,-0.010878449,-0.046787348,-0.028306322,-0.06166064,-0.00405353,0.07001719,-0.027364664,-0.003117258,-0.022560552,0.0010107222,-0.009816349,-0.005734497,-0.03224516,-0.04816818,0.050755464,0.01972979,-0.0074942536,-0.00033341875,-0.07349336,-0.036195375,0.051761825,-0.04408248,0.014215245,-0.015938597,-0.0021098806]	Keywords: delay-tolerant algorithms, AdaGrad, parallel setting, update delays, past gradients\nKey Objects: algorithms, gradients, update delays\nRefers to Images: None\nHypothetical Questions:\n- How do delay-tolerant algorithms improve upon AdaGrad in a parallel setting?\n- What are the challenges of update delays in distributed SGD?\n- How do these algorithms adapt to the variations in update timing?\n---\nSummary:\nMcMahan and Streeter extended AdaGrad for parallel settings by creating delay-tolerant algorithms that adjust to both past gradients and update delays, and these algorithms have demonstrated practical effectiveness.\nOriginal Text:\n### 5.3 Delay-tolerant Algorithms for SGD  \nMcMahan and Streeter [12] extend AdaGrad to the parallel setting by developing delay-tolerant algorithms that not only adapt to past gradients, but also to the update delays. This has been shown to work well in practice.\nContextualized Text:\nIn parallel stochastic gradient descent (SGD) settings, delay-tolerant algorithms, developed by McMahan and Streeter [12], improve upon AdaGrad by accounting for both past gradients and the delays inherent in distributed updates. These algorithms have proven effective in practical applications.	{"tags": ["optimization", "distributed", "SGD"], "doc_id": "fcf16df1-a5f5-4e83-a754-b52bb8f91a00", "summary": "McMahan and Streeter extended AdaGrad for parallel settings by creating delay-tolerant algorithms that adjust to both past gradients and update delays, and these algorithms have demonstrated practical effectiveness.", "doc_type": "text", "entities": ["AdaGrad"], "keywords": ["delay-tolerant algorithms", "AdaGrad", "parallel setting", "update delays", "past gradients"], "key_objects": ["algorithms", "gradients", "update delays"], "contextual_text": "In parallel stochastic gradient descent (SGD) settings, delay-tolerant algorithms, developed by McMahan and Streeter [12], improve upon AdaGrad by accounting for both past gradients and the delays inherent in distributed updates. These algorithms have proven effective in practical applications.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "5 Parallelizing and distributing SGD", "h3": "5.3 Delay-tolerant Algorithms for SGD"}, "hypothetical_questions": ["How do delay-tolerant algorithms improve upon AdaGrad in a parallel setting?", "What are the challenges of update delays in distributed SGD?", "How do these algorithms adapt to the variations in update timing?"]}
69561b40-0711-442b-8bc8-cd9cc3b737cb	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.044316642,-0.019650113,0.0036115306,0.028634075,0.04950234,0.08187719,-0.025898596,0.044930577,0.0010106884,-0.0030484984,0.0018141941,0.014648196,-0.012047505,0.002978761,-0.02459006,0.041668102,0.026249286,0.09363312,0.048476867,-0.035386022,0.031914804,0.039876264,-0.05112187,0.004436999,-0.0038475683,0.04125796,0.038453285,-0.053721186,0.00016560605,0.015232126,0.0039985813,-0.011203685,0.031346146,-0.004866999,0.03749516,-0.020888694,-0.015109878,-0.000536389,-0.013167128,-0.03969085,-0.00061953673,0.06101638,0.0040220856,-0.023783706,-0.04574888,-0.005547521,-0.100349136,-0.012237643,-0.0063061863,-0.01745465,-0.030302586,0.034937765,-0.04572035,0.02586076,-0.042426515,-0.03267685,-0.004971551,-0.033153336,0.0058091125,-0.0668219,-0.07831181,0.03578986,-0.031088691,-0.0002895259,0.0017158615,-0.054952417,-0.008949114,0.012931203,0.008337492,0.17844355,0.0026340112,-0.0066331946,-0.01324868,-0.06164735,0.0660198,0.054320153,-0.043689415,0.0015583747,-0.05127434,-0.0066296053,0.023414936,0.0125863,-0.03960487,-0.026450507,0.070312895,-0.011458023,-0.046569552,-0.030773047,0.075845875,-0.055973552,-0.04200667,0.014772663,0.025430558,0.046441965,-0.022384357,-0.03420385,-0.00018416563,-0.025349626,0.019418767,-0.014828587,0.0416291,-0.018589675,0.06510881,0.08829384,0.03415583,-0.007935152,-0.011267411,-0.015895652,0.032021362,0.013143764,-0.02229146,0.025481364,-0.037800167,-0.01562377,-0.064278625,-0.031656466,-0.07167653,-0.011224058,-0.0021707627,-0.01716306,0.0070168977,-0.045547575,0.008047884,-0.008207265,0.06453371,0.070224196,0.024240905,0.0044187214,-0.015363536,-0.018732145,0.045749385,-0.0031452933,-0.016507123,0.056654878,0.00026754246,0.021024482,0.050392807,0.00611446,0.010699316,-0.024193343,0.015558044,-0.011937196,-0.037490804,0.039697845,-0.008525643,0.056708124,-0.05899234,0.049962908,-0.025149958,0.0668103,0.022570712,0.029868584,0.053259555,-0.020071426,0.030778637,-0.018866546,-0.020788759,-0.062143806,-0.02400095,-0.02613438,-0.046275232,-0.06464217,0.025366018,0.07415107,0.06669675,0.11544218,0.010645127,0.019096827,0.0027088742,0.030701889,-0.014283939,0.0010747446,0.014645005,-0.021133477,0.027011128,0.015765024,0.0049428353,0.0033160006,-0.032218467,-0.023203116,-0.035433922,0.059973825,0.009984169,0.025693806,0.00085484824,-0.0029823647,0.0026052538,0.10566521,-0.042615045,-0.029731255,0.0040413747,-0.028170567,-0.040325075,0.016995735,0.040057205,0.027587919,0.020867862,-0.030613024,-0.022106268,-0.03517739,0.014019125,-0.0029162636,-0.010297707,0.0034520426,0.008620622,-0.023535574,0.0160999,-0.04171306,0.011987293,0.054050148,-0.018923642,-0.008324917,0.0014412144,0.06296576,0.027308784,-0.0028967184,-0.0065351035,0.035841703,0.043400973,-0.009995322,-0.05991141,-0.019496398,-0.008833914,0.036880247,-0.058101848,-0.04940621,-0.02862567,0.0030743766,0.018566415,-0.033175193,0.0018644679,0.032626845,-0.038789943,0.011319939,0.00039339214,0.009082136,0.011502052,-0.045610774,0.008757382,0.025858048,0.05144911,0.0031884424,-0.044691805,0.028621873,-0.033649623,0.024904646,0.032865427,0.07196937,0.011490425,0.072340004,-0.012530125,0.054602794,0.050918166,-0.07539182,-0.006596592,-0.025472196,0.0106998235,0.0075741364,0.035420608,0.010117741,0.027295083,0.024460424,-0.0063071027,0.03265813,0.03623701,0.0138505865,0.018257426,0.031239692,-0.031579487,0.012486715,0.0027395939,0.06130447,0.032603953,0.027087986,-0.02315788,0.06696482,-0.00024028023,0.0022923376,0.09830891,0.011891152,-0.015435696,-0.025061926,0.020073783,-0.0048880987,-0.017283587,0.00609503,0.01803647,-0.015555239,-0.039483566,0.028028116,-0.026236732,-0.03758634,-0.0023679805,0.019496845,0.014579153,0.04587154,-0.025917154,-0.08805905,-0.0047644237,0.034087088,0.062409803,0.014378456,0.039015573,0.046114057,0.022924423,0.027336307,0.001503579,0.021571353,-0.002874601,0.021215139,0.0628741,-0.0012752227,-0.01831134,0.007955808,0.024092322,0.04004212,0.080263115,-0.030131364,-0.020003611,0.032520868,0.04524501,0.034143113,-0.0023620827,0.031995367,-0.009558803,-0.010199746,0.02553368,-0.041108713,-0.092683196,0.046804145,-0.068924226,0.017220864,0.04089892,-0.00018722017,-0.016512837,-0.04217833,0.016824953,0.05009369,0.050664414,0.050304025,0.018238457,-0.032572813,0.029475726,-0.011868296,-0.011478804,0.007813017,0.030303193,-0.022832252,0.044196304,0.013120919,-0.005975749,0.016834252,0.0529465,-0.0025897264,0.011249861,-0.043213777,0.03553844,0.028577246,0.014829298,-0.004654123,0.0031875875,-0.050719105,0.04364681,-0.0411094,0.04838389,-0.02229062,-0.06968882,0.014079473,0.010734196,0.0029170457,0.009822138,-0.04882248,0.03463364,-0.02384495,0.014468525,0.030294355,0.03309519,0.024923552,-0.014175149,-0.03792095,-0.009091529,0.015599069,-0.09152998,-0.055983037,0.029374132,0.005751394,-0.01582975,0.07917959,0.013890007,0.0045803403,-0.0650382,0.0085987365,0.022333534,0.03687838,-0.03206473,-0.008363729,0.021476252,0.059321113,-0.039129164,0.0027334793,0.0007898785,-0.011143312,-0.063056774,0.056774974,-0.018939933,-0.02319407,-0.0863688,0.040209904,0.05744039,0.03543755,0.05971511,-0.056616824,-0.033277206,-0.0010764186,-0.030555975,0.008047872,-0.0177055,-0.0015790737,-0.023651937,0.009160833,0.0075421846,0.033442914,0.06662638,0.050181225,0.03384233,-0.017146343,-0.03311797,-0.0028662614,-0.0034270536,-0.052830614,-0.032721568,-0.052268554,-0.0018213413,-0.0052456073,0.023101285,0.006690239,-0.056363907,0.049065687,-0.06603038,0.034356408,0.04139266,0.035942536,0.028508423,0.013203965,0.034583114,-0.012958324,0.046838876,-0.037638657,0.02053832,-0.0007378716,-0.022734847,-0.020319715,0.033692524,0.035458125,0.037645042,-0.029764757,0.005671041,-0.020914787,-0.012471068,0.0045095026,-0.04131326,0.030507775,0.031311825,-0.04681612,-0.016620368,-0.018318038,0.02659682,-0.03393024,-0.05010084,-0.04272796,-0.022413213,0.0093675265,-0.053620245,0.09164919,-0.048967704,0.00074629264,-0.0017506363,0.058562256,0.01540897,0.035248816,-0.07412979,-0.040431786,0.04685059,0.018244928,-0.0052632843,-0.0023971659,-0.020343078,0.014093808,-0.026979094,-0.014250976,0.013192496,0.027685313,1.899093e-05,-0.008412987,0.008660089,-0.029808749,0.0009937927,0.0059178583,0.02676947,-0.008520204,-0.039991677,-0.027692148,0.07622578,0.031064708,0.0108624445,0.013580406,0.004031302,0.04485089,-0.016253162,0.023296148,0.0025323753,0.031970266,0.0032141402,-0.0117045855,-0.006302501,-0.0050214864,0.028659765,-0.023419058,-0.006300114,-0.001963375,-0.005006578,-0.035867047,0.010807878,0.044276502,0.022892471,0.012696172,0.014770038,0.041202925,0.060119756,-0.057439543,0.046640202,-0.017720774,-0.018975321,-0.005885022,-0.0510563,-0.0026364014,-0.0030639681,-0.022317866,0.012540752,-0.0026956482,-0.00859082,-0.0778791,0.0033123856,-0.03191936,0.038316745,-0.08981241,-0.044188708,0.024500804,-0.03355924,0.08692749,0.0017452921,-0.0075761057,0.008289872,0.021166777,-0.008669644,0.043818075,0.09213719,0.01925009,-0.038203064,0.049583193,0.01493619,0.024265612,-0.03426094,-0.052643746,-0.04563587,0.013956579,-0.007527587,-0.039254237,-0.022181058,0.019540597,0.0105859805,0.018376648,-6.3072905e-05,0.02923719,-0.040327195,0.061016552,0.044680897,-0.006681982,-0.00979747,-0.014948981,0.020425493,0.029107288,0.082471415,0.029749768,-0.0143741565,0.028806357,0.019066289,-0.024913754,0.044730555,-0.017310522,0.035229634,-0.030847276,0.013791092,0.02073541,-0.025221437,-0.015574363,0.048958484,-0.014447443,-0.022466553,-0.0353335,-0.052779038,-0.014000806,-0.01356901,-0.048310217,0.007810022,0.0017175488,-0.033450365,0.071773425,-0.0152288675,0.07217646,-0.065784924,0.014897253,-0.044212125,0.0589865,-0.06275906,-0.039742094,0.030036371,0.003409778,-0.030644076,-0.038880955,0.059802454,0.037995663,-0.028119631,0.054753076,-0.017677044,-0.016807796,-0.013238364,-0.027146818,-0.081899196,0.015224955,0.009705401,-0.020664223,0.09422795,-0.008755478,-0.051576287,0.037777226,0.0004081435,-0.08430167,-0.004236572,0.018733032,0.014239414,-0.005877868,0.028524082,0.04388946,0.002288831,-0.061491564,-0.013687187,-0.047500584,-4.8005313e-05,0.014842184,0.031057494,0.05048447,0.0017113247,-0.05015972,0.0070786406,-0.021557912,-0.034173306,0.042903412,0.028865524,0.011319063,0.005533332,0.017498292,-0.028039865,-0.074202724,-0.021494588,0.0023893276,0.029469537,0.018344043,0.021344336,0.014711604,0.005215603,-0.0011682656,-0.037026923,-0.01571538,-0.014352133,0.020649102,-0.0017125282,0.013386334,0.0074475617,-0.016685843,0.04702931,-0.010445926,0.02603306,0.06753843,0.048434246,-0.014420983,-0.023445971,-0.024161141,0.015686315,0.015424209,0.0022446217,0.024190785,-0.002357964,0.035975255,-0.048119374,0.040972054,-0.019559596,-0.038793113,-0.0077944435,0.052959643,0.024227161,0.009807688,0.03755517,0.009960971,-0.061554916,0.065284185,-0.03405753,-0.021654304,-0.0015627115,0.012275902,-0.0015227384,-0.080056526,-0.019151585,-0.031729233,0.016085962,-0.024565846,-0.0024048248,0.028446365,-0.021659428,-0.0052464413,-0.026946729,-0.055731256,0.0021319601,0.005015131,-0.032692872,0.0401142,0.0009828189,0.0023517776,-0.058195885,-0.0019658785,0.03689121,0.024146985,-0.038790002,0.004963928,-0.03275999,-0.006302101,-0.040353302,0.06786675,0.06924297,-0.011225998,-0.036053505,-0.013031217,0.03716122,0.0123605635,-0.022972126,0.01402412,-0.054741,-0.008637553,0.022516267,0.03929745,0.026759854,0.012852328,-0.03929892,-0.040499635,-0.042668425,-0.0017198652,0.036801774,0.047377285,0.023609357,0.043791812,-0.07386345,0.024102712,-0.061626576,0.043524988,-0.03125703,-0.036965586,-0.0090003805,0.00043187352]	Keywords: machine learning models, TensorFlow, large-scale deployment, framework\nKey Objects: TensorFlow, framework, machine learning models\nRefers to Images: None\nHypothetical Questions:\n- What is DistBelief and why is it relevant to TensorFlow?\n- How does TensorFlows distributed architecture improve performance?\n- What are Send/Receive node pairs and how do they facilitate communication in TensorFlow's distributed version?\n---\nSummary:\nTensorFlow is Google's open-sourced framework, built upon DistBelief, used for implementing and deploying large-scale machine learning models.\nOriginal Text:\n### 5.4 TensorFlow  \nTensorFlow 14 [1] is Google's recently open-sourced framework for the implementation and deployment of large-scale machine learning models. It is based on their experience with DistBelief and is already used internally to perform computations on a large range of mobile devices as well as on large-scale distributed systems. The distributed version, which was released in April 2016 15 relies on a computation graph that is split into a subgraph for every device, while communication takes place using Send/Receive node pairs.\nContextualized Text:\nTensorFlow is Google's recently open-sourced framework for the implementation and deployment of large-scale machine learning models. It's based on Google's previous work with DistBelief and is used internally for computations across mobile devices and large distributed systems.  A distributed version, released in April 2016, utilizes a computation graph divided into subgraphs for each device, with communication handled via Send/Receive node pairs.	{"tags": ["deep-learning", "framework", "TensorFlow"], "doc_id": "69561b40-0711-442b-8bc8-cd9cc3b737cb", "summary": "TensorFlow is Google's open-sourced framework, built upon DistBelief, used for implementing and deploying large-scale machine learning models.", "doc_type": "text", "entities": ["Google", "DistBelief"], "keywords": ["machine learning models", "TensorFlow", "large-scale deployment", "framework"], "key_objects": ["TensorFlow", "framework", "machine learning models"], "contextual_text": "TensorFlow is Google's recently open-sourced framework for the implementation and deployment of large-scale machine learning models. It's based on Google's previous work with DistBelief and is used internally for computations across mobile devices and large distributed systems.  A distributed version, released in April 2016, utilizes a computation graph divided into subgraphs for each device, with communication handled via Send/Receive node pairs.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "5 Parallelizing and distributing SGD", "h3": "5.4 TensorFlow"}, "hypothetical_questions": ["What is DistBelief and why is it relevant to TensorFlow?", "How does TensorFlows distributed architecture improve performance?", "What are Send/Receive node pairs and how do they facilitate communication in TensorFlow's distributed version?"]}
94a2f5d5-d379-4ab5-aa00-9d89bda364e3	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.022793721,-0.011166461,0.00985027,0.038801476,-0.021389132,0.061879147,-0.010441947,0.05890047,0.067543015,-0.05938479,0.01097221,-0.01590849,-0.003445361,-0.011256316,-0.026211398,0.036518972,0.020989029,0.034525834,-0.024175141,-0.04615092,0.035999402,0.04225889,0.007870494,0.0004195398,-0.0033789105,0.028497139,0.013520096,-0.0063855373,-0.03320188,0.022958044,-0.005497724,-0.06485569,0.029665878,0.0035816403,0.0010260395,0.019819392,0.019473338,-0.06774786,-0.0005948887,-0.020157648,-0.0254245,0.042576507,-0.055642266,-0.0351294,-0.043312322,-0.060712226,-0.11566843,-0.0365789,0.014254085,-0.020769367,-0.02515379,0.01805931,-0.02660606,-0.03847098,0.020425092,-0.026021598,0.0063470956,-0.04104457,-0.005844917,-0.10937134,0.020073285,0.032949902,-0.009311821,0.0021796676,-0.015549742,-0.004572403,0.020564904,0.023567118,0.020878116,0.10314637,-0.047262385,-0.012867353,-0.017247016,-0.028988693,0.13223553,0.08199464,-0.0055673397,0.028277883,-0.051831797,-0.024181502,-0.0045920787,0.082499295,-0.058767788,-0.06992207,0.0898101,-0.0069677993,-0.015404935,-0.017004386,0.08853345,-0.051521864,-0.033082087,0.027789447,-0.0152134625,0.047825716,-0.020658022,-0.089032635,0.001419813,-0.012844357,0.020959912,-0.0636859,0.037856624,-0.008460127,0.05281231,0.096755885,0.031378094,-0.007747186,-0.019912776,-0.029321939,0.028254416,0.0060352627,-0.013157816,0.013062051,-0.011531951,-0.0021975248,0.0024141571,0.065560274,-0.00813914,0.028613012,0.015848288,-0.01879935,0.03186459,0.00758615,0.0016544211,0.015827546,0.06063451,0.021575065,0.010996779,-0.006581897,-0.020635944,0.013042135,0.04610902,0.0068065193,-0.04555496,0.025138257,-0.020087807,0.0033475033,0.098406374,-0.013579841,-0.005435845,0.009681266,0.018351372,-0.05447654,-0.031465184,-0.006666795,-0.06953553,0.055767026,-0.08201028,0.024505591,-0.038013674,0.05765899,0.027894292,-0.021166721,0.013626354,-0.014651436,-0.03487035,0.022072544,0.0060009295,-0.059149344,-0.009292691,-0.031344768,-0.028174542,-0.051263563,-0.027090045,0.033997085,0.029482989,0.06806222,0.008244575,0.00873105,0.0029644736,0.014122944,-0.03764295,0.014520545,-0.035123084,-0.0058983336,0.014915885,0.03372013,0.01177102,0.009740971,0.01535195,-0.018334951,0.017788973,0.0064037424,0.011864488,0.033868644,-0.0516853,0.03215281,-0.02308098,0.06311744,-0.021107985,-0.0076934993,0.008263787,-0.01695564,0.00057643204,0.0018691904,0.011798573,0.040384352,0.06062545,-0.055489022,-0.020842623,-0.018261597,-0.041764703,-0.03725045,-0.032142825,-0.024925932,0.032619175,-0.0222847,0.034506977,-0.0012540981,-0.0014534685,0.038876392,0.021672588,-0.05104866,-0.041760452,0.028871534,0.01631593,-0.014529277,-0.025516119,0.020995902,-0.008740416,-0.012474197,-0.04542704,-0.009699815,-0.035423048,0.03983201,-0.012711455,0.021406127,-0.050735228,0.03364836,-0.013071723,-0.042923402,0.023681574,0.006210041,-0.05541602,0.014025088,0.0040989104,0.018662231,0.026481247,-0.013243678,-0.034490682,0.03558676,0.01303123,0.048117064,-0.0005803561,0.07998673,0.011496383,0.015813971,-0.0018615491,0.06303369,-0.011574551,0.028735109,-0.028239824,0.053003058,0.038996045,0.012489579,0.0030065728,-0.07659422,0.0131362835,0.02945761,-0.006752676,-0.008922356,-0.022345522,-0.026800245,0.005010812,0.038399853,0.012785589,0.016773112,0.025139408,0.056349747,-0.045826852,-0.013380175,-0.015108704,0.060058802,0.029531548,-0.03158106,-0.036525723,0.006127156,0.020196972,-0.021582494,0.009930759,-0.03508123,-0.011830559,-0.0461251,0.0008993944,0.00088090467,0.00476154,-0.0043880492,0.020338178,-0.018002091,-0.056575622,-0.0017768093,0.0016499864,-0.041999478,-0.058917116,-0.03080518,0.0031306315,0.0011882541,0.007532824,-0.03640844,0.025105301,-0.00026296158,0.12398188,-0.0005546201,-0.048198543,0.01891759,-0.0017195484,0.038671542,-0.06327681,0.015101677,0.045783833,-0.033241108,0.021289267,-0.024336895,-0.027106915,0.0064057764,0.048621528,0.06890589,0.054698665,-0.034682345,-0.043487202,9.190424e-05,0.03078905,0.021385668,0.015523714,0.043762248,0.05328502,-0.009331442,0.011207131,-0.013163544,-0.053863242,0.039080963,-0.09641313,0.078830734,0.04693293,-0.0064848387,0.007992619,0.015217264,-0.006075231,0.043950893,-0.024880374,-0.0069359774,-0.0362019,-0.06685168,0.008267073,-0.0101340655,-0.010777094,0.043999583,-0.009901118,0.007139491,0.036307387,0.017389316,0.008231602,0.015361893,0.048007935,0.008323764,-0.011136619,-0.054195922,-0.017953524,0.054268084,0.026887264,-0.005399243,0.063440114,-0.05477769,0.08646671,-0.054275066,0.040982455,-0.011780452,-0.058063462,0.028813912,0.04108955,-0.045587637,-0.029067436,-0.025745586,0.063329436,-0.027115954,-0.065625206,0.02072286,0.020227086,0.008144165,-0.028661402,-0.0032938037,0.0011749057,-0.008543803,-0.04419534,-0.050257146,0.018442554,0.0030880575,0.010235257,0.08990534,-0.015742175,0.047742147,-0.068500355,0.006876711,0.022394914,0.024870573,-0.043886274,-0.015471345,-0.002273676,0.051097915,0.041148696,-0.0398518,-0.023141306,0.053547505,-0.02644315,0.024672773,0.06858514,0.03959187,-0.051507186,0.044939574,0.01993099,0.02443849,0.036066465,-0.007863742,0.0041309777,-0.02159923,0.018644407,-0.00912008,0.0066661867,0.039898314,-0.03930269,-0.023957727,-0.025805956,0.038528368,-0.0002371172,0.040589057,0.03554724,-0.0122121535,-0.017842237,0.016802642,-0.022701498,-0.048350703,0.0482589,-0.024508854,-0.006286468,0.0478936,-0.023356115,0.0033723516,-0.045405496,0.08175512,-0.052758884,-0.010316443,0.08029,-0.016762564,0.019012386,-0.017272653,-0.029677432,-0.035913985,0.012551894,0.002267883,-0.03227039,-0.008226254,0.033278298,0.019537125,0.02440971,-0.020611228,0.04085684,0.03359355,-0.00015349688,-0.046830546,-0.0068523604,0.0024260208,0.0035170577,0.040884245,0.018273693,-0.015077712,0.017431865,-0.0101328995,0.014580338,-0.046491828,-0.027011132,0.040249497,0.0027613349,-0.0023627947,-0.057740647,0.0014589379,0.014236305,-0.02286411,-0.03237406,0.051781897,0.030191299,-0.0024735837,-0.07103644,-0.059910744,0.023601979,0.007637315,0.0021467484,0.013240921,-0.019694138,-0.06148693,-0.020480836,0.014911892,0.016300058,0.04917753,0.0026204048,0.0342852,0.039016176,-0.065940596,-0.034369703,0.028517934,0.004644683,-0.047981918,-0.014322531,-0.028611826,0.079851665,0.0565816,-0.04094711,0.00651018,-0.016793562,-0.0024708437,-0.026426416,0.09366491,0.016408391,-0.037953123,0.012243472,0.010524675,-0.087735176,0.035112128,0.0049787913,0.004675806,-0.029046971,0.040892635,-0.0064314194,0.034993995,0.045616824,-0.0041348827,0.020972092,-0.040248904,0.020227354,0.009790737,0.05633236,-0.07550481,-0.010840882,-0.05148302,-0.025985043,0.01461807,-0.041941363,-0.06294656,0.021096325,-0.054625347,0.021317191,0.023890862,0.0032516923,-0.015215304,-0.030099778,0.0064327414,0.06813611,-0.025896113,0.012574793,0.0002638767,0.010195087,0.060686592,-0.005957881,0.015300373,-0.012799399,-0.013437919,0.00092878065,0.02180007,0.088752925,0.063117296,-0.011560827,0.039346244,-0.009960134,0.021645285,-0.022855924,-0.05595725,-0.055368304,0.013839703,-0.007567632,0.013425182,-0.031687785,0.02709209,-0.019084025,0.0002512009,0.010398812,0.04464691,0.04160954,0.034151856,0.038018055,-0.0045470702,0.002407632,-0.013098492,0.040753834,0.04269747,-0.009749625,-0.000199449,-0.0070538423,0.045814514,0.011641319,-0.05108107,-0.021714387,0.025887134,-0.00623801,-0.041123684,-0.021974288,-0.017368613,0.013100593,-0.022254668,-0.002144443,0.07871704,0.018231332,-0.022345694,-0.0033586917,-0.0020354,-0.006623753,-0.009909644,-0.0017368027,-0.015067505,-0.008863744,0.021448486,-0.034594502,0.05884442,-0.084239006,-0.0018285039,-0.03649651,0.05235719,0.023843963,-0.023212949,-0.022948984,-0.0029550048,-0.02435417,-0.023740439,0.0969394,-0.008344332,0.00060041406,0.008130426,0.00087477814,-0.013424745,0.01012943,0.0048783235,-0.025532162,-0.021315642,0.014578481,-0.021191407,0.047985803,-0.035813645,0.018865827,0.045288265,-0.008893207,-0.031510502,-0.0062432312,0.01192445,0.032586806,0.02137946,-0.006206332,0.024553314,0.050783165,-0.06282864,-0.019169006,-0.072012134,-0.0045511522,-0.023676064,0.055921346,0.05200673,-0.052592732,-0.051512785,-0.0073209135,0.0022805105,-0.056991935,0.039037537,0.03299598,0.06938567,0.059449438,0.036984764,0.014601164,-0.07758937,-0.028724123,-0.11740951,0.034576274,-0.042736303,0.046917416,0.024721088,-0.012716964,0.0010293829,0.005111669,0.019315401,-0.012603703,0.01971203,0.024451707,-0.042008545,0.0043990216,-0.019725503,0.017892286,0.016565735,-0.011757522,0.044898357,0.034801293,-0.068472795,-0.04990502,-0.054412298,0.007263315,0.037668146,0.013949477,0.004085827,0.047998626,-0.009595774,-0.021408668,0.063806705,-0.015404196,-0.030848974,-0.04249788,0.04137308,0.034881752,0.014644843,-0.03340025,-0.048313104,-0.08325329,0.009984657,-0.0052604545,-0.058370743,0.024913445,-0.045935493,-0.023292912,0.023168553,0.0042614033,-0.008368757,0.030596497,-0.00059516414,0.038426362,0.035602707,0.0050475397,0.0049642767,-0.040411916,-0.022750337,-0.043759648,-0.01761224,0.000728664,0.035687268,-0.0011701325,0.01438167,-0.025672482,-0.016286915,0.023660362,-0.010142929,0.0014346277,0.07427339,-0.045327917,0.028663214,-0.029359305,-0.012849358,0.0473779,0.015816318,0.0006054673,0.01387643,0.038567413,0.004440489,0.03697327,0.0034526654,-0.025795853,-0.031446073,0.017005393,-0.015583809,-0.019216018,0.036517452,-0.019707832,-0.046989188,-0.049402658,-0.031694517,0.027647715,0.033605248,0.027462088,0.01816725,-0.07616264,0.020717928,-0.006134899,0.030086353,-0.0029387583,0.029304123,0.011358226,0.052842658]	Keywords: self-attention, masked self-attention, cross-attention, queries, keys, values\nKey Objects: Attention Mechanisms, Queries, Keys, Values\nRefers to Images: ./images/a-survey-to-transformers/image_1.png\nHypothetical Questions:\n- What is the purpose of masking self-attention in the decoder?\n- How does cross-attention facilitate communication between the encoder and decoder?\n- What is the significance of using Queries, Keys and Values in the attention mechanism?\n---\nSummary:\nThe vanilla Transformer utilizes three distinct types of attention mechanisms: self-attention (in the encoder), masked self-attention (in the decoder), and cross-attention (between encoder and decoder).\nOriginal Text:\nFig. 1. Overview of vanilla Transformer architecture  \n  \nIn Transformer, there are three types of attention in terms of the source of queries and key-value pairs:  \n-  Self-attention . In Transformer encoder, we set Q = K = V = X in Eq. (2), where X is the outputs of the previous layer.\n-  Masked Self-attention . In the Transformer decoder, the self-attention is restricted such that queries at each position can only attend to all key-value pairs up to and including that position. To enable parallel training, this is typically done by applying a mask function to the unnormalized attention matrix A = exp(  D k $\\_{T}$) , where the illegal positions are masked out by setting $\\_{ij}$ = - if i\n-  Cross-attention . The queries are projected from the outputs of the previous (decoder) layer, whereas the keys and values are projected using the outputs of the encoder.\nContextualized Text:\nThe vanilla Transformer architecture employs several attention mechanisms to process sequential data. Specifically, there are three types: self-attention, which is used in the encoder; masked self-attention, utilized in the decoder to prevent attending to future positions; and cross-attention, used to connect the encoder and decoder.	{"tags": ["architecture", "attention", "NLP", "transformer"], "doc_id": "94a2f5d5-d379-4ab5-aa00-9d89bda364e3", "summary": "The vanilla Transformer utilizes three distinct types of attention mechanisms: self-attention (in the encoder), masked self-attention (in the decoder), and cross-attention (between encoder and decoder).", "doc_type": "text", "entities": ["Transformer"], "keywords": ["self-attention", "masked self-attention", "cross-attention", "queries", "keys", "values"], "key_objects": ["Attention Mechanisms", "Queries", "Keys", "Values"], "contextual_text": "The vanilla Transformer architecture employs several attention mechanisms to process sequential data. Specifically, there are three types: self-attention, which is used in the encoder; masked self-attention, utilized in the decoder to prevent attending to future positions; and cross-attention, used to connect the encoder and decoder.", "mentioned_images": ["./images/a-survey-to-transformers/image_1.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.1 Vanilla Transformer"}, "hypothetical_questions": ["What is the purpose of masking self-attention in the decoder?", "How does cross-attention facilitate communication between the encoder and decoder?", "What is the significance of using Queries, Keys and Values in the attention mechanism?"]}
9b4e2ed2-4388-4777-8436-ea25be85d5af	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02491808,0.03675229,-0.008087808,0.026690064,-0.033099014,0.06779365,0.017355446,0.034207568,-0.0019514617,-0.030825738,-0.023744157,-0.017529385,0.015110082,0.012900775,-0.0116674835,-0.01671722,0.013321275,0.051206317,-0.015482194,-0.004760177,-0.0008415014,0.0038528994,0.044333488,-0.039480973,-0.014583813,0.04830237,0.04705079,-0.031475484,0.029621234,-0.002967625,-0.018494204,-0.01416684,0.03458387,0.0076148603,0.029314645,-0.0017341578,0.019432507,-0.060929947,-0.009033407,-0.026832486,-0.09184183,0.01697891,-0.028444933,-0.019561011,-0.013095913,0.012757672,-0.01788001,-0.061562408,0.0023296932,-0.0126255285,0.0069852015,0.033307455,-0.04431686,0.037677485,-0.024499517,0.014494018,0.005438871,-0.011160406,-0.0080720335,-0.07686212,-0.09243826,0.01608111,0.000395317,-0.011829415,0.070163734,-0.015663836,-0.027999759,0.013060633,0.039427225,0.1390482,-0.045029383,0.016641323,-0.02019364,-0.09099795,0.11517159,0.043758087,0.001034285,-0.025653994,-0.053928528,0.02181902,-0.004130773,0.057939485,-0.01032933,-0.0029276009,0.07772202,-0.037513837,-0.062543795,-0.05055815,0.08010104,-0.06830275,-0.028996566,-0.02098952,-0.022944167,0.048580423,-0.022658315,0.0053041745,-0.011948622,-0.05908863,0.06652411,-0.047399368,0.018109696,-0.005780326,-0.007630047,0.09083618,0.022300169,0.011070608,0.02899198,-0.031185802,0.037218925,-0.049412146,0.0017440532,0.037623357,-0.024763996,0.011715477,-0.05873554,-0.01682922,-0.050901923,0.033069123,0.013723659,-0.033515595,0.005614919,-0.009853754,0.010306041,0.027736163,-0.0034744795,0.06159506,0.025881115,-0.02632185,-0.029726826,-0.013812584,0.02265527,-0.025494164,0.0148945125,0.019030873,0.0051225587,0.005624378,0.050297797,-0.009620472,-0.011645189,-0.049566712,-0.02575437,-0.015573128,-0.054136958,0.021886436,-0.0060873283,-0.019938393,-0.07633607,0.0059010657,-0.026360098,0.053171344,0.0026665297,-0.00648774,0.040058237,-0.0231268,-0.04210199,-0.034661867,-0.008879374,-0.0024660653,0.014711098,-0.019445263,-0.03759563,-0.011666697,-0.001163382,0.028706945,0.08637785,0.12795456,-0.010709603,0.043400396,-0.03134667,-0.02013627,-0.019217279,-0.017825384,0.0426584,-0.07959716,0.005260629,0.019560762,0.022153003,-0.006086226,-0.012652737,0.0284888,-0.040452946,0.035015546,-0.012526185,0.015475612,-0.005650934,0.03504789,0.012351714,0.06198789,0.012004386,-0.030842189,0.032259293,0.0055257645,0.037231494,-0.016376426,0.01977794,0.035395578,0.028766938,-0.016326882,-0.004661564,-0.08579616,-0.0024899412,0.034909885,-0.0065313703,-0.01740388,0.010998647,-0.04535392,-0.02927223,-0.02184379,-0.042925227,0.009553117,-0.015579122,0.005179822,0.018064495,0.043541342,0.008360132,0.03093417,-0.0011512148,0.03714748,0.07471716,-0.025615506,-0.0637085,0.015092808,-0.040024992,0.07009884,-0.03112464,0.007857961,0.009988586,-0.01658404,0.039320648,0.0135872755,0.013244653,0.018827807,-0.050041147,0.06424845,-0.037259705,-0.0045308718,-0.008052228,0.0154246725,0.025638148,0.025071315,0.04939039,0.03037956,0.026117655,0.082637124,-0.024760002,-0.027928406,0.002444591,-0.012945097,-0.016506301,0.018862909,0.00065136456,0.044056393,0.022493165,-0.01706393,-0.00022176222,-0.0030204412,0.02766282,0.023432579,0.013977139,-0.03218123,0.033317354,-0.00039710768,-0.014240909,-0.020033775,0.031003358,-0.006055505,0.008624623,0.010817479,-0.034131173,0.060019147,0.018859606,0.019753296,0.03144688,-0.0076633478,-0.0062287487,0.023270864,-0.0020625545,-0.018063068,0.06401122,-0.06268136,-0.005843695,0.019176666,0.015559191,-0.0021813056,0.0031737988,0.029137127,-0.013179671,-0.03574634,-0.028343422,-0.013105854,0.014394357,-0.08969183,0.023180597,-0.007599688,-0.014729716,0.025511608,-0.0005807353,-0.069287114,-0.032287795,0.012135873,0.11433583,0.012204275,0.018914813,0.017678859,-0.014533012,0.020456078,-0.04606062,0.012803015,-0.006328799,0.016749188,0.0120445555,0.01256898,-0.06295198,-0.025459938,0.024629356,0.037469674,0.038393285,0.0070916233,0.0067791394,-0.008914447,0.059072938,0.014810921,-0.012661571,0.046931367,0.016761728,0.02320961,0.014918767,-0.040556833,-0.10307769,0.050900497,-0.05586894,0.020548778,0.061234485,-0.011494638,-0.008948783,-0.025732897,-0.003211819,-0.0127846785,0.032291632,-0.023631297,0.036043283,-0.036188792,0.0037737293,0.01437992,-0.010792615,0.010515061,-0.04280267,0.0007198355,0.005036782,0.05032,-0.005553064,0.027036063,0.02529802,0.03230847,-0.015666002,-0.011961823,-0.025333937,0.0219544,0.042486288,-0.032243755,0.07926711,-0.05279495,0.035254627,-0.042743623,0.008202148,0.0039093927,-0.062203962,0.013996565,0.01438739,-0.008516555,0.0059922067,-0.023805484,0.06741059,-0.006469421,-0.0015911488,0.10874334,0.040784504,-0.0019626217,-0.0057030525,-0.07630714,-0.017386844,-0.0005054107,-0.011181262,-0.03452687,0.037419546,-0.014980248,0.024814202,0.08291459,-0.07842979,0.011599134,-0.035165556,0.016800733,0.015875842,0.028723506,-0.024556238,-0.0039503933,0.024767535,0.01707591,0.009598942,-0.0025037564,0.03991318,0.034318738,-0.060652718,0.03697678,0.0009071231,0.045195308,-0.08380377,0.042816833,0.028818155,-0.021375487,0.047650192,-0.07807293,-0.047617514,0.008010664,0.0015617663,0.0024268553,-0.02508886,-0.0034775361,-0.025115639,-0.024953477,-0.010720041,0.02763449,-0.018405462,0.04924628,0.0010555792,0.020467386,-0.013535971,0.052570574,0.008501562,-0.091415174,0.011621637,-0.010372884,0.015074089,0.057071846,0.038813416,-0.0035287468,-0.028945375,0.06507762,-0.032586116,0.051550716,0.038458753,0.021555383,0.03898401,-0.0080673015,-0.015385621,0.013431988,0.0265172,0.020738242,-0.0069400095,-0.037649307,0.020080544,0.045439415,-0.009875434,0.016944356,0.023762923,-0.009964759,-0.015647708,-0.0088697625,-0.018897798,0.031519484,-0.012579071,0.036986437,-0.008884569,0.009392134,-0.0017755171,-0.025613625,-0.02713834,-0.064721525,-0.0026942512,-0.030554833,-0.02623089,-0.0022870179,-0.06583137,0.021398544,-0.029264579,0.002024348,0.0077107158,0.037081197,0.032815985,0.013330867,-0.09376174,0.00032304245,0.029449798,0.0044208844,0.010253646,-0.030779535,-0.020874776,0.0032479435,-0.021344125,-0.008861263,0.029256351,0.02301963,0.008337393,-0.010693445,0.039487667,-0.028750645,0.08278837,-0.033002555,0.0061755637,-0.018675715,-0.02746841,-0.01776564,0.04920786,0.030056192,-0.013247114,0.044707008,0.03249755,0.07446539,-0.012251185,0.05581681,-0.016219625,0.03358228,0.021860175,-0.01616883,0.018505028,0.0013349786,0.023625012,-0.0049641435,0.07740724,0.020768067,0.008513692,-0.024693219,0.02022808,-0.04668505,0.022576883,0.01360223,0.0530155,-0.0036170497,-0.011244265,-0.055573326,-0.0034718292,0.0054244734,-0.0065276255,0.019326808,-0.08567209,-0.06343706,0.015075718,-0.022009352,-0.01615546,-0.06254649,-0.03895643,-0.0021921303,0.012231776,-0.033411287,0.07657835,-0.030248804,-0.026579585,0.008614888,-0.0066311425,0.08846425,0.026966507,-0.015533331,-0.0034838482,0.0029889354,-0.019333936,0.008969337,0.049384397,-0.022829218,0.022604821,0.052214198,-0.012066695,-0.021438546,-0.03900426,-0.09957212,-0.008397653,-0.0029500911,-0.010801253,-0.011931626,0.0028497968,0.013814305,0.010063698,-0.013552921,0.019484928,-0.06128382,-0.007285101,0.096920945,0.010282716,-0.031197252,-0.0062742303,0.047966354,0.03356817,0.009719286,0.033096563,0.015905485,-0.03729176,0.03639818,-0.0053059743,0.00409032,-0.025687572,-0.0131678665,-0.004812962,-0.043159638,0.018691475,0.018807478,-0.029700682,-0.027881833,0.059992064,0.002226316,0.01554869,-0.036306582,-0.05323358,0.0028367427,-0.03145503,-0.048891384,0.009077632,-0.019614987,0.027129855,-0.004715219,-0.045102976,0.050728627,-0.05833924,-0.030415831,0.010221046,0.005248313,-0.041329905,-0.0030858817,0.0112876585,-0.019919675,-0.03630241,-0.0039652092,0.011962813,0.032636378,-0.032898363,0.0125264535,0.002503964,0.02935583,-0.07365653,-0.026879786,-0.013618386,-0.0024527842,-0.0013508007,-0.013972403,0.05625682,0.027012436,-0.005333369,-0.009797583,-0.015975012,-0.091721565,-0.04013803,-0.010845563,0.014093236,0.016467897,-0.014333897,-0.0057948087,-0.021895021,0.02572812,-0.0056533436,-0.002739245,0.028459152,-0.07328786,0.054878067,0.08551753,0.010395094,-0.052745342,-0.038255893,-0.06063794,0.025677979,0.045876518,0.023232795,0.05265595,0.04874677,0.04241975,0.016462155,-0.066963494,-0.056778554,0.048198212,0.0014832153,0.016515156,0.028491154,-0.027490545,-0.024488816,-0.02248149,-0.018826107,-0.029840112,-0.00885115,-0.013393703,-0.033215545,-0.031774756,-0.012979198,-0.072878405,-0.00302096,0.03715481,-0.023447704,0.108638324,0.04447518,-0.07997255,-0.0902272,-0.03002026,-0.004341498,-0.053773884,-0.02449815,0.0048733703,0.049487457,0.014218113,0.011192229,0.044305936,0.02702474,-0.027797928,-0.030368175,0.09611936,0.021934742,0.022285167,0.008404077,-0.048994333,-0.07961128,0.059229825,-0.0055828826,0.026829058,0.021115279,-0.007890851,-0.029638138,-0.045391366,-0.0155343795,0.019606233,0.017781682,-0.006281478,0.020764437,-0.015826259,-0.026468493,0.03099291,-0.049212568,-0.0069014723,0.035755005,-0.007428082,-0.0033892707,0.049772814,0.023341427,0.009122899,-0.04670336,0.0152285155,0.04871148,0.031705774,0.036373857,0.031680126,-0.034657635,0.0035445227,-0.045816194,-0.010423778,0.047592655,-0.059564497,-0.038854893,-0.037734758,0.058653466,0.0010717538,-0.02557234,-0.040565897,-0.02447818,-0.032539863,0.006308698,0.007593253,-0.006119406,0.0047351066,0.0059571844,-0.03453367,-0.017930055,-0.03627484,-0.042697344,0.036708623,0.05071302,-0.015019873,-0.041058607,-0.058627307,0.005620582,0.07882222,-0.035398807,0.0038136253,-0.06439659,-0.0240013]	Keywords: Elastic Averaging SGD, EASGD, asynchronous SGD, parameter server, local optima\nKey Objects: parameters, workers, local variables, parameter space, local optima\nRefers to Images: None\nHypothetical Questions:\n- How does EASGD differ from standard asynchronous SGD?\n- What is the role of the parameter server in EASGD?\n- Why would allowing local variables to fluctuate further improve optimization?\n---\nSummary:\nElastic Averaging SGD (EASGD), proposed by Zhang et al., connects the parameters of workers in asynchronous SGD using an elastic force centered around a variable stored on the parameter server.\nOriginal Text:\n### 5.5 Elastic Averaging SGD  \nZhang et al. [23] propose Elastic Averaging SGD (EASGD), which links the parameters of the workers of asynchronous SGD with an elastic force, i.e. a center variable stored by the parameter server. This allows the local variables to fluctuate further from the center variable, which in theory allows for more exploration of the parameter space. They show empirically that this increased capacity for exploration leads to improved performance by finding new local optima.\nContextualized Text:\nIn the context of asynchronous SGD, Zhang et al. proposed Elastic Averaging SGD (EASGD) to improve parameter optimization. EASGD links the parameters of the workers to a center variable stored on the parameter server, allowing local variables to fluctuate more freely and potentially explore a broader parameter space, which can lead to finding better local optima.	{"tags": ["optimization", "distributed training", "SGD"], "doc_id": "9b4e2ed2-4388-4777-8436-ea25be85d5af", "summary": "Elastic Averaging SGD (EASGD), proposed by Zhang et al., connects the parameters of workers in asynchronous SGD using an elastic force centered around a variable stored on the parameter server.", "doc_type": "text", "entities": ["Zhang et al."], "keywords": ["Elastic Averaging SGD", "EASGD", "asynchronous SGD", "parameter server", "local optima"], "key_objects": ["parameters", "workers", "local variables", "parameter space", "local optima"], "contextual_text": "In the context of asynchronous SGD, Zhang et al. proposed Elastic Averaging SGD (EASGD) to improve parameter optimization. EASGD links the parameters of the workers to a center variable stored on the parameter server, allowing local variables to fluctuate more freely and potentially explore a broader parameter space, which can lead to finding better local optima.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "5 Parallelizing and distributing SGD", "h3": "5.5 Elastic Averaging SGD"}, "hypothetical_questions": ["How does EASGD differ from standard asynchronous SGD?", "What is the role of the parameter server in EASGD?", "Why would allowing local variables to fluctuate further improve optimization?"]}
a4ad26a3-20de-4c62-b495-50559e971201	abe8c200-bfa1-4355-947e-23ea618c310d	[-9.3871844e-05,-0.013952395,-0.047836866,-0.0056454553,-0.022537611,0.026978754,-0.0045386297,0.05310704,0.06278714,-0.0069986745,-0.030268488,-0.0264313,-0.0014263451,0.038351137,-0.04374733,0.020284452,-0.0024998935,0.09902224,0.008578218,-0.031320095,0.06516479,0.06447939,0.019465799,0.015803352,-0.012568536,0.024861233,0.05486214,-0.008457648,0.027222676,0.027755203,-0.03401568,-0.006187365,0.024433963,-0.007145062,0.017992811,0.021244727,-0.027867729,-0.048115432,0.015104062,-0.03689207,0.011903869,-0.0006835584,0.0061210045,-0.022347096,0.029987099,-0.028122809,-0.04672458,-0.037950076,0.03119319,-0.03232202,-0.03374478,0.07143624,-0.015552663,-0.0030944552,-0.055575226,-0.002476107,-0.04149962,-0.008551742,0.024111185,-0.012572919,-0.063845836,0.009538934,0.025104463,-0.01699785,0.066540435,-0.028612463,-0.029040022,0.036350623,0.028072122,0.09861612,-0.015192686,0.032578826,-0.0028108465,-0.10439762,0.113321714,0.07362246,-0.04746695,-0.03110191,-0.012981665,-0.0045218756,-0.00013912491,0.0047977115,-0.04444002,-0.0046635345,0.08331411,0.0078076865,-0.09275432,-0.058088467,0.020161014,-0.06039833,-0.10563499,0.00094961916,-0.052618988,0.032434803,-0.035519406,-0.020039467,-0.006346097,-0.019161794,-0.022017073,-0.015603587,0.04195896,0.022976113,0.03739156,0.08333157,-0.007910015,0.021866139,-0.012435745,0.004542231,-0.01604232,-0.028909048,0.020539094,0.01935768,-0.003970422,-0.005199798,-0.058516156,-0.043308273,-0.056057908,-0.005775747,-0.025611581,-0.02534863,0.043636885,-0.031218085,-0.051154926,-0.005505171,0.014476652,0.040072024,0.0061139017,-0.0155941835,-0.039140727,0.006236329,0.024845388,-0.010118422,-0.0010534474,-0.0064181434,0.016412614,0.005606387,0.05478472,0.0066657253,0.020499554,0.017697293,-0.022808997,-0.014177259,-0.058014248,0.019356342,0.043854784,0.038448025,-0.07228803,0.038260646,-0.023046602,0.019649299,-0.014470024,0.02120713,0.001937471,-0.076408476,-0.03577551,-0.04973834,-0.047949016,-0.010982055,0.024364222,-0.042789143,-0.029885508,0.020822564,-0.00607289,0.018799024,0.05033529,0.09131835,-0.011778649,0.014116716,-0.04424124,-0.004749286,-0.017493179,0.01991774,0.036736947,0.014394438,0.0085711805,-0.013789142,0.008983083,-0.008714144,0.022115389,0.015975574,0.02124836,0.005218435,0.031094588,-0.01016878,0.0016256013,0.03328855,-0.035712413,0.02879536,0.014099545,0.019141395,0.060563866,0.01476084,-0.0068668025,0.01332195,0.030380018,0.032995462,0.04121797,-0.017882073,-0.019807506,0.004707377,-0.03626864,0.03275228,-0.008261339,-0.015736151,0.026758475,-0.001324733,-0.0030135622,-0.078714,-0.023258515,0.039034136,-0.050042816,0.0009917654,-0.004592847,0.03710893,-0.0036404845,0.011350322,0.006208565,-0.0039224895,0.071262866,-0.0151673015,-0.076133884,0.0036984156,-0.028116927,0.03787015,-0.019079747,-0.028111298,0.026260365,0.0050797667,0.06864079,0.020339565,-0.00041608312,0.031970195,-0.05794891,0.023072062,0.01880096,0.012110548,0.033115916,0.01920555,0.07067215,0.07610818,0.022909418,0.020571576,0.008235034,0.091494046,0.08074364,-0.006632235,0.023371793,0.020528238,-0.00021315124,-0.006398355,-0.058897857,0.04528891,0.066952735,-0.024623713,0.016850255,-0.006911302,0.012094982,-0.010413031,0.06380593,-0.017283335,0.014327774,-0.026700014,-0.053534307,0.07646061,0.021431563,-0.0050775832,0.00029655368,0.006925292,-0.025178708,0.058989696,0.025017256,0.014038091,0.00864916,-0.030864324,-0.032423034,0.07522886,0.017292876,0.0140402205,0.033074398,0.006716113,0.0033033628,0.040739298,0.06414271,-0.0024730077,-0.004472355,0.0416216,0.024128517,-0.013870088,-0.016602723,0.03172903,0.01910629,-0.021270648,-0.009590234,-0.02604501,-0.07000602,0.052364264,-0.021845883,-0.00017332751,0.010054393,-0.017280877,0.08122607,-0.029357411,0.026659137,-0.035831414,-0.011269671,-0.024757564,-0.08010537,0.0112093305,0.007176291,0.038371086,0.032966107,-0.016112939,-0.030639382,0.036271334,-0.00044873034,0.037787784,0.03574965,-0.005100474,-0.0034208042,-0.010285624,0.06276848,-0.012276478,-0.035706192,0.035546027,-0.0020806612,0.020434598,0.027239121,-0.01017164,-0.078417204,0.038161404,-0.0465174,0.022414938,0.03519108,0.0026691828,-0.024381584,0.004758221,0.0042801714,0.009170236,0.09641171,-0.019056939,-0.005189172,-0.007940392,0.018616615,0.026698641,-0.025119489,-0.0063925073,-0.037182603,-0.0018122622,0.0032276271,0.011006382,-0.005655166,0.04366892,0.065958805,0.017397288,0.0044982447,0.009236934,0.027205708,-0.03179532,0.040731195,-0.04389101,0.03756681,-0.073035546,0.09481041,-0.05290259,0.02823757,0.022220053,-0.04105424,-0.00807421,-0.014451287,-0.0106980475,0.08268911,0.05105244,-0.0028700472,-0.056358866,-0.009272583,0.03439386,0.044414487,0.015252568,0.022750024,-0.062564164,-0.011614677,0.020459352,-0.03697618,-0.047354475,0.030416677,-0.00470222,0.016801063,0.09038898,-0.010812302,-0.027344367,-0.030882452,0.011304158,0.022664478,0.021944597,-0.023555327,-0.03320185,-0.011092958,-0.0061403452,0.046044163,-0.025957411,0.02281579,0.064749174,-0.07550568,0.058729514,-0.0053079287,-0.005243676,-0.03515287,0.058082055,0.05649438,0.01557067,0.013396566,-0.047099154,-0.028427951,0.022115521,-0.042789698,-0.0076009477,0.01395782,0.012003111,-0.053113613,-0.0038362707,-0.025084082,0.051811293,0.025210068,0.049929317,-0.007580954,0.06271445,-0.0027854932,-0.012131722,-0.034440145,-0.02517272,0.033239488,0.063115336,-0.014489531,0.017957367,0.018016327,-0.032308653,0.005398001,0.047606166,0.0036842963,0.06232389,0.03582606,-0.03736221,0.051371556,0.008479028,-0.0127067175,-0.002745071,0.033720482,-0.024423465,0.0043392037,0.008567863,0.00021074773,0.056855727,0.0054754023,-0.00850744,0.018261407,-0.020871872,-0.0025835235,-0.04602721,-0.059765857,-0.0007364307,0.0064507984,0.022018177,0.062546074,-0.010933795,-0.0035114447,-0.0233366,-0.008982179,-0.064583056,-0.0050011226,0.021922646,-0.046514265,0.01138148,-0.066597946,0.031219326,0.0019253677,0.0047475747,0.0033603702,0.013639809,0.061443046,0.03113135,-0.04921651,-0.029209316,0.03038266,0.025954781,0.0037942182,-0.026405236,0.006734181,0.0010561827,-0.011020982,-0.009857442,-0.0093398215,0.03554967,-0.056395322,0.017136524,-0.0013400487,-0.0908058,-0.007697732,0.01064379,0.023277525,-0.05949291,-0.012169985,-0.025377708,0.04534116,0.035349686,-0.0185169,-0.01772955,0.015447036,0.07610518,-0.017102953,0.035116628,0.019249959,0.030052548,-0.0032736815,-0.010463821,0.019302733,-0.00213697,-0.019989528,-0.022691092,-0.0002674842,-0.06265403,-0.0116515495,0.03027448,0.005504619,-0.00019648208,0.032825414,-0.010309454,-0.026673518,0.062934436,0.027381431,-0.017072976,0.017893478,0.022060482,0.015504775,-0.0101175485,-0.07775578,-0.036222465,0.0150554115,0.021333609,0.028683882,-0.015379797,-0.038973063,-0.01446925,-0.009269628,0.017418282,0.10213761,-0.067908384,0.006316544,0.054308854,-0.0037079512,0.06309097,0.040185787,0.033004586,0.0073138825,-0.048648134,-0.029539766,-0.014166028,0.052808005,-0.031545877,-0.040598836,0.036519602,0.028731754,-0.012254921,-0.013750315,-0.059310425,-0.059487067,0.012452851,0.014795211,-0.026901796,-0.03926252,-0.025412628,-0.006092885,0.017771311,0.023554083,0.0006765648,-0.009178974,0.10148467,0.0006814426,-0.02568584,0.0020316187,0.028418561,0.04384031,-0.010571573,0.090601355,0.030914309,-0.0066302586,0.018578151,0.045407303,0.040557716,-0.04282691,0.015971135,0.0015638515,0.01647988,0.021540293,-0.00041711656,0.006024937,-0.015985755,0.05499798,0.012649423,-0.016191304,-0.017602718,-0.03842777,0.015092346,-0.03594899,-0.042913374,0.017288595,-0.059318155,-0.020307545,0.018356033,0.023305094,0.04753823,-0.10250808,-0.046779584,-0.0036105467,-0.041299585,-0.09120068,-0.028395925,0.010670879,-0.01664069,0.0009952486,-0.02056344,0.052829206,-0.0060218936,-0.0035441078,0.010724944,-0.026232293,-0.031472813,-0.054594185,3.4802266e-05,-0.021432867,0.0069049154,0.003976299,-0.005739034,0.0785482,-0.014288402,0.011711124,0.02041655,0.047624256,-0.017021008,0.013293862,0.07982955,0.03701076,-0.0072250296,-0.043153644,0.08396439,0.024822606,0.034431957,-0.0051216213,-0.04406235,-0.021877756,-0.046014845,0.0076991646,0.05926841,-0.0010327788,-0.062981844,-0.063628584,-0.0127523625,-0.035997935,0.0007039696,0.022753326,0.055170503,0.052591745,0.048188142,0.01779706,-0.05671333,-0.03423628,0.02962719,0.041253217,0.019663196,0.0156299,0.0016788701,-0.03433616,-0.031574335,-0.0046423213,-0.023071907,-0.024037257,-0.016264996,-0.06925851,0.03157338,-0.037907615,-0.054952066,0.04773652,0.0047961716,0.03236591,0.05545584,0.04384025,-0.026753368,-0.06020735,-0.058123056,-0.007454892,-0.005856883,0.045877874,-0.026312122,-0.00521751,5.8104935e-05,0.019479936,0.06363368,0.024607409,-0.053349633,-0.041629184,0.080587566,-0.025525158,0.05830989,0.042387147,-0.047669332,-0.032346725,0.06637399,-0.012408561,0.012659016,0.023361763,0.00211641,0.00012808658,-0.010568239,-0.029978087,0.010413573,-0.020450355,0.019846546,0.011872719,0.04462694,-0.0022530893,0.030565385,-0.019299181,-0.011608247,0.0016466315,0.004101738,-0.007725022,0.024012305,0.027505323,0.046862766,0.010833739,0.026425837,0.006920895,-0.009438226,0.05274039,0.050347257,-0.0036616833,0.04188346,-0.04010191,0.06206472,0.06869613,-0.06039933,0.0029253957,-0.030861752,0.07183571,0.0056972294,-0.012638435,0.011951797,-0.0044263722,-0.000739896,-0.017976567,-0.029543411,0.018371308,0.0027867905,-0.0057086293,-0.033888288,-0.049911696,-0.033744562,-0.029252186,0.033212744,0.029043388,-0.0081334235,-0.04634539,0.058083635,-0.012543091,0.03829265,-0.044788644,0.015132469,0.0025991648,-0.037167676]	Keywords: Stochastic Gradient Descent, SGD, optimization algorithms\nKey Objects: SGD, optimization strategies\nRefers to Images: None\nHypothetical Questions:\n- What are some examples of these additional optimization strategies?\n- How do these additional strategies complement existing optimization algorithms?\n- Where can I find more information on these optimization tricks (reference [11])?\n---\nSummary:\nThis section introduces additional strategies for optimizing Stochastic Gradient Descent (SGD) that can be used in conjunction with other optimization algorithms.\nOriginal Text:\n## 6 Additional strategies for optimizing SGD  \nFinally, we introduce additional strategies that can be used alongside any of the previously mentioned algorithms to further improve the performance of SGD. For a great overview of some other common tricks, refer to [11].  \n14 https://www.tensorflow.org/  \n15 http://googleresearch.blogspot.ie/2016/04/announcing-tensorflow-08-now-with.html\nContextualized Text:\nTo further improve the performance of Stochastic Gradient Descent (SGD), this section introduces additional optimization strategies that can be implemented alongside other algorithms. For a more detailed overview of common optimization tricks, refer to reference [11].	{"tags": ["optimization", "machine learning", "algorithms"], "doc_id": "a4ad26a3-20de-4c62-b495-50559e971201", "summary": "This section introduces additional strategies for optimizing Stochastic Gradient Descent (SGD) that can be used in conjunction with other optimization algorithms.", "doc_type": "text", "entities": ["TensorFlow"], "keywords": ["Stochastic Gradient Descent", "SGD", "optimization algorithms"], "key_objects": ["SGD", "optimization strategies"], "contextual_text": "To further improve the performance of Stochastic Gradient Descent (SGD), this section introduces additional optimization strategies that can be implemented alongside other algorithms. For a more detailed overview of common optimization tricks, refer to reference [11].", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "6 Additional strategies for optimizing SGD"}, "hypothetical_questions": ["What are some examples of these additional optimization strategies?", "How do these additional strategies complement existing optimization algorithms?", "Where can I find more information on these optimization tricks (reference [11])?"]}
f6625cfd-0f0b-4b26-8d3e-ef20fa47a22c	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.013752617,0.009713355,-0.012095889,0.034722824,-0.03966168,0.03646375,0.03357983,-0.02739399,0.0309239,-0.014062251,-0.009055239,0.012446133,0.02560687,0.010223341,-0.009315339,0.03213847,0.0044590253,0.08103258,-0.031078823,-0.039144862,0.071614325,0.00051828846,0.014772432,0.010775492,0.04033511,0.03514734,0.06862223,0.0017449614,0.005010889,-0.058019836,-0.057677694,-0.014158822,0.021523247,0.008288684,0.019986914,0.020660933,0.053464036,-0.022304054,-0.049122103,-0.061173774,-0.031425904,0.046999525,-0.028181817,0.005823542,-0.026364887,-0.06967639,-0.045881405,-0.03218032,-0.011684935,-0.046040833,-0.01618996,-0.0017046253,-0.05631241,-0.0053090113,-0.051267035,0.0021015701,-0.00590622,-0.026344217,0.017939763,-0.07188234,-0.03710589,0.020886233,0.01701949,-0.019846406,0.052429825,-0.055435203,-0.003890518,0.001810538,0.05870561,0.10526366,-0.012564609,0.013913911,-0.054324046,-0.08996783,0.13942543,0.01207595,-0.031410262,-0.050720036,-0.03488868,-0.0013733597,0.009883641,0.05002868,-0.02388452,-0.032831904,0.08290758,-0.0055753463,-0.04419016,-0.027986627,0.04975256,-0.014742115,-0.05913288,0.0012526676,-0.05804616,0.07406121,-0.045764286,-0.07750799,0.011012492,-0.048025593,0.04137792,-0.017074332,0.04566057,-0.018142955,0.0661318,0.07207019,-0.009561796,-0.00015258005,-0.07125701,-0.02554952,0.0015682067,-0.036218565,0.0061137876,0.0026520952,-0.0436919,0.00062532164,-0.069493145,-0.043398745,-0.053981077,-0.012904106,-0.040302087,-0.056710817,0.028370915,0.002651405,-0.041859515,0.021358684,0.0765232,0.010360818,-0.0073240967,0.008977745,-0.03236053,0.009065381,0.044291407,-0.025323983,-0.019183848,0.017110804,0.017018918,-0.021116693,0.036387354,0.024465062,-0.017347615,-0.0066316086,-0.013075336,-0.028606175,-0.012163506,0.019327216,-0.010134741,0.008319143,-0.053804085,0.03070641,-0.043320943,0.07821239,0.060746633,0.015833963,0.057080347,-0.0011114147,-0.0056725736,-0.055145904,-0.02662975,-0.0022586167,-0.019086108,-0.055007663,-0.06300728,-0.04873469,0.012556028,0.031046394,0.052685894,0.081784695,0.016056422,0.035618227,0.008891203,0.017035961,-0.034703378,-0.0017171364,-0.01779211,-0.020069592,-0.017555363,-0.0073768725,0.029558804,-0.026738936,-0.027374772,0.008630856,-0.0078032166,0.017984012,-0.0112890415,0.03855434,-0.009621542,0.03007348,0.020835802,0.04215806,0.009492672,-0.015806083,0.003960939,0.0034447524,0.024884196,0.0074858274,-7.018591e-05,0.038624365,0.026007634,-0.010621743,-0.001489998,-0.021509646,0.003135323,0.042287063,-0.014508225,-0.00074609555,0.022017486,-0.017598607,-0.03598668,-0.035729013,-0.06874554,0.014443705,-0.044739548,-0.0026432914,-0.01885737,0.031836607,-0.015487567,-0.005161234,0.007374639,0.066316105,0.020495107,0.028481077,-0.023321984,0.018303456,0.040662065,0.034676865,-0.042022716,-0.0022229014,-0.014543668,0.002718121,0.016119348,0.024074737,0.012507804,0.0098306695,-0.027338888,0.083636984,-0.008739635,0.03224791,0.011601775,-0.031190233,-0.013164079,0.0622227,0.023171136,0.012449727,-0.008874049,0.09271736,0.03038343,-0.025002532,0.004264069,0.044608697,0.009816291,0.05204832,-0.044801515,-0.013366651,0.034895122,-0.0056764088,-0.005309251,-0.009135875,-0.028207302,0.04766891,0.056356627,-0.035064284,0.02170769,0.02099208,0.039195806,0.02466801,-0.0031217672,-0.017236045,0.028468316,-0.008768479,-0.0101935025,-0.019341268,0.053390022,0.05108568,-0.0058057243,-0.023439515,-0.032320123,0.050897278,0.03266963,0.00877562,0.04419648,-0.017873637,-0.02472275,0.062818564,0.0074546584,0.011575428,0.014816178,0.015149799,-0.037244406,-0.015084761,-0.03995796,0.020508021,-0.0018295633,-0.08015646,0.03958345,0.012903742,-0.05384161,-0.026605299,0.01330098,-0.017396556,-0.0015056244,0.0063078245,0.061623,-0.018487124,0.042203404,-0.016673433,0.056785926,0.027677443,-0.0877662,0.023780452,0.0076297466,0.0021245917,0.046508484,-0.0068742507,-0.0198308,0.02513355,0.011040025,0.054208435,0.09731018,-0.030328203,0.027635887,0.04443774,0.033759587,-0.04294094,-0.00892927,0.037969016,-0.0056674695,-0.029306605,-0.008055112,0.014572194,-0.0712891,0.07323429,-0.030623315,0.03953949,0.037503287,0.017917993,-0.0029800793,0.019546865,-0.020805039,0.0412258,0.079843834,0.018778354,0.0020134225,-0.047726814,0.005958811,0.016170086,-0.03064165,0.057729915,-0.046441663,-0.012156777,0.008179146,0.061025638,-0.03972853,0.04686107,0.04233319,0.022227215,0.041700132,-0.008372758,4.0521012e-05,0.034438338,0.021503275,-0.0235428,0.055786446,-0.061207272,0.054616928,-0.030261287,0.03507774,0.0068860026,-0.0645223,-0.0052909264,0.029420484,-0.033597007,0.037877914,-0.0171909,0.024043124,-0.079157256,0.0078020706,0.004809346,0.025429936,0.023907201,-0.022597473,-0.024546714,-0.024628185,0.0025937755,-0.012119279,0.011379192,0.036577977,0.013142701,-0.0029906076,0.05260103,-0.016368099,0.044897463,0.0069128033,0.022270426,0.031744827,0.0017075236,-0.036994983,-0.0282794,-0.012523861,0.013104889,0.04868914,-0.0006159909,-0.00093718746,0.025091736,-0.051212873,0.03627402,0.017607499,0.0014827806,-0.05582326,0.03400108,0.05403952,-0.021715675,0.019615518,-0.08330366,-0.039680447,-0.004131588,-0.0040381826,-0.005037126,-0.016462523,0.0026228027,-0.028416919,-0.0044785235,-0.0248296,0.05369522,0.023811556,0.023972007,0.011851296,0.011524579,-0.015982796,0.025898762,-0.0675265,-0.052708372,0.0027575206,-8.3358056e-05,-0.031239342,0.062137745,0.058069233,0.027173335,-0.03402689,0.057509262,-0.050532676,0.01327551,0.082126066,-0.004041177,0.0199312,-0.013317478,0.042327613,0.020610008,0.004067241,0.004471935,0.0038660376,-0.04570688,-0.0069879023,-0.0016320781,-0.016075024,-0.032539185,0.013677358,-0.027257664,-0.012086299,0.015724748,-0.028157791,-0.010734169,0.028872833,0.027553912,0.0071223616,-0.003800664,0.0048490902,0.014410097,-0.014288602,-0.039859917,0.017589247,0.022381367,0.008090736,0.0472036,-0.028517785,0.012118798,-0.03981481,0.006434606,-0.023250844,0.020894837,0.056354452,-0.012078822,-0.030680053,-0.013388415,-0.03286611,0.04243039,-0.032039616,-0.029956033,-0.038456865,0.0065905866,-0.044693887,0.03294482,-0.03373425,0.025802352,0.00843994,0.027592177,0.019736432,-0.0062088408,-0.033162124,-0.017815758,-0.008197819,-0.021976206,-0.037507616,0.010522952,0.09657115,0.06234628,-0.034807887,0.017819615,0.008180858,0.08563961,-0.018384362,0.012825271,-0.0021524702,0.015020922,0.04304715,-0.054131273,-0.04356505,0.026501494,-0.0062484955,0.013556433,0.037046365,-0.0055129714,0.0020899312,0.023127478,-0.012506756,0.019249259,-0.012783541,0.01893271,0.0045797694,0.022900496,0.06517204,-0.06587368,-0.028619045,-0.006262648,-0.0055447696,0.0305509,-0.11809972,-0.018651415,-0.007144061,-0.0071982853,0.024263902,-0.016288973,-0.023446642,-0.0035473185,0.01445253,-0.028380467,0.059996855,-0.054107964,-0.0058480497,0.033299286,0.0035101806,0.09177647,0.049325783,0.0097473,0.03646209,-0.0031429906,-0.026295964,-0.009358881,0.027912805,0.0052928184,0.014674317,0.056768008,0.028106451,0.0041834903,-0.05068203,-0.066672586,-0.0326312,-0.0016865513,-0.018325716,-0.06696177,-0.0068513127,0.042774465,0.01244354,-0.019514153,-0.04460292,0.0023341447,-0.041418254,0.07179209,0.036599074,-0.010059904,-0.025827443,0.08418491,0.016943956,0.007958211,0.055290207,0.002518302,0.0044424636,0.013786043,-0.020811299,0.043571223,0.022998117,0.004119372,-0.022919178,-0.02243781,-0.03256472,0.021269038,-0.05640763,-0.045354716,0.020232152,0.004684722,0.052118916,-0.051492415,-0.04616147,-0.019017896,-0.03075655,-0.040633436,0.05935553,0.0038719024,0.0063205683,0.03139305,-0.008199256,0.030739756,-0.09013324,-0.013108345,0.013917012,-0.008505833,-0.06508555,0.0047242125,-0.0058565433,-0.04749615,-0.0049399436,-0.043718144,-0.01176973,0.03767667,-0.027839549,0.02830544,-0.029106636,0.03773315,-0.0625377,-0.020291675,-0.024962045,0.0077007706,0.014484195,-0.030653851,0.06504051,-0.05752544,-0.023144891,0.02761957,0.029126702,-0.08266623,-0.05194742,0.008504727,-0.013960906,0.012292336,0.03505056,0.051299736,0.060680278,-0.028602432,-0.027310848,-0.063148774,-0.032014173,-0.044095676,0.015327764,0.055074476,0.018493084,-0.0030294906,-0.04979288,-0.057016034,-0.0217142,0.02823962,0.001597804,0.041129924,0.075159,0.045362696,-0.04462266,-0.054180656,-0.02181985,0.06807645,0.010960762,0.006611531,-0.018851297,-0.016328294,-0.054935105,0.0014192659,0.0048794947,-0.026298538,0.06307741,0.0312739,-0.02655237,-0.021339582,-0.02608562,-0.029882792,0.020568868,-0.020106755,-0.01883342,0.056137893,0.036447473,-0.044529207,-0.08076038,-0.011516824,-0.036069445,-0.0021522539,-0.012467581,-0.008914846,0.012275241,-0.01951858,-0.016630638,0.031162973,0.055672314,-0.017954303,-0.056393135,0.015597852,-0.0007728697,0.055470135,0.024143558,-0.08324906,-0.068400875,0.08166015,0.014833881,-0.046739273,0.00838975,0.00398999,-0.021508329,-0.022150721,0.01743976,-0.045135476,-0.013625035,0.011245685,0.032161668,0.005440183,-0.02020625,0.021233156,-0.035996433,-0.016969185,-0.023422467,0.018091856,0.0051943013,0.04790422,-0.024062665,0.014753387,-0.02225196,0.012336804,0.02767094,0.060384378,-0.022524178,0.046768293,0.030794758,0.03399775,-0.045207575,0.019406723,0.07679661,-0.057094652,-0.01441405,-0.0048504765,0.06993405,0.03801055,0.01975618,0.008103229,-0.051794898,-0.010800237,0.0046678837,0.03661761,0.01914055,-0.0005153332,-0.0363984,-0.04816539,-0.0499303,-0.016265519,-0.0043869144,0.03440556,0.055374153,0.041590124,-0.0192621,-0.043797117,-0.059983388,0.040081274,-0.017471178,0.04022441,-0.0040947995,-0.0043275277]	Keywords: training data, optimization algorithm, curriculum learning, shuffling, epoch\nKey Objects: training examples, optimization algorithm, curriculum learning\nRefers to Images: None\nHypothetical Questions:\n- Why is it generally recommended to shuffle training data?\n- What is curriculum learning, and when might it be beneficial?\n- How did Zaremba and Sutskever's work demonstrate the effectiveness of curriculum learning?\n---\nSummary:\nTo prevent bias in the optimization algorithm, it's generally recommended to shuffle training data after each epoch, though curriculum learning can sometimes improve performance by presenting examples in a meaningful order.\nOriginal Text:\n### 6.1 Shuffling and Curriculum Learning  \nGenerally, we want to avoid providing the training examples in a meaningful order to our model as this may bias the optimization algorithm. Consequently, it is often a good idea to shuffle the training data after every epoch.  \nOn the other hand, for some cases where we aim to solve progressively harder problems, supplying the training examples in a meaningful order may actually lead to improved performance and better convergence. The method for establishing this meaningful order is called Curriculum Learning [3].  \nZaremba and Sutskever [21] were only able to train LSTMs to evaluate simple programs using Curriculum Learning and show that a combined or mixed strategy is better than the naive one, which sorts examples by increasing difficulty.\nContextualized Text:\nDuring training, it's often beneficial to avoid presenting training examples in a specific order, as this can bias the optimization algorithm. Therefore, shuffling the training data after each epoch is generally recommended. However, in scenarios where progressively harder problems are targeted, supplying examples in a meaningful ordera technique known as curriculum learningcan sometimes lead to better performance and faster convergence.  Researchers Zaremba and Sutskever successfully trained LSTMs to evaluate simple programs using curriculum learning.	{"tags": ["optimization", "training", "machine learning"], "doc_id": "f6625cfd-0f0b-4b26-8d3e-ef20fa47a22c", "summary": "To prevent bias in the optimization algorithm, it's generally recommended to shuffle training data after each epoch, though curriculum learning can sometimes improve performance by presenting examples in a meaningful order.", "doc_type": "text", "entities": ["LSTMs"], "keywords": ["training data", "optimization algorithm", "curriculum learning", "shuffling", "epoch"], "key_objects": ["training examples", "optimization algorithm", "curriculum learning"], "contextual_text": "During training, it's often beneficial to avoid presenting training examples in a specific order, as this can bias the optimization algorithm. Therefore, shuffling the training data after each epoch is generally recommended. However, in scenarios where progressively harder problems are targeted, supplying examples in a meaningful ordera technique known as curriculum learningcan sometimes lead to better performance and faster convergence.  Researchers Zaremba and Sutskever successfully trained LSTMs to evaluate simple programs using curriculum learning.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "6 Additional strategies for optimizing SGD", "h3": "6.1 Shuffling and Curriculum Learning"}, "hypothetical_questions": ["Why is it generally recommended to shuffle training data?", "What is curriculum learning, and when might it be beneficial?", "How did Zaremba and Sutskever's work demonstrate the effectiveness of curriculum learning?"]}
6091a521-62aa-47ff-ac8c-abb374aa0e34	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.018702902,-0.0069658444,-0.01389972,0.0024522361,-0.024383169,0.02678412,0.044640172,0.02726546,0.012383231,-0.03344963,0.026652815,-0.034598216,0.0035463092,-0.009354098,-0.047507625,0.007916433,0.009626853,0.10215558,-0.008200761,-0.014576134,0.039925016,0.045917727,0.015936425,-0.00468086,0.019038625,0.047623668,0.04755745,0.01550218,0.020969242,-0.007017478,-0.02117539,0.015230979,0.010893604,0.012345671,-0.016442731,-0.010212196,0.019391594,-0.037783217,-0.027625611,-0.0323355,-0.045781713,0.063617714,-0.060554016,-0.060375117,0.00054550485,-0.034672454,-0.11061384,-0.003164297,-0.035730835,-0.0056723906,-0.0377081,0.056368504,-0.04136209,0.036689896,-0.06986886,0.0039399993,0.010293501,-0.027436359,0.004226818,-0.028552884,-0.026511902,0.009196479,0.021379359,0.027276922,0.044022106,0.008656223,0.012826359,0.025326667,0.03764242,0.109291516,-0.027312908,0.009650711,-0.04315508,-0.07100089,0.10788027,0.011831967,-0.09322199,-0.027178133,-0.036461115,-0.024647722,0.046634514,0.022077605,-0.037952814,-0.046153523,0.06282378,0.00017369991,-0.042276893,-0.050711118,0.046718225,-0.06435235,-0.027513139,-0.007038158,-0.008084227,0.031353142,-0.051434692,-0.012392283,0.054669145,-0.029367503,0.0080629345,-0.035106037,0.057516217,-0.0057557942,0.06419861,0.094541535,0.028502528,-0.030446941,-0.0531165,-0.024354888,-0.016188169,0.06593988,-0.014482802,0.039697792,-0.060885087,-0.0049953917,-0.017000038,-0.023054842,0.00193333,0.018178586,0.0024102102,0.0023732584,0.065555155,0.032881133,-0.02117448,0.028781395,0.018865438,0.010710999,0.015138946,-0.026315747,-0.026312899,0.026980747,0.079043716,-0.035462864,-0.056866743,0.0318297,-0.016522633,-0.042591073,0.04601071,0.011277828,-0.009074609,0.012298358,-0.01720642,-0.046697784,-0.034867737,0.0033140047,-0.040095367,-0.009080182,-0.0756826,0.0658025,-0.013092662,0.049726043,0.014037663,-0.0060234508,0.0128521435,-0.0025133216,-0.05762358,-0.008175865,-0.012683369,-0.021278484,0.010993521,-0.01809102,-0.039708387,-0.031719092,-0.019142348,0.095276564,0.013240186,0.12345574,-0.019419197,0.0029469393,0.0026454397,0.018086404,-0.039370343,-0.037533466,0.032929197,-0.010384562,0.033473145,-0.022079272,0.022710124,-0.061730206,-0.005462954,0.03904291,-0.023530846,0.051675193,-0.03480468,0.026048228,-0.04815173,0.02065529,-0.0009131365,0.044082083,-0.0100198295,-0.034543246,0.030653888,-0.0025368063,-0.008040466,-0.007395849,0.02761479,0.0011468919,0.029333135,-0.005434047,-0.012379862,-0.0062432033,0.01941327,0.015903823,-0.028796561,-0.065724164,0.04384165,-0.046706885,-0.0031950858,-0.017938113,-0.043940086,0.064235955,0.0104567325,0.006952964,-0.013855436,0.013307167,-0.041793007,0.0037994168,-0.056859557,0.0256108,0.033877496,-0.008046427,-0.04862911,0.004992615,-0.029525071,0.047429137,-0.05277966,0.015227678,0.002486789,-0.00014414487,0.04512941,-0.018087678,0.006277233,0.06188998,-0.04477259,0.024624625,-0.01680381,0.04170442,0.0052266824,-0.016356682,-0.013949849,-0.0044193952,0.06283952,0.03663817,0.0038181087,0.08253147,0.011541962,-0.014144454,0.030384762,-0.0019177174,-0.029259294,0.061417364,-0.022071704,0.029118441,0.019310432,-0.03671455,-0.021111926,-0.019466054,0.012728204,0.069892295,0.02432175,-0.035068825,-0.01700885,-0.041986804,0.0067415456,0.013755173,0.008758596,0.009905121,0.012848096,0.008726408,-0.03743864,-0.0037312796,0.0053712283,0.068670094,-0.024768861,-0.011589731,-0.029409152,0.050630476,0.012693187,-0.04514613,-0.00427303,-0.033073008,0.028384315,0.01968048,-0.04634943,0.004149291,0.022661516,-0.02041359,-0.05516752,-0.047364973,-0.034472495,0.045309354,-0.0039467835,-0.036642943,0.0126799615,0.037620336,-0.01036096,-0.023957355,-0.010829764,0.015796348,0.0034343547,0.042207748,0.10399864,-0.03583277,0.019535828,0.011221236,0.045495074,0.0064313724,-0.08009413,0.021005878,0.028124819,-0.00838711,0.041134514,0.023878714,-0.014933026,0.013687495,0.002171643,0.06505628,0.11521467,-0.022069348,0.02093985,0.054892275,0.06191507,0.023424106,0.012646728,0.04356559,0.017475393,-0.043537695,-0.007474336,0.030024152,-0.061484206,0.1135364,-0.04649848,0.028576525,0.069935344,-0.0014588408,0.03324317,0.024192808,0.0047562323,0.017895335,0.019053912,-0.008919063,-0.0055699237,-0.08260691,-0.02615918,-0.0014958747,-0.016516859,0.017993689,-0.04058659,-0.018550111,0.05525885,0.0842012,-0.016732706,0.03782492,0.05374257,-0.0059670084,0.0394851,-0.019889874,0.021688402,0.02590347,0.024961894,-0.021577882,0.03744106,-0.03149652,0.058773383,-0.008462201,0.05336282,0.021814719,-0.004145681,0.021304894,-0.028244352,-0.03165131,0.012555364,-0.018777078,0.03670216,-0.019144362,0.008288842,0.026211318,0.027313782,-0.0073493393,0.009269839,0.009170244,-0.020187281,0.03689747,-0.016966872,-0.025204098,0.041340984,0.006523785,0.0014698287,0.09294079,-0.03548252,-0.016891161,-0.041207734,0.0014942463,0.058702923,-0.022687053,-0.05872899,-0.0138816545,0.01525749,0.025248319,-0.019068167,0.014005291,0.035244692,0.023674563,-0.05522926,0.06378511,0.03272739,0.0015932298,-0.09048578,0.0073502953,0.043543912,0.007350471,0.040459663,-0.08534566,-0.06100103,0.025826836,-0.04425158,-0.016555455,-0.011070316,0.01018979,-0.027796483,-0.019062482,-0.014563113,0.032832094,0.023447676,0.05166112,0.018911764,0.014655907,-0.020785812,0.025005601,-0.006620776,-0.020509133,-0.039021157,-0.062036138,-0.03071923,0.056590218,0.017056031,0.028093431,-0.05016386,0.08520082,-0.047188327,0.013334564,0.026944406,-0.03715211,0.07301542,-0.0044279154,-0.020533439,-0.000719977,0.02800393,-0.029451407,-0.0335441,-0.007327735,-0.009397163,-0.031698555,0.010513164,-0.0036370622,0.012432826,-0.04004327,0.009441939,-0.02913349,0.018030632,-0.015662717,0.034063227,0.01925775,0.009263927,0.036433768,-0.009004176,-0.024033252,0.012931034,-0.039865334,-0.028222281,-0.017509904,0.023452537,-0.015573406,-0.0077393944,0.047340054,-0.009663169,0.021760693,0.021871662,0.017212203,0.027770583,0.0048651425,-0.052254874,-0.04399631,0.03939088,0.04052411,-0.01876014,0.025227033,-0.04732381,-0.021934258,-0.04813246,-0.007350744,-0.005030238,0.04877744,0.03740589,0.00493789,0.006929722,0.0015454878,-0.0020959382,-0.008241635,0.060588256,-0.010838437,-0.013133356,-0.017861836,0.08318132,0.04400792,0.017358199,0.01741822,0.020867195,0.07738859,-0.021838676,0.070929654,-0.0010507401,-0.010399982,0.0261534,-0.0062775626,-0.028512223,0.014997961,0.012901323,0.04455513,0.0025839964,0.0046864348,-0.02656964,0.014487601,0.021113783,0.048398636,-0.0136050545,0.0016778017,0.034523483,0.017185895,0.046329655,-0.09604398,0.024326408,-0.055513702,0.0043702545,0.0094443355,-0.0728386,0.014056965,0.016109698,-0.020562675,-0.008445923,0.010775663,-0.03881157,-0.0083017545,0.0097069265,-0.018936176,0.060289517,-0.063234836,0.03826605,0.032654323,-0.0058081066,0.114694014,0.04231513,0.0024937112,0.014728551,0.017455505,-0.019242363,0.019744247,0.03248241,-0.003349017,-0.04248515,0.046131432,0.007821227,-0.029810151,-0.008716862,-0.0010888265,-0.047535986,-0.057049267,-0.022578288,-0.043615207,-0.058404937,0.0027158929,0.021188969,-0.00801102,0.0074358652,-0.0040059797,-0.009859719,0.036474768,0.025422616,0.01583163,0.007977754,0.03622049,0.02948162,-0.009211801,0.046003297,0.01707637,-0.04863897,0.0067385673,-0.022551348,-0.008980126,0.029725617,0.03150706,0.027778635,-0.085673474,0.027713574,-0.021793699,-0.030708026,-0.039502226,0.0027198473,0.001700381,0.033420794,-0.020440452,-0.062237427,-0.0037190593,-0.0061743343,-0.040654354,0.01992796,-0.0038774386,0.01103741,0.03788728,-0.04909586,0.073384196,-0.084993854,-0.025202971,-0.00815554,0.005209497,-0.044254355,-0.02379959,-0.017854558,-0.036670938,0.016067713,-0.054419488,-0.00265563,0.0443486,-0.000989733,-0.010820261,-0.025164774,0.036347505,-0.010837212,0.021278117,-0.0109071145,0.01782616,0.0075792926,-0.01425794,0.084100105,-0.075781666,-0.036822952,0.0088457875,0.0009807677,-0.06072535,-0.06907804,0.025672728,-0.02729709,0.033245336,0.023216123,0.04559526,0.007917533,-0.011161781,-0.05161828,-0.065011084,-0.027801495,-0.026705494,0.020902928,0.08590089,0.0154168485,-0.017759526,0.002758383,0.006886098,-0.032991577,0.03015048,-0.005376915,0.02602659,0.02293978,0.05027732,-0.032395836,-0.054067586,-0.046820853,-0.046017233,0.036932684,0.013103868,0.01330035,0.04478972,-0.0051070475,0.0071711224,-0.000114580274,0.017723514,0.03496588,-0.023073941,-0.067587525,0.022785654,-0.0048747812,-0.02090662,0.019614073,-0.04457931,0.021147221,0.06937625,0.0015630784,-0.023446571,-0.04833579,-0.04952312,-0.003352575,-0.02753088,0.008098693,-0.006285965,-0.007039443,0.048320204,-0.012238053,0.06274106,0.03634481,-0.02976936,-0.0034945006,0.028602218,0.01726033,0.006315627,0.012567219,-0.046442285,-0.055370986,0.036657225,-0.0023475685,-0.010574207,0.01217752,0.029489297,-0.03169103,0.021628978,-0.010272014,-0.02116631,0.0227764,-0.011664329,0.032688934,0.0083927335,-0.0044152117,0.015661232,-0.048563987,-0.036339898,0.009768879,0.024316967,0.019863993,0.03069625,-0.0014151804,0.042260975,-0.061909735,0.020737136,0.046577375,0.021353051,-0.0030135592,0.030091153,-0.013717154,0.046011735,-0.024411215,0.05107247,0.066341475,-0.005792804,0.005594803,-0.012095048,0.054436862,0.048668448,-0.011088473,-0.054948322,-0.01615774,-0.038043156,0.001587528,0.017030895,0.019857224,0.0514693,-0.017595986,-0.045781545,-0.038809538,-0.003927342,-0.043385558,0.027822921,0.038636878,-0.019948866,-0.08524032,-0.05200626,0.028342385,0.072076686,-0.023322793,0.024164854,0.019441605,-0.005024578]	Keywords: batch normalization, parameter normalization, learning rates, Dropout\nKey Objects: parameters, mini-batch, learning rates, Dropout\nRefers to Images: None\nHypothetical Questions:\n- Why is it important to normalize the initial values of model parameters?\n- How does batch normalization improve training speed and stability?\n- What is the connection between batch normalization and the need for Dropout?\n---\nSummary:\nBatch normalization reestablishes parameter normalizations for every mini-batch, allowing for higher learning rates and reducing the need for Dropout.\nOriginal Text:\n### 6.2 Batch normalization  \nTo facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.  \nBatch normalization [9] reestablishes these normalizations for every mini-batch and changes are backpropagated through the operation as well. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout.\nContextualized Text:\nTo facilitate learning, it is typical to normalize initial parameter values with zero mean and unit variance. During training, this normalization is lost, which can slow down the process and amplify changes. Batch normalization reestablishes these normalizations for each mini-batch. This technique allows for the use of higher learning rates and reduces the need for Dropout, acting as a regularizer within the model architecture.	{"tags": ["optimization", "regularization", "deep learning"], "doc_id": "6091a521-62aa-47ff-ac8c-abb374aa0e34", "summary": "Batch normalization reestablishes parameter normalizations for every mini-batch, allowing for higher learning rates and reducing the need for Dropout.", "doc_type": "text", "entities": [], "keywords": ["batch normalization", "parameter normalization", "learning rates", "Dropout"], "key_objects": ["parameters", "mini-batch", "learning rates", "Dropout"], "contextual_text": "To facilitate learning, it is typical to normalize initial parameter values with zero mean and unit variance. During training, this normalization is lost, which can slow down the process and amplify changes. Batch normalization reestablishes these normalizations for each mini-batch. This technique allows for the use of higher learning rates and reduces the need for Dropout, acting as a regularizer within the model architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "6 Additional strategies for optimizing SGD", "h3": "6.2 Batch normalization"}, "hypothetical_questions": ["Why is it important to normalize the initial values of model parameters?", "How does batch normalization improve training speed and stability?", "What is the connection between batch normalization and the need for Dropout?"]}
121f4e8c-9bbe-44a4-8dc3-3d47ffa4b31e	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.034665205,-0.01320653,-0.0082281185,0.02065966,-0.038653724,0.057552386,-0.014728283,0.06679491,-0.01549538,0.0024541395,0.022744238,-0.05618098,0.024103414,0.015554128,-0.044120997,0.028889347,-0.0018029731,0.1212808,-0.0073854737,0.0421968,0.03689721,0.04415251,0.00048555635,0.002921113,-0.0013032816,0.044804774,0.03187426,0.0024291566,-0.03403424,-0.00120136,-0.058339857,-0.0118042,0.048605353,0.001084351,0.03451301,-0.032620955,0.022544282,-0.034225132,0.0011643237,-0.044054642,-0.021785457,0.05087377,-0.03386734,-0.043579597,0.006827097,-0.052898172,-0.034021597,-0.018114394,0.0047235494,0.0086607635,-0.00092659204,0.010375761,-0.072657794,0.041252587,-0.025078038,0.016686186,0.035465565,-0.028057184,-0.010331964,-0.0037005914,-0.003760457,0.02588895,-0.012676387,0.022941263,0.03280356,-0.0023382297,0.009207659,-0.03360337,0.075780764,0.10827643,-0.0073672514,0.031122837,-0.051681258,-0.063595034,0.12427208,0.016974939,-0.032750085,-0.04564352,-0.0005310332,-0.036464147,-0.0043876786,0.06445068,-0.056850296,-0.06615721,0.051117778,-0.0138273565,-0.04323669,0.000559884,0.062371682,-0.0491803,-0.01245512,-0.020047983,-0.0059331623,0.031327132,0.009085782,-0.051857457,0.01785651,0.03858801,0.015385433,-0.014287025,0.046716962,-0.029992122,0.023850346,0.07823464,-0.021528317,0.005669356,-0.034399636,-0.07999688,0.00031053205,0.016231006,-0.057599686,-0.01539849,0.012226514,-0.0042612427,-0.07182929,-0.006883268,0.009816568,-0.013560421,-0.038156837,-0.039760742,0.043981507,0.017262448,-0.0663567,0.049170546,0.01619998,0.041456778,0.023014367,-0.0359845,-0.02408573,-0.0047536674,0.014959567,-0.034023102,-0.06935729,0.054705083,-0.01809553,-0.004700288,0.015995758,-0.012076233,-0.010993285,0.032863878,0.00068483,-0.042496353,-0.012509988,-0.0064062136,0.014787948,0.06871132,-0.08301681,0.025856785,-0.015828252,0.08765006,0.014171568,0.048320275,0.035015386,-0.025442924,-0.026813865,-0.011497501,-0.014445714,-0.028015554,-0.011565704,-0.02946466,-0.015862064,-0.04877848,-0.016766282,0.09690351,0.023374941,0.0805741,-0.013341586,0.037843842,0.012082685,0.0079912795,0.02413063,-0.040232852,0.010419249,-0.021781288,-0.028566044,-0.029958252,0.013029765,-0.023276063,-0.008828616,0.023750056,-0.055812895,0.038356937,-0.01612894,0.0046590767,0.029124511,0.060593218,0.02863355,0.013499622,0.014023236,-0.06132517,-0.0033124762,-0.028276807,-0.011188709,-0.002211546,0.01117095,0.01586858,0.047225058,-0.04197036,-0.0009318685,-0.045196418,0.0172131,0.0060757883,-0.04156628,0.05019836,0.0012778442,-0.018421644,0.024333037,-0.011260163,-0.020127486,0.015021707,-0.032648288,-0.0004901364,0.01814558,0.022793373,0.024496585,0.0032043224,-0.069875486,0.014717374,0.030219294,-0.000480777,-0.03596504,-0.011089696,-0.00216521,0.021092534,-0.017034087,-0.038659927,-0.022260834,-0.015581495,0.03331057,-0.0036231894,0.001837048,0.0068646315,-0.007918691,0.034066968,0.0016998065,-0.028334904,-0.0052924915,-0.013658813,0.0051366775,0.022736719,0.0016232737,-0.021116901,0.0023199527,0.08018029,0.032946154,-0.036231596,-0.024168808,0.00886318,0.0033630747,0.0012845371,-0.053469893,0.02389015,0.0027277758,0.027380964,0.03929656,-0.016316343,0.004784897,0.06735342,-0.0020198305,-0.037240952,0.013268465,-0.010956646,-0.01746678,0.03624529,0.001749657,0.02547675,-0.020244496,-0.028780445,-0.04744671,0.015380293,0.039968487,0.039313175,0.0026661337,-0.037358046,-0.03714012,0.012050303,-0.052494932,-0.03385312,0.031251207,0.0033751987,0.008941967,0.036183923,-0.029349558,0.0012282373,-0.0049169404,0.014299628,-0.02446772,-0.01692882,-0.019112887,-0.010620898,-0.0025625573,-0.077456184,-0.021291943,0.004220746,0.010185972,0.01664739,-0.020448653,-0.030288324,0.013734003,0.06149645,0.08936129,-0.08975248,0.014322167,0.00048528076,0.0062826187,0.0012153137,-0.08581964,0.05005623,0.028745731,-0.02489556,0.031657387,-0.0008316099,0.03098453,-0.018805908,0.0017362147,0.0018563052,0.082158655,-0.030293118,0.024868948,0.049023245,0.049122263,0.0041344087,0.0166813,0.012812239,-0.03137235,-0.011663042,0.017291488,0.00014392997,-0.07715106,0.03354283,-0.026732313,0.007336366,0.05941152,-0.023708725,0.005148723,-0.012932278,-0.039674807,0.04051084,0.0054925797,0.009914671,-0.05451599,-0.08902359,-0.017406294,0.017343188,-0.0379591,-0.014991074,-0.079643026,-0.018586932,-0.01071524,0.006399437,-0.018702531,0.023732103,0.073017895,0.035787124,0.0034897728,0.02444905,-0.04435462,-0.015301187,0.06635832,0.002620073,0.019610725,-0.06352454,0.04462836,-0.052146655,0.06904617,-0.0065552527,-0.03388789,-0.0068383664,0.03644909,-0.03892056,0.0022086322,0.030201709,0.042819247,-0.045596916,0.010098443,0.030634655,0.028431451,-0.022384686,0.016603313,-0.0174287,-0.053019118,0.007801895,-0.028470382,-0.029789798,0.07817636,0.05326628,0.048695043,0.07645109,-0.045505602,-0.01614481,0.022503035,-0.018349053,0.06206951,-0.059640184,-0.012077094,-0.086153395,-0.023997514,0.029513612,-0.018144153,-0.00039522923,0.01529232,0.036301848,-0.04946564,0.059889022,-0.0063954643,0.0025827643,-0.09128892,0.02585736,0.07561946,-0.0049848934,0.08644873,-0.04654059,-0.04429601,-0.0016159464,-0.00226839,-0.021514751,-0.0018208682,0.0055603725,-0.033813104,-0.009789965,-0.012676766,-0.006933099,0.0646002,0.03257567,0.024718946,0.012415238,-0.0010530319,-0.015181076,0.012269677,-0.0038165695,0.02643626,-0.039850682,-0.0391729,0.011186425,0.032990023,-0.00839469,-0.06307524,0.07939286,-0.0050122207,0.031533007,0.018059665,-0.019601434,0.014739406,-0.04347552,0.0041537317,-0.025411714,0.038592663,-0.004106297,-0.019052299,-0.018449906,0.031809118,0.04873229,0.0024386914,0.02948083,0.06366979,-0.043271203,-0.03601614,0.01244914,-0.038293656,0.024964914,-0.0018197827,-0.049149927,-0.025475688,0.011462587,0.015827077,-0.047148332,-0.044527378,-0.013334374,-0.053675286,-0.0015566073,0.051877372,-0.011204297,-0.06700339,0.04030696,-0.023559164,-0.03161604,0.018416172,-0.009362152,0.017782085,0.005042105,-0.033924762,0.007421472,0.0023138667,-0.018879317,-0.019575436,-0.0020246957,-0.027203506,-0.025822181,-0.031822752,-0.04368399,-0.0052264207,0.035315745,-0.03391776,0.028225984,0.058639444,-0.0045242882,-0.003913988,0.0054712123,0.037774544,0.0268525,0.020490464,0.021481814,0.08215914,0.049785692,0.012514481,0.003938055,0.009930783,0.0039440077,-0.056091543,0.049850233,0.024965772,0.025118826,0.008582422,-0.02354031,0.026286151,0.040096104,0.024244696,0.02175509,0.01315942,-0.03882769,-0.011803642,0.026667766,0.014799917,-0.0028967676,-0.04412282,-0.0064888946,0.012028641,-0.008209216,0.023334527,-0.036580376,0.0065178024,-0.004655881,-0.02099315,0.028628057,-0.044521622,0.0442945,0.016670309,-0.009889829,-0.03059895,-0.03720952,0.021941846,-0.041243102,0.015121349,-0.0030643824,0.027448444,-0.06362707,0.029336395,0.013897212,0.032778755,0.016763683,0.047800925,-0.01407881,0.04188928,0.00646594,-0.06455212,0.006503789,0.007125934,-0.02811818,0.025603522,0.007642688,0.012756481,-0.035251494,-0.03942634,-0.037373416,-0.045320608,-0.015201333,0.0006705035,-0.0800592,-0.025553359,-0.008527806,0.03216017,-0.0058524422,0.04454452,0.009562999,-0.048158117,0.032662004,0.04246057,-0.041090034,-9.65447e-05,-0.010781037,0.052849106,-0.016792748,0.07118106,-0.016541777,0.026722316,-0.023076955,0.016833711,0.02972089,-0.05468782,-0.008654108,-0.028755195,-0.025183812,-0.0029933418,0.014857698,0.0014695228,-0.03140135,0.028662752,0.0064805574,0.059138265,-0.053796463,-0.03221564,-0.045260783,-0.033508927,-0.033295512,0.029635167,-0.0048772967,0.056133974,0.025660703,0.047251012,0.019367682,-0.017502286,-0.029521573,-0.021479145,-0.03638752,-0.08713282,-0.049346704,0.0053759986,0.008221904,-0.011158975,-0.072921306,0.06481483,0.018891027,-0.014493704,-0.061146326,0.020017514,0.05014054,0.013298207,-0.0681001,-0.024571093,0.031967796,0.016981427,0.012400771,0.060640566,0.013218167,-0.0586781,0.0130262,0.09059739,-0.0216474,-0.069121234,0.022784635,0.005871909,0.020304786,-0.07063585,0.06616975,0.0553777,-0.08236133,-0.022868523,0.016910786,-0.023277419,-0.0381182,0.033905156,0.0465263,-0.015032091,-0.008922488,-0.012953948,-0.0033629234,0.01741503,-0.011744389,0.008763752,0.003491963,0.080006346,0.08885555,0.02459232,-0.031136155,-0.072253145,-0.011557574,0.013851119,0.031885937,0.04117505,0.04432352,-0.00849348,0.03356517,0.042542256,-0.005914906,-0.017600285,-0.005369978,-0.06942017,0.037533294,0.0088217575,-0.018486978,-0.012991141,0.04444505,-0.020715356,0.03404495,0.03143366,-0.010810086,-0.06954884,-0.039117247,0.028142165,0.057702255,0.04295354,-0.011913751,0.02670312,0.049314238,0.00080526667,0.044718146,0.068211,-0.027342698,-0.08192457,-0.0350134,0.03609137,-0.010529775,0.03621123,-0.066294275,0.002701414,0.047258668,0.01573034,-0.01780638,-0.0063981996,-0.0067291865,-0.047741953,0.022182789,0.008497496,-0.019936476,-0.0009084634,0.003424255,0.013193415,0.0034965614,0.0010412368,-0.0005311047,-0.0051626647,-0.05798126,0.010507731,-0.010239476,0.013723866,0.054148786,0.012861917,0.013840314,-0.049666982,-0.017582541,0.028183557,0.047395654,-0.037658058,0.030280365,-0.028269246,-0.04459427,-0.0550892,0.04171709,0.015387257,-0.051323768,0.024639633,-0.055552594,0.056108523,0.04248649,-0.039098892,-0.016849635,-0.044855285,-0.036947142,0.0062056947,0.01844248,0.0019587728,-0.018168937,-0.017228698,-0.025462665,-0.008642599,-0.055886835,-0.06285634,-0.0008903987,0.00036003932,-0.012062224,-0.10981592,-0.054793365,-0.049229003,0.035058312,-0.012362073,0.005289787,0.048427228,-0.018696047]	Keywords: early stopping, validation set, training, optimization\nKey Objects: validation error, training process\nRefers to Images: None\nHypothetical Questions:\n- What does Geoff Hinton mean by 'beautiful free lunch' in the context of early stopping?\n- Why is it important to monitor error on a validation set during training?\n- What constitutes 'patience' when using early stopping?\n---\nSummary:\nEarly stopping, described by Geoff Hinton as a 'beautiful free lunch,' involves monitoring validation error during training and halting the process when improvement stagnates.\nOriginal Text:\n### 6.3 Early stopping  \nAccording to Geoff Hinton: 'Early stopping (is) beautiful free lunch' 16 . You should thus always monitor error on a validation set during training and stop (with some patience) if your validation error does not improve enough.\nContextualized Text:\nDuring training, it is recommended to use early stopping, a technique described by Geoff Hinton as a 'beautiful free lunch.' This involves monitoring error on a validation set and stopping the training process, with some patience, if the validation error does not improve sufficiently.	{"tags": ["optimization", "training", "validation"], "doc_id": "121f4e8c-9bbe-44a4-8dc3-3d47ffa4b31e", "summary": "Early stopping, described by Geoff Hinton as a 'beautiful free lunch,' involves monitoring validation error during training and halting the process when improvement stagnates.", "doc_type": "text", "entities": ["Geoff Hinton"], "keywords": ["early stopping", "validation set", "training", "optimization"], "key_objects": ["validation error", "training process"], "contextual_text": "During training, it is recommended to use early stopping, a technique described by Geoff Hinton as a 'beautiful free lunch.' This involves monitoring error on a validation set and stopping the training process, with some patience, if the validation error does not improve sufficiently.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "6 Additional strategies for optimizing SGD", "h3": "6.3 Early stopping"}, "hypothetical_questions": ["What does Geoff Hinton mean by 'beautiful free lunch' in the context of early stopping?", "Why is it important to monitor error on a validation set during training?", "What constitutes 'patience' when using early stopping?"]}
69d0ffbc-b22e-4ea6-8f04-3d9d816ea12d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0011223622,-0.0050736023,0.00809737,0.033964917,-0.016696561,0.07466749,0.038563773,0.011201354,0.007415728,-0.054501038,-0.014194882,-0.047950428,0.0008040518,-0.027516855,-0.02715037,0.01329384,-0.021036439,0.077953465,0.0044529415,-0.027873967,0.08914511,0.03596648,0.021814456,0.022854708,0.026271876,0.05028647,0.016451174,0.03550515,0.021048987,-0.017953878,-0.0020457082,0.0012144875,-0.04776028,0.010431769,0.011523287,0.006774743,0.0028147853,-0.011860026,0.0011455236,-0.04308823,0.0055726683,0.028668307,-0.04988367,-0.011793599,0.024228234,-0.030391099,-0.047712456,-0.065682895,-0.011744782,-0.014830603,-0.030946322,0.081249066,-0.028970081,0.0004614775,-0.015199994,-0.0030684092,0.0029235631,-0.023971247,0.018800545,-0.036810417,-0.042694762,0.0018281207,0.03348644,-0.028740669,0.06194003,-0.032341722,0.009926641,0.07699603,0.037695386,0.13617231,0.027882602,0.056335352,-0.04324863,-0.067560054,0.11356368,0.045106217,-0.020157296,-0.008602461,-0.05149436,-0.0010596762,-0.014764346,0.015070408,-0.016388165,-0.044190817,0.049928956,-0.002717006,-0.07085513,-0.067760035,0.0682426,-0.03580805,0.0085021,0.0022604866,0.024072887,0.05473698,-0.027023876,-0.030652724,0.04236201,-0.029593427,-0.012002717,-0.04276245,0.060001627,0.0053058458,0.046677973,0.14299387,0.030900063,-0.0075186095,-0.03357513,-0.06254866,0.034140132,0.028546933,-0.040022727,0.07709206,-0.039579045,0.02934789,-0.007091709,-0.032987054,-0.050257985,0.016053755,0.020667681,-0.0022048291,0.07411426,-0.022949057,-0.01073463,0.031951603,0.04359924,0.029172944,-1.7398233e-05,-0.006122039,-0.025984671,-0.020091195,0.00663079,-0.016078003,-0.014481984,-0.019306555,-0.0038264915,-0.0018805075,0.07355041,-0.032238126,-0.010573615,0.011703201,-0.0011774802,-0.007617928,-0.033547442,-0.016459111,-0.009840076,0.012807697,-0.07041043,0.021183642,-0.024965642,0.03388944,0.030055387,-0.0485059,0.0496968,-0.05303822,-0.048211213,-0.063556455,-0.028012045,-0.0023085072,0.0049831024,0.0012186198,-0.024934966,-0.015916254,0.0034164072,0.06076734,0.072038926,0.104150906,0.009086446,-0.023029752,-0.01620118,-0.00028718752,-0.013164843,-0.02506339,0.042317137,0.006091047,0.024914147,-0.049677104,0.034788072,-0.048712812,-0.01840977,-0.006973671,-0.053045202,0.013552526,-0.044671636,-0.011748095,-0.04908379,-0.009471696,0.0063518304,0.047482748,-0.079473026,-0.038292903,-0.0029380505,-0.016326332,-0.013458477,0.0018021748,0.010800483,0.04576989,0.025930708,-0.040470257,0.014254683,0.00063710794,0.02003113,0.00944148,-0.034998365,0.014619105,-0.021146297,-0.06209411,0.033195175,0.024738172,-0.015065597,0.05009252,0.0007502971,0.018812452,-0.015325307,0.042453546,-0.017703444,0.0016055601,-0.0855963,-0.007176919,0.05232227,-0.010306184,-0.041512795,-0.014426232,0.052135758,0.029577065,-0.03338424,-0.011782581,-0.020921338,-0.024798771,0.023639947,-0.020227654,-0.016005367,0.01699089,-0.051806547,0.05563676,-0.06601005,-0.003932979,-0.007019304,-0.00551984,0.0052202744,0.02999986,0.060608216,0.026790284,-0.03809835,0.06234862,0.003986582,-0.03106216,0.009506561,0.022598432,-0.015961196,0.02776063,-0.050738234,0.029772941,0.038514912,0.002381943,-0.0018877884,-0.034315802,0.04876322,0.021771839,0.020057175,-0.05023983,-0.025241813,-0.042958144,0.011564258,0.0052226353,0.019074716,0.0025834432,0.04506885,-0.017155714,-0.032843173,0.0034293968,0.042102933,0.03206855,0.0042628823,-0.0249911,-0.04893843,0.041596416,0.03592812,-0.007921246,0.023237715,-0.0039424472,0.026056282,0.0007819426,0.04081329,-0.00053882657,-0.01635643,0.016264556,-0.005800609,0.031769674,-0.052248415,0.022636017,0.016288267,-0.030114558,0.02710406,0.029628629,-0.042074732,0.005651171,0.004748638,-0.019765455,0.04958028,0.045153975,0.087140635,-0.018191459,-0.018127745,0.027785242,0.006310634,0.02491643,-0.049819168,0.033682678,0.029909324,0.036736615,0.024168093,-0.009737534,-0.00844262,-0.015180511,-0.030309264,0.04566506,0.07150419,-0.017283378,0.020857789,0.035673212,0.030697094,0.021364937,-0.009115368,0.054810528,0.04079995,-0.008643067,-0.023256995,-0.008747112,-0.08168754,0.07863362,-0.060860183,0.06349835,0.05347648,0.0021445707,-0.00025358936,-0.03192503,-0.04299085,-0.016323378,0.050755866,0.02077478,-0.020344589,-0.05236432,0.038051598,-0.010384415,0.00987137,-0.0017942666,0.059666645,0.011436,0.050445415,0.034386482,0.00749023,0.030803697,0.05736737,-0.0064388984,0.052978,0.03230394,0.025591072,0.040785775,0.015414254,-0.0047240816,0.043792777,-0.057250198,0.049060963,-0.04846309,0.046219964,0.021971652,-0.016187875,0.032922797,0.039304756,0.017990474,0.04682916,-0.04036117,0.07661158,-0.05667482,-0.0022336417,0.050646022,0.059897594,0.016271451,-0.019375773,-0.054088213,-0.0038112258,0.044589747,-0.025588825,-0.016692331,0.012697723,-0.036084525,0.0058186157,0.09664191,-0.018351678,0.0050459695,-0.037348874,0.03363716,0.0050066747,0.0045081554,-0.047664005,0.012248583,0.04184141,0.029336102,0.021771492,-0.01961992,-0.009106722,0.019041615,-0.07761678,0.03922959,0.035687294,-0.013208132,-0.052717548,0.0011440403,0.0142966835,-0.0028358446,0.06304974,-0.032353126,-0.058177505,0.008063622,-0.04686835,-0.031877697,-0.025875483,0.017785758,-0.026422434,-0.0048410506,0.025300262,0.03794532,0.032699447,0.0177457,0.012979297,0.014976142,-0.032171965,0.02226296,-0.022628138,-0.0443866,0.00648619,-0.04838036,0.01327095,0.050052576,0.041337203,-0.025974939,-0.032585807,0.07897951,-0.056453217,0.04107598,0.040740304,-0.020874435,0.028860705,0.0037816018,0.022884706,-0.014496062,0.037290405,-0.031907514,0.0009618666,-0.009543591,-0.00847358,-0.018135801,0.020699136,0.0110036,0.02436574,-0.017878648,0.020752473,-0.038468845,0.0019723708,-0.005971114,0.01731418,-0.0075432323,-0.0012010008,0.018515442,0.02333556,-0.03351512,-0.00893452,-0.05389808,0.008071753,-0.010701343,0.012575174,0.023165716,-0.03484956,0.07509485,-0.0020851025,0.032940127,0.023377204,0.015847612,0.0110011045,0.0013780168,-0.0718574,-0.016772795,0.036043987,0.019259455,-0.011858073,-0.04395116,-0.056109563,-0.044415575,-0.014211546,-0.0033136932,0.045333005,0.03575426,-0.008849095,-0.021711363,-0.008760196,-0.045038275,0.019879073,-0.055733655,0.028548341,-0.019949168,-0.009531713,0.01151161,0.05607428,0.03841981,-0.008688428,0.044328876,0.032152943,0.08890612,0.001602154,0.0072342614,0.04275695,0.027751246,0.038968332,-0.016183216,-0.008007477,0.040361244,0.027984621,-0.018336,0.023071462,-0.014552014,-0.0037348918,0.02673584,0.017176729,0.022779342,0.03424247,-0.0066412324,-0.008507175,0.031346366,0.052571587,-0.04558407,0.03247956,0.0013333095,0.025797507,0.012182215,-0.10332227,-0.04106642,-0.017150884,-0.032560356,0.0121184,0.023143008,-0.022421615,-0.05322168,0.018287007,-0.020557173,0.06684431,-0.04139775,0.027619967,0.021506762,0.02873511,0.10085869,0.034099676,-0.046777107,0.016499039,-0.041961607,-0.0018126336,0.002263837,0.04181368,-0.0044643222,-0.009035746,-0.003590832,0.03651344,-0.030360121,0.013996734,-0.020081911,-0.0072407005,0.006265599,-0.00023576681,-0.06418952,-0.015662646,0.005910392,-0.017074868,0.026284518,-0.018396754,-0.00024247043,-0.004204711,0.07425366,0.044461463,-0.005783669,-0.0045938254,0.013708843,0.010969343,-0.0041011428,0.03802757,-0.00084884756,-0.0047639557,0.059002522,0.016108772,0.027502595,-0.012776289,0.034049653,-0.026181912,-0.037068345,-0.028093,-0.031942,-0.019834192,-0.038523503,0.029724143,-0.01664555,0.0508329,0.00608999,-0.09447318,-0.0037802563,-0.028945856,-0.009726246,0.048104744,0.005609466,-0.0063871234,0.03282241,-0.020960916,0.0586784,-0.09164181,-0.023464235,-0.017875358,-0.0056352,-0.10934891,-0.0046617365,0.0122967595,-0.030847315,-0.037709653,-0.03643336,-0.002869551,0.031780627,-0.029969228,0.046572424,-0.008132924,0.013497076,0.022000425,-0.022229912,0.018586066,0.017445361,0.030003281,-0.030450737,0.0803278,-0.07606401,-0.020335412,-0.017946014,0.025078986,-0.08137068,-0.025184765,0.0455845,-0.02044035,-0.0029991914,-0.0073205894,0.026432283,0.01960909,-0.036470234,-0.017526554,-0.06993188,0.042612437,0.007182967,0.021242913,0.089535765,0.048897993,-0.026899155,-0.056187753,-0.019219471,0.00142936,0.03613532,0.011666099,0.016487237,0.057434916,0.062107734,-0.037315384,-0.07154146,-0.06314517,-0.028441252,0.05455007,0.042314272,0.069527455,0.0033280212,-0.026061269,0.0002964642,-0.011042843,-0.012755951,-0.0043513654,-0.0099072,-0.014464506,0.028174438,0.0060657617,-0.012582699,0.01910256,0.022117216,-0.006272166,0.068192914,0.04292366,-0.028525375,-0.0864016,-0.065274864,-0.020692643,0.0076479088,0.011699134,-0.022338947,0.0034098616,0.02218298,-0.0254528,0.03524207,0.030270914,-0.04762994,-0.045782767,0.025331693,-0.0052770926,0.057851207,-0.010556921,-0.05071883,-0.054631434,0.017956045,-0.01575697,-0.04136066,-0.036994606,0.010258106,-0.008682892,-0.029555222,-0.011921404,-0.019335,-0.0050965846,0.014030184,-9.845579e-05,0.047470607,-0.010984063,0.0057485034,-0.042507585,-0.04444983,-0.02105051,-0.015904367,0.008538425,0.057753056,0.013945466,0.039087906,-0.016788257,0.020407703,0.019716596,-0.005160096,-0.0059630247,-0.013907151,-0.018132068,0.04693099,-0.027762422,0.0039775083,0.055023637,-0.015722156,-0.0012179747,-0.01963261,0.071929336,0.054807138,-0.0062521715,0.015952386,-0.039562788,-0.029640323,-0.0423778,-0.031386048,0.004124909,0.030874996,0.002595008,-0.051151074,-0.03292422,-0.01068134,0.0073963497,0.060465887,0.023021445,-0.021977574,-0.06830681,0.03329466,0.0096147945,0.057153314,-0.03364629,0.005193676,0.046757378,-0.015571657]	Keywords: gradient noise, Gaussian distribution, variance schedule, local minima\nKey Objects: gradient updates, variance, local minima, networks\nRefers to Images: None\nHypothetical Questions:\n- Why is gradient noise helpful for training deep networks?\n- What is the purpose of annealing the variance of the noise?\n- How does gradient noise relate to finding better local minima?\n- What are the potential drawbacks of adding noise to gradient updates?\n---\nSummary:\nNeelakantan et al. [13] proposed adding Gaussian noise to gradient updates, with a variance schedule, to improve network robustness and training, particularly for deep networks.\nOriginal Text:\n### 6.4 Gradient noise  \nNeelakantan et al. [13] add noise that follows a Gaussian distribution N (0 ,  2 t ) to each gradient update:  \n<!-- formula-not-decoded -->  \nThey anneal the variance according to the following schedule:  \n<!-- formula-not-decoded -->  \nThey show that adding this noise makes networks more robust to poor initialization and helps training particularly deep and complex networks. They suspect that the added noise gives the model more chances to escape and find new local minima, which are more frequent for deeper models.\nContextualized Text:\nTo enhance network robustness and training, particularly for deep and complex networks, Neelakantan et al. [13] introduced a technique called gradient noise. This method involves adding Gaussian noise, following a distribution N (0, ), to each gradient update. The variance of this noise is annealed according to a specific schedule, providing the model with more opportunities to escape and discover new local minima.	{"tags": ["optimization", "deep learning", "robustness", "training"], "doc_id": "69d0ffbc-b22e-4ea6-8f04-3d9d816ea12d", "summary": "Neelakantan et al. [13] proposed adding Gaussian noise to gradient updates, with a variance schedule, to improve network robustness and training, particularly for deep networks.", "doc_type": "text", "entities": ["Neelakantan et al."], "keywords": ["gradient noise", "Gaussian distribution", "variance schedule", "local minima"], "key_objects": ["gradient updates", "variance", "local minima", "networks"], "contextual_text": "To enhance network robustness and training, particularly for deep and complex networks, Neelakantan et al. [13] introduced a technique called gradient noise. This method involves adding Gaussian noise, following a distribution N (0, ), to each gradient update. The variance of this noise is annealed according to a specific schedule, providing the model with more opportunities to escape and discover new local minima.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "6 Additional strategies for optimizing SGD", "h3": "6.4 Gradient noise"}, "hypothetical_questions": ["Why is gradient noise helpful for training deep networks?", "What is the purpose of annealing the variance of the noise?", "How does gradient noise relate to finding better local minima?", "What are the potential drawbacks of adding noise to gradient updates?"]}
d2cc3341-0eb1-4f93-ab91-fd8839928155	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0044272803,-0.02980065,-0.038095027,-0.011732966,-0.019024964,0.049669717,-0.032567687,0.026521172,0.038112346,-0.07031359,-0.00040364024,-0.024046907,-0.028966976,-0.013690175,-0.056670334,0.015499598,-0.010592023,0.083006255,-0.012457784,-0.015198833,0.0702262,0.04881461,0.0613949,0.0047564497,-0.0291013,0.0036518977,0.041400056,0.007532152,0.008957801,0.0007933114,-0.05109906,-0.011358069,-0.00423821,0.0041250694,0.0060698963,0.0116520515,-0.013181395,-0.02460833,-0.0074262074,-0.073433466,-0.05274711,0.028759783,0.00348683,-0.03301285,-0.0125882,-0.07249886,-0.050834473,-0.016922813,-0.007940836,-0.0073033744,-0.029435167,0.061016314,-0.03530863,0.028115353,-0.038451564,0.025413737,-0.049068596,-0.02680846,0.015506322,-0.025954442,-0.04315764,0.03531604,0.025853358,-0.0038912261,0.06162561,-0.043283533,-0.023863025,0.039483514,0.04981636,0.097976066,-0.0014953307,0.073134765,-0.0075495364,-0.10016588,0.10746757,0.045314535,-0.039710317,-0.065724626,0.0041512237,-0.0234528,0.030575411,0.04270163,-0.062395256,-0.029390577,0.061349563,0.00691205,-0.06792209,-0.028192215,0.09562286,-0.062894054,-0.041438267,-0.014387717,-0.057213906,0.07469385,-0.032121062,-0.01121587,0.031873595,-0.02956918,-0.016105697,-0.022212192,0.03643858,-0.004981783,0.005847653,0.09445278,0.0023778565,0.0440791,-0.044327024,-0.0032128892,0.0071381815,-0.0025881287,0.02617271,0.028254988,-0.030184954,0.009038625,-0.027621549,0.0013317885,-0.0068187434,0.024178838,-0.008753995,-0.055972207,0.06969895,-0.01829378,-0.022783756,0.012762437,0.060590707,0.04751364,-0.046921067,0.003689409,-0.060166184,-0.014693559,0.008742927,-0.01097725,-0.0462919,-0.030169027,0.018957872,-0.047055863,0.011291678,-0.0008658599,-0.010955307,-0.01048371,-0.051348563,-0.04207268,-0.036333438,-0.00591115,0.0020506424,0.025757777,-0.08694745,0.04801802,-0.01875564,0.06573253,-0.009067581,-0.004075131,0.053636786,-0.030316457,-0.062459864,-0.064470805,-0.037192814,0.01911328,-0.01094331,-0.02449049,-0.03412912,0.012420461,0.0039481716,0.033734385,0.04195534,0.11583567,-0.018353136,0.03741947,-0.007859578,0.01058354,-0.047874957,-0.010294899,-0.0009749435,0.010455097,0.0044183917,-0.044906642,-0.0030291001,-0.023385063,-0.01800514,-0.00036457612,0.014147065,-0.005265638,-0.043285705,-0.0032398803,0.017790781,0.019477895,-0.030600348,0.03728114,0.03107778,-0.022560174,0.021465803,0.018953348,-0.0094066225,-0.007890416,0.044360828,0.00052543,0.023372373,0.002842401,0.009233128,-0.009853174,0.0045226356,0.047012918,-0.04230488,-0.033881582,0.039443888,-0.05684609,0.004979369,-0.03654458,-0.09163396,0.022747971,-0.037565153,0.020955961,0.011461057,0.032384582,-0.00044058415,-0.010586506,-0.030169502,-0.022324199,0.024477772,-0.0014663965,-0.052654766,0.006857877,-0.03190506,0.041443795,-0.037846103,-0.01954794,0.026326379,-0.005464267,0.072116114,0.0063052364,-0.019870315,0.041124392,-0.030733572,0.036890045,0.019355744,0.0033856735,0.016369728,0.0005524195,0.0051476024,0.043607138,0.033397444,0.036465123,-0.0024939994,0.1056573,-0.015091475,-0.040499974,0.012255665,0.020707557,0.037961215,-0.00047352246,-0.07633332,0.003058383,0.006057459,-0.017839653,-0.015705423,-0.011524232,0.06318294,0.040974107,0.05448684,-0.041766044,-0.037297823,-0.031159133,-0.009823247,0.040144525,0.006084543,0.051398546,0.031618893,0.025740555,-0.02256165,0.06118049,-0.010486983,0.028292084,0.03702252,-0.041369032,-0.03353438,0.04403467,0.013580574,0.016151726,0.019386819,0.0053849285,0.05386964,-0.0027140025,0.025958985,-0.0019273241,-0.02371169,-0.0009206694,0.003571385,-0.024476286,-0.031308338,0.03827315,0.024263125,-0.047214042,0.054365613,-0.0077178646,0.00051468087,0.033660613,-0.0052224537,-0.027833572,0.06449329,0.022621384,0.07709618,-0.06560864,0.028969368,-0.008391967,0.018842397,-0.0067146225,-0.09555999,0.029390112,0.029462293,0.023040432,0.06907963,-0.0042727776,0.007317465,0.00973543,-0.0078968,-0.021560501,0.065743156,0.01587616,0.0031600061,0.003779981,0.034577474,-0.016152648,-0.026774615,0.04636938,-0.008219499,-0.0048964485,0.0314858,-0.017691866,-0.049854398,0.07189752,-0.05141274,0.04016297,0.04236444,-0.008759031,-0.019180495,0.044684764,-0.074330315,0.041094184,-0.014733783,-0.008992484,-0.032930333,-0.030513233,-0.039595965,-0.021618605,-0.034390748,-0.02760976,-0.049365584,0.03263247,0.003189791,0.035289526,-0.038343668,0.06262775,0.060132716,0.040259417,0.0072181425,0.016906014,-0.0037472623,0.03233168,0.04262455,-0.02619431,0.01562932,-0.048928667,0.0661812,-0.06473993,0.054850888,0.032194525,-0.043709435,0.011569927,-0.033033617,-0.019198295,0.027937729,0.0047044815,0.03083502,-0.09341514,-0.0050069545,0.019781424,0.04028352,0.045012575,0.00854468,-0.08253666,-0.017121641,0.017646322,-0.005783275,-0.041709393,0.0052736937,0.008163273,0.028295275,0.050140683,0.008693974,-0.009164273,-0.021310609,0.01441918,0.013584913,0.019270504,-0.049379665,-0.017491996,0.017208477,0.044872966,0.05351512,-0.05818711,-0.028060064,0.09498374,-0.05866424,0.064800024,-0.02358631,0.014769558,-0.07172684,0.017696438,0.03932784,0.009307237,0.043216918,-0.044410173,-0.05129622,0.00952974,7.3277647e-06,0.010482972,0.01693386,0.014613165,-0.03778676,-0.021035431,-0.03874366,0.06346558,0.03534781,0.045009967,0.021449277,0.02489382,-0.017189272,0.028995065,-0.01879985,-0.033005357,0.018680636,0.009752741,-0.04166055,0.073331095,-0.0047211647,-0.012348267,-0.01817972,0.04061268,0.0038052576,0.0485508,0.029872527,-0.0051600183,0.0373544,0.016598161,-0.01898569,0.023586765,0.0035510487,0.008181498,0.021189528,0.010722652,-0.0016394196,-0.01792416,-0.0035403103,-0.0016227576,0.029341504,-0.008924811,0.014371878,-0.014409026,-0.0072945333,0.01757454,-0.022242434,0.06517938,0.00040321925,0.05005743,-0.015221659,-0.011133219,-0.045982804,-0.06183485,0.02401985,0.008631484,-0.035912726,0.06827878,-0.036048323,0.01378956,-0.0044590333,0.04643417,0.04344762,0.05025606,0.09595035,0.012252465,-0.022676656,-0.024456525,0.03359096,0.05490317,0.024627967,-0.06615972,-0.015853517,0.009620419,-0.02254461,0.03182507,-0.011363174,-0.012732883,-0.0032891687,0.040674593,-0.007135744,-0.054750238,0.004721248,0.0025562427,0.028841443,-0.06858202,-0.039232306,-0.040041663,0.08464031,0.07095937,-0.021489851,0.01283936,-0.0072215656,0.070914164,0.010400479,0.05222671,-0.014717699,0.020366853,0.0012379788,-0.020594018,0.005673068,0.022748077,0.004023726,-0.008786445,0.01802477,-0.009582756,-0.017584637,0.058493212,-0.0003769894,-0.038880758,0.008071697,-0.038520582,-0.018435933,0.011623043,0.07037897,-0.0017563977,-0.015615729,-0.012768291,0.008575314,0.0071322336,-0.06668836,-0.006982176,-0.017169101,-0.031404693,0.023908397,-0.013974318,-0.03350089,-0.016275689,-0.03314673,0.027179621,0.052449718,-0.06659198,0.037506606,0.02130651,-0.009155946,0.09004856,0.020719582,0.015629055,0.049490463,-0.043607205,0.015930887,0.039600395,-0.03211837,-0.004528654,-0.011919784,0.026359187,0.009264079,-0.0038758796,0.027442265,-0.050506722,-0.023272093,0.0018652242,-0.01688793,-0.06850153,-0.041930214,0.0070944326,0.011770254,0.012679778,0.006740401,0.022048501,-0.018971115,0.044816494,0.053159736,-0.008860411,-0.04169825,0.045810267,0.029422425,-0.019382544,0.041597433,0.017589763,-0.0064929174,0.04863931,0.03379468,-0.040791556,0.01323696,0.00743377,-0.0032573165,-0.00746509,-0.011765182,-0.03606755,-0.017803976,-0.056998014,0.05816593,0.02760478,0.011035978,-0.013357714,-0.004286779,0.043807823,-0.06139121,-0.026034072,0.03152413,-0.003380948,-0.01811767,-0.0038141403,-0.02921808,0.046959866,-0.049116753,0.0011328413,0.001492822,-0.010271863,-0.085068695,-0.02147391,-0.033412784,-0.019788202,0.015776677,-0.04460323,0.017513525,-0.0062833917,-0.0474784,0.04937229,-0.07424225,-0.025005633,-0.048698593,-0.002045935,-0.032825097,0.005891291,0.022858514,-0.0113494275,0.042359203,-0.039575193,-0.038509525,0.03605688,-0.0165496,-0.035737123,-0.022018492,0.04096937,-0.034975428,0.0048850686,-0.003425495,0.042902157,0.025312997,-0.015968995,-0.07033439,-0.061511155,0.020333793,-0.043062136,0.034176037,0.07581639,0.019906836,-0.016894277,-0.060548943,-0.036560442,-0.03633721,0.012065939,0.033722512,-0.011281619,0.06757652,0.045419354,0.021172633,-0.030284015,-0.016678678,0.023984974,0.061386984,0.0055888942,0.014598294,0.040389977,-0.027390778,-0.02071119,-0.013766023,-0.0382126,-0.0016001754,-0.01597692,0.0005425688,0.023054259,-0.027830668,-0.06090246,0.05794123,-0.039699335,0.015074388,0.10357796,0.007244416,-0.027210623,-0.064383596,-0.056181893,-0.002580064,-0.030885717,-0.0013578956,-0.03641193,0.012527282,0.020802818,-0.0030158907,0.03102911,-0.0058181826,-0.011961794,-0.012372474,0.040868763,0.012597209,0.0251662,0.052263826,-0.080129854,-0.0498207,0.05889219,0.010268818,0.00750105,-0.006446731,0.006728561,-0.0035974816,0.018858176,0.0013246171,-0.011887458,0.015409663,0.0006532388,0.021856256,0.010592663,0.0042215064,0.011866215,-0.03731843,-0.039599936,0.020571208,0.028952163,-0.01885839,0.03655732,0.0013306055,0.072205424,-0.01204219,0.0050145197,0.079611614,0.0048947707,0.028633565,0.026890548,0.025459329,0.03876337,-0.010497782,0.015208434,0.03240755,-0.031481408,-0.009223062,-0.011801047,0.05363187,0.056713965,-0.020841299,0.027886888,-0.020154707,-0.038740676,0.011337149,-0.040298656,-0.010750721,-0.0051524267,0.032248925,-0.027687903,-0.046037406,-0.027753405,-0.021389863,0.032618172,0.035872713,-0.005621036,-0.043051716,-0.011992476,0.00044014637,0.027438598,-0.057860635,0.014104191,-0.008189353,0.0271324]	Keywords: gradient descent, SGD, optimization algorithms, minibatch gradient descent\nKey Objects: gradient descent, SGD, optimization algorithms\nRefers to Images: None\nHypothetical Questions:\n- What are the advantages of minibatch gradient descent over other gradient descent variants?\n- How do algorithms like Adam improve upon standard SGD?\n- Why is shuffling considered a useful strategy when optimizing SGD?\n---\nSummary:\nThis article provides an overview of gradient descent optimization algorithms, covering variants of gradient descent, optimization algorithms for SGD (like Momentum and Adam), and strategies to improve SGD's performance.\nOriginal Text:\n## 7 Conclusion  \nIn this article, we have initially looked at the three variants of gradient descent, among which minibatch gradient descent is the most popular. We have then investigated algorithms that are most commonly used for optimizing SGD: Momentum, Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam, AdaMax, Nadam, as well as different algorithms to optimize asynchronous SGD. Finally, we've considered other strategies to improve SGD such as shuffling and curriculum learning, batch normalization, and early stopping.  \n16 NIPS 2015 Tutorial slides, slide 63, http://www.iro.umontreal.ca/~bengioy/talks/ DL-Tutorial-NIPS2015.pdf\nContextualized Text:\nThis article concludes by summarizing the discussed gradient descent optimization algorithms. It initially explores the three variants of gradient descent, with minibatch gradient descent being the most popular. The article then details commonly used optimization algorithms for Stochastic Gradient Descent (SGD), including Momentum, Adam, and others, concluding with a review of techniques to enhance SGD, such as shuffling and early stopping.	{"tags": ["optimization", "machine learning", "deep learning"], "doc_id": "d2cc3341-0eb1-4f93-ab91-fd8839928155", "summary": "This article provides an overview of gradient descent optimization algorithms, covering variants of gradient descent, optimization algorithms for SGD (like Momentum and Adam), and strategies to improve SGD's performance.", "doc_type": "text", "entities": [], "keywords": ["gradient descent", "SGD", "optimization algorithms", "minibatch gradient descent"], "key_objects": ["gradient descent", "SGD", "optimization algorithms"], "contextual_text": "This article concludes by summarizing the discussed gradient descent optimization algorithms. It initially explores the three variants of gradient descent, with minibatch gradient descent being the most popular. The article then details commonly used optimization algorithms for Stochastic Gradient Descent (SGD), including Momentum, Adam, and others, concluding with a review of techniques to enhance SGD, such as shuffling and early stopping.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "7 Conclusion"}, "hypothetical_questions": ["What are the advantages of minibatch gradient descent over other gradient descent variants?", "How do algorithms like Adam improve upon standard SGD?", "Why is shuffling considered a useful strategy when optimizing SGD?"]}
684b65ab-5067-4383-bfe4-142e35bdc81b	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0032008912,0.0019049954,-0.06977194,-0.004785431,-0.03511094,0.032112025,0.0019406175,0.04441141,0.06763803,-0.031123055,-0.013584018,0.01508118,0.012260003,-0.008675719,-0.054520354,0.047326665,0.0044813976,0.075142995,0.040702663,0.020086387,0.027784193,0.039922833,-0.022595681,-0.0045128036,-0.017419508,0.0068281293,0.0374,-0.013170613,0.019715523,0.046026815,0.033648897,-0.035662826,0.023009192,-0.0018669316,0.021841224,0.014984848,-0.005144078,0.03616876,0.027312333,-0.06552007,-0.008117286,0.032803457,-0.03618629,-0.018947955,0.01749078,-0.058035463,-0.03528105,-0.024159215,0.01825499,-0.0043048323,-0.019149944,0.032639906,-0.03972904,-0.014104178,-0.015875451,-0.03350892,-0.0016758728,0.0035797714,0.022187782,-0.074457146,-0.07903153,0.061116427,0.02161208,-0.048480842,0.04180296,-0.068002954,-0.07586411,-0.014268445,-0.003241801,0.09375347,-0.014076335,0.06240466,0.013943141,-0.10252838,0.0996883,0.04445988,-0.000504534,-0.015660139,-0.065332584,0.011274231,0.041446466,0.042776212,-0.049804572,-0.03644303,0.06539781,0.036685575,-0.08132239,-0.03525662,0.08212298,-0.032289352,-0.020666918,0.0025086743,-0.023429185,0.06467991,-0.022837184,-0.051992737,0.0035235847,-0.049618736,-0.046514608,-0.05352035,-0.018076217,0.03418454,0.043136794,0.098420545,0.020599375,-0.035957932,-0.05632199,-0.023318006,-0.0038542845,-0.012989103,0.024315277,0.0008382726,-0.018578103,0.011538876,-0.040543623,-0.026200185,-0.06365186,-0.0065202625,-0.052244175,-0.04776469,0.04287982,-0.019282874,0.0212644,-0.0010803506,0.03240647,0.05267568,-0.023974372,0.027643252,-0.0391452,-0.015879892,0.002012265,-0.0020885104,-0.0093523385,0.0084693255,0.05786617,-0.0059112953,0.07334427,-0.04653174,-0.017313845,0.009985729,0.011569023,-0.046520904,-0.018950075,0.022391802,0.0002728226,0.03701341,-0.043524303,0.017196428,-0.019215038,0.037657037,0.0096114725,0.022073615,0.028570032,0.014514001,0.0076602446,-0.0028385625,0.01830478,-0.043912124,0.014085012,0.008008033,-0.064123884,-0.038956925,0.024861246,0.02420516,0.045155138,0.11507492,-0.0069539244,0.01479668,0.03240251,0.043529052,-0.018442996,0.020633958,-0.018523829,0.009674732,0.035656486,-0.031386893,0.04500273,0.05633085,-0.02514541,0.0018391677,-0.0028363036,0.025218897,-0.01918701,0.021356337,0.0011475078,0.029338129,-0.02630769,0.049635697,-0.008921665,-0.055170685,0.016625185,-0.0062830867,0.017946757,0.0057794545,0.03835345,-0.005790591,-0.007622312,-0.036597647,-0.048815485,-0.04332745,-0.019814108,0.04234343,0.005384135,0.005420799,0.019035475,0.0008244737,-0.049841326,-0.039714783,-0.035425067,-0.041364837,-0.0013155424,0.0103243925,0.010197263,0.063751325,0.021018112,0.00019969289,-0.016746365,0.02207469,0.021056207,0.022681726,-0.074306004,0.015257368,-0.023758551,-0.0015744043,-0.071240745,-0.014982937,-0.030501246,0.035356842,0.038213816,-0.027314952,0.0022236265,0.021073326,0.0036277578,0.018555071,-0.0027950974,0.026719118,0.005380746,0.0024979047,0.02822409,0.06920825,0.0061362362,0.03593428,0.051997047,0.0952813,-0.007914878,0.015552005,0.06152092,0.06640295,0.016946409,0.04927387,-0.01926784,0.0064840373,0.012922258,-0.023447072,0.03561898,-0.017003782,0.022308944,0.020633262,0.06252368,0.0063474094,-0.017020974,-0.023640968,0.014638208,-0.0043404475,0.022113245,0.042209473,0.01266727,0.03019496,0.017649557,0.042026527,0.03987512,0.031430144,0.016646625,-0.023601886,-0.0996259,0.029621057,0.0013551944,0.043160792,0.034868762,-0.0114355935,0.0413675,-0.0044622933,0.021071322,-0.02071632,0.010305596,-0.031397674,-0.02809412,-0.02703685,-0.05484197,0.015442588,0.037855882,-0.052756637,0.038225345,-0.014279688,-0.021422733,0.056465838,0.01952396,-0.05031178,0.023406154,0.04580365,0.09459424,-0.014548004,0.06994952,-0.024186593,-0.017695375,-0.008477404,-0.094985686,0.03188043,0.027078837,0.023683459,0.03469329,-0.018283445,0.025388585,-0.053003293,0.011846465,0.019635016,0.07877152,0.022383694,-0.0024637808,0.032813936,0.04370764,0.031789497,0.006831507,0.07258571,0.0015009546,-0.03381549,-0.018174214,-0.020065399,-0.08305629,0.05320853,-0.02998072,0.027379254,0.02245482,0.01973108,0.0015888509,-0.030007457,0.015311821,0.021169724,0.03506958,0.047595937,0.04101991,-0.064686425,0.017210616,-0.009130248,-0.042137094,0.008413823,0.025283774,0.03466958,0.039849196,0.042805497,-0.01702858,0.020538999,0.058578458,0.0058996137,-0.0153067345,-1.9230789e-05,0.007204387,0.032905035,-0.032967914,0.02546039,0.055524062,-0.029411355,0.06976156,-0.035402283,0.0446345,-0.013700453,-0.07000196,-0.022417868,0.022191212,-0.028611835,0.017380435,-0.0031440603,0.017988658,-0.05682859,-0.0042364337,0.019026693,0.041145466,0.041506067,-0.030362327,-0.06919345,-0.02154932,0.034626517,-0.023311289,-0.010305921,-0.024157729,0.041087635,-0.010456107,0.051339548,0.012162086,0.030671217,-0.036697876,0.0125254765,0.016328366,-0.02167842,-0.009100582,-0.014674668,0.021450028,0.0012602352,0.012287363,-0.039441466,-0.0076178717,-0.009532268,-0.059808046,0.031149022,-0.0107552335,0.0096809035,-0.071031794,0.05180839,0.09674923,-0.009057275,0.030719863,-0.0727412,-0.034305245,-0.005735816,-0.026812723,-0.043830466,0.012602177,-0.001797497,-0.050115492,0.0046265987,-0.023342002,0.066583045,0.03017838,0.054165747,0.0291022,-0.04004547,-0.028180448,0.037605766,-0.017752245,-0.037341926,-0.00012705555,-0.041723028,-0.004496055,0.013787308,0.03591512,-0.0015748723,-0.029007124,0.060805812,-0.05453925,0.024478542,0.013520383,-0.037846897,0.017903117,0.014309092,-0.012438485,-0.007595975,0.007864837,0.0068005435,0.017917948,0.009270545,-0.006840242,0.044216394,0.019501116,-0.008273284,0.007291694,0.033513423,-0.061054435,0.007972665,-0.038595635,-0.031421907,-0.0071781627,0.06366691,-0.005328635,0.002519158,-0.0428965,-0.024200458,-0.03132993,-0.05803946,-0.053209536,-0.013063753,0.0015638486,0.03135287,-0.041048765,0.0046708984,0.03197739,0.009619312,0.014037673,0.039492514,0.019711494,0.011655301,-0.04168765,-0.036367435,0.02488989,0.07120425,0.0014080517,0.0014919766,-0.025022963,0.010438123,-0.037989553,0.018592082,0.02569834,0.024806242,0.03063667,-0.012203724,-0.016675487,-0.032625146,-0.02005728,-0.0102161,0.00088439387,-0.009432309,0.0007449368,-0.0014389332,0.07797375,0.0194131,-0.00220393,0.053948894,0.03716801,0.044887334,0.026026893,0.022108832,0.031109016,0.027464116,0.014163313,0.007485751,-0.02500571,0.06282551,0.02308175,-0.005989242,-0.015725372,-0.00604947,-0.0019689568,-0.021086995,-0.011589748,-0.023706667,0.013891258,0.00090884,-0.036031637,-0.011851651,0.011096662,-0.05801647,-0.005840237,0.0070030014,-0.0022448536,0.030745717,-0.06793452,-0.052761987,7.600816e-05,-0.06549479,-0.0038419955,-0.017013784,-0.06158545,-0.037194792,0.0027164994,-0.032343946,0.09413778,-0.05521026,-0.0030735468,-0.010099063,-0.006572784,0.088607274,0.046106216,-0.0032747937,0.012456723,-0.016089762,-0.023250967,0.040666305,0.04433541,-0.017637067,-0.0030176332,0.035628635,0.04725221,0.029989593,0.008631004,-0.029720495,-0.008654553,-0.01464749,-0.03248287,-0.04598755,-0.03981916,0.03268126,0.025515435,0.02504336,0.0011836594,0.040814493,-0.00096602074,-0.0060206093,0.038487274,-0.049061235,0.014726055,-0.019462295,0.040844765,-0.008324042,-0.0012943438,-0.022339579,0.025857208,0.035402965,-0.0120688435,-0.026370365,0.012148671,-0.0012950257,0.022783192,-0.014023242,-0.003017237,0.0048194584,-0.045502115,0.0029449654,0.09292767,0.013636239,0.04248735,-0.02655332,-0.011008279,-0.013100045,-0.022307064,-0.0016953466,0.034421086,0.01920906,-0.027496172,0.05327502,-0.040675156,0.0778681,-0.0980569,-0.031879876,0.017711684,-0.013540411,-0.10403446,-0.0009327258,0.035961878,-0.032729335,0.007181637,-0.07808048,0.020868665,0.04464844,-0.06234587,0.034265604,-0.04011742,-0.02903716,-0.015270184,-0.047015734,-0.034722254,0.004955931,-0.026847115,-0.009336545,0.07405995,-0.00941567,-0.016546205,0.06554075,-0.020819668,-0.04083602,-0.01910498,0.023217887,-0.021947369,-0.0057697045,0.012244785,0.032036573,0.027235072,-0.027291503,-0.032954488,-0.06643118,0.017616415,-0.0072777304,-0.049361654,0.0065976456,0.039038222,-0.04993276,0.003945789,-0.022020947,-0.027821302,-0.0053020786,0.016453598,0.02970235,0.07741693,0.061076008,-0.016203683,-0.082607396,-0.06911328,0.025262682,-0.02533749,-0.008368547,0.023885133,0.017496137,-0.038868688,0.034044072,-0.0040917853,-0.042991295,0.025297545,0.0006596418,-0.011848527,0.047497198,0.017725548,-0.045265064,0.021983292,-0.049344283,0.044957332,0.06259728,0.043181036,-0.021533042,-0.074408956,0.00013893856,-0.0076666307,-0.031017663,0.0064523173,-0.008392611,0.010676108,0.00953103,-0.004980291,0.07130351,0.022723638,-0.009065456,-0.029327858,0.02621696,-0.05132973,0.03861607,0.05907927,-0.06294089,-0.056872264,0.027027635,-0.01745931,-0.022457099,0.016124004,0.03807879,0.007491488,-0.02732157,-0.08099944,0.018812481,0.04447408,0.021660367,0.040149193,0.02263318,0.014199539,0.03540782,-0.04005674,-0.059046842,0.0113209495,0.015149586,-0.04124257,0.026146026,0.021820726,0.06681119,-0.02753502,0.018166615,-0.012275146,0.017558018,0.025627699,0.016797375,0.009040635,0.06370446,-0.04964892,-0.0056746905,0.0589756,-0.032467052,-0.025643852,0.014791573,0.012830672,0.02062323,0.015841622,0.01373104,-0.015699128,-0.037932713,-0.024533937,0.017246258,0.011465109,0.04279315,0.02895598,-0.044954948,-0.04868237,-0.024794212,-0.030028757,0.048726197,0.042276412,0.016294476,-0.06379576,0.022431595,-0.048861414,0.04097535,0.026264474,-0.008872813,0.023716481,-0.032191414]	Keywords: references, TensorFlow, recurrent networks, curriculum learning, machine learning\nKey Objects: references, machine learning systems, optimization algorithms\nRefers to Images: None\nHypothetical Questions:\n- What common themes or areas of focus emerge from these references?\n- How do these referenced works contribute to the broader field of machine learning optimization?\n- Are there any connections or dependencies between these referenced papers?\n---\nSummary:\nThis section presents a list of references including works by Martin Abadi, Yoshua Bengio, and others, covering topics from large-scale machine learning with TensorFlow to advances in optimizing recurrent networks and curriculum learning.\nOriginal Text:\n## References  \n- [1] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Man, Rajat Monga, Sherry Moore, Derek Murray, Jon Shlens, Benoit Steiner, Ilya Sutskever, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Oriol Vinyals, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems. 2015.\n- [2] Yoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu. Advances in Optimizing Recurrent Networks. 2012.\n- [3] Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. Proceedings of the 26th annual international conference on machine learning , pages 41-48, 2009.\nContextualized Text:\nThis section provides a comprehensive list of references related to gradient descent optimization algorithms. It includes foundational works such as Abadi et al.s work on TensorFlow for large-scale machine learning and Bengio's research on optimizing recurrent neural networks and curriculum learning techniques.	{"tags": ["optimization", "references", "machine learning"], "doc_id": "684b65ab-5067-4383-bfe4-142e35bdc81b", "summary": "This section presents a list of references including works by Martin Abadi, Yoshua Bengio, and others, covering topics from large-scale machine learning with TensorFlow to advances in optimizing recurrent networks and curriculum learning.", "doc_type": "text", "entities": ["TensorFlow", "Martin Abadi", "Yoshua Bengio"], "keywords": ["references", "TensorFlow", "recurrent networks", "curriculum learning", "machine learning"], "key_objects": ["references", "machine learning systems", "optimization algorithms"], "contextual_text": "This section provides a comprehensive list of references related to gradient descent optimization algorithms. It includes foundational works such as Abadi et al.s work on TensorFlow for large-scale machine learning and Bengio's research on optimizing recurrent neural networks and curriculum learning techniques.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "References"}, "hypothetical_questions": ["What common themes or areas of focus emerge from these references?", "How do these referenced works contribute to the broader field of machine learning optimization?", "Are there any connections or dependencies between these referenced papers?"]}
ef6697d0-2021-49b9-9bf5-fc718df196a4	abe8c200-bfa1-4355-947e-23ea618c310d	[0.009754681,0.027448542,-0.04875254,-0.00040722147,-0.04645584,0.058976334,0.011804366,0.041208625,0.057892997,-0.044099253,-0.019528212,-0.015182281,0.03022526,-0.013968575,-0.03500072,0.012049671,-0.017046342,0.08726706,0.04517178,0.01062282,0.062209394,0.022564078,-0.015279613,0.001457017,-0.026521629,0.015292616,0.048344154,-0.038016714,0.027998649,0.028541021,0.017057851,-0.029509638,0.0044743707,-0.0026108492,0.018571835,0.020612953,0.015539563,0.042238526,0.006773458,-0.09617702,0.0044359057,0.022080103,-0.023158917,-0.014699066,0.026629305,-0.063079245,-0.019033197,-0.036208756,0.0100762015,-0.04426665,-0.011054673,0.01414076,-0.026098268,-0.0048133596,-0.033073336,-0.0033172006,0.0144993095,0.0018124674,0.014699034,-0.081108384,-0.045603305,0.043213073,-0.016341608,-0.04832751,0.07131522,-0.058744352,-0.08463628,0.04601928,0.02249455,0.09917298,-0.015884543,0.050662696,-0.019248432,-0.09162605,0.11774614,0.053253256,0.013962856,-0.019254588,-0.054656092,0.009255833,0.01687184,0.030536352,-0.045327306,-0.016671007,0.07102699,0.028294737,-0.06731765,-0.02084992,0.08675628,-0.021633612,0.0005560706,0.023311753,-0.016424587,0.072738014,-0.012099437,-0.014429173,0.011663406,-0.02996208,-0.023256049,-0.04755084,0.017686317,0.004813758,0.06484883,0.11338046,-0.008950971,0.024857273,-0.06237206,-0.012498379,-0.014939237,-0.020093065,0.013299707,0.028048128,-0.03448714,0.0015237705,-0.036184557,-0.0600626,-0.0584142,0.015229065,-0.037045486,-0.053804286,0.01602176,-0.0062426995,-0.016466154,0.008968593,0.04910225,0.05307077,-0.032284025,0.03226036,-0.04912303,-0.02758135,0.016527228,-0.02052734,-0.033985082,0.020453965,0.08377409,-0.021397395,0.02951742,-0.05317737,-0.0045453697,0.0041393773,0.0036400612,-0.036204908,-0.02044831,0.01524026,-0.016788928,0.03537634,-0.08504692,0.0292638,-0.023179408,0.025308974,0.042119145,0.02329854,0.044003397,0.016974961,-0.012662407,-0.029652508,0.006506309,-0.027994618,0.013582478,-0.0069867293,-0.07626379,-0.030166386,-0.00066697923,0.050454877,0.021225203,0.1172107,-0.0022476586,0.0059484267,0.017771896,0.039617363,-0.029951036,0.015484918,-0.019098073,-0.019965736,0.008101019,-0.02166821,0.043446824,0.008105401,-0.035565224,-0.025183573,-0.005706599,-0.0011353997,-0.014574061,-0.0035590355,-0.024794739,0.025934901,-0.008868195,0.063511245,0.011237602,-0.039904673,0.023898393,0.001842038,0.021940008,0.013383381,0.02518894,0.002455527,0.012790904,-0.039686885,-0.013404998,-0.056249913,0.011621778,0.04249337,-0.020437645,0.017235048,0.024130018,-0.01019023,-0.04503224,-0.029621346,-0.060088754,-0.011487112,-0.032947548,0.023409862,-0.004254139,0.036893684,0.000628609,0.031174365,-0.030573495,0.019340003,0.016335338,0.054940883,-0.080638595,0.0037569504,0.000977108,-0.001589044,-0.062269147,0.0028847537,0.0004022607,0.02824042,0.057251807,-0.042100802,-0.0068794787,0.013913391,-0.021922182,0.0497037,-0.0025007343,0.006692256,0.04168369,0.023139002,0.005874065,0.02552458,0.022180703,0.05428586,0.015669174,0.088339254,0.007688037,0.0060887146,0.04696565,0.050661094,-0.008452654,-0.0007069087,-0.037585083,0.018303042,-0.005294232,0.01506402,0.014110127,0.008113275,0.03747369,0.016492112,0.080609776,-0.030647844,-0.010857325,-0.034440164,0.019625412,-0.011760651,0.009328367,0.04983324,0.010397984,0.011231806,0.021427033,0.007069347,0.033379342,0.0338447,0.014984888,-0.007733392,-0.098574646,0.02416613,0.008134246,0.04764026,0.043016143,-0.028467903,-0.003659613,0.009207943,0.019196134,-0.020355232,0.017883483,-0.005318838,-0.037079997,-0.019053798,-0.05730725,0.0026304517,0.044090025,-0.04689851,0.036500134,0.06194067,0.0059484374,0.0031189225,0.01696578,-0.06864647,0.016763791,0.025695693,0.09585731,0.0007113275,0.05349617,0.016109154,0.007131293,0.0022385078,-0.102277696,0.014218411,0.038177874,0.00058796315,0.06182586,-0.010920958,-0.003800113,-0.01875484,0.008487143,0.017896617,0.05999373,0.008983264,0.004256626,0.035964426,0.034554046,0.041949634,-0.0012695333,0.048727114,0.0047686817,-0.0231846,-0.023195727,-0.0143342605,-0.08535372,0.034892663,-0.00796682,0.031516965,0.032352407,-0.004893725,-0.018801559,-0.0131847765,-0.006112891,-0.0044260668,0.044698793,0.025979484,0.022048019,-0.06763823,0.037144464,-0.0016402471,-0.007218102,-0.0060788034,0.013064473,0.025881246,0.023053946,0.058320466,-0.0029224,0.020214386,0.052385624,-0.003347932,-0.015716553,0.017423548,0.006449097,0.05515894,-0.0055949837,0.02964393,0.04993014,-0.063750274,0.082682855,-0.05263425,0.061374296,0.0058637843,-0.061392285,-0.017906208,0.043109473,-0.026758732,0.019413669,-0.01774589,0.03736108,-0.056264285,-0.01742746,0.01040712,0.05447029,0.010630047,-0.038068697,-0.06877854,-0.03603037,0.036850862,-0.005032145,-0.003511523,-0.010281362,0.039736778,0.02277309,0.073019914,0.001453614,0.027327405,-0.0020337873,0.028408319,0.028257031,-0.014743025,-0.023778422,-0.01924092,0.030192874,0.0009456073,0.035138953,-0.045314647,0.0054075615,-0.007125206,-0.065307155,0.034390505,0.025166497,0.013630916,-0.031135038,0.03505558,0.08654985,-0.013561735,0.051608454,-0.066329196,0.011919654,0.002824228,-0.02502904,-0.050174795,0.018983806,-0.000509462,-0.04226939,-0.012480584,-0.03960912,0.06929104,0.011128298,0.0098118605,0.015119992,-0.012720175,-0.05719151,0.036108732,-0.026813058,-0.032874875,0.009146567,-0.018258173,-0.025345206,0.017553044,0.017497327,0.017024036,-0.028900528,0.08133142,-0.050234843,-0.004884833,0.031617787,-0.043202326,0.022933086,0.022218985,-0.027207013,-0.025314491,0.0027581346,0.0049803453,-0.0049463306,0.010335899,-0.008153143,0.009562245,0.017805818,-0.0005834425,-0.0025010444,-0.004826191,-0.05125883,-0.0183668,-0.033578042,0.0068456177,0.011996129,0.06283516,-0.022289956,-0.025313562,-0.0058039417,-0.0025380608,-0.019570332,-0.041042496,-0.018179324,-0.019238941,-0.0072824596,0.045353394,-0.04607166,0.0041885963,0.0013856026,0.006713819,0.031111894,0.0300293,0.04508927,0.008676206,-0.017878117,-0.012195552,0.021592557,0.07756217,0.010349067,-0.025212156,-0.04697579,0.013191928,-0.0374214,0.0091666505,0.022494627,0.025217552,-0.009649345,-0.016346948,0.023915337,-0.037535504,-0.0204757,-0.03508469,0.008041562,-0.027062267,-0.023668969,-0.014559908,0.06475394,0.030811256,0.0142283095,0.028990569,0.02081737,0.052562132,0.01867577,0.005920591,0.027623313,0.023545459,0.026985306,-0.023681065,-0.017147757,0.054069117,0.027755609,-0.012170699,0.010443872,-0.004860743,0.018192042,-0.015641315,-0.0012327636,-0.0051028454,0.032993123,-0.009753937,0.008634209,0.036217697,0.029563429,-0.066456415,-0.0069551715,0.0015902591,-0.0010214054,0.032040663,-0.12350244,-0.060210958,-0.027037123,-0.07121245,0.0033621837,-0.02450055,-0.063074954,-0.023827517,0.0026909662,-0.045719314,0.101398975,-0.035305936,0.029202051,0.031922843,-0.00029951986,0.0694421,0.038273424,-0.02176411,0.04200554,-0.012090157,-0.023999523,0.010757769,0.029971926,-0.02862885,0.007816187,0.02171255,0.059630536,0.005114451,0.010167481,-0.037352547,-0.024393357,-0.009865502,-0.040368263,-0.057419956,-0.04116152,0.02535894,0.020774348,0.010975424,-0.017125264,0.046058517,-0.0056644813,0.024660122,0.03303637,-0.015560365,-0.0039080884,-0.024162348,0.043709006,0.018047359,0.01951423,-0.028683921,0.009954237,0.045147095,0.00142882,-0.0019689635,0.044654332,-0.0056848633,-0.0026008135,-0.019352354,0.003897757,0.013544995,-0.06474216,-0.0052115507,0.08883131,0.015999552,0.07105793,-0.034526307,-0.030879207,-0.013598094,-0.008664675,-0.010283856,0.035448134,0.011509348,-0.0119289635,0.045464963,0.0012021019,0.07671497,-0.090899564,0.0016975313,0.019878402,-0.001531262,-0.08769467,0.0037302447,0.02613375,-0.020643214,-0.009394956,-0.07760505,0.050289672,0.027415091,-0.025449065,0.033911675,-0.064066686,-0.025768911,-0.033133436,-0.032722995,-0.028394422,0.0026459626,0.0018249137,-0.0006732447,0.08487259,-0.056677785,0.0013533538,0.022030393,-0.026526622,-0.06394056,-0.033049818,0.016527941,-0.02950097,-0.020495918,0.006601797,0.05068368,0.023586681,-0.01733166,-0.019988552,-0.07553468,0.029328812,-0.036265213,-0.026253866,-0.00034184213,0.048122626,-0.032549683,-0.024564967,-0.022876801,0.0030976962,-0.0004896258,0.027854664,0.01572323,0.07585285,0.09027158,-0.069099925,-0.047351085,-0.077241,0.00646444,0.0015112509,-0.00082271814,0.015563148,0.012417179,-0.03308786,0.035690866,-0.016037215,-0.04814522,0.010347704,0.008274615,-0.05255613,0.03464753,0.00925488,-0.027989404,0.011077953,-0.03418389,0.009956199,0.03673954,0.002606686,-0.046350453,-0.06328776,-0.0068314946,-0.019882204,-0.034549505,0.015734928,-0.009119421,-0.008224069,0.0071857553,-0.011217279,0.056857686,0.051559933,-0.034666732,-0.032823842,0.025607081,-0.031787604,0.038746484,0.04182623,-0.058196638,-0.049327463,0.010169589,-0.0048460322,-0.051548824,0.013732588,0.033007402,0.0028699005,-0.0069306814,-0.08402174,-0.0061626425,0.046839617,0.029620335,0.029513724,0.008437688,-0.02308559,0.03843998,-0.024342697,-0.020683695,-0.016312461,0.01846228,-0.040984407,0.036598455,0.033637974,0.035257865,-0.03345937,0.023387956,-0.0073331846,0.02550075,0.017032932,0.009044966,0.030514969,0.07681927,-0.051266115,-0.018356757,0.062223785,-0.02608755,-0.014402676,-0.0020512063,-0.002203409,0.03574318,-0.0044009183,0.026353916,-0.06159124,-0.058304306,-0.0144939525,0.0317422,0.016494526,0.024474898,-0.01262231,-0.0479054,-0.055521555,-0.041279107,-0.028180648,0.047644008,0.026420325,-0.011892687,-0.05223148,0.008422184,-0.056470517,0.042064283,0.011580311,0.008766108,0.023698155,-0.058666233]	Keywords: optimization, references, curriculum learning, deep networks, saddle point problem\nKey Objects: Optimization Techniques, Deep Learning, References\nRefers to Images: None\nHypothetical Questions:\n- What is the significance of curriculum learning in training machine learning models?\n- How do saddle point problems hinder the optimization process in deep learning?\n- Why is it important to scale deep networks, and what challenges does this pose?\n---\nSummary:\nThis list of references includes seminal works exploring various optimization techniques, from curriculum learning to tackling saddle point problems and scaling deep networks.\nOriginal Text:\n- [3] Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. Proceedings of the 26th annual international conference on machine learning , pages 41-48, 2009.\n- [4] C. Darken, J. Chang, and J. Moody. Learning rate schedules for faster stochastic gradient search. Neural Networks for Signal Processing II Proceedings of the 1992 IEEE Workshop , (September):1-11, 1992.\n- [5] Yann N. Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional nonconvex optimization. arXiv , pages 1-14, 2014.\n- [6] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large Scale Distributed Deep Networks. NIPS 2012: Neural Information Processing Systems , pages 1-11, 2012.\n- [7] Timothy Dozat. Incorporating Nesterov Momentum into Adam. ICLR Workshop , (1):2013-2016, 2016.\nContextualized Text:\nThe following is a list of references related to gradient descent optimization algorithms. It includes key contributions such as Bengio's work on curriculum learning, Dean's exploration of large-scale distributed deep networks, and Dauphin's research into saddle point problems, alongside other significant advancements in the field.	{"tags": ["optimization", "references", "machine-learning"], "doc_id": "ef6697d0-2021-49b9-9bf5-fc718df196a4", "summary": "This list of references includes seminal works exploring various optimization techniques, from curriculum learning to tackling saddle point problems and scaling deep networks.", "doc_type": "text", "entities": ["Adam", "TensorFlow", "NIPS", "ICLR", "Neural Networks"], "keywords": ["optimization", "references", "curriculum learning", "deep networks", "saddle point problem"], "key_objects": ["Optimization Techniques", "Deep Learning", "References"], "contextual_text": "The following is a list of references related to gradient descent optimization algorithms. It includes key contributions such as Bengio's work on curriculum learning, Dean's exploration of large-scale distributed deep networks, and Dauphin's research into saddle point problems, alongside other significant advancements in the field.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "References"}, "hypothetical_questions": ["What is the significance of curriculum learning in training machine learning models?", "How do saddle point problems hinder the optimization process in deep learning?", "Why is it important to scale deep networks, and what challenges does this pose?"]}
16e48581-b2fc-4683-9a3a-ae5b244ed889	abe8c200-bfa1-4355-947e-23ea618c310d	[0.030157017,0.0021359017,-0.045605328,0.0019675838,-0.048572753,0.05713999,0.029123878,0.05040142,0.013713452,-0.059569232,0.0047461516,-0.008259644,-0.0040725893,0.0011684503,-0.038188584,0.027575782,-0.021328593,0.08830515,0.026414026,-0.001922368,0.05250221,0.038843844,0.025237577,0.0071414453,0.009692825,0.028994981,0.034470897,-0.008709102,0.0117547065,0.019577105,-0.01131266,-0.021393035,0.0065414533,0.0013707911,0.0002692904,-0.0120097445,-0.0059550665,0.013733115,-0.0028335305,-0.08280566,-0.0020281472,0.03356548,-0.029562367,-0.035681486,0.03622215,-0.05468719,-0.04814265,-0.04034191,0.019334808,-0.009040843,-0.018594109,0.051903665,-0.019873114,-0.014124753,-0.05948078,0.00779949,-0.022761237,-0.011210667,0.015592243,-0.069282986,-0.04253997,0.030255105,0.015258882,0.003767693,0.031391576,-0.06417202,-0.06356248,0.041523796,0.03831347,0.092197314,0.011232768,0.042694133,-0.00048889726,-0.09034471,0.10742513,0.04299935,-0.028120523,0.0032193074,-0.049455356,-0.0033382326,0.012593618,0.03984471,-0.035066716,-0.047121428,0.06847512,-0.004669643,-0.062199097,-0.029083934,0.099540845,-0.039043028,-0.033643726,-0.028909162,-0.039002083,0.05680541,-0.017805085,-0.017069267,0.030802816,-0.04963141,0.0034457156,-0.0401746,0.005376487,0.022874482,0.051207755,0.13222603,0.005991257,-0.022306465,-0.0578535,-0.013871834,-0.011651755,0.003868715,0.031012459,0.032363653,-0.032516904,-0.002867124,-0.039777454,-0.021519192,-0.00054620433,0.03546271,-0.04608358,0.0066704103,0.05259903,-0.0013611554,-0.003988475,-0.0029209289,0.036758613,0.032678887,-0.010988974,0.010994273,-0.06356787,-0.01387047,0.022251386,0.0033197459,-0.034276728,0.018217137,0.041543175,-0.022172926,0.05853693,-0.04416378,0.0065674772,0.013066911,-0.0041238675,-0.032630593,-0.03031315,0.014021296,-0.026068669,0.025430413,-0.058943894,0.04703461,-0.01891499,0.049339544,0.003691681,0.02754872,0.04098206,-0.011449034,-0.017588072,-0.02782727,-0.020577438,-0.007491283,0.018902406,-0.012104769,-0.058067013,-0.027295753,-0.0130899735,0.043372083,0.02561151,0.1269323,-0.009865033,0.010195775,0.024889689,0.027144194,-0.04407748,0.020035176,-0.002493159,-0.002035008,0.01746455,-0.02136638,0.037255395,0.0071562594,-0.03982799,0.0030115254,-0.026548568,0.004003839,-0.0029079514,-0.0014321342,-0.016483638,0.012706271,-0.010967373,0.05314806,-0.008297686,-0.050464075,0.023247864,-0.009101753,0.012704022,-0.0037002803,0.04089508,0.013179809,0.00074596965,-0.039471224,-0.039359223,-0.052093346,0.038317937,0.041759424,0.00046277783,-0.009622574,0.012176812,-0.024548797,-0.0068791667,-0.0439789,-0.060852967,0.0136504015,-0.02184726,0.019071378,0.018678853,0.043393426,0.024695784,0.011721081,-0.012151139,0.007378996,-0.0011057251,0.013610264,-0.06828881,-0.010786552,0.0044781948,0.0070737526,-0.04912005,-0.009989571,-0.014788202,0.00067088474,0.07556457,-0.019469667,0.008798111,0.02885788,-0.022529602,0.038537994,-0.018477725,0.011738155,0.022618636,0.012775948,-0.02414449,0.036052756,0.03776277,0.06996933,0.02728587,0.085845575,0.031217529,0.013228426,0.04376806,0.06596569,-0.015175386,0.03760833,-0.0121299615,0.019993866,0.0039583603,-0.024090895,-0.015828485,-0.0039070263,0.041601826,0.018917542,0.08711522,0.0059601073,-0.0104814535,-0.065791674,0.015873108,-0.009957918,0.008798801,0.047927447,0.012554896,0.030148836,0.001303004,0.039573923,0.009640387,0.026374727,0.015222204,-0.03162717,-0.091478184,0.031438705,0.030857254,0.0038703517,0.035439804,3.4876834e-06,0.008762188,-0.011705722,0.0016135739,-0.033281755,0.022053335,-0.04108884,-0.03792601,-0.023388851,-0.011384452,0.022746928,0.030803075,-0.033716246,0.019089343,0.020507595,-0.01004375,0.016662844,0.0045999447,-0.027427519,0.03744441,0.037637938,0.07487951,0.0011363992,0.03995809,0.01391768,0.015798474,0.0061945077,-0.09684674,0.03254147,0.042394433,0.012613069,0.058546174,-0.014405526,-0.027352896,-0.0058377637,0.042013526,0.035018712,0.098639525,0.021828342,-0.00081903493,0.061768956,0.064998545,0.0025250323,-0.016340889,0.070408374,0.0036205517,-0.0066094613,-0.011085817,0.023986755,-0.0917073,0.0791472,-0.0445246,0.041953158,0.02572918,-0.0077664615,-0.0012663399,-0.02071648,0.00257181,0.010213632,0.0187362,-0.0006902813,0.0033989616,-0.077887066,-0.010791342,-0.006310544,-0.030139068,-0.025843214,-0.012764438,0.024004795,0.037741255,0.08520121,-0.023129696,0.033929184,0.05311982,0.009226662,0.0465103,-0.01396939,0.048995934,0.028669931,0.018497849,-0.024901671,0.04360576,-0.035937242,0.122974895,-0.036566045,0.056891672,0.0004893661,-0.026257236,-0.033013463,0.027047224,-0.036827415,0.015232148,0.000823386,0.009150008,-0.063965455,-0.005007596,0.018916244,0.023181804,0.00592041,-0.009142632,-0.059850696,-0.050892,0.008003164,-0.023633266,-0.017185465,0.0110042775,0.047397,0.025512934,0.07771183,-0.022081075,-0.034964018,0.016481986,-0.026198663,0.027299171,0.0076709283,-0.050770044,-0.0183718,0.030167788,0.023345774,0.05582514,-0.0056736134,0.039063383,0.00956765,-0.05791394,0.049434602,0.060720067,-0.00036689834,-0.0691111,0.02187686,0.056287143,-0.0113824,0.022564676,-0.08885392,-0.024950448,0.0059544663,-0.026427262,-0.02148413,-0.003005085,-0.008070753,-0.0312847,-0.020346925,-0.008079836,0.0577986,0.03732694,0.03476792,0.0061504957,-0.0019661915,-0.047845,0.036189713,0.009464171,-0.048459403,-0.014855686,-0.02855395,-0.03682119,0.033536054,0.01707167,0.028888868,-0.053260844,0.06892277,-0.056252565,0.023485791,0.019557076,-0.026241248,0.05822334,-0.00869777,-0.041078154,-0.021676162,0.026232816,-0.010638616,0.016134549,0.012561534,-0.024343707,0.006829494,0.018027361,0.009663259,-0.011898871,-0.02837274,-0.043048233,-0.014662048,-0.045834932,-0.008404587,0.032334186,0.056154944,-0.019925147,0.002452752,0.009564933,0.0019620424,-0.029137667,-0.03401965,-0.012773089,-0.005726333,0.016621832,0.023772215,-0.02111721,0.0062476406,-0.0012087199,0.033346202,0.028161732,0.015491382,0.07107523,-0.0043818583,-0.01290932,-0.037678197,0.022512728,0.06432701,0.006264011,-0.012533952,-0.014888438,0.0021931324,-0.056861043,0.008568899,0.036849536,0.03500377,0.00533101,0.011673309,0.027110377,-0.048927598,-0.017201057,0.007568319,0.03192854,-0.028191434,-0.027557101,-0.01908513,0.08336709,0.012729991,0.03270584,0.007273151,0.03762997,0.060074043,0.020901509,0.02950178,0.021539649,0.009180712,0.005161837,-0.049186755,-0.0028221002,0.054111023,-0.0068707466,0.009693772,0.02118841,-0.005686812,0.0049688555,0.0022624626,-0.016029216,0.014584155,0.0013745434,-0.017404955,0.015372031,0.037134126,0.034900136,-0.062983185,0.026930707,-0.0024247037,0.031608988,0.039945155,-0.10847598,-0.032482572,-0.0049676956,-0.04184529,-0.030166637,-0.0070579625,-0.03691646,-0.041019194,0.0029511189,-0.030676922,0.09294624,-0.06812241,0.026930835,0.047133006,-0.011584845,0.053932287,0.03958592,0.0065259715,0.024735626,0.013004071,-0.01408072,0.025628474,0.037995894,-0.028193533,0.0040591382,0.05056014,0.056458276,-0.020793237,-0.01859928,-0.057638533,-0.01534761,-0.026527263,-0.023511887,-0.055472903,-0.04582249,0.022508247,0.026295152,0.01196623,0.02285752,0.020351628,-0.009264819,0.06788497,0.06299531,0.008288166,-0.00064578786,-0.022531377,0.014783718,0.03350296,0.025352182,-0.028523717,-0.003913958,0.018086657,-0.017772308,-0.007885226,0.0038274492,-0.0027266387,-0.008567125,0.011814091,0.012399439,-0.01458487,-0.019273074,-0.04712595,0.0811977,0.0065400447,0.051693633,-0.031619396,-0.05020037,0.003354473,-0.029276077,-0.015457148,0.039029088,-0.015262373,-0.01771431,0.009339101,-0.018220002,0.05795663,-0.09126972,-0.026411695,-0.030403629,0.012651811,-0.09450104,-0.016985891,0.008017867,-0.028744133,-0.014234568,-0.10073996,0.027136225,0.008385576,-0.04774174,0.018179482,-0.035397034,0.012471937,-0.014773386,0.016594049,-0.04581924,0.034479458,0.020718355,-0.026277069,0.10157167,-0.049411252,-0.022086652,0.0246162,0.024180083,-0.03662575,-0.053505555,0.02914617,-0.006933892,-0.0015167455,0.0057689976,0.053697757,0.013928881,-0.005193071,-0.037977826,-0.10855889,0.026036812,-0.0277731,-0.024299039,0.048788313,0.036850844,-0.033491813,-0.04765986,-0.019809142,-0.024432233,-0.0061468533,-0.003789001,0.010645157,0.0314308,0.06249623,-0.056595154,-0.054081425,-0.03160963,0.0026646017,0.039730407,-0.0031537483,0.028917572,0.015524414,-0.010401952,0.019795436,-0.043275006,-0.034238167,0.028317664,0.02128864,-0.029614331,0.009686209,0.011955384,-0.027341887,0.04544014,-0.024545131,0.011340113,0.04848195,0.021515222,-0.019579574,-0.076841354,-0.019983452,-0.022184582,-0.004815467,0.02671428,-0.018686252,0.019269332,0.031439085,-0.026588283,0.028288323,0.011548937,-0.022652516,-0.043500476,0.040974226,-0.028759139,0.031036684,0.03836621,-0.055396754,-0.081317924,0.016355008,-0.003559131,-0.027279075,-0.010563199,0.021533864,0.0055012754,-0.00023331202,-0.029654413,-0.015398379,0.028089035,0.02320004,0.0338515,0.006148448,-0.0037711596,0.039105054,-0.025521476,-0.045599405,-0.00095826434,0.043760087,-0.012929092,0.041422993,0.016145963,0.048298255,-0.080503576,0.015429288,0.009311607,0.04397204,0.014524498,0.023499845,0.019236831,0.08289363,-0.05558927,0.016501619,0.067250036,-0.008585567,-0.027329806,-0.03148976,-0.009281191,0.05039734,0.02100778,-0.009085955,-0.033594534,-0.024051799,-0.00066737796,0.0036455004,0.037965275,0.024645463,-0.030358318,-0.054635867,-0.035562307,-0.052326277,-0.06438886,0.04588705,0.0367867,-0.02060713,-0.08980321,0.0155971,-0.03248431,0.07773324,-0.010884088,0.02977921,0.029693954,-0.024770305]	Keywords: optimization algorithms, Adam, batch normalization, backpropagation, references\nKey Objects: optimization algorithms, references\nRefers to Images: None\nHypothetical Questions:\n- What is the significance of Nesterov momentum within the Adam optimization algorithm?\n- How does batch normalization contribute to the training process of deep neural networks?\n- Why are these particular optimization algorithms and techniques important for training deep learning models?\n---\nSummary:\nThis section of the references lists several key works focusing on optimization algorithms, including modifications to Adam, adaptive subgradient methods, batch normalization, and efficient backpropagation techniques.\nOriginal Text:\n- [7] Timothy Dozat. Incorporating Nesterov Momentum into Adam. ICLR Workshop , (1):2013-2016, 2016.\n- [8] John Duchi, Elad Hazan, and Yoram Singer. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. Journal of Machine Learning Research , 12:2121-2159, 2011.\n- [9] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv preprint arXiv:1502.03167v3 , 2015.\n- [10] Diederik P. Kingma and Jimmy Lei Ba. Adam: a Method for Stochastic Optimization. International Conference on Learning Representations , pages 1-13, 2015.\n- [11] Yann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus Robert Mller. Efficient BackProp. Neural Networks: Tricks of the Trade , 1524:9-50, 1998.\n- [12] H. Brendan Mcmahan and Matthew Streeter. Delay-Tolerant Algorithms for Asynchronous Distributed Online Learning. Advances in Neural Information Processing Systems (Proceedings of NIPS) , pages 1-9, 2014.\nContextualized Text:\nThe following list provides references for optimization algorithms used in deep learning, including a method for incorporating Nesterov momentum into Adam and explorations of batch normalization for accelerating training, alongside fundamental works on backpropagation and other related techniques.	{"tags": ["optimization", "references", "deep learning"], "doc_id": "16e48581-b2fc-4683-9a3a-ae5b244ed889", "summary": "This section of the references lists several key works focusing on optimization algorithms, including modifications to Adam, adaptive subgradient methods, batch normalization, and efficient backpropagation techniques.", "doc_type": "text", "entities": ["Adam", "Batch Normalization"], "keywords": ["optimization algorithms", "Adam", "batch normalization", "backpropagation", "references"], "key_objects": ["optimization algorithms", "references"], "contextual_text": "The following list provides references for optimization algorithms used in deep learning, including a method for incorporating Nesterov momentum into Adam and explorations of batch normalization for accelerating training, alongside fundamental works on backpropagation and other related techniques.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "References"}, "hypothetical_questions": ["What is the significance of Nesterov momentum within the Adam optimization algorithm?", "How does batch normalization contribute to the training process of deep neural networks?", "Why are these particular optimization algorithms and techniques important for training deep learning models?"]}
eddf4b75-ff52-469f-b36a-8f76b2f0db2b	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0008118957,0.0037348242,0.011704997,0.04183213,0.0027448281,0.047615908,0.031728428,0.044251207,0.01706358,-0.042465873,-0.026814442,-0.03932464,-0.018207641,-0.045225736,-0.019223407,-0.017827567,-0.022843296,0.094323695,0.023072613,-0.0072849677,0.081054196,0.048091676,-0.010802653,0.018752003,0.014993749,0.028980087,0.030741768,-0.0041039553,0.041320927,-0.019389912,0.006013643,-0.01906104,-0.028753249,-0.016106926,0.056881886,0.009720242,-0.0193982,-0.01612987,6.584363e-05,-0.052497845,-0.0060899253,0.010299867,-0.032085203,-0.030759897,0.050638396,-0.027841836,-0.04885927,-0.056174234,0.014133731,-0.023021601,-0.023353798,0.059388362,-0.021056429,0.0033272936,-0.047040995,-0.0009762529,-0.0105250105,0.007258949,0.015209355,-0.08112098,-0.06538372,0.031968206,0.040346134,-0.030059949,0.048646018,-0.038449958,0.0035030614,0.053177703,0.04762707,0.12693305,0.015476724,0.08475393,-0.013342225,-0.08306008,0.109138556,0.04979365,0.01212682,-0.02664488,-0.040763937,-0.004131182,0.030154461,0.040630788,-0.012414054,-0.012728169,0.05989822,-0.0064556696,-0.050083723,-0.021124182,0.071927756,0.025471255,0.001101948,0.0010278253,0.020929407,0.044919427,-0.015946124,-0.01575087,0.006273432,-0.019476594,-0.05290372,-0.021407627,0.01652288,0.030296212,0.08770091,0.13602829,0.035412736,0.012400395,0.0003045001,-0.02329456,0.008583212,0.0060784635,0.011276196,0.088211834,-0.024330677,0.013972755,-0.02483931,0.0044989903,-0.038808838,0.029398305,0.025964402,-0.016775463,0.06897467,-0.0021411774,-0.0015139723,0.012559998,0.045854073,0.03571328,0.012869685,0.006679464,-0.028235877,-0.0117201125,0.0010070112,-0.009421701,-0.023779608,-0.001743363,0.034837946,-0.018761167,0.06409419,-0.06568949,-0.025308551,0.011454544,-0.010377916,-0.0016698012,-0.039775893,-0.018583242,-0.0052061193,0.015457723,-0.08159872,0.004601014,-0.011587026,0.042699702,-6.836154e-05,-0.02105769,0.0414223,-0.016679807,-0.048543848,-0.05663922,-0.055767994,-0.0054949364,0.008142933,-0.005046106,-0.037403487,0.0057818047,0.018383874,0.07885427,0.050725196,0.12176765,0.0061546643,-0.006417931,0.028410181,-0.004943247,-0.010997553,-0.015733898,0.033162817,-0.03142998,0.0017295587,-0.024819402,0.04114977,-0.035975367,-0.02875768,-0.003159301,-0.03922818,0.024407096,-0.017713957,-0.012748815,-0.04562055,-0.0025168122,0.008198667,0.059601236,-0.02860929,-0.052850276,0.01883437,-0.0069704847,-0.03933154,-0.006976891,0.03821243,0.03578562,0.0387647,-0.035253577,0.01493487,-0.045793336,0.02781539,0.07107274,-0.031715613,-0.011656984,0.035362706,-0.0642832,0.038575247,-0.028615154,-0.043193925,0.016600976,0.0074393013,0.04415661,0.03063019,0.07877001,-0.010972998,0.02301052,-0.07741925,-0.021382626,0.06538092,-0.0019499814,-0.071479894,0.0063764825,0.011511861,0.023059433,-0.05395961,0.019561762,-0.0044424427,-0.01531238,0.055234224,-0.012805839,-0.005677106,0.024577482,-0.045988604,0.063205816,-0.04952974,-0.008699214,-0.0042243097,0.017329935,-0.02088235,0.04664657,0.03241687,0.028533729,0.009924508,0.079123296,0.020090355,-0.010711258,0.0061792405,0.0070136455,-0.0039582867,0.013748931,-0.04718529,0.020675112,0.06415521,-0.015938286,-0.0072473655,-0.0052488414,0.0620233,0.030314827,0.034157477,-0.04588597,-0.014985503,-0.047784373,0.013605223,0.001729253,0.008482579,0.01796427,0.038982783,-0.00636414,-0.0156398,0.030002665,0.043758582,0.04161384,0.009596695,-0.013491837,-0.033854127,0.007113535,-0.0079089375,-0.0008964139,0.05699655,-0.05950775,0.023658875,0.028222715,0.016160259,-0.018156251,-0.028053379,-0.006340447,0.010579977,0.021399055,-0.03714225,0.04047048,0.030707313,-0.03569309,0.013164411,0.05370677,-0.019081995,-0.0092996145,0.0020816613,-0.015249082,0.020854294,0.030804962,0.089017555,-0.0077445065,0.03298662,0.062337644,-0.0021077134,0.004756186,-0.07842388,0.030199789,0.014354756,0.06563507,0.032350887,-0.00013933814,0.00019656919,-0.020067336,-0.0042380635,0.044934005,0.07847755,0.006138475,0.02423274,0.020146836,0.04015376,0.021138048,-0.041567083,0.041246336,0.0142205255,-0.010556146,0.01619621,-0.02014495,-0.05145795,0.061135046,-0.047481827,0.033849202,0.044407036,-0.033986405,0.0115736555,0.0026119037,-0.034571845,0.0077483593,0.029532935,0.04860821,-0.00059116026,-0.06860297,0.014192052,-0.028874839,-0.008682694,-0.012993835,0.04529867,0.019875687,0.027952157,0.041812107,-0.016439466,0.056606963,0.032581683,0.0035498617,0.03880813,0.029002381,-0.012114141,0.03673598,0.041229546,-0.030951641,0.021516047,-0.040343292,0.09321332,-0.035125285,0.036068764,-0.028851708,-0.021774327,-0.011487367,0.043568734,0.029665796,0.024270235,0.0011517063,0.05731203,-0.08653059,0.029453844,0.04612396,0.059406947,0.0579861,-0.043617256,-0.0630951,0.0055671986,0.046155095,-0.02508655,-0.024391962,0.0025616747,0.005688769,0.027969146,0.078654155,-0.0055730813,0.012819241,-0.03243659,0.013448014,0.027987909,0.021977292,-0.04480119,-0.019817555,-2.5124711e-05,0.027228015,0.026911614,-0.020964893,0.02417386,0.0035545817,-0.0630748,0.025835825,0.02929171,0.013751371,-0.02748763,0.040886547,0.043362677,-0.024307033,0.06540531,-0.031746563,-0.06044766,-0.009055869,-0.027447902,-0.040649086,-0.028033417,0.0196247,-0.017699646,-0.04021492,0.011676501,0.058953144,0.055683523,-0.003285738,-0.0010481655,0.009348245,-0.038378146,0.020395245,-0.032487914,-0.013736543,-0.018512644,-0.03694069,0.0028248485,0.052903723,-0.007275274,-0.017673992,-0.057704233,0.07829777,-0.036363807,0.031383116,0.046444975,-0.018986516,0.02312758,0.020121409,0.00891168,-0.03788677,0.03960491,-0.017221969,-0.0016450789,-0.028886072,0.013979038,0.008583736,0.043372978,0.0438002,0.048494846,-0.024765646,-0.017588194,-0.03304847,0.012183731,0.03654658,-0.0053665806,0.035640337,-0.006338015,7.904854e-05,0.014645883,-0.03886845,-0.06238769,-0.053774264,-0.015115826,-0.03121596,0.006132951,0.04435824,-0.029679215,0.03216217,0.0042691496,0.011939211,0.016828554,0.009572084,0.040737044,-0.0205283,-0.019080453,-0.012904108,0.033181574,0.074166484,0.009410125,-0.01817753,-0.03523103,0.004655605,-0.03353542,0.016925832,0.0068309717,0.026869394,-0.01050011,-0.016664712,0.0034214496,-0.016615383,0.01757104,-0.051123537,0.027736196,-0.020434797,-0.06632597,0.0030188789,0.039035145,0.06917786,0.03317967,0.014818522,0.051542353,0.07576014,-0.017287308,0.008880449,0.027077956,0.0009898917,0.039558195,-0.020856807,-0.0019626485,0.020833323,0.056151513,-0.039233517,0.037391547,-0.020543572,-0.019169915,0.0036346265,0.014588822,0.022079237,0.041958615,-0.018893905,-0.03993328,-0.009037452,0.05314914,-0.010673073,0.003461359,-0.023519378,0.002370123,0.022563329,-0.095720366,-0.02166855,-0.052836318,-0.032594133,0.000116861556,0.016880263,-0.022222746,-0.024406921,0.0005215851,-0.01021345,0.057330374,-0.04778002,0.026918983,0.05307969,-0.002271616,0.09007996,0.016591696,-0.037839595,0.008872001,-0.03487318,-0.04342192,-0.022957668,0.022383206,-0.009047121,-0.011505636,0.03908492,0.03312735,-9.796386e-05,-0.0034852957,-0.031930503,-0.0028363334,-0.0039019801,-0.025797227,-0.052181546,-0.0075993966,0.020301603,-0.003438385,-0.0033976203,-0.0010734792,0.024351435,-0.048641462,0.048201274,0.042008944,0.0055356147,-0.009390618,-0.0029085833,-0.013859286,0.016333485,0.018655824,-0.0026721603,0.0023104025,0.05304699,0.008331278,-0.024532262,-0.020881882,-0.0011234186,0.0054928795,-0.021138493,-0.006453263,-0.021128276,-0.016724875,-0.0033885017,0.059695803,0.009188156,0.036386974,-0.021276826,-0.037031535,0.011743147,-0.042360455,-0.0125680985,0.035476644,0.004347332,0.018304843,-0.0025485663,-0.048656292,0.06395171,-0.07596926,-0.024974871,-0.028298266,0.014789937,-0.07133846,-0.018263234,0.0059102844,-0.051556017,-0.04528164,-0.029860394,-0.016268367,0.011564291,-0.06026102,0.01485118,-0.044421375,-0.004661649,-0.004271062,-0.016773392,0.0093155345,0.020728294,0.0073869685,-0.016306613,0.08540051,-0.060911935,-0.03308749,0.00018067507,0.012465258,-0.07731086,-0.024410123,0.043697283,-0.039622184,0.000712223,-0.014368639,0.016066743,0.013072183,-0.033313107,-0.040095117,-0.08348666,0.035565197,-0.027556403,-0.004914625,0.07429906,0.04750021,-0.032606248,-0.06109936,-0.053872213,-0.0061746025,0.041558404,0.0051760087,0.0067974348,0.044250257,0.08314354,-0.034963876,-0.061332375,-0.06164057,-0.024576837,0.009894054,0.024391841,0.075166665,-0.01705474,-0.04325079,-0.0046893307,-0.016328705,-0.021968696,0.02377343,-0.047000274,-0.031054482,0.029972173,0.013081576,-0.011503113,0.012646179,0.008971117,0.0054373126,0.07810239,-0.0073838956,-0.0149876,-0.09435382,-0.026610069,-0.029734842,0.00017475119,0.014248391,0.0017229154,0.025350383,0.015787939,-0.039099656,0.054882754,0.034809645,-0.062682524,-0.033966202,-0.0035719632,-0.0011660539,0.056769393,-0.010366813,-0.052557547,-0.07502188,0.005438331,-0.007183028,-0.046962366,-0.02664283,0.03685838,-0.0041301134,-0.004290581,-0.02565749,-0.014426419,0.026202973,0.025956346,-0.02551145,0.015654478,-0.025016375,0.018540915,-0.05238874,-0.023384178,0.0021568295,0.028076401,-0.00037501508,0.011209553,0.04009424,0.064516746,-0.024968997,0.0038529753,0.018849334,0.0032938905,-0.004182372,0.01433182,0.020502772,0.05830422,-0.03758173,0.020292083,0.06706184,0.0023491941,0.018949134,-0.022262812,0.052859392,0.036627397,-0.026269587,0.018873705,-0.04573793,-0.039130628,0.002682953,-0.01586608,-0.0011869913,0.02812099,0.011934951,-0.044685442,-0.02874994,-0.0043394696,-0.018347468,0.09251955,0.024158733,-0.07404306,-0.07547204,-0.010307363,-0.028790334,0.054166544,-0.025528915,0.0059444895,0.04842409,-0.0032513165]	Keywords: gradient noise, stochastic gradient descent, parallelizing, learning\nKey Objects: learning, networks, gradient noise, stochastic gradient descent\nRefers to Images: None\nHypothetical Questions:\n- Why might adding noise to gradients be beneficial for training very deep networks?\n- What are the advantages of a lock-free approach to parallelizing stochastic gradient descent?\n- How does Hogwild! address the challenges of parallelizing training?\n---\nSummary:\nNeelakantan et al. (2015) explored how adding gradient noise can improve learning in very deep networks, while Niu et al. (2011) introduced Hogwild!, a lock-free method for parallelizing stochastic gradient descent.\nOriginal Text:\n- [13] Arvind Neelakantan, Luke Vilnis, Quoc V. Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and James Martens. Adding Gradient Noise Improves Learning for Very Deep Networks. pages 1-11, 2015.\n- [14] Yurii Nesterov. A method for unconstrained convex minimization problem with the rate of convergence o(1/k2). Doklady ANSSSR (translated as Soviet.Math.Docl.) , 269:543-547.\n- [15] Feng Niu, Benjamin Recht, R Christopher, and Stephen J Wright. Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent. pages 1-22, 2011.\n- [16] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global Vectors for Word Representation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing , pages 1532-1543, 2014.\n- [17] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks : the official journal of the International Neural Network Society , 12(1):145-151, 1999.\nContextualized Text:\nThe following references detail various optimization techniques, with Neelakantan et al. (2015) investigating the benefits of adding gradient noise to improve learning in very deep networks, and Niu et al. (2011) presenting Hogwild!, a lock-free approach to parallelizing stochastic gradient descent.	{"tags": ["optimization", "deep-learning", "algorithms"], "doc_id": "eddf4b75-ff52-469f-b36a-8f76b2f0db2b", "summary": "Neelakantan et al. (2015) explored how adding gradient noise can improve learning in very deep networks, while Niu et al. (2011) introduced Hogwild!, a lock-free method for parallelizing stochastic gradient descent.", "doc_type": "text", "entities": [], "keywords": ["gradient noise", "stochastic gradient descent", "parallelizing", "learning"], "key_objects": ["learning", "networks", "gradient noise", "stochastic gradient descent"], "contextual_text": "The following references detail various optimization techniques, with Neelakantan et al. (2015) investigating the benefits of adding gradient noise to improve learning in very deep networks, and Niu et al. (2011) presenting Hogwild!, a lock-free approach to parallelizing stochastic gradient descent.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "References"}, "hypothetical_questions": ["Why might adding noise to gradients be beneficial for training very deep networks?", "What are the advantages of a lock-free approach to parallelizing stochastic gradient descent?", "How does Hogwild! address the challenges of parallelizing training?"]}
61bc5d3c-0456-4abc-a269-34b018d9d292	abe8c200-bfa1-4355-947e-23ea618c310d	[0.03466635,-0.009503561,-0.02229801,0.0068044313,-0.019542044,0.03481227,0.022730086,0.030748194,0.076234415,-0.039493863,-0.037250355,-0.021213656,-0.0019979621,0.010292,-0.040189203,0.0044188844,-0.014237837,0.096518874,0.047793698,-0.012344097,0.052845098,0.033662993,0.016357958,0.017416596,0.009064282,0.020917699,-0.0017502231,-0.013280746,0.0159171,0.017447172,0.015699994,-0.041181613,-0.0010462608,-0.012118916,0.012710287,8.829751e-05,-0.03018343,0.032059193,0.004920971,-0.07023847,-0.006824746,0.034040526,-0.01935648,-0.009255975,0.053672798,-0.08803779,-0.0110930735,-0.03810189,0.028020585,-0.032272015,-0.034885008,0.05353662,-0.029509943,0.014688312,-0.06825141,0.02786961,-0.034742497,-0.020535773,0.003938778,-0.050680213,-0.04113003,0.04300284,0.026923206,-0.017947193,0.06566986,-0.0766051,-0.044676427,0.019387718,0.02605918,0.09329721,0.009196571,0.0867729,-0.018820176,-0.0948235,0.08413983,0.041974895,-0.01608191,-0.025527364,-0.033396963,0.0059827487,0.022029893,0.05708659,-0.041348077,0.0022485221,0.062923566,-0.0068021775,-0.07131289,-0.051077932,0.07676555,-0.057480406,-0.03736855,-0.03721023,0.0008963586,0.055473357,-0.03138016,-0.02424201,0.01634324,-0.03411197,-0.0076640267,-0.03642448,0.011524982,0.05233867,0.029768202,0.12322628,0.011813245,0.013448708,-0.03848981,-0.020851042,0.0019500941,0.0073214658,0.014742207,0.024201546,-0.05106807,0.0033977279,-0.04765604,-0.012299036,-0.04079866,0.0134711685,-0.030303834,-0.041852716,0.06958898,-0.021334173,-0.02698297,0.0301013,0.0432774,0.055035196,-0.024125982,0.016380386,-0.04146209,-0.00036357672,-0.00032459345,0.01833916,-0.017253883,0.018011065,0.047347896,-0.026636988,0.06611527,-0.061813552,-0.0035808997,0.01715518,-0.010187215,-0.022309795,-0.024323473,0.019269025,-0.009250474,0.03329902,-0.058430977,0.051310606,-0.0010816758,0.028097428,0.014397867,0.02205598,0.066010766,-7.849274e-05,-0.020185482,-0.07139235,-0.03795586,0.0032786431,0.03434253,0.008627659,-0.058191095,-0.011216495,0.010926012,0.050606064,0.016192907,0.14343259,-0.007963925,0.012849103,0.03288262,0.004726697,-0.019088285,0.008530417,0.026642734,-0.015613425,0.021807702,-0.039694164,0.031113733,0.0146679375,-0.063593864,0.022654597,-0.02336985,-0.008609867,-0.025754215,-0.0070985197,0.003925622,0.056822874,-0.019220077,0.04664269,-0.007520676,-0.06166813,0.020814808,-0.02228007,-0.0076902416,0.004592855,0.024467813,0.019925913,-0.00015026989,-0.05594153,0.005741133,-0.032211382,0.01815313,0.04237406,-0.013733339,0.02771036,0.030395055,-0.020054009,0.0016915884,-0.046166465,-0.037235584,-0.0007718342,-0.04218109,0.02492137,0.03322722,0.052882854,0.023213588,0.038011916,-0.014690909,-0.013579262,0.015559861,0.004579411,-0.07889448,-0.01684966,-0.004931476,0.030111907,-0.020783687,-0.0102654975,0.01072483,0.029655652,0.060125243,-0.0321856,0.012167509,0.05144626,-0.041076675,0.04022639,-0.025402972,0.024725731,0.005067065,0.0033131985,0.012249157,0.070669346,0.031155713,0.051686168,0.034683164,0.07022732,-0.014110181,-0.01510625,0.05770927,0.045891732,-0.013324235,0.01772351,-0.011651632,0.0052987533,0.032207206,-0.014324068,0.002539025,-0.022501525,0.045613166,0.02330097,0.067275904,-0.04431639,-0.019714598,-0.04430874,-0.0035438307,0.0129296975,0.02859376,0.0500073,0.008321013,0.044600423,-0.0013147809,0.037883125,0.02136207,0.028302139,0.0014100403,-0.032352876,-0.08498757,0.04747684,-0.0017050022,0.025744852,0.03501435,-0.002755213,-0.002467808,0.019207677,0.03106327,-0.037885554,0.022517227,-0.029190755,-0.008348043,-0.0037105,-0.013130992,0.009882992,0.021698052,-0.011036927,0.029699942,0.010916228,-0.017237673,0.018728124,0.02617462,-0.04922034,0.05691254,0.03336784,0.07189308,-0.016381247,0.04363258,0.006562188,0.017236687,0.0036720198,-0.08593923,0.038285047,0.027320046,0.024196353,0.04505594,-0.031846553,-0.011668795,0.0004152408,-0.012251654,0.012891032,0.05340712,0.014582135,0.017302062,0.040596224,0.026138594,-0.011007596,-0.028692672,0.07250426,0.00074661314,-0.01704229,-0.034782525,-0.031484015,-0.075353995,0.049661398,-0.04927969,0.04952344,0.004529336,0.008544659,0.0038154388,0.00066545117,-0.009639838,0.01043911,0.04149061,0.009670167,-0.0017135589,-0.07346494,0.02558939,-0.003670181,-0.018591287,-0.02751124,0.004807368,0.017077986,0.02213166,0.07051334,-0.016040163,0.03666185,0.06692769,0.03221438,0.0278489,0.026189724,0.0040375385,0.0057144454,0.026834734,-0.00916132,0.024314981,-0.045875277,0.078337766,-0.037240688,0.049565926,-0.013625413,-0.04791435,-0.02843909,0.023861555,-0.015007795,0.036511645,0.0019658762,0.053514548,-0.08253624,-0.013915408,0.019856088,0.037804138,0.04836749,-0.01963202,-0.077831715,-0.030007938,0.032666463,-0.02842927,-0.032464553,0.009553748,0.03397207,0.02453079,0.06418624,-0.013621728,-0.02911108,0.0023004254,-0.00036123153,0.03126965,0.022860486,-0.029518068,-0.016589427,0.042675015,0.026703544,0.03719332,-0.026287,0.025343424,0.014678118,-0.06531541,0.031711787,0.0022358936,0.0010640717,-0.061974704,0.01337107,0.06947169,-0.031812802,0.028869571,-0.08020761,-0.044094373,0.021200696,-0.0067537464,-0.022214022,0.0039266,-0.0058822474,-0.038688317,0.0031200233,-0.037578072,0.070483364,0.033933494,0.04004568,0.012843592,-0.0036751607,-0.046651445,0.049026765,-0.0025885063,-0.02623763,0.030264487,0.002037467,-0.020101193,0.03617281,0.007341908,0.012486027,-0.034626342,0.060677275,-0.0707729,0.020097012,0.028024385,-0.038611405,0.03455896,-0.015387671,-0.014394986,-0.017513335,0.027265204,-0.011887976,0.012193207,-0.0002395556,-0.0018504331,0.019618504,0.011377459,0.016360989,0.025540262,-0.013367524,0.0017826511,-0.0106675,-0.024236355,0.019817015,-0.0037483997,0.055598535,-0.030944493,-0.0049337405,-0.00023144642,-0.01341939,-0.05997947,-0.05702481,-0.0056682494,-0.0027295249,-0.03486103,0.035681993,-0.026593382,0.020398106,0.020860039,0.013884508,0.030630087,0.01773735,0.0551158,0.03629701,-0.05597127,-0.03880684,0.0227591,0.08654408,0.02377991,-0.02631433,-0.0092655625,-0.020663798,-0.032837685,0.018617462,0.00859973,0.031666346,0.00043295466,0.024486935,0.011646761,-0.08489873,0.008425701,-0.004663002,0.0072747488,-0.0421313,-0.013366374,-0.0080980975,0.07263155,0.019856695,0.032158036,-0.0020345184,0.038520575,0.066728234,0.00166007,0.04325451,0.022796843,0.002951536,-0.014626545,-0.033654943,0.0026990217,0.04500983,0.0037773552,-0.025785252,0.02378235,-0.033047955,0.0077156527,0.002309437,-0.018972743,-0.011371352,0.0015926063,-0.003929982,-0.05787376,0.029590959,0.061146863,-0.04178931,-0.00069155265,0.011111296,0.02120777,0.034432337,-0.08305511,-0.041067485,-0.0026246298,-0.060659133,-0.004848073,-0.0024852352,-0.037427645,-0.04947329,-0.013605062,-0.009816586,0.07718038,-0.059118655,0.038827468,0.025820618,0.007647756,0.068947315,0.042963505,0.0063121193,0.03467728,-0.013264774,0.0025612595,0.00374373,0.010874346,-0.015878037,0.014821975,0.0501038,0.054826517,0.014407562,0.003227547,-0.03897208,-0.0053591053,0.014724077,0.002052381,-0.07151087,-0.021600215,0.030855572,0.026096052,0.021057997,0.0124436,0.035504527,-0.036459975,0.07349085,0.055800352,-0.023332147,-0.049370248,0.016775027,0.027364612,0.038016256,0.05799519,-0.0031107634,0.019490302,0.041112207,0.0029061583,-0.004935717,0.0036555487,-0.034510378,0.008783102,0.02005905,-0.017535033,-0.0062059355,-0.024042461,-0.03325619,0.06401609,-0.018374074,0.033009898,-0.022072371,-0.04003996,0.035934545,-0.03343165,-0.01755291,0.033832993,-0.016612055,-0.042799767,0.02279278,-0.0136701,0.06410972,-0.07306278,-0.022672988,-0.026408263,-0.00757407,-0.12525444,-0.005486342,0.0084830215,-0.013197963,0.006142755,-0.054842424,0.019892061,0.022243185,-0.06057996,0.048186228,-0.03453779,-0.01570384,-0.015125613,-0.008736465,-0.05592449,0.014191925,-0.0070347507,-0.027463883,0.083115324,-0.05993534,-0.03965212,0.030354297,0.018466748,-0.057070415,0.0001925735,0.061539292,-0.043699093,-0.0032148534,-0.024804622,0.036781356,0.005698622,-0.028906839,-0.021392668,-0.08764779,0.02956607,-0.017431423,-0.0037626706,0.051406223,0.030566817,-0.044968836,-0.043591063,-0.037532926,0.013531203,-0.0049315062,0.054999,0.009701445,0.042311262,0.056669787,-0.056603923,-0.05408175,-0.037881836,0.024469422,0.033798166,0.0090496065,0.07950301,0.02295301,-0.044410113,0.012315147,-0.04603558,-0.05511261,0.0028275226,0.04418932,-0.04360665,0.033644486,-0.0015879461,-0.04214234,0.06422527,0.0064541413,0.0064060665,0.07359974,0.022072438,-0.032156274,-0.063356794,-0.019425761,-0.046386454,-0.012153195,0.006466704,-0.039805092,-0.0045264834,0.0060237143,0.005244811,0.033407528,0.008741608,-0.012279702,-0.027488187,0.026571581,-0.03352888,0.036092065,0.019219879,-0.066789955,-0.09756224,0.029001238,-0.01643842,0.00030203006,0.0059008114,0.04306238,0.03459296,-0.022635778,-0.03952952,0.0026408941,0.026046138,0.03570385,0.014946113,0.018068107,-0.012538473,0.019113274,-0.035817765,-0.054134965,-0.0022625702,0.023222147,-0.044919863,0.028325105,0.035194676,0.06397352,-0.008528236,0.0033555282,-0.025097972,-0.013186511,0.0063178437,-0.00018750326,0.0034573998,0.057367507,-0.022668371,0.01702953,0.0371419,-0.029479593,0.011609027,0.0012622538,0.011580312,0.017557504,0.011950118,0.018889012,-0.04604774,-0.019012878,0.011300262,-0.0060621663,0.00039904396,0.018057473,0.01070607,-0.05420337,-0.061992913,-0.040910274,-0.0279667,0.06609706,0.028062567,-0.029158838,-0.08654776,0.0037972922,-0.030551601,0.04392106,0.0025296279,0.02125965,0.028344186,-0.04351648]	Keywords: gradient descent, optimization algorithms, momentum, adaptive learning rates, references\nKey Objects: references, algorithms, optimization\nRefers to Images: None\nHypothetical Questions:\n- What specific advancements did the 'momentum term' introduced by Ning Qian contribute to gradient descent?\n- How do the approaches described in these references relate to modern optimization techniques used in deep learning?\n- What common threads or challenges are evident across these early explorations of optimization algorithms?\n---\nSummary:\nThis section lists several references related to gradient descent optimization algorithms, including works on momentum terms, stochastic approximation methods, and adaptive learning rate techniques.\nOriginal Text:\n- [17] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks : the official journal of the International Neural Network Society , 12(1):145-151, 1999.\n- [18] Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathematical Statistics , 22(3):400-407, 1951.\n- [19] Ilya Sutskever. Training Recurrent neural Networks. PhD thesis , page 101, 2013.  \n- [20] Richard S. Sutton. Two problems with backpropagation and other steepest-descent learning procedures for networks, 1986.\n- [21] Wojciech Zaremba and Ilya Sutskever. Learning to Execute. pages 1-25, 2014.\n- [22] Matthew D. Zeiler. ADADELTA: An Adaptive Learning Rate Method. arXiv preprint arXiv:1212.5701 , 2012.\n- [23] Sixin Zhang, Anna Choromanska, and Yann LeCun. Deep learning with Elastic Averaging SGD. Neural Information Processing Systems Conference (NIPS 2015) , pages 1-24, 2015.\nContextualized Text:\nThe listed references provide a foundation for understanding gradient descent optimization algorithms. They include works exploring momentum terms, stochastic approximation methods, and other techniques for improving the efficiency and stability of network training. These papers contribute to the development of more effective optimization strategies for machine learning models.	{"tags": ["optimization", "references", "deep-learning", "machine learning"], "doc_id": "61bc5d3c-0456-4abc-a269-34b018d9d292", "summary": "This section lists several references related to gradient descent optimization algorithms, including works on momentum terms, stochastic approximation methods, and adaptive learning rate techniques.", "doc_type": "text", "entities": ["SGD", "ADADELTA", "Neural Networks"], "keywords": ["gradient descent", "optimization algorithms", "momentum", "adaptive learning rates", "references"], "key_objects": ["references", "algorithms", "optimization"], "contextual_text": "The listed references provide a foundation for understanding gradient descent optimization algorithms. They include works exploring momentum terms, stochastic approximation methods, and other techniques for improving the efficiency and stability of network training. These papers contribute to the development of more effective optimization strategies for machine learning models.", "mentioned_images": [], "section_hierarchy": {"h1": "An overview of gradient descent optimization algorithms", "h2": "References"}, "hypothetical_questions": ["What specific advancements did the 'momentum term' introduced by Ning Qian contribute to gradient descent?", "How do the approaches described in these references relate to modern optimization techniques used in deep learning?", "What common threads or challenges are evident across these early explorations of optimization algorithms?"]}
f8fc7437-d4a2-4451-af76-3e85a1f518ff	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.043353893,0.029328927,0.049316466,-0.024962462,-0.011766103,0.04606996,0.016395407,0.046490707,-0.014110174,-0.014688507,-0.013482874,0.030106865,0.049741425,0.0004271472,-0.017128216,0.0005086634,0.017903099,0.04239412,-0.011572455,0.016817871,0.0050539216,0.03844425,-0.029950732,-0.025327973,0.012779588,-0.011488949,0.021593593,0.008073218,0.042236276,0.023280056,-0.0013635597,-0.083079986,0.009987422,0.015528572,0.012045786,0.0183299,0.00038914048,-0.054820653,-0.021173421,0.00072991906,-0.0572279,0.011275001,-0.060404733,0.016497448,-0.019584654,-0.0027561055,-0.07447014,-0.07597483,-0.02074088,0.004694592,-0.008472973,0.015928145,-0.058811214,-0.039455473,0.066049606,-0.040091913,-0.057071093,-0.045931272,0.002415972,-0.07104484,-0.03369917,0.004254486,-0.04879221,-0.06544941,0.035991427,-0.025876138,0.04447899,0.047361817,-0.009744772,0.121603906,-0.036984652,-0.034300342,-0.02156688,0.018217871,0.09471258,0.0011702694,-0.0070263958,-0.059804518,-0.042075492,-0.0013923275,-0.03583218,0.054388616,-0.017919343,-0.015401472,0.08244515,0.019810287,-0.038860913,-0.07006952,0.086595304,-0.08247385,0.009110902,-0.020925634,-0.03196777,0.029207816,0.004733395,-0.07463841,0.017360639,-0.07193787,0.03420987,-0.022806892,-0.02730976,-0.024568195,0.05742234,0.072043575,0.041106634,-0.067982905,-0.0070754318,-0.045096494,0.031130359,-0.00906445,-0.007358428,0.010845997,-0.002287028,0.006330008,-0.023471523,-0.054919492,-0.043382213,-0.017992096,0.0067253625,-0.0445815,-0.022117127,0.028480787,-0.0013776846,0.027770141,0.075728685,0.055112813,0.0037139894,0.025968317,0.017226543,-0.0243496,0.008457748,0.014933988,0.064858235,0.0037079647,-0.021646312,-0.020814173,0.10867288,-0.036830403,0.007848906,0.0009759547,0.013017485,-0.040049925,-0.023423072,0.0501098,-0.0012800123,0.0028607012,-0.021565642,0.01998966,0.010492705,0.020864738,0.03748202,0.012382995,0.023953272,-0.009841431,-0.021963093,0.0075550843,-0.040676963,-0.08881581,-0.010379438,-0.021466095,-0.005710908,-0.04344542,-0.041416153,0.035138037,0.046876993,0.07009298,0.030934867,0.023420902,0.002585439,0.02218221,-0.06102023,-0.016632387,0.030535031,0.006590377,-0.00051588763,-0.026369426,0.0005744003,0.006322869,-0.023081468,0.020450039,-0.0008087052,-0.009436804,-0.0330633,0.0017864612,0.011975821,0.008314682,-0.007722645,0.07817548,-0.02797316,0.023651792,0.019272275,0.004632334,0.0049235784,0.011045586,0.045191057,0.04121641,0.048032008,-0.08580321,-0.029699948,0.007582102,-0.004789673,0.0009144132,-0.0011812672,-0.005309628,0.0014845382,-0.011738466,0.0026241594,0.03606021,-0.021303827,-0.0066374163,0.033972804,0.013028056,-0.016744196,0.056056425,-0.013406625,-0.025962736,0.03881564,0.02612911,0.046878763,-0.05113172,-0.028229387,0.07553906,-0.0058575,0.009839197,-0.0039692195,-0.027046781,-0.059919056,0.009879109,0.0033894207,-0.026847756,0.039490834,0.06037851,-0.007952423,0.024010185,-0.029047113,-0.017808178,-0.015850654,-0.005694747,0.011944398,0.0474239,0.028521843,0.030777385,-0.014578797,0.05718474,-0.037550736,0.011580589,-0.0068880026,0.03401796,-0.0032346519,0.031017018,0.000931067,0.044425312,0.03231094,0.051591523,0.03221893,-0.037830442,0.053894784,-0.00431995,0.043996207,0.0054469686,0.0147721525,0.020274226,-0.015111959,0.016032387,0.003414731,-0.009976538,-0.01240323,0.006559151,0.043289114,-0.026713446,0.02131247,0.041227594,0.045936864,-0.016083233,-0.033027258,0.026644252,0.0021703038,0.057744052,0.043776285,-0.038957894,0.02816928,-0.017000075,-0.015700242,0.014679373,-0.03320382,-0.05887003,0.008248272,-0.039291333,-0.063760325,-0.004508289,-0.015985355,-0.020409726,0.027449926,-0.021504976,0.025676034,0.033163406,-0.012718539,-0.028352244,-0.040864468,0.03718265,0.07920619,0.022135837,-0.007732439,0.0059554847,-0.038963836,0.0001743479,-0.028521769,-0.0009541361,0.027338827,-0.007063277,-0.018309703,0.010718111,0.006443136,0.0023155003,0.06895475,0.11844199,0.034274854,-0.09436287,-0.014347238,0.020213205,0.04007247,-0.03894189,-0.026683142,-0.0027598718,0.027833464,0.01482369,-0.007802232,-0.0029472492,-0.07318969,0.046571072,-0.057040267,0.0353005,0.071957394,-0.0023810975,-0.039755225,0.022228722,-0.012870879,0.004964926,0.040040035,0.020419104,0.015706152,-0.08399305,-0.029997516,0.030163782,0.01833013,0.047692537,0.029939678,0.04577614,0.005676552,0.05106947,0.01292601,0.015418801,0.053962633,-0.012979571,0.003106567,-0.043667886,0.033734713,-0.050075427,0.04615285,0.03009937,0.07468287,0.0058609173,-0.0054437434,-0.05624002,0.013569043,0.025813282,-0.05657629,-0.026519336,0.02629316,-0.008831616,-0.030007387,-0.039316315,0.061632346,-0.039368443,-0.00011834065,0.018697673,0.084423065,0.03624611,-0.010767875,-0.023789518,-0.025956389,0.0072687143,-0.011401743,-0.014502517,0.035834126,0.036106,0.027183877,0.02204996,-0.015104613,0.06509153,-0.009708037,-0.017770588,0.01745495,-0.026829422,-0.01521251,0.0317976,0.017931854,0.02466574,-0.006000082,-0.07143114,0.023914633,0.042522278,-0.019171987,0.041131105,-0.03854132,0.0017417228,-0.044431552,0.014617132,0.009062584,0.040901184,-0.0021625,0.014802042,-0.066544056,-0.0027627526,-0.041207794,-0.014355536,-0.023464534,0.05543811,-0.05865479,-9.74682e-05,-0.063580014,0.07555259,0.0070721437,0.034743622,-0.0036567948,-0.029371949,0.0007194316,0.027916582,-0.006126765,0.004426668,0.033500686,-0.019292006,-0.02692859,0.015415971,0.011841915,-0.03792655,-0.07305693,0.048348907,-0.013348378,0.052444838,0.053241156,-0.0010595545,0.011551254,-0.007680318,0.0056211757,-0.029133718,0.013829627,0.054570347,-0.0021163037,-0.029514218,0.00429641,0.01984268,0.006211304,-0.018714802,-0.029420214,-0.0074870326,-0.01947156,0.056552872,-0.04178075,-0.018592928,-0.04283708,0.030881498,0.028140854,-0.009207534,0.017158998,-0.060471084,-0.0027418064,-0.0021373387,0.021973543,0.042203017,0.012854186,0.033961482,-0.028120507,-0.03243106,-0.00050144317,-0.003343855,-0.034453277,0.07570907,0.023131186,0.0011452648,-0.111767575,-0.03341684,-0.021802764,0.03641415,-0.025706455,-0.021973576,-0.03923965,-0.068205945,-0.03471194,0.021890407,0.02396462,0.05035984,0.02934758,0.03175685,0.038022522,-0.030208828,-0.004584342,0.03440318,-0.044373382,-0.052264016,-0.06659365,0.03739748,0.078313105,-0.011762411,-0.042810574,0.017027626,-0.016398236,0.011772266,-0.038972598,0.059396364,0.030839505,0.022670424,0.001340382,-0.00066338474,-0.026171323,0.007454541,-0.018718274,0.005694793,0.032830223,0.040621646,0.0017124311,0.007412531,0.05968371,-0.045710307,0.01145489,-0.021807358,0.034870993,0.022327723,0.07681101,-0.022662405,0.06994567,0.031074153,-0.0016575813,0.01423316,-0.0026068096,-0.007807269,0.06494111,-0.0536153,-0.02225746,-0.044878162,-0.0020770824,0.007543567,0.0070694964,-0.015881045,0.08147847,-0.022109749,0.010649814,-0.049690776,-0.022873865,0.08881398,-0.020277461,-0.027045663,-0.0041366643,-0.047343384,0.0015904368,0.0300697,0.06588584,0.055409707,-0.03201963,0.044752464,-0.028429402,0.017692829,0.025965894,-0.05221346,0.010471007,-0.04235089,-0.03938077,-0.012184744,-0.021517126,0.003481964,0.008056206,0.01962506,-0.030239848,-0.027369037,-0.009069916,0.045892656,0.020926174,-0.04406783,0.007435314,-0.030760562,0.056928225,-0.0040752105,0.0074922806,-0.015910817,0.044140212,0.013399769,0.0074848933,-0.024324771,-0.009763945,-0.028330542,0.033220742,-0.017205017,0.018714786,0.03023397,-0.07590479,-0.042413197,0.0110834595,0.020874677,0.058754914,-0.018136792,-0.019712022,-0.02244736,-0.03921229,-0.032180913,0.007762495,0.029462038,0.033464566,-0.03054858,0.009192299,0.01104393,-0.03922526,-0.014404934,-0.02298385,0.013126315,-0.016427454,0.034089543,-0.0359443,-0.024182105,-0.04504954,-0.061003365,0.013524528,0.049422596,-0.03013663,-0.031018531,-0.017266275,-0.020486077,-0.0028900655,-0.022897344,-0.004692128,0.0133784,-0.04887928,-0.0119678285,0.07520919,0.00357069,-0.008877755,0.043419346,0.029342897,-0.0868898,-0.033846226,-0.0065354644,0.05714454,-0.023055736,-0.028516036,-0.0016230161,-0.012357474,-0.0113364775,-0.020158306,-0.047911506,0.008204713,-0.04442749,-0.06490361,0.08755336,0.051823944,-0.078652725,0.040255878,-0.05988774,0.005879677,-0.027704658,0.0020741788,0.041443817,0.021218093,-0.005878326,0.0022245059,-0.0706403,-0.0116698,-0.050656933,0.021899046,-0.030497126,0.025615789,-0.011272794,-0.028310917,-0.024126384,-0.028572004,-0.014648551,-0.0064615095,-0.008085708,0.006739003,-0.04662656,-0.047143683,0.033794925,0.020740222,-0.03719429,-0.0120610725,0.086748146,0.031742774,-0.023002386,-0.032110155,-0.052391175,-0.0050988006,-0.0042633414,0.030304324,0.027448185,0.009413538,-0.025083704,0.009120664,0.00063154136,-0.01547174,-0.0720783,0.039205857,0.0462232,-0.011368591,0.042564783,-0.00092528516,0.003118997,-0.102648005,0.018478177,0.015449839,-0.0623204,0.027636405,0.016725881,-0.030131625,-0.0376194,0.004730613,0.018437527,0.009123807,-0.018262414,0.052911144,0.037436485,0.0044479496,-0.018937297,-0.05093577,0.034258418,-0.009917789,0.01922557,-0.011399232,0.044958003,-0.011127852,0.016017074,0.0036514103,-0.023957092,0.008707883,-0.018190503,0.008986578,0.031083994,-0.068047814,-0.047343947,0.0022659304,0.004259134,0.041224036,-0.009396565,0.039480336,0.008406321,0.07207166,0.056026,-0.08651167,-0.01906143,0.0016911759,-0.08918241,0.006545621,0.026230358,-0.010211309,0.00063188723,0.04053343,-0.019370735,-0.011370312,-0.057942085,0.06506679,0.02905859,-0.018465681,0.057369843,-0.03511942,-0.003409776,-0.041669335,-0.004930611,-0.007178989,0.03879425,0.029834593,-0.01783708]	Keywords: Transformer, survey, variants\nKey Objects: Transformer variants, survey\nRefers to Images: None\nHypothetical Questions:\n- Why is a survey of Transformer variants needed?\n- What is meant by 'X-formers'?\n- What types of areas do Transformers impact?\n---\nSummary:\nThis survey paper, authored by Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu, focuses on providing a comprehensive review of Transformer variants.\nOriginal Text:\n# A Survey of Transformers  \nTIANYANG LIN, YUXIN WANG, XIANGYANG LIU, and XIPENG QIU $^{}$, School of Computer Science, Fudan University, China and Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, China\nContextualized Text:\nAuthored by Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu from Fudan University, this survey paper aims to provide a systematic review of Transformer variants, also known as X-formers, which have seen widespread adoption across various AI fields.	{"tags": ["NLP", "deep-learning", "survey", "transformer"], "doc_id": "f8fc7437-d4a2-4451-af76-3e85a1f518ff", "summary": "This survey paper, authored by Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu, focuses on providing a comprehensive review of Transformer variants.", "doc_type": "text", "entities": ["Tianyang Lin", "Yuxin Wang", "Xiangyang Liu", "Xipeng Qiu", "Fudan University"], "keywords": ["Transformer", "survey", "variants"], "key_objects": ["Transformer variants", "survey"], "contextual_text": "Authored by Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu from Fudan University, this survey paper aims to provide a systematic review of Transformer variants, also known as X-formers, which have seen widespread adoption across various AI fields.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers"}, "hypothetical_questions": ["Why is a survey of Transformer variants needed?", "What is meant by 'X-formers'?", "What types of areas do Transformers impact?"]}
4f73b69a-54f8-4e7b-891a-26a7efcec584	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0263816,0.03419228,0.0313092,0.02505659,0.019696137,0.05748604,0.006683751,0.03660559,0.016359087,-0.021771602,-0.001607344,0.010492436,0.012922075,0.010462109,-0.017360674,0.01794146,0.0051799193,0.06063427,0.0032384687,-0.009613117,0.056433577,-0.004130782,0.0029139167,-0.029104948,0.039245415,-0.0041464544,0.034600332,-0.01082908,0.031835057,0.011577021,-0.014450155,-0.056981795,0.04202118,0.006713205,-0.014639788,0.0141495755,0.029012512,-0.051468175,-0.012243523,-0.010991592,-0.04128053,0.037796892,-0.08703302,0.024129326,-0.06931643,-0.019406935,-0.09548434,-0.04248273,0.0144897755,-0.018021172,-0.023684176,-0.008816179,-0.044492114,-0.048565146,0.03741034,-0.015504115,-0.03770955,-0.023344122,0.02111379,-0.07039983,-0.035110064,0.014573761,-0.048905622,-0.03973928,0.04546547,-0.03544925,0.013065194,0.03294623,0.041246567,0.118681245,-0.0013693523,-0.032332215,0.009970991,-0.013623978,0.114426546,0.020477107,-0.027796604,-0.05799771,-0.047774836,0.00012588568,-0.052384913,0.10090218,-0.059398048,-0.031163894,0.096325815,0.021595327,-0.04061134,-0.05969039,0.07088513,-0.02838334,0.023764197,-0.006328725,-0.025407242,0.04541674,-0.016682198,-0.09566723,0.014606621,-0.047208376,0.038066536,-0.03358749,-0.010742514,-0.02162889,0.028239433,0.08110097,0.048042264,-0.0015178443,-0.017245583,-0.043505464,0.04482428,0.0062237736,-0.022110987,0.03019779,0.00021573642,0.0051751416,-0.029303866,-0.02727138,-0.050733265,0.0056212395,0.042039987,-0.0048279976,-0.015088151,-0.007053059,-0.009256434,-0.01910306,0.04939939,0.048588842,0.023181887,0.025246736,-0.018880054,-0.043450546,0.0012956297,-0.015040304,-0.0041934703,0.025371267,-0.018876282,-0.013620132,0.11114502,-0.013342322,0.00591601,-2.2781314e-05,-0.004417306,-0.045159753,-0.006617043,0.007131345,-0.02369172,0.005044164,-0.018825352,-0.006515789,-0.006022178,0.041531067,0.044759553,-0.010268125,0.036132272,-0.012581754,-0.03240192,0.0139299575,-0.045332592,-0.0752296,0.0033147507,-0.006914592,-0.02371071,-0.024288796,-0.046585012,0.023028484,0.013103307,0.081441276,0.03535923,0.037107892,0.01702403,0.024986511,-0.04092,6.974073e-05,-0.015769817,-0.013566112,-0.009821243,-0.0133082215,-0.009312641,-0.009626122,-0.011211438,-0.026734773,-0.015561971,-0.018857107,-0.006077917,0.029735763,0.021717358,0.012209082,-0.020825801,0.04215282,-0.06048308,0.021817239,0.02778747,-0.043437786,0.001182016,0.020899303,0.050276753,0.03602854,0.05488086,-0.05196062,-0.045483064,0.033480234,0.0013545558,-0.008348797,0.011086157,-0.022668006,-0.020095294,0.0151737,-0.0063328855,-0.0075709242,-0.01407435,0.006882701,0.036347892,-0.035038963,-0.030406248,0.091644205,0.033559877,0.007944808,0.026941188,0.031065179,0.031227069,-0.0022703102,-0.03268784,0.032252677,-0.0060450407,-0.004188665,0.009599177,-0.0057487935,-0.030566467,0.04017977,0.00085316465,-0.055332467,0.0724923,0.034142062,-0.034821503,0.014461503,0.02647365,0.055647843,-0.010830371,-0.014144074,0.039617807,0.05461517,0.03448981,0.023219112,-0.043744538,0.060680248,0.006280546,-0.027542511,0.009748331,0.025028523,-0.0035581635,-0.015122533,0.016617246,0.04505503,0.047729112,0.013608091,0.0442271,-0.03399797,0.048038114,0.01408346,0.04948569,-0.0051242583,-0.011050411,-0.009195083,-0.011337319,0.01965369,0.01344762,-0.03168521,0.01107374,0.026376722,0.036965214,-0.021236839,0.043731704,0.060260832,0.01833263,-0.02626561,-0.039747834,0.016103495,0.010579126,0.03365335,0.03017999,-0.025196342,-0.0009099007,-0.05259625,0.015881922,0.00018328227,0.0009731184,-0.042448338,0.02042179,-0.051903207,-0.13047433,-0.004439384,-0.011491487,-0.016174609,0.010243409,-0.007100913,0.031878047,0.036079135,-0.009028853,-0.021685978,-0.0347933,0.031840444,0.09036297,-0.008200778,0.011348698,0.00035801498,-0.024793351,0.002413368,-0.0625838,-0.013414463,0.003551447,-0.015384069,-0.0037875178,0.029047873,-0.042966224,0.008940669,0.039990656,0.09712021,0.06894951,-0.079795964,0.022114102,-0.00567187,0.043100756,-0.011427216,-0.04134959,0.008964182,0.040013734,-0.011882901,-0.0017579003,-0.0016868209,-0.07256895,0.04463957,-0.080650404,0.065740116,0.066610426,-0.01312588,-0.018930038,0.0156190945,-0.00079871115,0.035344746,0.020626755,0.019225566,0.024299411,-0.06554372,-0.0152255185,-0.0049359556,0.017659415,0.032283287,-0.02046637,0.0219151,-0.02132393,-0.0059839073,0.0010048588,0.03543,0.026819548,-0.00011075501,0.009997526,-0.035711262,0.02699809,-0.01909801,0.04957572,0.0071780602,0.058229763,0.010186675,0.049758058,-0.010670883,0.020709025,0.033641968,-0.035988525,-0.04143477,0.012639438,-0.03787859,-0.015615007,-0.020281954,0.059763294,-0.028325366,-0.0025381534,-0.004399735,0.07932538,0.027783457,-0.021017468,-0.014422703,-0.017697822,0.02915755,-0.024767688,-0.018174317,-4.8964874e-05,0.022156721,0.03376224,0.008165716,-0.027152482,0.033377543,-0.032214947,-0.01675692,0.013345665,-0.0003066629,-0.032231238,-0.0090227695,0.03190154,0.041257232,-0.015076364,-0.043410454,-0.006033416,0.056596097,-0.0288029,0.04515177,0.013067373,0.02618207,-0.04682632,-0.013449316,0.05214325,0.027228225,0.00015024058,0.04385087,-0.056483477,0.026868956,-0.018657148,-0.0038344876,-0.0050712773,0.07741111,-0.05637109,0.0029349758,-0.045370523,0.067024335,0.0025526886,0.04497968,0.016046172,-0.019567082,-0.033527452,-0.0025791123,-0.016027132,0.042450916,0.050169732,-0.057049144,-0.03516855,0.048257727,0.06457104,0.010209214,-0.07584825,0.05492757,-0.06539441,0.049782738,0.07037417,0.014871944,0.021011973,-0.0070680603,0.016935432,-0.014858683,-0.0114676,0.048485894,-0.036073204,0.0022119337,-0.025210785,0.024307234,-0.0037129775,-0.009211474,0.024832133,0.026932148,-0.009592694,0.02522896,-0.023885071,0.0050856094,-0.020098709,0.04881537,-0.008582433,0.0072995536,0.026880037,-0.04839672,0.01609191,-0.036793716,0.015992492,0.026659587,0.019462878,0.02872764,-0.03629522,-0.024672018,-0.029892951,0.0055022347,-0.005284577,0.058970276,0.036011204,0.012542406,-0.05599401,-0.028295677,0.0024403415,0.062627405,-0.04492972,-0.03553715,-0.0312723,-0.06478098,-0.043348007,0.027618388,0.011530349,0.052256986,0.042264342,0.025232391,0.05767015,-0.04029356,-0.013514911,0.033763673,-0.028400334,-0.065470316,-0.030032607,0.0080372775,0.07235051,0.0220396,-0.007021592,0.015797723,-0.022963932,0.0060261963,0.0002051365,0.063580215,0.01770976,0.01467006,0.013654003,-0.037253696,-0.008516029,0.024383118,-0.0017482733,-0.014547857,0.02506717,0.03851311,-0.007921683,0.044980075,0.049762074,-0.010930598,-0.0068159113,-0.036363453,0.07035554,0.029646035,0.10820005,-0.042985667,0.08312756,0.003997746,-0.03655547,0.007618367,-0.010672419,0.00719854,0.06600589,-0.06075944,-0.035836175,0.030075843,0.009174093,0.0021293133,0.024976363,-0.034806784,0.07436442,-0.07002539,0.0064485106,-0.064397074,0.012304141,0.06031186,-0.017187456,-0.012365174,0.0009057446,-0.07513311,-0.012012779,0.0134720085,0.06794832,0.040524326,-0.006240505,0.05288647,0.018740943,0.033633396,0.024696322,-0.039966334,-0.00955724,-0.003752461,-0.019315202,0.012660048,-0.0021077467,0.026368856,0.0011330951,0.020080926,-0.01646317,0.009439055,-0.02489695,0.059709273,0.047857743,0.008777889,0.04759395,-0.00037844075,0.05352488,0.026049623,-0.023796728,0.018185558,0.038065623,0.012551765,-0.0055118855,-0.02056513,0.033761077,-0.007998431,0.0063269534,-0.030403525,0.04565684,0.021369731,-0.05930488,-0.044444248,-0.0016932562,0.05568063,0.048546635,-0.03595384,-0.056185663,-0.015458894,-0.036546797,-0.038967066,0.02224331,0.008645413,0.013359017,0.00053742796,-0.066231854,0.055733234,-0.09582354,0.01832949,-0.022130286,0.009242089,-0.009021953,0.0019047835,-0.006048483,-0.035988532,-0.03476488,-0.043202177,0.022499679,0.08765197,0.0015634192,-0.036267046,-0.0044485205,-0.03940831,-0.022301827,-0.040470224,-0.0069083176,-0.018237839,-0.021630265,-0.0063515636,0.03609765,-0.012882139,0.0031139213,0.030344993,-0.0055133314,-0.05275069,-0.0151206395,0.0005452554,0.043531965,0.017742677,0.008979138,0.037934773,0.04251491,-0.044252347,-0.03386663,-0.014494391,0.007132652,-0.01155454,-0.03219449,0.058634017,0.021863462,-0.051071845,0.010067367,-0.046896815,-0.017252674,-0.011781832,0.026615558,0.0716581,0.029436704,0.0242815,-0.029388744,-0.10713489,-0.011357606,-0.026869124,0.033854075,-0.06819411,0.026318837,-0.03805542,-0.022334475,-0.039943032,-0.012091089,0.012118181,0.025146628,0.0067700315,-0.007722053,-0.038054556,-0.089205645,-0.013450412,0.039264664,-0.064527914,-0.01838652,0.06416731,0.028960807,-0.063019074,-0.007256564,-0.05165172,0.010782266,0.0037557364,0.03643475,0.0329879,0.029578408,-0.04710102,-0.007339458,0.03734083,-0.003077987,-0.04898374,0.012963109,0.04338514,-0.0058453865,0.04457862,-0.00047954405,-0.004840223,-0.09817716,0.04276766,0.005145249,-0.09687756,0.041268922,-0.009941316,-0.01584762,-0.024076728,0.0055461004,-0.038294606,-0.020794524,0.01554832,0.026925744,0.022959495,0.02378246,-0.0026595031,-0.01699721,0.013186518,-0.016945463,-0.010864061,-0.030048227,0.034541726,-0.012442036,-0.01568417,0.020124529,-0.012962175,0.014215827,-0.048188154,0.02438086,0.041060273,-0.058903262,0.013606803,-0.0050760633,0.001648835,0.04805468,-0.028138028,0.032729138,-0.01996099,0.057718154,0.04090434,-0.049381554,0.027429266,-0.016099611,-0.06813604,0.0088657085,0.0149622625,-0.00049372297,0.004032372,0.045115434,-0.02142698,-0.05181553,-0.009578419,-0.000120957346,0.029379994,-0.011516152,0.01041333,-0.035545107,0.07075343,-0.021685954,0.021896476,-0.018061664,-0.031377368,0.015288945,-0.03631226]	Keywords: Transformer, X-formers, variants, survey, literature review\nKey Objects: X-formers, variants\nRefers to Images: None\nHypothetical Questions:\n- Why is a survey of Transformer variants (X-formers) needed?\n- What are the key areas that this survey will cover regarding X-formers?\n- What distinguishes a 'vanilla' Transformer from its variants (X-formers)?\n---\nSummary:\nThis survey aims to provide a comprehensive review of Transformer variants (X-formers) due to their widespread success and the lack of a systematic literature review in the field.\nOriginal Text:\nTransformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.  \nCCS Concepts:  General and reference  Surveys and overviews;  Computing methodologies  Artificial intelligence.\nContextualized Text:\nDue to their success in areas like natural language processing, computer vision, and audio processing, Transformers have garnered significant interest from researchers. As a result, numerous Transformer variants, also known as X-formers, have emerged. This survey addresses the need for a comprehensive review of these X-formers, outlining their architectural modifications, pre-training techniques, and applications.	{"tags": ["survey", "deep-learning", "AI", "transformer"], "doc_id": "4f73b69a-54f8-4e7b-891a-26a7efcec584", "summary": "This survey aims to provide a comprehensive review of Transformer variants (X-formers) due to their widespread success and the lack of a systematic literature review in the field.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["Transformer", "X-formers", "variants", "survey", "literature review"], "key_objects": ["X-formers", "variants"], "contextual_text": "Due to their success in areas like natural language processing, computer vision, and audio processing, Transformers have garnered significant interest from researchers. As a result, numerous Transformer variants, also known as X-formers, have emerged. This survey addresses the need for a comprehensive review of these X-formers, outlining their architectural modifications, pre-training techniques, and applications.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers"}, "hypothetical_questions": ["Why is a survey of Transformer variants (X-formers) needed?", "What are the key areas that this survey will cover regarding X-formers?", "What distinguishes a 'vanilla' Transformer from its variants (X-formers)?"]}
19bdc731-d305-40f4-af5d-5a346a51b569	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01446695,0.002457297,0.03155907,0.028221099,0.0013419612,0.055905893,-0.045868993,0.055954818,0.051194713,-0.045279697,0.0065669743,0.020110976,0.015119676,0.000933295,-0.005057148,0.015858015,-0.004351743,0.04750721,-0.027725454,-0.04393819,0.051477823,0.019839238,0.033974707,-0.010479952,0.041928545,0.0112818,0.042092096,-0.04793529,0.011958534,0.008394497,-0.016854737,-0.059423964,0.014195023,-0.013106705,-0.01592127,0.0030487622,0.01108466,-0.044636007,0.0031067673,-0.026004575,-0.042944554,0.05942493,-0.055337332,0.005389829,-0.049811542,-0.051114127,-0.099251285,-0.009334583,0.016952327,-0.0069844467,-0.014478676,-0.015828101,-0.029259378,-0.034560725,0.01721657,-0.041047394,-0.036668915,-0.04646514,0.028808339,-0.056576792,-0.03241894,0.008342613,-0.05664571,-0.013007632,0.029851936,-0.01744704,0.03355984,0.04018697,0.042167213,0.1343033,0.0066140103,-0.016434336,-0.007690688,-1.3476416e-05,0.111103,0.03537705,-0.047351,-0.03876498,-0.043245368,0.017111247,-0.06326824,0.08912333,-0.041237008,-0.023484033,0.10076156,0.02394336,-0.04660518,-0.044760108,0.0637305,-0.01761445,0.03749406,-0.016685784,-0.03291895,0.10087138,-0.038967002,-0.07921774,0.027231397,-0.047776356,0.013648199,-0.07189434,0.0012727351,-0.018127985,0.03715733,0.07202189,0.039143138,-0.02067251,-0.0070625613,-0.05369676,0.03734624,0.036921445,-0.010600217,0.015736677,0.013145665,0.021683687,0.012182256,0.0016411224,-0.049894348,-0.00870218,0.018111909,-0.013233267,0.010951444,-0.004949493,0.007175948,0.014158319,0.06692666,0.06916577,-0.009359078,0.022651942,-0.041859385,-0.015606879,0.023767522,-0.02025772,-0.008399026,0.014961943,-0.017213995,0.0057437588,0.09773378,-0.014824955,0.03425793,0.007201947,0.03209006,-0.045816347,-0.013655695,0.042606615,-0.01953321,0.03981185,-0.029578647,0.01175166,-0.0051057427,0.016120898,0.039309986,-0.0008489191,0.042595055,-0.03037869,-0.026919687,-0.018178603,-0.0287849,-0.043496117,0.014353935,-0.01598625,-0.058513235,-0.07729539,-0.05269586,0.040705733,-0.0022528355,0.09267628,0.03708443,0.025605392,0.028496442,0.018044826,-0.04166335,-0.0014727266,-0.0014609823,-0.03867176,0.014998762,0.004002042,-0.0060700285,-0.030831527,-5.3357206e-05,-0.022074437,-0.0037178714,-0.010324471,-0.029172733,0.0681175,0.004520118,0.02207425,-0.026647018,0.04891037,-0.018451083,-0.004223044,0.04155176,-0.022845961,0.03566133,-0.019366967,0.052191243,0.01534288,0.048403043,-0.048013147,-0.053012785,0.023345418,-0.02078127,0.015093186,0.005757961,-0.049153537,0.018882958,-0.023972943,0.0069431667,0.01080278,-0.028974332,0.02109604,0.011470677,-0.04250663,-0.020834232,0.07262601,0.014036092,-0.0026781277,-0.016817141,0.024149677,0.027706007,-0.024494264,-0.0383371,0.0074186777,-0.038083553,0.012951345,-0.02715415,-0.01226381,-0.014294385,0.04006716,0.009369694,-0.068195865,0.06649861,0.04131068,-0.0119241355,-0.010248342,-0.008966808,0.015366258,-0.0048935767,-0.008374051,0.04647203,0.031543672,0.0112651335,0.01768382,-0.027946535,0.06639504,0.0050610225,0.0008825314,-0.030436022,0.052887563,0.036478978,0.014537516,-0.024071757,0.0615391,0.04440119,0.02621602,0.017697344,-0.026056567,0.022011427,0.0009456591,0.075739846,-3.4296863e-05,-0.0030592906,-0.010092824,0.015822418,0.02171631,0.016171483,-0.014622271,-0.009676834,0.046059478,0.02298917,-0.0277871,0.013259501,0.053028908,0.014282667,-0.021823704,-0.05503764,0.043301094,-0.008750979,0.035418633,0.032222893,-0.0020522699,0.010970674,-0.030470481,0.005005686,0.008995932,-0.0009379471,-0.03336933,0.04180704,-0.053477995,-0.09869268,0.008005549,-0.0013186755,-0.017671665,0.039257232,-0.052646123,0.04371166,0.040569723,-0.014153925,-0.018801177,0.017672805,0.041831627,0.09163832,-0.0151158795,-0.005163378,-0.02037663,-0.0009354203,0.0045423442,-0.071795195,-0.0061366837,-0.015409373,0.006239637,-0.0035854124,-0.016375246,-0.006970731,-0.014154969,0.06782349,0.090786174,0.06916484,-0.056530952,0.010663061,0.018019922,0.028988818,0.008227053,0.0052573723,0.040771198,0.022639547,-0.010032104,0.022798272,-0.02664076,-0.1055766,0.046982106,-0.068937175,0.06041098,0.072279066,-0.014655496,-0.013787039,0.02053611,-0.0368733,0.027289392,0.0014995335,0.018798407,0.0053931675,-0.06195807,-0.0093396185,-0.017326264,0.0072394633,0.031843305,-0.0116626825,0.020426901,0.020831518,0.024563922,0.016187338,-0.0052152704,0.030050075,0.022892008,-0.018438324,-0.05826281,-0.0024315696,-0.0038066062,0.03997198,-0.0011034966,0.06520615,-0.016196439,0.034015663,-0.027496263,0.017322576,0.0180085,-0.06538053,-0.0043734214,-0.0054008695,-0.006799244,0.012316218,-0.016133599,0.070478536,-0.014369362,-0.017068684,0.018515522,0.045817178,0.02526753,-0.017683685,-0.04319875,-0.0310042,0.012934297,-0.032302804,-0.024915775,0.01333253,0.018931909,0.010728652,0.02936777,-0.0069392687,0.027547896,-0.05849071,0.003182831,0.026021695,0.006459166,-0.037134476,-0.001221366,0.0047936756,0.036745217,0.017878423,-0.08140522,-0.010379014,0.030630056,-0.047502086,0.04907579,0.0029223452,0.0028500704,-0.079354085,0.02167866,0.059229072,0.031853184,0.027408998,0.023660745,-0.02738566,0.00096085784,-0.032592267,-0.0048028426,-0.016691817,0.04218634,-0.07974541,0.02854974,-0.04179044,0.070130326,0.020203454,0.050926846,0.013393865,-0.0011190365,-0.033895724,0.0010888474,-0.0074102087,0.013632841,0.023583656,-0.02741037,-0.028100029,0.04321732,0.038795955,0.012338048,-0.06754794,0.059032913,-0.04663764,0.017913513,0.06336376,-0.005010084,-0.01654948,-0.0176369,0.0145995775,-0.0031267656,-0.02550521,0.0366191,-0.029643174,-0.0043831933,0.019383216,0.014152685,0.03769665,0.009876237,0.020757193,0.010167141,-0.008425329,0.049479436,-0.05827111,-0.03628651,-0.0318415,0.018638497,0.017272433,-0.009636282,0.031594746,-0.047342822,-0.00635801,-0.012726572,-0.0031347922,0.021289753,0.018431809,0.03567306,-0.048771977,-0.022649227,-0.003926078,0.0023198577,-0.005620601,0.069229774,0.054076213,0.040673174,-0.087346904,-0.031236228,0.0072064493,0.047218397,-0.047553975,-0.06145448,-0.04752856,-0.059084058,-0.0014979019,0.042898487,0.014388201,0.031882826,0.02452096,0.033588395,0.011950486,-0.051909536,0.0072816466,0.02637645,-0.01053613,-0.05942906,-0.04798193,0.019146366,0.07790912,0.012671894,-0.04490847,0.013666782,-0.006039007,0.0058605787,0.002826753,0.07366187,0.05346879,-0.015548045,-0.0032771113,0.0032119076,-0.050362438,-0.00062217965,-0.020706598,-0.008793065,-0.019863384,0.057758953,0.0002964392,0.022765223,0.037240345,-0.032597534,-0.010790575,-0.02846552,0.007465747,0.023149358,0.09937881,-0.054821804,0.06423694,-0.0072331587,0.008325786,-0.014258364,-0.021347722,-0.036184736,0.06520074,-0.08083276,-0.033096652,0.020098072,0.0041028988,-0.034455307,-0.0017946016,-0.00059094897,0.06068618,-0.04424174,0.0036460254,-0.037996776,-0.016590906,0.10665512,0.006560405,-0.008408738,-0.006914864,-0.04402064,0.011283832,0.08260564,0.06000428,0.029457152,-0.010849657,0.061484315,-0.018010346,0.05739436,0.03543546,-0.03432493,-0.021585643,-0.018255819,-0.031678226,0.0073982854,-0.04475343,-0.00022259801,0.0064775334,0.013952084,0.025387134,0.008009808,-0.01360619,0.052929863,0.018923245,-0.022026261,0.015405764,-0.04553593,0.044111468,0.021677408,-0.015588254,-0.0078065763,0.02387927,0.0074549485,0.022689309,-0.0037563904,0.018624816,-0.014840876,-0.024263574,-0.02693546,0.00060725864,0.031109786,-0.065357566,-0.0378942,0.015349174,0.029584108,0.06515626,-0.011656218,-0.0436154,0.02377159,0.001570551,-0.07252137,-0.001330228,0.012259056,0.0023298669,0.004896972,-0.029545411,0.055136662,-0.08312211,-0.045116317,-0.04467371,0.016411869,-0.009102077,-0.010854281,-0.01403639,0.022524294,-0.02602956,-0.03444811,0.04148103,0.06010132,0.0020718172,-0.0012354874,-0.008847036,-0.03002852,-0.020937782,-0.0104948515,-0.03858829,-0.00645358,-0.015654914,-0.0010052018,0.066227876,-0.051843215,-0.0011441266,0.007892775,-0.012405917,-0.07052531,0.00036829288,0.013917594,0.028584246,-0.013574065,-0.057531986,0.052230835,0.05986046,-0.059128616,-0.05181308,-0.034929823,0.0024445013,-0.026978409,-0.012727742,0.07955185,0.058010273,-0.059449118,-0.006287421,-0.020810273,-0.06754602,-0.00586589,0.02365492,0.041972924,0.027126266,0.02163965,0.010084784,-0.082269534,-0.008033819,-0.03821698,0.044983707,-0.034123883,0.016848065,-0.040329345,-0.033000994,-0.002729258,-0.0103889555,-0.00972828,0.009104505,0.00874402,0.030334277,-0.008059597,-0.071557075,0.025487166,0.034046944,-0.028726738,-0.0019045001,0.059880517,0.01849862,-0.049385436,-0.035487853,-0.05612934,0.007616706,-0.0004577945,0.024425413,-0.00073634024,0.030238163,-0.042343143,-0.019738557,0.061877273,-0.017684769,-0.029701378,0.009235221,0.026313145,0.019854218,0.032086506,-0.020522125,0.008725694,-0.10363416,0.0205022,0.0095019145,-0.09089058,0.007953491,0.00050870393,-0.024079723,-0.005505022,0.02366433,-0.011369022,-0.046081513,-0.0051862844,-0.0058527226,0.024250299,0.018036429,-0.013802216,-0.014338601,-0.025179911,-0.017700236,0.021253206,-0.022014316,0.071796186,-0.04776605,0.027202398,-0.011220281,0.017796349,0.021230271,-0.03232447,0.018977135,0.0229138,-0.062508285,0.013376905,0.0034432984,0.0044360077,0.0018795431,-0.018527359,0.03137279,-0.019552631,0.046423428,0.039153975,-0.037844107,-0.008960675,-0.041040838,-0.068582766,0.026934454,0.02583911,-0.003166163,0.0048477394,0.042202644,-0.016567852,-0.05957727,-0.026873974,-0.011057217,0.0547059,-0.012170407,0.009976136,-0.01906881,0.04010556,-0.030663053,0.024972329,-0.039099734,-0.016984964,-0.0016814292,-0.007110798]	Keywords: Transformer, Self-Attention, Pre-trained Models, Deep Learning\nKey Objects: Transformer, Self-Attention\nRefers to Images: None\nHypothetical Questions:\n- What are some examples of architectural modifications applied to Transformers?\n- How do pre-training techniques influence the performance of Transformer variants?\n- What are the main application areas where Transformer variants are being utilized?\n---\nSummary:\nThis survey focuses on Transformer variants, categorizing them and providing a comprehensive review of their architectural modifications, pre-training approaches, and applications within the broader fields of artificial intelligence.\nOriginal Text:\nCCS Concepts:  General and reference  Surveys and overviews;  Computing methodologies  Artificial intelligence.  \nAdditional Key Words and Phrases: Transformer, Self-Attention, Pre-trained Models, Deep Learning\nContextualized Text:\nThis survey provides a comprehensive review of various Transformer variants, categorized by architectural changes, pre-training techniques, and applications. It focuses on the broader field of artificial intelligence, covering areas such as natural language processing, computer vision, and audio processing.	{"tags": ["AI", "deep-learning", "survey", "transformer"], "doc_id": "19bdc731-d305-40f4-af5d-5a346a51b569", "summary": "This survey focuses on Transformer variants, categorizing them and providing a comprehensive review of their architectural modifications, pre-training approaches, and applications within the broader fields of artificial intelligence.", "doc_type": "text", "entities": [], "keywords": ["Transformer", "Self-Attention", "Pre-trained Models", "Deep Learning"], "key_objects": ["Transformer", "Self-Attention"], "contextual_text": "This survey provides a comprehensive review of various Transformer variants, categorized by architectural changes, pre-training techniques, and applications. It focuses on the broader field of artificial intelligence, covering areas such as natural language processing, computer vision, and audio processing.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers"}, "hypothetical_questions": ["What are some examples of architectural modifications applied to Transformers?", "How do pre-training techniques influence the performance of Transformer variants?", "What are the main application areas where Transformer variants are being utilized?"]}
56da03aa-4b2f-41e0-86c7-1c6b14897f61	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.030749517,0.027099567,0.030856946,0.033762768,0.033975486,0.070250645,-0.0010177555,0.07344293,0.010216702,-0.026642991,-0.03716636,0.03005238,0.024857355,-0.00046466038,-0.034551233,0.015430767,-0.0058772606,0.055510815,0.017914377,-0.046737824,0.05187502,-0.0048699803,0.008652059,-0.031522628,0.031818766,-0.00973585,0.021913627,-0.03142859,-0.02616939,0.039511673,0.02922416,-0.0033622421,0.0018985579,0.020824147,0.023542453,-0.004066321,0.015097982,-0.006495499,-0.014146207,-0.00013590085,-0.047740877,0.08422618,-0.079326384,0.008326441,-0.009555442,-0.0447278,-0.09909041,-0.029045552,0.03366031,-0.008856709,-0.05075451,-0.045384742,-0.023686426,-0.03554587,0.025896123,-0.0034626361,-0.020752173,0.005206109,0.02294783,-0.07480746,-0.035246197,-0.0050811633,-0.0515378,-0.007055573,0.02057169,-0.041111495,-0.020923324,-0.02125613,0.033585023,0.15153778,-0.047338597,0.009463489,-0.007415503,-0.06254391,0.087333456,0.012590191,-0.029528623,-0.015477044,-0.050741892,-0.025692515,-0.013262092,0.07227045,-0.057024118,-0.043474957,0.08862995,0.010633402,-0.065734245,0.011535461,0.05915964,-0.0364824,-0.030527236,-0.0053823856,0.0022246952,0.062136352,-0.029230533,-0.06560882,0.012845259,-0.0014743885,0.016782528,-0.038140856,0.013062514,-0.019341435,0.11740252,0.067475475,0.019968178,-0.004024172,0.00051920826,-0.06397007,0.047422025,0.05949678,-0.01175875,0.02204483,0.0065163877,-0.003499001,-0.022035869,0.01542133,-0.023651158,0.010608021,0.012786532,0.014287098,-0.02847943,-0.033641394,-0.0026421028,-0.011712136,0.063361645,0.066903144,0.011067813,0.014187073,-0.0064244266,-0.0030796428,0.021772498,0.0043126987,-0.023473438,0.00583172,0.025994292,0.012604536,0.061801143,0.018116387,0.008386902,0.040466465,0.009506625,-0.07366496,-0.024407215,0.022115825,-0.0046300613,0.014838391,-0.042611625,0.0027887276,-0.03094599,0.07751722,0.035911508,0.01876363,0.03767789,0.017276594,-0.008862265,-0.03818933,-0.017603891,-0.074362025,-0.014560253,0.0036546395,-0.044365898,-0.04383314,-0.050102398,0.032204203,0.03308111,0.090239644,-0.012874436,0.020040356,0.05496172,0.03983679,-0.034349676,0.0010763525,0.048056196,-0.058467492,0.021539574,-0.0011726263,-0.0018929145,-0.02321155,-0.040651925,-0.018985488,-0.0033800206,0.06401631,0.014089263,0.05017145,-0.046277065,0.022860311,0.03380046,0.09572788,-0.040644053,0.00012753965,0.041237302,0.009568801,-0.00032872878,0.010955046,0.014911988,0.018412195,0.039191157,-0.053459898,-0.09618053,-0.0042946003,0.03466884,-0.002246736,0.02154407,-0.003899203,0.0065915682,-0.0103898365,0.036909137,-0.03251728,-0.044760887,0.015461463,0.008809377,-0.014793125,-0.026670575,0.056707136,0.028766938,-0.016911954,0.013050342,0.021938855,0.019200806,-0.0073217936,-0.05364959,0.027797189,0.019137366,-0.016114242,-0.037938025,0.025984988,-0.020350488,0.03237454,0.0210121,-0.04223812,0.019761998,0.05364599,-0.02657083,0.016225977,-0.0053342544,0.055127885,-0.04282649,-0.008543537,0.00787347,0.027785175,0.04894228,0.02522969,-0.044310987,0.048581924,-0.015030441,-0.007129602,0.042567354,0.026859889,0.01043197,-0.0139735965,0.01930213,0.056246392,0.06181676,-0.026637796,0.020824632,-0.06490641,0.011410632,0.039484642,0.03136626,0.011199131,-0.012638112,0.020384036,-0.0024392123,0.017802823,0.033838864,0.021770116,0.021591151,0.025404606,0.036104936,-0.027986947,0.05288826,0.0656607,-0.0043285177,-0.0077530784,-0.022111367,0.029466415,-0.012344081,-0.0030948806,0.047592394,-0.034568965,0.015734138,-0.04730556,0.048540413,-0.012173641,0.015335731,-0.04008101,0.008527597,0.0013318211,-0.038874567,-0.030005563,-0.0032939601,-0.05964513,0.022273188,-0.016216144,0.026862454,0.03689142,-0.030053586,-0.03577309,-0.027086267,0.04146799,0.049990065,-0.056264468,0.035448186,0.036735132,0.0063511566,0.037527546,-0.057006627,-0.007829176,0.041800465,-0.0021447605,0.0043174713,0.03384354,-0.0017068225,0.002972002,0.030273225,0.06909045,0.09189349,-0.05003302,-0.0040208767,-0.0016095091,0.04308178,-0.033858113,0.0049489792,0.015927278,0.026928946,-0.050608624,0.021019751,0.010762373,-0.059150208,0.050894555,-0.07899504,0.082933396,0.040729742,-0.02147193,0.008196933,-0.012391342,0.027892366,0.07612605,-0.04433601,0.06327993,-0.019700458,-0.07465986,-0.0005200615,-0.01812449,0.012000943,-0.0048310487,0.012116577,0.014315804,0.021841243,-0.015362456,-0.016728848,0.056150783,0.017985852,-0.0020401904,0.02372386,-0.029480377,0.009653054,0.01064375,0.0035593614,0.0029020128,0.05245202,-0.022425689,0.025242927,0.032723058,0.008399842,-0.026527826,-0.07814775,-0.033774253,0.04499567,-0.03524862,-0.020847013,-0.03395841,0.0019064959,-0.0047859563,-0.008641849,0.0046330406,0.06972156,0.011630212,-0.025208464,-0.013091393,-0.031114202,0.036301572,-0.066309586,-0.051343348,0.0068903603,0.042439155,0.022854501,0.03336447,0.0060188784,0.024904175,-0.03993405,-0.004572399,0.042244576,0.016508285,-0.008945009,0.011697978,0.053178344,0.062770106,-0.033985056,-0.032957114,0.024010483,0.023786204,-0.047208145,0.060140442,0.036959846,0.053926334,-0.02568182,0.026716506,0.055988867,0.08666754,0.0017212667,-0.0090652555,-0.06243551,0.048424143,-0.028332327,-0.0049270247,-0.025488984,0.04778857,-0.049989834,0.028108085,-0.02580011,0.038549367,0.042095125,0.077476874,0.025021126,-0.021227712,-0.03237663,0.00784105,-0.029982846,-0.027851071,-0.03199591,-0.0647843,-0.011529652,0.07027581,0.07170467,0.023999898,-0.071651794,0.044589177,-0.07235752,0.039892897,0.05360747,0.0013529183,-0.0042860312,-0.0167577,0.0052264733,-0.025613502,0.026181392,0.012731083,-0.0357368,-0.0019120282,0.014210402,-0.009186699,0.019019632,0.019040966,0.051010586,-0.0061018793,-0.030772904,0.006231016,-0.032645058,0.009410834,-0.025543617,0.043627582,0.0036869573,-0.016031923,0.014290727,-0.009122504,-0.005828594,-0.010597342,-0.00048704434,0.0012603144,-0.008709154,0.0029592663,-0.0058474266,0.010415489,-0.035629723,-0.02277007,0.0020640383,0.00017273279,0.009564237,0.01963447,-0.045885775,-0.05477782,0.011939647,0.0791764,-0.018113002,0.01390457,-0.01706018,-0.03275693,-0.05061774,0.029069435,-0.019399615,0.043595355,0.018252885,0.027317263,0.025400707,-0.019771991,-0.04077324,0.009884112,0.048845235,-0.065289475,-0.022355309,-0.019008458,0.06827175,0.046198092,0.012666983,-0.017903697,0.011380577,0.0036345753,-0.016564177,0.024320347,0.00898799,0.0059927604,-0.017854042,-0.014242609,-0.017458012,0.023641871,0.0011668974,-0.020089595,0.01757231,0.06969007,-0.038994677,0.020240702,0.023731515,-0.008975712,0.025764294,-0.0075059133,0.025598062,0.051380068,0.06134932,-0.07698109,0.020574125,-0.014814565,-0.057595015,0.009937923,-0.058855183,-0.017052319,0.03081984,-0.06035749,0.0050927266,0.010857261,0.0009849074,-0.037808564,0.0035993007,-0.0025488664,0.05189597,-0.09148494,-0.016648442,-0.035948414,-0.041837946,0.057165492,0.011896842,-0.015552249,-0.0130240545,-0.055624314,-0.022701155,0.034601975,0.06543202,-0.0043439497,-0.022178153,0.071123846,0.014852584,0.014431393,-0.020191625,-0.050616387,-0.0055458476,-0.025554415,-0.033486802,0.017422182,0.025753347,0.044215705,0.0090238135,0.028006013,3.2471416e-05,-0.0051495023,-0.032792624,0.08649734,0.02401032,0.012578777,0.037559763,-0.016194154,0.003715959,0.024715053,0.029445104,0.041519336,0.0073258304,0.0005259672,0.027215542,-0.04945822,-0.009095715,0.00084140635,-0.012063374,-0.006540489,0.009500134,0.018235357,-0.008318411,-0.025294915,0.027789738,0.014647969,-0.013016854,-0.03664544,-0.09448676,-0.017577989,-0.037894927,-0.06523973,0.005207468,0.012943414,0.015557223,0.03861544,-0.082515106,0.0733572,-0.07934282,0.02861869,-0.056941263,0.017005406,0.037270404,-0.012315253,-0.033892278,-0.039440513,-0.0007391468,-0.04402538,0.039731223,0.04994819,-0.026465723,-0.002518163,0.017272063,0.0030704779,-0.02666562,-0.0031229723,-0.009030321,-0.007884404,-0.0142073715,-0.016512971,0.04367518,0.009200224,-0.041702274,0.014055278,-0.036567815,-0.08290064,0.013294565,0.026068125,0.024769647,0.023352236,-0.02273902,0.021936001,0.0570562,-0.057903945,-0.014868185,-0.02103074,-0.016786875,-0.015223373,0.031135542,0.046494156,0.0012684944,-0.03235543,0.041693628,-0.001879441,-0.051578168,0.0010889326,-0.013696903,0.0476238,0.015328295,0.01020638,-0.026858462,-0.08080916,-0.04408603,-0.059350837,0.0046532475,-0.041291904,0.056943398,-0.009791263,-0.0037649567,-0.011226773,-0.02850707,0.013734226,0.004144996,0.031635106,0.002151826,-0.022691816,-0.00971745,-0.012049343,-0.0009884271,-0.019533081,-0.0010761739,0.051405337,0.043528374,-0.049960125,0.005326856,-0.054306738,-0.0118123805,0.004737754,-0.014048174,0.033962183,0.018741982,0.0082294,-0.0023446567,0.03159853,-0.0019258909,-0.0014453514,-0.03253357,-0.00059873745,0.05815969,0.04380239,-0.03045489,0.013592306,-0.116495416,0.06773074,-0.018515775,-0.04913581,0.07342656,0.06711385,0.0023402155,-0.010844634,0.030804561,-0.029978333,-0.034018815,0.055687547,0.0027670858,-0.011618287,0.027096298,-0.035832684,-0.024392562,0.015433414,-0.020133331,-0.034674045,-0.0076224096,0.045017276,0.0077083376,-0.007928737,0.005143533,-0.030128224,0.010974476,-0.017915735,-0.015779335,0.02262079,-0.062019184,0.0065672537,7.9699246e-05,0.014737912,0.035326887,0.036763366,0.031137487,0.005173756,0.036447126,0.02442061,-0.032807007,0.012438091,-0.03141935,-0.04707973,0.03571181,0.018513042,-0.026885414,0.03673644,-0.016909603,-0.015396948,-0.018510858,0.00528425,-0.008834009,0.05006445,0.038133603,0.027282933,-0.08158571,0.026775764,-0.033859774,0.072070226,0.02664665,0.00014448162,0.02699485,0.0038703824]	Keywords: Transformer, deep learning, machine translation, natural language processing, pre-trained models\nKey Objects: Transformer model, sequence-to-sequence model\nRefers to Images: None\nHypothetical Questions:\n- What were the initial applications of the Transformer model?\n- Why has the Transformer become so popular in NLP?\n- What does it mean for a model to be 'pre-trained'?\n---\nSummary:\nThe Transformer is a widely adopted deep learning model, initially designed for machine translation, that has become a go-to architecture, particularly in Natural Language Processing and for pre-trained models.\nOriginal Text:\n## 1 INTRODUCTION  \nTransformer [137] is a prominent deep learning model that has been widely adopted in various fields, such as natural language processing (NLP), computer vision (CV) and speech processing. Transformer was originally proposed as a sequence-to-sequence model [130] for machine translation. Later works show that Transformer-based pre-trained models (PTMs) [100] can achieve state-ofthe-art performances on various tasks. As a consequence, Transformer has become the go-to architecture in NLP, especially for PTMs. In addition to language related applications, Transformer has also been adopted in CV [13, 33, 94], audio processing [15, 31, 41] and even other disciplines, such as chemistry [114] and life sciences [109].  \nDue to the success, a variety of Transformer variants (a.k.a. X-formers) have been proposed over the past few years. These X-formers improve the vanilla Transformer from different perspectives.\nContextualized Text:\nOriginally proposed as a sequence-to-sequence model for machine translation, the Transformer has become a prominent deep learning model, widely adopted across various fields including natural language processing and computer vision. Its success has led to the development of Transformer-based pre-trained models that achieve state-of-the-art performance.	{"tags": ["deep-learning", "architecture", "NLP", "machine translation"], "doc_id": "56da03aa-4b2f-41e0-86c7-1c6b14897f61", "summary": "The Transformer is a widely adopted deep learning model, initially designed for machine translation, that has become a go-to architecture, particularly in Natural Language Processing and for pre-trained models.", "doc_type": "text", "entities": ["Transformer", "NLP", "CV"], "keywords": ["Transformer", "deep learning", "machine translation", "natural language processing", "pre-trained models"], "key_objects": ["Transformer model", "sequence-to-sequence model"], "contextual_text": "Originally proposed as a sequence-to-sequence model for machine translation, the Transformer has become a prominent deep learning model, widely adopted across various fields including natural language processing and computer vision. Its success has led to the development of Transformer-based pre-trained models that achieve state-of-the-art performance.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "1 INTRODUCTION"}, "hypothetical_questions": ["What were the initial applications of the Transformer model?", "Why has the Transformer become so popular in NLP?", "What does it mean for a model to be 'pre-trained'?"]}
a395f039-ef52-4e79-a966-dd6d32b27e53	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.032173414,-0.0037971688,0.046250958,0.051101517,-0.011485014,0.059737615,0.027831065,0.03688453,0.0127466405,0.018507874,0.0023687463,0.040770378,-0.0064063645,0.0051735784,-0.029567257,0.028872514,0.0024343256,0.0872817,0.01281678,-0.0017563789,0.035599284,-0.01981959,0.03331936,-0.026783478,0.0011431072,-0.024659965,0.028587826,0.0153352935,-0.017721122,0.018410493,0.013346756,-0.050534837,0.037350472,0.021826334,0.032057166,0.016095063,0.0072743944,-0.036086958,-0.03068659,-0.0077882116,-0.06728145,0.05742266,-0.059166364,-0.021400694,-0.061137952,-0.033668984,-0.078249514,-0.03211947,0.03358821,-0.02979835,-0.010204565,-0.016798818,-0.043616842,-0.031508654,-0.013000947,-0.0007863343,0.0011102989,-0.0059817242,-0.008129765,-0.07356356,-0.0105783325,-0.017089738,-0.024561536,-0.0049595037,0.03427177,-0.026754173,0.01254051,0.0008242457,0.0071754707,0.12539834,-0.017091462,-0.013215792,-0.020801071,-0.04234043,0.110603005,0.039910384,-0.03469905,-0.036141325,-0.030647745,-0.03632378,-0.054443464,0.10714529,-0.053433064,-0.058854356,0.078659736,0.024721822,-0.05610577,-1.6757198e-05,0.08238659,-0.044930268,-0.0058561917,0.02558778,0.016138958,0.040450476,-0.018583108,-0.1280288,-0.007804493,-0.04598045,0.043633144,-0.044242643,0.025877826,-0.0005981837,0.055456247,0.07804167,0.040231146,-0.019813456,0.0022086392,0.008878992,0.03260738,0.015895473,-0.028256219,0.045676503,-0.039407954,-0.020929242,-0.04722156,-0.018152697,-0.027809959,0.0132497335,0.017023675,0.015780505,0.006696351,-0.006563485,-0.017431987,0.03322421,0.04837212,0.039559267,0.031627867,-0.022926293,-0.001904536,0.012869815,0.05028594,-0.045005642,0.012151692,0.013602201,0.00032482896,0.013772499,0.10099924,0.020993078,0.0051461924,0.013202624,0.010263677,-0.05194396,-0.033142064,0.03947906,-0.02889159,0.029482607,-0.045203723,0.023308078,-0.027291032,0.04631464,0.06729515,-0.030333757,0.03741126,-0.015528655,-0.012605333,0.021394758,-0.036959972,-0.060695857,-0.0017630928,-0.022466805,-0.018075308,-0.04493859,-0.0031503069,0.035704184,0.026442437,0.10593766,-0.004564469,0.0038969268,0.03075725,0.02240856,-0.046972625,0.007343949,-0.016969915,-0.021604031,-0.015703063,0.0024207723,0.001166086,-0.02215743,-0.023955284,-0.058404677,-0.0046281503,0.023043154,-0.015210283,0.057659205,-0.03659742,0.02699634,0.032232776,0.05804108,-0.034858204,-0.01718535,0.029124988,-0.015201411,-0.006819763,0.0015232619,0.030125977,0.038171202,0.047902104,-0.07907255,-0.018630119,-0.0255496,0.029407512,-0.007729245,0.016640974,-0.013948161,0.008254401,-0.0063684573,0.014727543,-0.009794425,-0.010844683,0.0060210433,-0.014191363,-0.010857818,-0.016838647,0.0877076,0.013490077,-0.008990838,-0.0022548835,0.02870118,0.055860538,-0.007519749,-0.020920327,0.027035179,-0.029272893,0.004519742,-0.0008571014,0.01368812,-0.0030666583,0.042069033,0.02261323,-0.022453938,0.039485417,0.04805504,-0.03210714,0.017038194,0.005861169,0.03766428,-0.015851513,-0.012381636,0.0031019524,0.00841626,0.059257794,0.039835796,-0.037943717,0.08305256,-0.001924769,0.009611259,0.00092137296,0.043935653,0.036945388,0.025286553,-0.0018141828,0.020215001,0.06223471,-0.0059264703,0.032541968,-0.07289918,0.018358363,0.022785155,0.02727988,-0.02674602,-0.012729013,0.0033619257,0.009134331,-0.0028265188,0.02908295,-0.0041314387,-0.03368741,0.0043693455,0.008908435,-0.03553512,0.041769892,0.025315631,0.01198236,-0.039608084,-0.024714626,0.017967831,-0.022894688,-0.028328657,0.044902384,-0.01935279,-0.02844508,-0.04176301,0.056348335,-0.004892165,0.038673017,-0.01871002,0.016593322,-0.02589762,-0.056855857,-0.002698708,-0.031659853,-0.049533058,0.022185154,0.0017748765,-0.0073369546,-0.005650193,-0.0117192315,-0.027884258,-0.016257152,-0.00019595756,0.10473931,0.0036388803,-0.036219694,0.010059557,0.015662594,0.02118683,-0.09166039,0.016103934,0.0330185,0.008734723,0.022952037,0.028475344,-0.016179163,0.013920431,0.033423454,0.05545372,0.08363579,-0.086200066,-0.0050231777,-0.042228594,0.03507097,-0.008797662,-0.027860412,0.037056927,0.026373232,-0.028336281,0.012990783,-0.018167865,-0.05905614,0.056323886,-0.08656579,0.058517434,0.049389403,0.026419219,-0.02067714,-0.006322994,0.030735118,0.048524264,0.004346306,0.008701002,0.0007749096,-0.07760914,-0.020446926,-0.014281334,-0.0343437,0.056907296,-0.03354823,0.024320327,-0.0059318845,-0.018925495,0.019459281,0.048541483,0.033478513,0.003484983,0.024913296,-0.039145708,0.03072165,-0.012662104,0.009071447,0.0055249445,0.02971141,-0.034671467,0.027672404,0.017872758,0.016044099,0.013508798,-0.059098754,-0.014891839,0.0402157,-0.04797806,-0.033797525,-0.009458745,0.046221223,-0.03957306,-0.013621258,0.025481215,0.08248117,0.04945155,-0.028110951,-0.0037704478,-0.012996435,0.0047145435,-0.037955347,0.012561801,0.013734664,0.012737607,0.006966087,0.045638446,-0.024176013,0.03763564,-0.015764458,-0.013832098,0.02414505,-0.0019016364,-0.023084676,0.028546026,0.0651615,0.0696009,0.0062678866,-0.018930452,0.017332774,0.038496513,-0.04478213,0.059763305,0.047899324,0.03671152,-0.08186868,0.017280659,0.06289041,0.05540667,-0.019612253,-0.033408843,-0.064958915,0.02519872,-0.0024995976,0.0024149828,-0.0275269,0.056898985,-0.0452623,0.006426538,-0.012045708,0.03283399,0.008868016,0.05364334,0.019765807,0.011627624,-0.03886996,0.035202894,-0.0111034475,-0.010773918,0.028734384,-0.06701332,-0.033260692,0.06648095,0.06289468,0.011700049,-0.06412749,0.07009793,-0.08805954,0.019693732,0.05284406,-0.0120727895,0.008905929,-0.04449092,0.015751222,-0.04654004,-0.012725221,0.019242648,-0.020317428,0.00740084,0.0074206223,0.013274333,0.020759398,-0.015311579,0.010042863,0.007254097,0.012658079,-0.0070305476,-0.061647426,0.016026806,-0.000706615,0.0128875645,-0.006779871,-0.0103614535,0.009920519,-0.03005948,0.011875197,-0.033975936,0.012900902,0.031496383,-0.0027927442,-0.016428575,0.0041548614,0.010855165,0.0100358445,-0.033105236,-0.010128049,0.0135537665,0.0028606586,-0.006505107,-0.060963828,-0.020493262,-0.003053931,0.03342291,0.015414073,0.006776736,-0.036660865,-0.042188726,-0.03386175,0.031675227,-0.016387641,0.066315584,0.028717292,0.023224788,0.03936023,-0.05660749,-0.004727292,0.021322113,-0.010198867,-0.04853221,-0.010604472,-0.02168754,0.078705706,0.073763825,0.0067263693,0.017148668,0.028608061,0.0087372335,0.027606003,0.029855737,0.0034330112,0.00913536,0.014160158,-0.060704153,-0.039516,-0.002081438,-0.021768969,-0.0034044804,0.01653942,0.054949272,-0.003945307,0.016540114,0.037630856,0.00057953165,0.034714337,-0.008274671,0.042167142,0.00753439,0.029298948,-0.06692869,0.021918632,-0.010654285,-0.028516483,0.016304929,-0.029637324,-0.0045101214,0.052205503,-0.03364176,0.013181456,-0.005563278,-0.011707875,-0.018526342,0.01333029,-0.01409296,0.048857078,-0.08328588,-0.028032213,-0.034204543,0.02897454,0.08098707,0.036688603,-0.016398713,0.004081448,-0.010867549,-0.027645439,0.04692569,0.059222627,0.030951723,0.011430696,0.07009756,0.011816651,0.032445062,-0.00313594,-0.0500932,-0.027403023,0.0194573,0.02079708,-0.00216945,0.017861025,0.023770204,0.027376924,0.021132348,-0.0072667273,-0.0009488348,0.001220726,0.051499613,0.011278523,0.0085646175,0.055836756,0.0017164281,0.057074543,0.012131599,0.04958369,0.029666109,-0.0031256238,0.03351239,0.053329136,-0.017690333,0.013670438,-0.0124084875,-0.034615602,-0.022879818,0.03824816,0.033751167,-0.009999983,-0.041163504,-0.00241744,0.050083406,0.03221836,-0.048910234,-0.04465524,-0.039573617,-0.05245867,-0.031323776,-0.032517914,-0.029002076,0.037437998,0.016765783,-0.06030989,0.10979994,-0.06369089,-0.011768693,-0.04725611,0.0012176888,0.008215179,-0.03627274,0.0118759405,-0.019775618,-0.026479604,-0.0672917,0.05871788,0.04694602,-0.052483067,-0.024340233,-0.0044584116,-0.0135310255,-0.0034023738,-0.034345627,0.027021535,-0.013858616,-0.002463856,-0.005351579,0.022398207,-0.0052775056,-0.007189471,0.064497195,-0.04272115,-0.073839195,-0.021773899,-0.0101210065,0.06023328,0.00297481,0.02616116,0.015167936,0.008492738,-0.021163752,-0.02702177,0.0025911669,-0.023813896,0.016046403,-0.0042256266,0.042003576,-0.037010416,-0.030482793,0.04474589,-0.030474888,-0.020529574,0.030929841,0.011568238,0.07344259,0.063205555,0.0011995223,-0.035810277,-0.09650335,-0.038542815,-0.06052072,0.01727164,-0.100725204,0.07967615,0.0026197778,0.025092619,-0.06130166,-0.012496524,0.015189947,-0.011187319,0.017855927,0.014413961,-0.05545088,-0.044696335,-0.03950699,0.018343063,-0.0599988,-0.047840197,0.053945176,0.043901898,-0.0762059,-0.018768977,-0.059138652,-0.0065249074,-0.0039799,-0.007011474,0.030488096,-0.0015134418,-0.007919468,0.024283564,0.036853697,0.017750885,-0.06004563,-0.022354005,0.053223025,0.033078272,0.025709946,0.021076117,-0.032986764,-0.08740472,0.04444206,0.0074613323,-0.077473894,0.034300886,-0.013368798,-0.019588897,-0.050015356,-0.019109905,-0.0072908895,-0.0051102214,0.0190153,0.041707724,-0.0036218439,-0.00021882069,0.019472787,-0.052701842,0.005600148,-0.011818769,-0.040659785,0.010968187,0.060481116,0.01212464,-0.033385374,0.0039456785,0.01708325,-0.018787589,-0.031566177,0.009946295,0.08033587,-0.03409224,0.010423323,-0.013204953,-0.0043823426,0.045937177,-0.004015411,0.0531071,0.0011410471,0.034665074,0.029449403,-0.01889652,-0.008804541,-0.025558213,-0.04770547,0.0077834516,0.017421078,0.040836778,0.006473714,-0.017520592,-0.03819456,-0.024767898,-0.013324496,-0.019478215,0.032783855,0.044030797,-0.034726612,-0.03359445,0.04234534,-0.032140795,0.06393038,-0.029341485,-0.01224038,-0.018597798,-0.051457092]	Keywords: Transformer, X-formers, model efficiency, model generalization, model adaptation\nKey Objects: X-formers, Self-attention module, Input data\nRefers to Images: None\nHypothetical Questions:\n- What are some of the primary challenges that motivated the development of X-formers?\n- How does the ability of the Transformer to make few assumptions about input data impact its ability to generalize?\n- Can you provide examples of methods used to address each of the three areas: model efficiency, model generalization, and model adaptation?\n---\nSummary:\nFollowing the initial success of the Transformer model, numerous variants, referred to as X-formers, have emerged to enhance its capabilities. These improvements generally focus on three key areas: model efficiency, model generalization, and model adaptation.\nOriginal Text:\nDue to the success, a variety of Transformer variants (a.k.a. X-formers) have been proposed over the past few years. These X-formers improve the vanilla Transformer from different perspectives.  \n- (1) Model Efficiency. A key challenge of applying Transformer is its inefficiency at processing long sequences mainly due to the computation and memory complexity of the self-attention module. The improvement methods include lightweight attention (e.g. sparse attention variants) and Divide-and-conquer methods (e.g., recurrent and hierarchical mechanism).\n- (2) Model Generalization. Since the transformer is a flexible architecture and makes few assumptions on the structural bias of input data, it is hard to train on small-scale data. The improvement methods include introducing structural bias or regularization, pre-training on large-scale unlabeled data, etc.\n- (3) Model Adaptation. This line of work aims to adapt the Transformer to specific downstream tasks and applications.\nContextualized Text:\nDriven by the widespread adoption and success of the Transformer model, researchers have developed numerous variations, known as X-formers, to address specific limitations and broaden its applicability. These X-formers aim to improve upon the original Transformer's design, primarily focusing on making it more efficient, improving its ability to generalize with limited data, and adapting it to various downstream tasks.	{"tags": ["architecture", "NLP", "deep-learning", "transformer"], "doc_id": "a395f039-ef52-4e79-a966-dd6d32b27e53", "summary": "Following the initial success of the Transformer model, numerous variants, referred to as X-formers, have emerged to enhance its capabilities. These improvements generally focus on three key areas: model efficiency, model generalization, and model adaptation.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["Transformer", "X-formers", "model efficiency", "model generalization", "model adaptation"], "key_objects": ["X-formers", "Self-attention module", "Input data"], "contextual_text": "Driven by the widespread adoption and success of the Transformer model, researchers have developed numerous variations, known as X-formers, to address specific limitations and broaden its applicability. These X-formers aim to improve upon the original Transformer's design, primarily focusing on making it more efficient, improving its ability to generalize with limited data, and adapting it to various downstream tasks.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "1 INTRODUCTION"}, "hypothetical_questions": ["What are some of the primary challenges that motivated the development of X-formers?", "How does the ability of the Transformer to make few assumptions about input data impact its ability to generalize?", "Can you provide examples of methods used to address each of the three areas: model efficiency, model generalization, and model adaptation?"]}
cd915ba5-f84e-4344-b4a9-679dbfe07d50	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0441541,0.00432461,0.050553832,0.022914458,-0.019843496,0.054961607,-0.0015856364,0.053661305,0.014294126,-0.011358225,-0.007917866,0.021902163,0.049460344,-0.002528494,-0.041227613,-0.0152331935,0.008733805,0.08281971,0.025508387,-0.029926406,0.042175476,-0.0012986562,0.04095334,-0.044578254,0.0331229,-0.03916444,0.02195603,-0.0030411556,0.03633392,0.0029585117,0.024175055,-0.044492416,0.0410608,-0.003671013,0.013372693,0.004508533,0.0061961403,-0.065019146,-0.03360446,-0.023655733,-0.037539255,0.030062895,-0.05511448,0.004534881,-0.02434276,-0.03553431,-0.07463243,-0.06618001,0.001499365,-0.014607421,-0.024566695,0.019861287,-0.03896191,0.002046191,-0.036851004,-0.0053480756,-0.028141834,-0.03715698,0.012066692,-0.08330406,-0.031700294,-0.028362922,-0.059913363,-0.01327798,0.030923694,-0.053051807,0.011900509,-0.009863037,-0.002286202,0.111350164,-0.011833717,0.008377251,0.00312016,-0.044360086,0.101242736,0.033014476,-0.06298078,-0.046836555,-0.06108504,-0.009185665,-0.06645842,0.06226699,-0.05730516,-0.014861259,0.08101664,0.010075284,-0.08319619,-0.020705957,0.07903678,-0.019457567,-0.00726944,0.007533154,-0.017592821,0.020967923,-0.023763957,-0.03958214,-0.009924138,-0.035381265,0.038401924,-0.015203898,0.018456109,-0.0109654805,0.06587746,0.047490083,-0.013838224,-0.026530253,-0.051147882,-0.004283125,0.0066084466,0.026043605,-0.013286202,0.04187833,-0.069288716,-0.012962615,-0.043475814,-0.03549254,-0.0127919875,0.033204302,0.003805215,-0.010725395,0.01913024,0.019265067,-0.038638767,0.048551667,0.06893195,0.04579759,0.02489755,0.007861811,0.003090994,-0.008639563,0.046351947,-0.021722117,0.030361308,-0.019806324,-0.00393012,-0.007452208,0.059825033,0.00059047126,0.015213498,0.029118266,0.010017989,-0.063235626,-0.038488686,0.031850312,-0.011923444,0.023337496,-0.031054743,0.016229833,-0.0033357323,0.053788826,0.048598953,-0.029706461,0.034582324,-0.030231318,0.0010705064,0.0063442183,-0.00710139,-0.04695904,0.01014823,-0.043017935,-0.011549955,-0.03601927,-0.02794499,0.07452108,0.04486281,0.08256056,-0.0047054673,0.039154775,0.037451297,-0.035535276,-0.07245602,0.013229414,0.018283077,0.03417192,0.024160858,-0.02457532,-0.028707173,-0.025279352,-0.012363549,-0.04883362,-0.0037854824,0.019197097,-0.044105057,0.012729082,-0.045850795,0.027256796,0.025323875,0.06160443,-0.020665603,-0.011738603,0.04746415,-0.013020102,-0.007178184,0.0040378994,0.058928903,0.04756693,0.04668549,-0.11054204,-0.069261774,-0.022969743,-0.012903152,-0.004857756,0.0073215,-0.005668494,0.0036958624,-0.045813665,0.010826466,-0.017955903,-0.05053502,0.0176455,-0.0073391395,-0.037307836,-0.03450239,0.10273873,0.017390568,-0.023840554,-0.016514882,0.028851433,0.06262153,-0.04840306,-0.04951686,0.016434412,-0.024977693,0.008550945,-0.0469093,-0.0007411136,-0.05186067,0.03815233,0.013541784,-0.030093463,0.04805023,0.04115911,-0.017535053,0.038367685,0.033429507,0.019223962,-0.022855684,0.02947247,-0.019199504,0.058329526,0.054179728,0.05231541,-0.008903669,0.07555597,0.0008144402,-0.0024976851,0.0019673607,0.041082475,0.009688761,0.0033706562,-0.01030032,0.037302863,0.04376262,-0.008742315,0.006192248,-0.042414658,0.043879956,0.0036568185,0.04654925,-0.006233645,0.033561636,0.010500719,0.016163211,0.010372094,0.004944167,-0.004288343,-0.04872404,-0.010370478,0.0034280408,-0.0064787343,0.040965118,0.032888617,0.008346928,-0.03265446,-0.038909175,0.03129675,-0.00867051,0.009375574,0.027237097,-0.04700044,-0.02658397,0.013843074,0.03541199,0.005222361,0.00751918,-0.0435979,0.04422376,-0.03357549,-0.0357027,0.011054876,0.0012977239,-0.029383555,0.07101884,-0.0062353825,-0.060959417,0.030988187,-0.016674975,-0.04943015,-0.041183,0.019924074,0.08833447,-0.019553851,-0.018395726,-0.015902609,0.021265175,-0.0061202473,-0.088400364,0.02182228,0.021714833,0.023377197,0.05093645,0.027883204,0.027320562,0.013414132,0.06067776,0.060143854,0.057332903,-0.059796873,0.022434574,0.0090497695,0.000989624,-0.01807536,-0.014677848,0.008955274,0.07098799,-0.032908842,-0.004888941,-0.018160505,-0.063544266,0.031505186,-0.060832854,0.048028048,0.058043137,0.0045353845,-0.05074896,0.006812905,0.009348935,0.04059934,0.026939625,0.007377566,0.0016941826,-0.10745103,-0.02913407,0.000109574365,-0.010494604,0.03332608,-0.030305887,0.051959213,-0.013149833,0.020431567,0.013427082,0.0025600146,0.026753448,-0.007179699,-0.016831188,-0.017333575,-0.013799342,-0.02557698,-0.0002727917,0.010394228,0.04494291,-0.029726172,0.04273675,-0.008043943,0.017030759,-0.005368275,-0.06706462,0.005581312,0.06427855,-0.0085784495,-0.042582925,-0.05743927,0.053966593,-0.04260604,-0.020471884,0.013016898,0.06298653,0.032043885,-0.025886131,-0.040118612,-0.020924216,0.0077553596,-0.00038256423,-0.01322786,0.028967803,0.048973642,0.008084055,0.05278705,-0.013223244,0.082879715,-0.030208789,-0.027731411,0.05699274,-0.028959522,-0.013941927,0.052611876,0.048454527,0.04421996,0.0015833251,-0.056533244,0.010682041,0.034692496,-0.042533282,0.049316593,-0.016083308,0.048357923,-0.09789158,0.03183541,0.03969033,0.033531353,0.017777184,-0.023964308,-0.07620556,0.00565825,0.0031592224,0.027139207,-0.004590281,0.049682423,-0.040299363,0.0024710312,-0.036517426,0.05345538,0.0074373856,0.08288781,0.030588768,-0.02385488,-0.036336266,0.0075003426,-0.037144125,-0.043180358,-0.003554042,-0.06953884,-0.026489997,0.034027934,0.05784454,-0.016336102,-0.056550402,0.05166493,-0.05615798,0.057511795,0.033622123,-0.02310499,0.01633739,-0.026153605,0.041960888,-0.021037977,0.02779251,0.00690513,-0.04382873,-0.024157688,0.006277672,0.027793722,-4.3103584e-05,-0.037951123,-0.013578289,-0.0029637255,-0.0035580646,0.02044125,-0.051794026,-0.0065857386,-0.041355,0.05530903,0.0007184032,-0.024140729,-0.026239185,-0.037475485,-0.04378121,-0.009590481,-0.010657202,0.011783297,-0.014551257,0.045818295,-0.017976979,0.001492658,0.025940428,-0.017767886,-0.030843731,0.036811788,0.041779757,0.02924894,-0.05548003,-0.026136275,-0.035547588,0.057538956,-0.014785184,-0.033580106,-0.033757143,-0.036117483,-0.032604992,0.006672497,-0.009491959,0.04790229,0.054003898,0.039272584,-0.006409228,-0.043201525,-0.0073475004,0.05437978,-0.006825521,-0.032837268,-0.049470972,0.02759473,0.094395846,0.022135189,-0.03932122,0.053658493,0.016182603,0.0033661267,-0.036541067,0.045748122,0.013274563,0.011959337,-0.01947117,-0.008841659,-0.03009882,-0.02907182,-0.04424565,-0.017167222,-0.0018989767,0.05548928,-0.0075161094,-0.0187785,0.018256325,-0.044136357,0.0053196074,0.012721204,0.047259722,0.023553507,0.041127644,-0.041717127,0.07166198,-0.020794805,0.0020729476,0.020989211,-0.03995436,-0.020404285,0.05201983,-0.061659373,0.019740866,-0.012270511,0.021016613,-0.020707192,0.0050183414,-0.013284969,0.081277095,-0.056371093,-0.030064153,-0.019612862,0.019939817,0.07385228,0.031761326,0.00057201425,-0.008056346,-0.04124046,-0.01087959,0.043982606,0.034067012,0.02912739,-0.024949694,0.07324578,-0.029848034,0.054554228,-0.021607956,-0.067820445,-0.040489566,-0.034499604,-0.010697663,0.0061711697,-0.06899204,0.0005555222,0.045550603,0.0079835,-0.012153606,-0.019541694,0.0021076445,0.049921304,0.023497604,0.0036055248,0.016873606,-0.034431566,0.032127228,0.0050889957,0.0037966794,0.02222447,0.009131071,0.026240256,-0.005079391,-0.011598972,0.009689471,-0.027666178,-0.031947773,-0.024589762,0.010894474,0.037485093,-0.061638467,-0.032012522,0.0010592723,0.012933957,0.046126496,-0.030027,-0.025598304,-0.011413759,-0.036501456,-0.047031127,0.024364507,0.014382505,0.0058632223,-0.016821502,-0.0471332,0.0686021,-0.021250896,0.023332717,-0.043523196,0.011729907,-0.0054856073,-0.028768925,0.0025433074,-0.03249075,-0.019513436,-0.09657704,0.048817288,0.018455561,-0.013300346,0.010901759,0.004707209,-0.013166255,-0.027881166,-0.0062101837,0.025698695,-0.0037960066,0.002931396,-0.009809369,0.057100255,-0.008797513,0.0008468256,0.05062609,-0.030748172,-0.072170936,-0.020600792,0.0048345863,0.06819679,-0.015266606,-0.021636812,0.0049303765,-0.0056831352,-0.011252173,-0.013195239,-0.007965133,-0.0113187125,-0.02382162,-0.020863071,0.048945375,0.0047150506,-0.044887673,0.057902977,-0.058860388,-0.023719398,0.02720598,0.0068125254,0.061611716,0.055665128,-0.0054619596,-0.0027193334,-0.0752702,0.009895705,-0.031500664,0.011896117,-0.015104389,0.07889809,0.022659943,-0.0057206843,-0.01382395,-0.033331063,0.008811659,0.019640537,0.001749564,-0.0094318185,-0.029283077,-0.03483126,-0.02676657,0.041180793,-0.026582027,-0.049315535,0.07281319,0.042591058,-0.048100095,-0.03186329,-0.0593604,0.018922485,0.010510549,0.012413532,0.028299458,0.03388982,0.0016449047,0.011377663,0.041203443,0.00407923,-0.049154818,-0.010669665,0.029683158,0.01715887,0.024915345,-0.037955023,-0.013794027,-0.07099492,0.061836958,-7.7511286e-05,-0.057511505,0.0457867,-0.013096458,-0.023824304,-0.03502455,0.035845164,0.011078376,0.0002397932,0.005490516,0.062626675,0.02955375,0.00016094637,-0.006209587,-0.07076551,0.03199492,-0.014135255,0.008020635,-0.024666993,0.05059524,0.010148811,0.017132184,-0.008310166,0.041455816,0.024846142,-0.022468774,0.03682962,0.06470982,-0.055354804,0.00201203,-0.022630248,-0.014906551,0.07524419,0.016165905,0.024547596,0.0033825699,0.018138468,0.0060774307,-0.018295366,-0.026571572,-0.043160852,-0.078847274,0.015797995,0.026662868,0.031290673,-0.013101576,-0.0042641773,-0.02974096,0.0076066284,0.011210235,-0.0020527092,0.035970703,0.020472694,-0.010608966,-0.05814771,-0.012316451,-0.05037137,0.04107808,-0.04192265,0.0035347247,0.01553489,0.012138174]	Keywords: model adaptation, downstream tasks, applications, Transformer variants\nKey Objects: Model Adaptation, Transformer Models\nRefers to Images: None\nHypothetical Questions:\n- What are some examples of specific downstream tasks where Model Adaptation is crucial?\n- How does Model Adaptation differ from other approaches to improving Transformer models, such as architecture modification or pre-training?\n- What are the potential challenges in adapting Transformer models to very specialized or unique applications?\n---\nSummary:\nModel Adaptation focuses on tailoring Transformer models to perform optimally on specific downstream tasks and applications, representing a key area of research within the broader field of Transformer variants.\nOriginal Text:\n- (3) Model Adaptation. This line of work aims to adapt the Transformer to specific downstream tasks and applications.  \nIn this survey, we aim to provide a comprehensive review of the Transformer and its variants. Although we can organize X-formers on the basis of the perspectives mentioned above, many existing X-formers may address one or several issues. For example, sparse attention variants not only reduce the computational complexity but also introduce structural prior on input data to  \n$^{}$Corresponding Author.  \nAuthors' address: Tianyang Lin, tylin20@fudan.edu.cn; Yuxin Wang; Xiangyang Liu; Xipeng Qiu, xpgui@fudan.edu.cn, School of Computer Science, Fudan University, Shanghai, China, 200433 and Shanghai Key Laboratory of Intelligent Information Processing, Fudan University, Shanghai, China, 200433.\nContextualized Text:\nA significant focus in Transformer research is Model Adaptation, which concentrates on modifying and customizing Transformer models to achieve optimal performance on specific downstream tasks and applications. This approach aims to improve the model's effectiveness in various use cases.	{"tags": ["NLP", "deep-learning", "architecture", "adaptation"], "doc_id": "cd915ba5-f84e-4344-b4a9-679dbfe07d50", "summary": "Model Adaptation focuses on tailoring Transformer models to perform optimally on specific downstream tasks and applications, representing a key area of research within the broader field of Transformer variants.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["model adaptation", "downstream tasks", "applications", "Transformer variants"], "key_objects": ["Model Adaptation", "Transformer Models"], "contextual_text": "A significant focus in Transformer research is Model Adaptation, which concentrates on modifying and customizing Transformer models to achieve optimal performance on specific downstream tasks and applications. This approach aims to improve the model's effectiveness in various use cases.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "1 INTRODUCTION"}, "hypothetical_questions": ["What are some examples of specific downstream tasks where Model Adaptation is crucial?", "How does Model Adaptation differ from other approaches to improving Transformer models, such as architecture modification or pre-training?", "What are the potential challenges in adapting Transformer models to very specialized or unique applications?"]}
c1b691c5-b7d2-475c-bb8f-9d50cf8800e9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.026513407,0.004583208,0.028875092,0.034944274,-0.011851546,0.048279677,-0.014301896,0.028226577,0.03395635,-0.012965385,0.009494584,0.030058995,-0.0046591936,0.017248396,-0.036954693,0.018195992,-0.0050750924,0.059604883,-0.0052813054,-0.060747564,0.054307207,-0.020409634,0.048184056,-0.010861853,0.027808622,-0.003916686,0.048959203,-0.020172277,0.017268486,-0.008276886,-0.003307765,-0.028716048,0.0362547,0.0022386564,0.0024870592,0.050257355,0.0048848973,-0.03982548,-0.05424822,-0.036517985,-0.055167288,0.05697372,-0.09582166,0.016637111,-0.09317898,-0.032712817,-0.07395689,-0.050113827,0.03238627,-0.015422062,-0.04080033,-0.0046089026,-0.0757025,-0.0015510166,0.007426054,-0.033503417,-0.024757778,-0.04368047,0.004426846,-0.053203054,-0.017958298,0.005133186,-0.045167495,0.0039394256,0.012730472,-0.033935916,0.017765569,0.035566807,0.04121207,0.13183466,-0.0027696695,-0.041406825,-0.02087268,-0.018851643,0.12900487,0.025134172,-0.07383336,-0.06305375,-0.006616351,-0.0072226543,-0.046062104,0.09949681,-0.057850108,-0.03844907,0.101834826,0.025575051,-0.051601373,-0.045657083,0.05822908,-0.039003912,0.01097842,0.010002414,0.0019389901,0.059579026,0.0066722888,-0.09574459,-0.019031001,-0.03941601,0.04205438,-0.03871904,0.018751986,-0.020302605,0.04936706,0.047525305,0.022184523,0.002223375,0.017857304,-0.032722265,0.03062407,0.04480015,-0.020148428,0.001639829,0.00046008953,0.003093967,-0.018224182,-0.0015585377,-0.033506785,0.0004838008,0.017318001,0.007570676,-0.008751036,-0.022986192,-0.023318771,0.025005357,0.048834234,0.032896943,0.004095714,-0.023050448,-0.019145193,-0.006251498,0.018823428,-0.009693825,0.020299604,0.011944445,-0.01987457,0.016951261,0.08623484,0.02864233,0.016690524,-0.00957481,0.006839473,-0.03563182,-0.041394297,0.008156367,-0.0042875283,0.029917307,-0.021160511,-0.008668378,-0.0034691219,0.041594304,0.0593598,-0.042161368,0.02515911,-0.028952144,-0.023929987,-0.026018206,-0.014925808,-0.040914968,0.013979376,-0.01861773,-0.032064,-0.055455968,-0.043484665,0.018551927,0.030260896,0.073991194,0.032026816,0.044345766,0.03325603,0.00437822,-0.023347115,-0.016102418,-0.0043062754,-0.042713344,0.0071847313,-0.010501378,-0.0020531071,-0.02856391,-0.023689544,-0.046944905,0.020226344,-0.014248733,-0.011632171,0.07572863,-0.008483136,0.01742735,-0.0027810559,0.06446685,-0.05429424,0.00020371369,0.054915626,-0.03027226,-0.008121109,0.04200708,0.029772943,0.03012068,0.064662434,-0.042382848,-0.06761669,0.0066837315,-0.03616125,0.00071374734,0.0038886,-0.047489043,-0.005024282,-0.017603653,0.0020204964,-0.014812708,-0.036519233,0.030667692,0.02067355,-0.048618145,-0.01564314,0.09025958,0.033051156,-0.013053151,-0.016272362,0.01697694,0.080915354,-0.013013571,-0.017866125,0.030664802,-0.01547021,0.013714181,-0.011019522,-0.021817183,-0.027545337,0.005828549,0.012932562,-0.047426883,0.052954156,0.008968745,-0.025257474,0.016168281,0.025385963,0.03618061,-0.027656078,-0.011837978,0.042820055,0.012978593,0.03519281,0.03371062,-0.05722916,0.06086608,0.0379454,0.004644538,-0.029076288,0.028619576,0.030468922,0.044086173,-0.014209375,0.055148482,0.061926868,0.012255804,-0.0015856462,-0.048257157,-0.0043611377,0.010909458,0.047752947,-0.018914012,-0.017466765,0.019323524,0.0015360739,0.01846981,0.010106066,-0.015482563,-0.00035793518,0.016267115,0.0026952333,-0.03952421,0.024948169,0.042786792,0.013482737,-0.04275676,-0.025258888,0.02210473,-0.008165986,0.016252093,0.03480828,-0.01506971,-0.0023740157,-0.021124423,0.04458766,0.025984388,0.0069714226,-0.026498249,0.038693316,-0.035107657,-0.07642677,-0.004400751,-0.019281667,-0.04011838,0.02151913,-0.007117642,0.013955865,0.021576082,-0.017144807,-0.025374051,-0.009684805,0.019999187,0.08343404,-0.018799549,-0.0029734487,-0.035432804,-0.008910762,-0.010915915,-0.07077244,-0.004829569,0.013918174,-0.0023167261,0.039001953,-0.02857931,-0.054922722,0.014671233,0.048347253,0.094048,0.0639052,-0.07499845,0.02898343,-0.029153379,0.02215363,-0.011532397,-0.039822765,0.050818805,0.044186242,-0.018139174,0.033003088,-0.014939096,-0.090069935,0.028526692,-0.09284098,0.03451528,0.07209523,0.038982995,-0.022645233,0.012359309,0.0048975106,0.04654986,0.006072304,-0.029894134,-0.009764118,-0.053204704,-0.003482143,-0.0018112104,-0.007069573,0.04603402,-0.04637203,0.050828423,0.010568997,0.024136242,-0.008313087,0.0011458829,0.02650029,0.032163963,-0.008065764,-0.029787352,0.011223286,-0.023632664,0.016358651,9.2654365e-05,0.054075025,-0.02945139,0.012724164,-0.000112894726,0.018935816,0.045652837,-0.060752533,-0.009465984,0.004700115,-0.02384645,0.016393384,-0.003033805,0.06404147,-0.010994308,-0.0069880574,7.063985e-05,0.05523644,0.008445519,-0.041516222,0.000340044,-0.0013077927,0.0026805855,-0.072345115,-0.0134691615,0.01171803,0.03731395,-0.009780893,0.03681262,-0.010182742,0.017236669,-0.026251823,0.0028072644,0.026782168,0.023538617,-0.00698798,0.0010767564,0.055807445,0.037916142,-0.0001922255,-0.047692496,0.027165277,0.06413804,-0.034933846,0.055928968,0.045287926,0.019184653,-0.069403894,-0.007028878,0.05507951,0.06177418,0.006404173,0.007634715,-0.05559351,0.006204759,-0.0500524,0.0010977181,-0.020746307,0.0555373,-0.06680653,0.028642185,-0.03009115,0.046397038,-0.0033804064,0.038202,0.019669283,-0.002611071,0.00551066,0.00065787806,-0.024298176,-0.033703215,0.03294963,-0.082512416,-0.038946018,0.049198467,0.0718211,-0.022375945,-0.04215492,0.07571426,-0.054756735,0.03044162,0.085994214,0.0033193938,0.034661684,-0.009011583,0.024071438,-0.018539632,-0.03398928,0.027422203,-0.030605454,0.005724073,-0.020245928,0.017282931,0.018442389,-0.031238241,0.030731238,-0.011591584,0.024934404,0.01657611,-0.043692775,0.015546762,-0.012623526,0.01306764,0.0050613414,0.017371003,0.0201175,-0.03792054,0.02091502,-0.022290763,5.3927397e-05,0.023632156,0.007739497,0.012588792,-0.031490095,-0.018817734,-0.021033593,-0.013661001,-0.020330396,0.09596807,0.037284683,0.026972463,-0.038208023,-0.037676245,0.008298704,0.046417464,-0.020273797,-0.030545328,-0.048977517,-0.051196117,-0.013530223,0.02130125,0.009469653,0.051472023,0.042525355,0.043709576,0.024248507,-0.0691619,-0.030606758,0.022634499,-0.0012581631,-0.05493642,0.008887191,0.02693853,0.08303983,0.051835425,-0.013333,0.014892437,-0.0006013086,-0.005764482,-0.005684789,0.081189334,0.04223941,0.01998756,-0.0019372875,-0.041670427,-0.01899383,0.005234002,-0.009315275,-0.014861476,0.030463994,0.061911758,-0.022847341,0.012211939,0.03610247,-0.021798398,-0.015314473,-0.0042810515,0.05117538,0.026036207,0.07983965,-0.022741118,0.052195136,-0.025682846,-0.012479156,0.006050356,-0.04523772,-0.019629156,0.047877446,-0.055209264,-0.0016060339,0.0022697612,0.016126284,-0.013137258,0.018763456,0.019588528,0.07509801,-0.056948446,-0.0055281827,-0.055964958,0.02424899,0.073380575,0.02206251,-0.009048616,-0.0051533426,-0.0576082,-0.01916712,0.06530141,0.03397389,0.02169868,-0.004995352,0.052504044,-0.031184686,0.043951716,0.020300882,-0.041189138,-0.052636538,0.021459961,0.019607792,-0.016187707,-0.012492753,-0.01223601,-0.012339542,0.014058103,-0.0032368,0.0010954901,-0.021236166,0.06755748,0.03687431,-0.005136795,0.027395753,0.011118881,0.044643976,0.012718293,-0.022742754,0.041996844,-0.0127641335,0.003061988,0.04899515,-0.036910933,0.013309714,0.026181145,-0.020996954,-0.054441247,0.018825032,0.039759636,-0.045467354,-0.035953302,-0.021341657,0.01615413,0.031487294,-0.019885851,-0.044970527,-0.0058572832,-0.01380895,-0.031981956,-0.020340454,-0.022511717,0.03843196,0.03057472,-0.051293783,0.0899804,-0.09086456,0.0026078147,-0.023912529,0.021675691,-0.013497829,-0.023978097,-0.008000055,-0.010710586,-0.04418266,-0.06054694,0.02914848,0.04595297,0.00816157,0.0022379935,-0.001786814,-0.009221887,-0.021296734,-0.037678283,-0.016702695,-0.0060541313,-0.007783271,-0.017523529,0.045738906,7.059862e-05,-0.022407249,0.016697861,-0.010718522,-0.07487441,-0.010339154,-0.019681327,0.04702965,0.020628683,-0.007104343,0.03807688,0.051434096,-0.06739501,-0.06366598,-0.021699814,-0.013463549,-0.028691808,0.033464424,0.072438635,-0.0037642107,-0.06696091,-0.00074437336,-0.012141426,-0.036790244,-0.008548403,0.013787997,0.055581167,0.04375028,0.013658144,0.012460608,-0.116982475,-0.0033473566,-0.022366153,0.046837952,-0.046563014,0.039434288,0.0259348,0.01373856,-0.02635446,-0.021243988,0.03389355,-0.0018618648,0.0051959883,0.0141680855,-0.028923053,-0.047741078,-0.008535456,0.026108075,-0.08237019,-0.038827877,0.04332825,0.033367153,-0.04381585,-0.012606133,-0.06627774,0.022095878,-0.009727186,0.020350501,0.012713596,-0.0038968245,-0.0427007,-0.0032557414,0.07318473,-0.010078707,-0.03231734,-0.008917453,0.034744807,-0.0028131567,0.009613821,0.006905488,0.016958661,-0.047291968,0.044919454,0.011361022,-0.08424715,0.03791365,-0.03664652,-0.029613292,-0.031871215,0.0001501978,-0.029981803,-0.044200655,0.022459934,0.012255176,0.009304028,0.0074669626,0.03667377,-0.019706085,0.016259508,-0.024437087,-0.012212627,0.0064621395,0.06580628,-0.028490767,-0.015016084,0.010264607,0.03340139,0.017506134,-0.036518827,0.04325824,0.07180304,-0.052582283,0.032566536,-0.021666177,-0.012137198,0.01596328,0.0010433267,0.025518853,-0.03962643,0.05744847,0.017044907,-0.0039347745,-0.013239482,-0.02862819,-0.06440451,-0.00014705856,0.044305902,0.033064563,0.0021405516,-0.023941288,-0.016195858,-0.053717688,-0.012173264,0.0013652709,0.050617866,0.020470671,0.015524468,-0.015642887,0.055452447,-0.02501073,0.048864026,-0.06513158,-0.0353604,0.0040628198,-0.0442216]	Keywords: Transformer, X-formers, architecture modification, pre-training, applications, taxonomy\nKey Objects: Transformer variants, architecture, taxonomy\nRefers to Images: None\nHypothetical Questions:\n- What are the three main categories used to classify Transformer variants?\n- Why is it helpful to categorize Transformer variants based on how they improve the original architecture?\n- How do these different categories of improvement reflect the challenges faced when applying Transformers?\n---\nSummary:\nExisting Transformer variants, often called X-formers, can be categorized by how they improve the original Transformer architecture, primarily through architecture modification, pre-training, and application-specific adaptations.\nOriginal Text:\nalleviate the overfitting problem on small datasets. Therefore, it is more methodical to categorize the various existing X-formers and propose a new taxonomy mainly according to their ways to improve the vanilla Transformer: architecture modification, pre-training, and applications. Considering the audience of this survey may be from different domains, we mainly focus on the general architecture variants and just briefly discuss the specific variants on pre-training and applications.\nContextualized Text:\nTo provide a structured understanding of Transformer variants (X-formers), this survey categorizes them based on their methods for improvement. These methods primarily fall into three categories: architecture modification, pre-training approaches, and adaptation to specific applications. This categorization will allow for a more methodical approach to understanding the diverse range of advancements built upon the original Transformer architecture.	{"tags": ["deep-learning", "NLP", "transformer", "survey"], "doc_id": "c1b691c5-b7d2-475c-bb8f-9d50cf8800e9", "summary": "Existing Transformer variants, often called X-formers, can be categorized by how they improve the original Transformer architecture, primarily through architecture modification, pre-training, and application-specific adaptations.", "doc_type": "text", "entities": ["Transformer", "X-formers"], "keywords": ["Transformer", "X-formers", "architecture modification", "pre-training", "applications", "taxonomy"], "key_objects": ["Transformer variants", "architecture", "taxonomy"], "contextual_text": "To provide a structured understanding of Transformer variants (X-formers), this survey categorizes them based on their methods for improvement. These methods primarily fall into three categories: architecture modification, pre-training approaches, and adaptation to specific applications. This categorization will allow for a more methodical approach to understanding the diverse range of advancements built upon the original Transformer architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "1 INTRODUCTION"}, "hypothetical_questions": ["What are the three main categories used to classify Transformer variants?", "Why is it helpful to categorize Transformer variants based on how they improve the original architecture?", "How do these different categories of improvement reflect the challenges faced when applying Transformers?"]}
e82d6260-748b-419c-82c7-2fb4bbaa73fc	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03045385,0.028963711,0.05392352,0.019749518,0.024706926,0.048115883,-0.018967673,0.021885265,0.006696784,0.002800741,0.0049319556,0.033531558,0.056718133,0.027748995,-0.017660642,0.027542552,-0.014094812,0.075839475,0.012758751,-0.051304203,0.062198132,0.05191619,0.05726541,-0.020713449,0.012944691,-0.022504976,0.03778,0.0048990645,0.007501618,-0.013577217,-0.021941636,-0.053135116,0.044007953,0.0017440466,0.005325655,-0.0026086068,-0.0032883633,-0.01453782,-0.016695265,0.0116152475,-0.043350633,0.05078851,-0.08508617,0.029637929,-0.060639724,-0.046255864,-0.080011375,-0.05037672,-0.020555286,-0.014058551,-0.05861983,0.015606816,-0.03685417,-0.057646982,0.024950135,-0.012499869,-0.032751393,-0.040791404,0.016960815,-0.06370444,-0.025456967,-0.0035655322,-0.045948762,0.0037708308,0.017933592,-0.013175603,0.020505344,0.0473087,0.02825232,0.117732815,0.0058428687,-0.024709811,-0.0092186835,0.004098097,0.1141796,0.031506345,-0.058734912,-0.0764711,-0.052101385,-0.014602309,-0.07421833,0.07222119,-0.049219064,-0.036463916,0.10038375,0.022173103,-0.02910299,-0.076268464,0.082194656,-0.028024234,0.032288216,0.008622243,-0.008131738,0.031268246,0.0036723898,-0.05172226,-0.0041834232,-0.07744899,0.022229249,-0.025333868,0.020825671,-0.014924287,0.067002356,0.068462946,0.01735055,-0.058657918,-0.0034095559,-0.05235739,0.029049868,0.04208691,-0.044136494,0.0018792652,-0.007162179,0.023019405,-0.017267693,0.006532381,-0.011638053,-0.030847479,-0.007135229,-0.00548698,-0.016507214,0.008796974,-0.017867181,0.007841992,0.064356156,0.047454372,0.009891762,-0.012104673,-0.026967028,-0.017592162,0.0026332883,-0.015547371,0.020180851,0.0015750082,-0.031819806,-0.0019830759,0.07896012,0.03194854,0.00016485537,0.0014833088,0.014820796,-0.06217934,-0.03717314,0.0119143035,-0.01284551,0.056342058,-0.005373665,-0.027411776,-0.016679034,0.053313866,0.036974154,-0.017417856,0.04256189,-0.009336209,-0.013489786,0.011062796,-0.017521555,-0.052739363,-0.013948773,-0.047537163,-0.013429683,-0.053428575,-0.049441166,0.023697298,0.013444225,0.07719638,0.028532878,0.048329305,0.040798504,-0.013941676,-0.06551981,-0.02023459,-0.010104529,-0.02052317,0.014569798,-0.0013962544,-0.0021916917,-0.036028136,-0.018371224,-0.02124044,0.035488516,-0.00064247404,0.016236208,0.056824986,0.0019272489,0.039316136,-0.0028522774,0.06681295,-0.027253304,-0.0064017996,0.06262845,-0.038272116,-0.009420212,0.012809019,0.066584766,0.04129383,0.051188618,-0.051186338,-0.05507609,0.028040256,-0.031613614,0.0056415442,0.042619895,-0.06737522,0.00736876,-0.014767982,0.0059763133,0.005720411,-0.021275865,-0.014139792,0.0021378945,-0.035409443,-0.013243905,0.053342402,0.02236098,-0.011509311,0.0018222876,-0.0087941745,0.04542211,-0.031143725,-0.03346992,0.03357424,-0.021022012,-0.0015529419,-0.0074148765,-0.004278728,-0.0401611,-0.030851327,-0.0046441467,-0.015065719,0.05024801,0.015101773,-0.05259471,0.009095369,0.002761655,0.022052273,-0.031389136,-0.005114594,0.06150093,0.027349642,0.048750196,0.02123134,-0.054526623,0.04941309,0.026885401,0.0235645,0.014105883,0.047008615,-0.0011449765,0.040575527,-0.008786673,0.04038952,0.031830627,0.023141833,0.0078067263,-0.021657396,0.006642204,-0.0064138738,0.02248992,-0.014140029,-0.004929448,-0.0061478554,0.026178988,0.026176665,0.002916322,0.007869702,0.0006731157,0.011295858,-0.009724237,-0.012917139,-0.02163905,0.050752696,0.013214315,-0.025455171,-0.019130943,-0.013006859,-0.01592257,0.03304989,0.03573279,0.015011319,-0.013569057,-0.037592165,-0.008719051,0.018131942,0.0040369984,-0.039822787,0.042181786,-0.023273395,-0.09654351,-0.03249888,-0.0029143484,-0.04454246,0.021925278,-0.057180937,0.016305,0.028754735,-0.013108603,-0.00975933,-0.0313059,0.0073429416,0.09631163,-0.0031502133,-0.016645743,-0.016371422,-0.017585406,-0.012851547,-0.073227726,0.025712384,0.016545177,-0.02069146,0.012253015,-0.021784322,-0.059868824,-0.0155154755,0.058663994,0.112049475,0.07402796,-0.08036969,0.01458627,-0.011481435,0.014472363,-0.038280327,-0.026261479,0.021933265,0.018066535,-0.015514289,0.03649947,0.013622893,-0.03344336,0.039253615,-0.119915515,0.038913734,0.081315696,0.008798393,-0.01623587,0.03922141,-0.0059561515,0.07367674,0.03467701,-0.035222683,-0.006206943,-0.03823408,-0.0451827,0.008094472,0.04686828,0.030278042,0.0016217738,0.009198443,0.029568374,0.0061514927,0.02939797,0.008081608,0.023960847,0.009309756,-0.0077469763,-0.03173408,0.003950885,-0.06274118,0.04759639,0.030651778,0.03755696,0.014810597,0.003133692,-0.02746869,0.025469553,0.025868334,-0.06806105,0.00084579235,0.007059784,-0.028180102,-0.030843707,0.0004542237,0.080454886,-0.045015786,-0.012353251,0.027343022,0.06372593,0.016518094,-0.013967088,-0.017668454,0.0065433076,-0.01870656,-0.06410119,-0.05241496,0.019659732,-0.0013757752,-0.011183998,0.018425355,0.017101685,0.035550408,-0.040864456,-0.036046006,0.022898389,-0.0007419098,-0.003422929,0.035299107,0.022615751,0.043826688,-0.024914194,-0.06315944,0.040150613,0.0422496,-0.036915276,0.033418577,0.02522551,0.018811628,-0.058715984,-0.014112222,0.023967192,0.05925491,0.020739984,0.04079135,-0.07110511,-0.0007122823,-0.040690962,0.027000768,-0.02028337,0.079073586,-0.037516672,0.027221313,-0.02994206,0.040392082,0.007840957,0.041631393,0.015522807,0.003692825,-0.024042947,0.03550407,-0.0008671715,0.0038692849,0.031182976,-0.07645842,-0.037133627,0.07747507,0.025783058,-0.0056351344,-0.08590428,0.06291857,-0.058363102,0.041225333,0.069178306,-0.038337052,0.028860085,-0.01011309,0.028352845,-0.016593505,0.023436768,0.055957504,0.011501724,-0.0029433435,-0.0008963892,-0.0011046268,0.028163606,-0.005092779,0.036850493,-0.024979085,-0.011426216,0.008684564,-0.033050004,-0.022093417,-0.03129634,0.054045454,-0.008949815,-0.0030149687,-0.002888873,-0.023295337,0.004584684,-0.030962687,-0.00398119,0.02932379,0.03719232,-0.0067834463,-0.016903905,-0.0153996125,0.02372919,-0.044731826,-0.017631216,0.09342258,0.039966863,0.034967907,-0.02912526,0.004808489,-0.015540151,0.03187832,-0.03343068,-0.02525494,-0.030281024,-0.026703559,-0.048236378,0.042787734,0.02599724,0.06487385,0.057045195,0.02532887,0.021824334,-0.059737004,-0.020817587,0.012691752,-0.006094813,-0.030948382,0.020085586,0.013214122,0.060039468,0.010467517,-0.06277761,-0.000104793995,-0.011133734,0.0119424295,-0.012253088,0.06330754,0.06472536,0.019595478,-0.017150894,-0.028452253,-0.025096223,0.027684093,-0.022499938,-0.041270196,0.02641726,0.049389135,-0.036854308,0.045032296,0.027396116,-0.020792637,-0.013959662,-0.021855785,0.08072386,0.011267544,0.09051769,-0.024360552,0.067823365,-0.03055542,0.0019023694,0.01685179,0.00075105834,-0.027064713,0.03975903,-0.07077299,0.0071429797,0.0054621813,0.006208551,-0.018983543,-0.0092880875,0.0112019535,0.046148065,-0.0537381,0.018675147,-0.04247861,-0.008121637,0.056667004,-0.048695672,-0.019698415,0.0023222046,-0.053091742,0.008528275,0.039982833,0.049594034,0.03920295,-0.037935667,0.028403874,-0.010440012,0.0083786165,0.014868394,-0.050173704,-0.026736874,-0.027396483,0.002904054,-0.011082631,-0.023165079,0.030424517,-0.016520362,0.0251535,-0.014196443,0.0062031397,-0.012231092,0.05918416,0.06585135,-0.035589393,0.05161925,-0.034341298,0.028474228,0.02761333,-0.02089199,0.006414983,0.027816577,0.0058178687,0.020769263,-0.005610379,0.050076216,-0.012947674,-0.0037111994,-0.05822155,0.039480075,0.027827034,-0.06190591,-0.03958169,-0.0005431053,0.015423709,0.0259891,-0.008466616,-0.049998477,0.010098082,-0.012696534,-0.044853155,-0.016687298,0.005358742,0.0055009546,0.0174692,-0.049367562,0.0413184,-0.051690623,-0.0045503983,-0.03199988,0.018434942,0.013129198,-0.005657743,-0.0054257317,-0.027196012,-0.056426995,-0.069456294,0.004859988,0.07375152,-0.0017810247,0.0005387621,-0.029352175,-0.009488473,-0.01798289,0.0034014515,-0.011751573,-0.00293317,-0.012572034,-0.002133187,0.036373105,-0.0319677,-0.0070508323,0.038780928,-0.03277375,-0.084761165,-0.02218003,-0.024641883,0.036551554,-0.011355978,-0.0024583393,0.048312414,0.06217806,-0.03839098,-0.045533057,-0.027689273,-0.03006321,-0.03275367,-0.024316007,0.038384814,0.01263368,-0.066679105,0.03253477,-0.0074889595,-0.058049586,-0.014221862,-0.013127444,0.052203093,0.025650838,0.0049915365,0.010297003,-0.08235936,-0.0031020362,-0.049617626,0.03840329,-0.0505965,0.08281579,-0.029449312,0.0026147417,-0.026181044,0.0046090716,-0.007724388,-0.0025104904,0.0135430405,0.034815475,-0.037425473,-0.04673605,0.033301733,0.021112781,-0.0543833,0.009305117,0.048703536,0.023008546,-0.053676926,0.0032928328,-0.04612841,0.009698108,-0.0029829196,0.023582583,0.052099228,0.019318694,-0.027138079,0.02246205,0.037587617,-0.010633573,-0.037411273,0.004327963,0.0010140883,0.0043616733,0.01939457,-0.008905148,0.01209946,-0.10693848,0.014987689,0.01799637,-0.07459393,0.025400436,-0.029105682,-0.028696787,-0.024730142,0.01766186,-0.033299588,-0.021415493,0.058140725,0.038049284,0.024383545,-0.007982451,0.006514264,-0.008683805,0.032221474,-0.0021821025,0.016683351,0.0026658794,0.040149298,-0.012582852,0.011655386,0.04533123,0.004224407,0.005653855,-0.0144617455,0.043025702,0.035501197,-0.062302724,0.014242676,-0.0056582834,0.0029139097,0.059005473,0.017585998,0.021637011,-0.025523337,0.07620089,0.033326246,-0.053029653,-0.015642261,-0.013882357,-0.09154601,0.0204377,0.0349102,0.013546677,-0.042138822,0.020882694,-0.023717191,-0.0079039605,-0.009375519,0.0073256833,0.030582394,-0.007451559,0.038194668,-0.021898901,0.049655806,-0.021128088,0.0364608,-0.036408093,0.02192741,-0.014137481,-0.04933494]	Keywords: Transformer, survey, architecture, variants, PTM\nKey Objects: Transformer architecture, Transformer variants\nRefers to Images: None\nHypothetical Questions:\n- What are the main areas of improvement being explored in Transformer variants?\n- How are Transformer variants being categorized in this survey?\n- What specific components of the Transformer architecture will be examined in detail?\n---\nSummary:\nThis survey will proceed by first introducing the architecture and key components of the Transformer model, then clarifying the categorization of Transformer variants, and finally reviewing different modification approaches.\nOriginal Text:\nThe rest of the survey is organized as follows. Sec. 2 introduces the architecture and the key components of Transformer. Sec. 3 clarifies the categorization of Transformer variants. Sec. 4 GLYPH 5 review the module-level modiGLYPHcations, including attention module, position encoding, layer normalization and feed-forward layer. Sec. 6 reviews the architecture-level variants. Sec. 7 introduces some of the representative Transformer-based PTMs. Sec. 8 introduces the application of Transformer to various diGLYPHeren t efGLYPHlds. Sec. 9 discusses some aspects of Transformer that researchers might GLYPHnd intriguing and summarizes the paper.\nContextualized Text:\nThis survey provides a comprehensive review of the Transformer model and its numerous variations. To structure the analysis, the following sections will be presented: an introduction to the core Transformer architecture and its components, a categorization of Transformer variants, and a detailed review of modifications and improvements to the model.	{"tags": ["survey", "architecture", "NLP", "deep-learning"], "doc_id": "e82d6260-748b-419c-82c7-2fb4bbaa73fc", "summary": "This survey will proceed by first introducing the architecture and key components of the Transformer model, then clarifying the categorization of Transformer variants, and finally reviewing different modification approaches.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["Transformer", "survey", "architecture", "variants", "PTM"], "key_objects": ["Transformer architecture", "Transformer variants"], "contextual_text": "This survey provides a comprehensive review of the Transformer model and its numerous variations. To structure the analysis, the following sections will be presented: an introduction to the core Transformer architecture and its components, a categorization of Transformer variants, and a detailed review of modifications and improvements to the model.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "1 INTRODUCTION"}, "hypothetical_questions": ["What are the main areas of improvement being explored in Transformer variants?", "How are Transformer variants being categorized in this survey?", "What specific components of the Transformer architecture will be examined in detail?"]}
0f6724ab-5273-4fe5-9ec9-b996cfa79f37	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.025418289,0.02562716,0.059210137,0.045359857,-0.0124767395,0.025145454,-0.028448947,0.035200357,0.047923826,-0.047592774,-0.0029736273,0.014390338,0.017310211,-0.0034411724,-0.026180789,0.063218296,0.031348646,0.090344466,-0.015143631,-0.027610162,0.044580366,-0.008280801,0.03193219,-0.027113697,0.0069455793,0.010940499,0.04234602,0.011483313,-0.042909015,0.047414206,-0.014302404,-0.008974667,0.028554138,-0.01963414,0.04285831,-0.009799515,0.0067316373,-0.02511047,-0.04318047,0.011326776,-0.022396987,0.04930101,-0.08981311,-0.011118773,-0.0016492233,-0.056477744,-0.10767931,-0.03036318,0.022176228,-0.027956918,-0.052520305,-0.021391492,-0.05170939,-0.016517876,-0.0212422,-0.004597153,0.004116022,-0.030446667,-0.014268742,-0.061894465,-0.03143652,-0.02312801,-0.013653786,-0.012107235,0.0009905546,-0.01672609,-0.016085666,0.02772139,0.029332213,0.13550808,-0.01766407,0.03180054,0.0066055614,-0.032772943,0.123451896,0.05518308,-0.06384481,-0.020709863,-0.06035523,-0.031059226,-0.04122025,0.064933166,-0.04555351,-0.048958033,0.08332718,-0.01261536,-0.036189426,-0.008733376,0.067291036,-0.06395235,-0.017360043,0.044785846,-0.0064710644,0.02308319,-0.013125968,-0.05888683,-0.030500347,-0.018913219,0.028161898,-0.022731025,0.07148822,-0.010157901,0.049651,0.09145693,0.015162836,-0.0019081087,-0.011053315,-0.031019475,0.049715348,0.04678297,-0.056709778,0.018576648,-0.04445932,0.010433678,-0.032065645,0.02568862,0.007086682,0.0077769803,0.03204447,-0.007916625,-0.012504013,-0.024722133,0.01673366,0.01715825,0.04940154,0.04149579,0.024510626,-0.008700703,-0.038135525,0.016471958,0.022423187,0.006645052,-0.007556706,0.021182362,-0.023019714,0.010886962,0.07957567,0.011615965,-0.010004683,-0.016958158,0.034768533,-0.07563733,-0.005184603,0.029932285,-0.008394337,0.04887383,-0.02839299,0.0122725535,-0.019363346,0.06369501,0.028292006,-0.030169357,0.040219463,-0.0155772,-0.022525966,0.026152607,-0.001755894,-0.035962973,-0.034818947,0.0048985095,-0.010592724,-0.040494077,-0.046838775,0.057620194,0.01801869,0.087501526,0.009549957,0.01378593,-0.0020246624,-0.0077825203,-0.037958283,-0.0035687622,-0.03941285,-0.0065802154,0.012779758,0.0032131793,-0.0037608915,-0.018020643,0.0025458075,-0.010273824,0.027738634,0.04883388,0.017261846,0.08134905,-0.049150083,0.03877829,0.022359759,0.03653637,-0.035759542,-0.033969495,0.041324794,-0.01555208,0.0018468228,0.043237273,0.025007363,0.041289512,0.03998007,-0.052284528,-0.09384696,-0.0018104191,-0.0066886744,-0.02035039,0.020672522,-0.02980255,0.02122297,0.011598322,0.027989179,-0.0022807163,-0.0069288164,0.039727304,0.0044903466,-0.07264461,-0.025375906,0.0057685934,0.012940154,-0.014794273,0.0077929567,0.01685291,0.00909302,0.0067904876,-0.033901278,0.008913433,-0.020640401,0.0054042353,-0.021481594,0.008908108,-0.090854816,0.019165313,0.010301093,-0.02317843,0.023349192,-0.0017032867,-0.08529625,0.0012906389,4.5419667e-05,0.07977957,0.003238269,0.013761225,-0.02063232,0.027668117,0.04092116,0.038482875,-0.020339606,0.057192285,0.03799129,-0.018414097,0.019777318,0.04366339,0.012132991,0.014774538,-0.01450105,0.05397036,0.038779687,-0.013680688,0.011333733,-0.06375321,0.0048211,0.014456368,-0.021504737,0.004289814,-0.033813637,-0.029377058,-0.023174873,0.019929888,0.038363125,-0.000113258226,0.0261312,0.026636263,-0.031258386,-0.034733612,-0.003550668,0.043805502,0.031666797,-0.015088961,-0.001219285,0.0016100935,-0.011839434,-0.034174703,0.051514126,-0.02025846,-0.009070261,-0.030154122,0.016361652,-0.0011018446,0.02223085,-0.01572077,0.018400298,0.011090053,-0.03901079,-0.029205468,-0.023646811,-0.028381662,0.0013884117,-0.045629386,-0.031864405,0.035400152,-0.036159858,-0.04034209,-0.0147442985,-0.002121267,0.11279454,-0.021976566,0.016405506,0.0024940711,-0.012883039,0.0026682953,-0.051076535,0.013325525,0.041431032,-0.011610607,0.016568469,-0.0072843316,-0.009022063,0.0070070135,0.0343048,0.09522416,0.054234654,-0.054160044,-0.022073898,0.0018554607,0.0052127335,0.025536753,0.03846182,0.0356907,0.05408006,-0.044898544,0.03417627,0.009567026,-0.06276594,0.06112818,-0.056487046,0.08294321,0.055396944,0.016177941,-0.0011519734,0.022616455,0.022343494,0.07112163,-0.00590559,0.006900683,-0.023003226,-0.07408544,-0.009107061,-0.010358686,0.03789158,0.043917038,0.013908631,-0.0069145206,0.002710835,-0.0064405347,-0.0070221713,0.017901743,0.035942532,0.017474566,0.0013868144,-0.027880266,-0.02495449,0.033035103,0.0007126459,0.019440044,0.060614012,-0.0648803,0.028668333,-0.038410407,0.043610595,-0.007281264,-0.05413986,0.02196228,0.019747991,-0.041932378,-0.047092214,-0.013796229,0.025533019,-0.061707873,-0.0029577888,-0.009952907,0.040753298,0.019918807,-0.015603464,0.00029627315,-0.02414644,0.009831155,-0.05791251,-0.07282833,0.018035613,0.009888958,0.0239641,0.05737714,-0.017793698,0.015613883,-0.044742882,0.011445261,0.037392486,-0.010202769,-0.031392112,-0.0004938708,0.014134782,0.059867725,-0.0119079845,-0.022487415,0.0036906996,0.027281312,-0.06698877,0.026948702,0.05167561,0.032251317,-0.045994233,0.01424916,0.02473986,0.008321009,0.059962012,-0.009400966,-0.0075907256,0.03206886,-0.039603733,-0.01582973,-0.018978462,0.06179681,-0.03542342,0.012621586,-0.006789916,0.010472541,-0.013977915,0.08110823,0.016524723,0.0044287276,-0.040246744,0.02740639,-0.02299977,-0.036295336,0.008120632,-0.057675496,-0.0063004834,0.0682381,0.061329383,0.020348486,-0.07675068,0.10593686,-0.07814177,0.034132708,0.07812871,-0.049417607,0.020259002,-0.029641373,0.034682006,-0.014081244,0.04033433,0.037985098,-0.02707542,0.010056542,0.02306884,0.008190172,-0.003190916,-0.017561097,0.035653297,-0.022230297,-0.0056934953,-0.033488583,-0.02142646,0.036173314,-0.03611541,0.049457595,0.007331361,0.0018478086,-0.001889283,-0.0011932667,-0.023803247,-0.03516306,-0.046780817,-0.0140651325,0.008320577,-0.0019507924,-0.025457723,0.05318101,-0.009671322,-0.0354079,-0.01736073,0.045635,-0.0029485074,0.009249495,-0.08327697,-0.010152718,0.042538617,0.032215964,0.0046282853,0.017687242,-0.07129894,-0.016319796,-0.048994247,0.0041764206,0.012969737,0.06892243,0.05665599,0.027719585,0.04408943,-0.042788558,-0.013982005,0.014516918,0.0102553135,-0.042781856,0.00748236,-0.0142264385,0.109833285,0.037959006,-0.027612666,-0.0032468315,-0.0016284634,-0.00711869,-0.018656014,0.05173722,0.022758069,-0.006585043,-0.0065124887,0.015565264,-0.070040576,0.028720038,-0.03289691,0.024613488,-0.020815684,0.030901523,0.013538378,0.038158096,0.03487566,0.037477862,0.0032396715,-0.01603962,0.04867984,0.005373779,0.061005004,-0.05274921,0.0059074108,-0.057458144,-0.00015845534,0.013637138,-0.01819696,-0.048569288,-0.012497692,-0.06654647,0.05019011,0.019728335,-0.0057502566,-0.036948677,-0.015261911,-0.012236163,0.061376978,-0.067462645,0.006997194,-0.0072504627,-0.024643501,0.043431092,0.024107942,-0.005098852,0.0074915644,-0.00032172838,0.017182082,0.025559012,0.050337613,0.046685953,0.0104016345,0.06437194,0.012678152,-0.017818445,-0.044158358,-0.060548905,-0.029907973,-0.010644024,0.02360722,0.008412395,-0.0046294695,0.020345895,-0.027385041,0.036513288,0.004885523,0.015665695,-0.014011485,0.06555943,0.03888579,0.008706126,0.05936976,-0.024829494,0.0046309168,0.023749745,0.026826296,0.004987084,0.0102033,0.023166692,0.035977196,-0.019302288,-0.011249289,0.030744113,-0.017012369,-0.05491366,0.005934098,-0.011167496,0.0010190863,-0.027142454,0.033210795,0.0415974,0.0290196,-0.030548276,-0.033636507,-0.025999788,-0.019326702,-0.006872858,0.008579636,0.00751376,-0.00732477,0.03372544,-0.07113877,0.061655473,-0.07922839,-0.008954656,-0.040610272,0.03146128,0.029841576,-0.048145626,-0.042663023,-0.030465063,-0.008898833,-0.08316667,0.03326574,0.013852539,0.013303567,-0.0011156086,0.0017400215,0.019670894,0.008201032,-0.008370395,0.003046197,0.019755842,-0.023293018,-0.01613076,0.038341496,0.012226456,0.011979246,0.019893065,-0.018277157,-0.040385295,-0.0070415814,0.033641335,0.046678245,0.046138417,0.016102808,0.043811917,0.051659934,-0.055341735,0.007729765,-0.06317426,-0.016102234,-0.025039693,0.048101522,-2.4687419e-05,-0.02394724,-0.04312958,0.003058369,-0.01306715,-0.069642134,0.026636269,0.00068357156,0.04591947,0.061306976,0.09051608,0.0017622688,-0.082642086,-0.027698386,-0.050931532,-0.0017636971,-0.08160313,0.094056666,-0.021507654,-0.0028102398,-0.010047042,-0.01802513,0.017190939,0.013943123,0.022263136,0.0051180418,-0.007608142,0.03587944,-0.026716065,0.023522908,-0.0005192805,-0.031167844,0.039067995,0.03005569,-0.07015189,-0.015746886,-0.04158896,-0.022089703,0.06772365,-0.01022758,0.03955229,0.011914804,-0.00051543256,-0.016509159,0.05566474,-0.0076229325,-0.009015197,-0.037348993,0.033051938,0.04088946,0.020680377,-0.02561039,-0.015535552,-0.12254771,0.047437917,0.0048588915,-0.00900193,0.06380045,-0.022088626,-0.023445655,-0.008615821,0.004815421,-0.019313473,0.023457667,-0.002494527,0.047337282,0.026744664,-0.018050503,-0.017922997,-0.047174122,-0.017635437,-0.0122197755,-0.024823004,-0.0040104766,0.024800029,0.009212861,-0.0026177852,-0.015108624,-0.053143367,-0.011861294,-0.0014691231,-0.003135844,0.04332483,-0.056453526,0.029040841,-0.00073015323,-0.0048733423,0.07916603,0.023719585,0.012358798,0.01998922,0.055099554,-0.0020237607,0.008322451,0.020457473,-0.045547605,-0.040506445,0.039729286,0.033150006,0.015532702,-0.0021312537,-0.0018505388,-0.022747034,0.0003293241,0.032620262,-0.038430616,0.046389725,0.011854237,0.03253217,-0.08404919,0.028059667,-0.042688664,0.037827417,-4.8069855e-05,0.013829329,0.048148587,-0.004139467]	Keywords: vanilla transformer, sequence-to-sequence model, encoder, decoder, blocks\nKey Objects: Encoder, Decoder, Blocks\nRefers to Images: None\nHypothetical Questions:\n- What is the fundamental structure of the vanilla Transformer model?\n- How do the encoder and decoder components of the vanilla Transformer work together?\n- Why are the blocks in the encoder and decoder considered 'identical'?\n---\nSummary:\nThe vanilla Transformer is a sequence-to-sequence model comprised of an encoder and a decoder, each containing a stack of identical blocks.\nOriginal Text:\n## 2 BACKGROUND  \n### 2.1 Vanilla Transformer  \nThe vanilla Transformer [137] is a sequence-to-sequence model and consists of an encoder and a decoder, each of which is a stack of L identical blocks. Each encoder block is mainly composed of a multi-head self-attention module and a position-wise feed-forward network (FFN). For building a deeper model, a residual connection [49] is employed around each module, followed by Layer Normalization [4] module. Compared to the encoder blocks, decoder blocks additionally insert cross-attention modules between the multi-head self-attention modules and the position-wise FFNs. Furthermore, the self-attention modules in the decoder are adapted to prevent each position from attending to subsequent positions. The overall architecture of the vanilla Transformer is shown in Fig. 1.  \nIn the following subsection, we shall introduce the key modules of the vanilla Transformer.\nContextualized Text:\nThe vanilla Transformer, introduced in reference [137], is a sequence-to-sequence model designed for tasks involving both encoding and decoding sequences.  It operates with an encoder and a decoder, each constructed from a stack of L identical blocks.	{"tags": ["architecture", "NLP", "transformer"], "doc_id": "0f6724ab-5273-4fe5-9ec9-b996cfa79f37", "summary": "The vanilla Transformer is a sequence-to-sequence model comprised of an encoder and a decoder, each containing a stack of identical blocks.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["vanilla transformer", "sequence-to-sequence model", "encoder", "decoder", "blocks"], "key_objects": ["Encoder", "Decoder", "Blocks"], "contextual_text": "The vanilla Transformer, introduced in reference [137], is a sequence-to-sequence model designed for tasks involving both encoding and decoding sequences.  It operates with an encoder and a decoder, each constructed from a stack of L identical blocks.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.1 Vanilla Transformer"}, "hypothetical_questions": ["What is the fundamental structure of the vanilla Transformer model?", "How do the encoder and decoder components of the vanilla Transformer work together?", "Why are the blocks in the encoder and decoder considered 'identical'?"]}
f57867ae-0b3c-4381-b38b-d1641e9db715	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017149605,0.010006493,0.01740748,0.013951007,-0.0041561867,0.017970067,0.0020937822,0.014340887,0.063856445,-0.02101578,0.0062423763,-0.037775796,0.020906908,0.044393823,-0.020624964,0.060889177,0.018815521,0.10259685,-0.0034205313,-0.053620383,0.043794546,0.0014984631,0.104562744,-0.02983836,-0.0001560362,0.042779624,0.08080552,0.05092719,-0.06321032,0.017960321,0.008692666,-0.015747447,0.030702293,-0.0020522543,0.033793468,0.0054759285,-0.016550317,-0.050000437,-0.02910937,-0.005203398,-0.008929748,0.046822987,-0.04017433,-0.022995103,0.009147832,-0.057430174,-0.098939724,-0.024908027,0.008552311,-0.026980534,-0.060179714,0.02573244,-0.036317404,-0.031317897,-0.0055196784,0.0011805055,-0.02754808,-0.017845072,0.024145493,-0.08042724,0.04218239,0.015496825,-0.038062003,-0.045596417,0.011143415,-0.0037647334,-0.014710839,0.004496898,0.06544414,0.10836411,-0.022727102,0.018442936,0.035860162,-0.0197326,0.13198566,0.075927965,-0.061877012,0.012210774,-0.06371176,0.009546451,-0.042787023,0.057116587,-0.05041378,-0.06732991,0.0837314,0.029169468,-0.023337653,0.00798817,0.06739799,-0.051498257,-0.02895247,0.04513277,-0.009395789,0.04810631,0.05041676,-0.0773761,-0.052790757,-0.02598413,-0.0026735016,-0.0127339205,0.028290449,0.010537698,0.03509267,0.07598546,0.0049802614,-0.026740449,0.020911723,-0.03128011,0.04075959,0.025849689,-0.06324858,0.04945415,-0.05480958,0.03416306,-0.07760345,0.020129079,-0.025649874,0.022534667,-0.002037927,-0.037467003,-0.016071994,0.020755114,-0.0087220045,0.00731359,0.01672128,0.010750384,-0.018689426,-0.006725335,-0.023487853,0.019141668,0.03492298,-0.0036352698,-0.017879343,0.033818197,0.0030306103,0.007650622,0.064055026,-0.022625709,0.0012405115,0.03770522,-0.0077572744,-0.031075813,-0.016612926,0.030256724,-0.008307536,0.09571474,-0.044083454,-0.03533582,-0.061873402,0.06757317,0.031075977,-0.046209347,0.027512325,-0.03626255,-0.029028395,0.047521606,-0.009722036,-0.07030287,-0.037343513,-0.043463394,-0.05224914,-0.06298125,-0.066954926,0.022227243,0.021849902,0.09113641,-0.013394475,-0.00060404994,0.033783756,0.04223705,-0.040256605,-0.024825877,-0.02894159,-0.015410367,0.047198102,0.01680353,0.01297383,-0.010817755,0.005567662,-0.00942669,0.005157488,0.031150358,0.043441165,0.050008688,-0.056477476,0.05761413,0.06469506,0.06641324,-0.021188524,-0.006670305,0.029655462,0.009649606,-0.025679173,-0.0032488473,0.021632437,0.02387812,0.059198074,-0.038856566,-0.06597399,0.007483085,-0.0140497945,-0.020467788,0.007256184,-0.03662777,0.033746712,0.015485182,-0.002358748,0.00090831757,-0.025635753,0.021186313,-0.012399525,-0.06112683,-0.022200998,0.017042316,0.015452766,-0.028948952,-0.016182994,0.037591483,0.011356645,0.012129254,-0.031257458,0.004473431,-0.048110604,0.010994472,-0.033524837,0.03117862,-0.039992683,0.007370885,-0.034316022,-0.0032880085,0.02138776,-0.028093193,-0.101556614,0.0019978231,-0.012298481,0.04970916,-0.00023625289,0.0003872081,-0.028612437,0.013950311,0.04117841,0.013638853,-0.051474776,0.07223399,0.041599736,0.027949242,0.050776057,0.097747,0.0035060851,-0.017127581,0.0034365023,0.010558299,0.03539077,-0.008396752,0.014595456,-0.037778657,-0.008775613,0.00094158575,-0.008944432,-0.025277149,-0.016961738,-0.03532805,-0.03655752,0.012836946,0.036920797,0.0296281,0.022682909,0.0319093,-0.03546729,-0.022585575,-0.0359738,0.030451192,0.031206658,-0.007960429,-0.029913636,0.0040000407,0.021645034,-0.0042867824,-0.0033505466,-0.0096598985,-0.040747408,-0.034909084,-0.021266755,0.020604374,0.00070776884,-0.02031954,0.02104769,-0.019642308,-0.029750895,-0.012946806,0.0056179273,-0.009965362,-0.041857574,-0.02112839,-0.025918407,0.014393309,-0.01765714,-0.0024287386,-0.003740638,0.010489144,0.12936832,-0.034431186,0.033461157,0.025230499,-7.182122e-05,-0.05739869,-0.07225762,0.042619083,0.09030157,-0.02174592,0.00800664,-0.01982132,-0.031437892,-0.0011677034,0.046638735,0.06522605,0.021009892,-0.023553943,-0.020410175,-0.00917072,0.027430505,0.032073185,0.001895039,-0.009165585,0.0096455095,0.0102718985,0.034578837,-0.0031857106,-0.023322016,0.045239255,-0.067224324,0.028613115,0.059128765,0.020519746,-0.0114875175,0.018148528,0.0030515017,0.014386965,0.014348621,-0.017783241,-0.01914578,-0.060783207,0.011775606,-0.01094362,0.02973335,0.010749111,-0.0025383527,-0.013521555,-0.023004225,-0.04009883,0.0406403,1.9740026e-05,0.0265579,-0.0038362215,-0.017400792,-0.007723167,-0.012262747,-0.003943155,0.023443554,0.016975,0.02123947,-0.030023165,0.052072227,-0.038232133,0.007680038,0.006525743,-0.042725824,0.0057758614,0.018175632,0.014382797,-0.06615929,-0.019955521,0.015419478,-0.037928686,-0.034982927,0.035288006,0.041283473,0.06357366,0.02839668,0.007331687,8.536583e-05,0.038411044,-0.042958785,-0.08548223,0.021939859,-0.017054062,0.0018601358,0.07435746,-0.035734564,0.03643786,-0.055858653,0.0059809964,0.039248988,-0.039111063,-0.045283847,-0.015547405,-0.0029057004,0.024668045,-0.028465645,-0.024271509,-0.00011695974,0.024560535,-0.06474086,0.03867856,0.03432473,0.020731555,-0.06699831,0.015837358,0.053597353,0.031173106,0.03981513,0.0076124254,0.014102636,0.0056503755,-0.08544721,-0.017742494,-0.014187792,0.085091665,-0.04193292,-0.047138445,-0.009435523,-0.00070168794,-0.014174088,0.0679575,0.0013261053,0.0074820253,-0.0124162445,0.040001962,-0.014654458,-0.010489285,0.01217935,-0.023972046,-0.0007040284,0.08007114,0.008982507,0.039417762,-0.07413648,0.060645983,-0.05503834,0.0032406575,0.058228407,-0.017162913,0.058250505,-0.0301351,0.056295678,-0.02522456,0.038082916,0.034095343,-0.058067553,-0.03858854,0.054530285,-0.0026898026,-0.002547845,-0.017708814,0.047449276,-0.051148582,-0.022801653,-0.021422885,-0.014902715,-0.020831786,-0.008189702,0.03720614,0.014485645,-0.04900094,0.00065209175,0.013560733,0.057651937,-0.048837196,-0.039518617,-0.027917877,0.027744345,-0.014751565,-0.030138673,0.0391518,-0.010376124,-0.03726913,-0.010687386,0.030991782,0.009045861,0.0122723635,-0.03757139,0.0009227654,0.027697545,0.026536718,-3.821465e-05,0.021741597,-0.039809056,-0.031886745,-0.04873472,0.039650176,0.014813175,0.05615753,0.03794113,0.0575259,0.03459224,-0.06293206,-0.033991806,-0.0005762288,0.014372477,-0.009638698,-0.0011064636,0.004106795,0.086613044,0.02277972,-0.039979022,0.00091730786,-0.016464986,-0.02553167,0.017694958,0.04462894,0.059429705,-0.063988045,-0.045360062,-0.0028038907,-0.035888992,0.024041979,-0.0028903575,0.017924123,-0.017459841,0.052158676,-0.009284106,-0.027555224,0.033833776,0.057655,-0.008164544,0.009093248,0.026470264,0.034487754,0.04993166,0.017497819,-0.01238667,-0.030477611,-0.0043184096,0.015444285,0.024774626,-0.05670671,0.030717496,-0.038135953,0.054837253,-0.030515645,-0.0035017568,-0.055985395,-0.005836022,-0.020369038,0.044074453,-0.022158122,0.021260722,-0.03283228,-0.028535241,0.07192806,0.02835658,0.012394653,0.03249092,-0.0023520873,0.015050457,0.04118945,0.033422984,-0.012271815,0.01217534,0.007784233,0.027265448,0.028945403,-0.024988282,-0.03550084,-0.034421664,-0.02305312,0.0064819544,0.0064400164,-0.040453732,0.017360246,-0.05165494,0.021494038,-0.025944771,-0.0089024445,0.026908,0.0060045747,0.03582236,0.00017336234,0.05179419,-0.006090264,0.0106672635,0.053092696,0.010490237,0.0071419175,0.0011988903,0.025183482,0.044153016,-0.011922763,-0.009911074,-0.014043326,0.0092577655,-0.059553687,0.014333638,-0.049104314,0.01604469,-0.03556589,0.0319294,0.030471751,-0.0038716807,-0.024099581,-0.022920204,0.032755893,-0.0066675534,-0.031982455,-0.011945624,0.0077184075,-0.019140646,0.0057536433,-0.029599274,0.12584807,-0.00029216672,-0.0007692912,-0.04442872,0.009113515,0.04876527,-0.045776974,-0.054029353,-0.026643407,-0.0008036767,-0.062677436,0.05405814,-0.0372375,0.017592693,0.0081299795,-0.032115474,0.014841011,0.0037533778,0.0024830687,0.00043466242,-0.01954677,0.00047025873,-0.0056673107,0.03335692,-0.015821189,0.000257048,-0.00922127,-0.02355735,-0.0032054202,-0.010159739,0.02979359,0.02934371,0.004343071,0.032432716,0.037150107,0.026797662,-0.00022618256,0.006526833,-0.05086432,-0.023483569,-0.015712477,0.044486366,0.018307308,-0.01760639,-0.061024778,0.0023573523,0.013447576,-0.060605444,0.042620957,0.00052923855,0.073284104,0.033073753,0.06855824,-0.017741838,-0.04387801,-0.023085022,-0.09739317,0.00080178154,-0.021165518,0.07231463,-0.03577961,0.027332973,-0.001118274,0.018170394,0.009561992,0.026235659,0.037219916,-0.014523301,-0.023448236,-0.013254136,0.011644432,-0.009043007,-0.012422905,-0.043293446,0.022833083,0.021906614,-0.08164423,0.003034685,-0.05879698,-0.04990557,0.03373468,-0.017730663,0.014143315,0.0023512708,0.012544311,0.017978845,0.04398019,0.007059783,0.027575614,-0.039603956,-0.008095108,0.0618833,0.017114421,-0.03283845,-0.00658633,-0.09513648,0.022132041,0.009192493,-0.018231254,0.06249875,-0.022915218,-0.030637445,0.007669777,0.011220807,0.013957981,-0.02046847,0.022256041,0.044665728,0.028748805,-0.011631442,0.036316995,-0.03636468,0.0043706046,-0.019245362,-0.006966064,-0.025038963,0.06489938,0.033884935,-0.0072763516,0.017361734,0.011782127,1.15025805e-05,0.010719326,0.05455843,0.07166585,-0.01671284,0.03401548,-0.033188313,0.014441538,0.06362899,0.003551326,0.045494836,-0.016832097,0.013501099,0.032403838,0.018742196,0.033779837,-0.038870867,-0.03425144,0.06017545,0.04773968,-0.0007955556,0.018500071,0.01768435,-0.019585215,0.018620234,0.035744056,0.010901105,0.04669147,0.009827948,0.02666254,-0.05589616,0.018432343,-0.065484956,0.044738427,-0.03960895,0.03089424,0.019938093,0.0036579475]	Keywords: attention mechanism, Query-Key-Value, QKV, scaled dot-product attention\nKey Objects: Attention Mechanism, Query, Key, Value\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of using Query-Key-Value pairs in the attention mechanism?\n- Why are the dot products of queries and keys divided by  D_k?\n- How does the attention mechanism contribute to the overall functioning of the Transformer?\n---\nSummary:\nThe following section introduces the key modules used in the vanilla Transformer architecture, beginning with an explanation of the attention mechanism which utilizes Query-Key-Value (QKV) models.\nOriginal Text:\nIn the following subsection, we shall introduce the key modules of the vanilla Transformer.  \n- 2.1.1 Attention Modules. Transformer adopts attention mechanism with Query-Key-Value (QKV) model. Given the packed matrix representations of queries Q  R N  D$\\_{k}$ , keys K  R M  D$\\_{k}$ , and values V  R M  D$\\_{v}$ , the scaled dot-product attention used by Transformer is given by 1  \n$$A t t e n t i o n ( Q, K, V ) = s o f t m a x \\left ( \\frac { Q K ^ { T } } { \\sqrt { D _ { k } } } \\right ) V = A V,$$  \nwhere N and M denote the lengths of queries and keys (or values); D$\\_{k}$ and D$\\_{v}$ denote the dimensions of keys (or queries) and values; A = s o f t m a x ( QK T ) is often called attention matrix ; softmax is applied in a row-wise manner. The dot-products of queries and keys are divided by  D$\\_{k}$ to alleviate gradient vanishing problem of the softmax function.\nContextualized Text:\nTo understand the vanilla Transformer model, it's important to review the model's key components. This section will begin by detailing the attention mechanism, which operates using a Query-Key-Value (QKV) model. This mechanism utilizes packed matrix representations of queries (Q), keys (K), and values (V) to calculate a scaled dot-product attention.	{"tags": ["architecture", "NLP", "attention"], "doc_id": "f57867ae-0b3c-4381-b38b-d1641e9db715", "summary": "The following section introduces the key modules used in the vanilla Transformer architecture, beginning with an explanation of the attention mechanism which utilizes Query-Key-Value (QKV) models.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["attention mechanism", "Query-Key-Value", "QKV", "scaled dot-product attention"], "key_objects": ["Attention Mechanism", "Query", "Key", "Value"], "contextual_text": "To understand the vanilla Transformer model, it's important to review the model's key components. This section will begin by detailing the attention mechanism, which operates using a Query-Key-Value (QKV) model. This mechanism utilizes packed matrix representations of queries (Q), keys (K), and values (V) to calculate a scaled dot-product attention.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.1 Vanilla Transformer"}, "hypothetical_questions": ["What is the purpose of using Query-Key-Value pairs in the attention mechanism?", "Why are the dot products of queries and keys divided by  D_k?", "How does the attention mechanism contribute to the overall functioning of the Transformer?"]}
121fda60-1994-4388-a82d-526129b3bde4	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0031423091,0.011690444,0.033217516,0.049286976,0.011643968,0.060458966,-0.003271837,0.063723,0.011535298,-0.0077262227,0.00023098856,-0.008369831,0.017593982,-0.0120519195,-0.019258915,0.046550382,0.03691207,0.0694234,0.00057888334,-0.037341908,0.035822064,0.05697363,0.034416825,-0.019815274,-8.83312e-05,0.015589817,0.040607642,-0.0028525784,-0.031217603,0.033181567,0.027792314,-0.05898004,0.043863475,0.009071268,0.068116516,-0.004186742,-0.009980045,-0.06916832,-0.0060188845,-0.0029794239,-0.036265492,0.046098854,-0.051723782,-0.022030128,-0.0034140311,-0.040077437,-0.100855626,-0.034134794,-0.003932886,-0.00559153,-0.026194675,-0.005675943,-0.051735464,-0.0048326445,-0.030670542,-0.022933602,0.0057764524,-0.06373961,0.017912855,-0.12303918,0.0032580008,-0.008258131,-0.00016798949,-0.0050486936,0.024032226,-0.02162689,0.013959226,-0.018077953,0.043079462,0.12793486,-0.03893109,-0.008236911,0.051349264,-0.053813513,0.08988149,0.054821957,-0.029243443,-0.011078971,-0.051305585,-0.04389938,-0.02092924,0.073809005,-0.06322097,-0.06039808,0.06969032,0.006254461,-0.02729627,0.031297028,0.04780941,-0.057675097,-0.01982833,0.022118792,-0.007472752,0.06901725,-0.027467662,-0.061385658,-0.0210839,-0.01863157,0.008818251,-0.045565147,0.013441433,0.0031874287,0.09978282,0.106875576,0.007569968,-0.043526243,-0.044286188,-0.05863632,0.0013250068,0.034809176,-0.013194183,0.03725294,-0.03082113,0.038177386,-0.0032473681,0.03783471,-0.035613243,-0.009181427,0.007276813,-0.00292465,-0.010885912,-0.007981641,-0.014809912,0.024513198,0.047384128,0.052009072,0.031930573,-0.012614651,-0.054572236,0.006928739,0.037526537,-0.0035263144,0.001513686,0.014282152,-0.029916538,-0.020055475,0.08286231,-0.016045542,-0.013453254,0.026164476,-0.0018118565,-0.037692312,-0.044791058,0.05239884,-0.023105327,0.06634574,-0.06851153,0.033520445,-0.029726164,0.06736097,0.04217857,-0.0059500826,0.02169277,-0.011050413,-0.02074829,-0.019784056,0.015069787,-0.05815824,0.011893067,-0.05048226,-0.046677873,-0.049344353,-0.01881308,0.055304468,0.038657,0.11530246,-0.009838514,-0.003870354,0.038665168,0.005503991,-0.03660022,-0.0013674874,-0.00083134655,-0.04139858,-0.008182395,0.0020812207,0.0053663026,-0.03116716,0.014931958,-0.017908296,0.004784244,0.031050144,0.018766781,0.03802823,-0.036767457,0.052194413,-0.0074038026,0.043849315,-0.006246776,-0.03944962,0.020250633,-0.0025168248,0.006808971,0.010973887,0.045061864,0.035002902,0.03961859,-0.08834291,-0.040903557,-0.05197072,-0.024983548,-0.007177644,0.014269002,-0.058253143,0.041034967,-0.031831037,-0.006362509,-0.049451668,0.009666579,0.03153998,0.0052628773,-0.055362515,-0.009610279,0.042929634,0.018848477,0.0004693894,0.019791601,0.0029985483,0.016409745,-0.021226069,-0.054499917,0.013895029,-0.07710392,0.019659176,-0.0015096444,0.02931153,-0.006531367,0.02527079,1.1199669e-05,-0.035007484,0.018227953,-0.021682652,-0.05366388,0.013150065,-0.010825852,0.020767983,0.024426099,0.017874792,-0.026928887,0.013946832,0.059522785,0.054220613,0.0023951465,0.09208156,0.037910562,0.0062500965,0.037042905,0.0387867,0.001374656,0.00039077393,-0.038326867,0.067872666,0.015935393,-0.020155804,-0.012807476,-0.07072553,-0.019947449,0.024150606,-0.0010463293,0.014777193,-0.009184117,-0.00038247663,0.0033355872,-0.016611492,0.0010488745,0.023471951,0.021707406,0.045626126,-0.055053387,0.00032889683,0.01379738,0.07184561,0.034993425,-0.02663215,-0.031545833,0.043163855,-0.0013920378,-0.027911363,-0.015663432,-0.015819374,-0.019420132,-0.025410445,0.029597271,-0.004566486,0.04398625,-0.0061803143,0.068142146,-0.006562885,-0.032556493,0.018899612,-0.014369195,-0.0067594475,0.0057956576,-0.008087992,-0.015552381,-0.010263439,-0.00023857066,-0.013724956,-0.033237223,0.0117624495,0.13226771,-0.045082696,-0.0055350508,0.035902888,0.019359251,-0.005712083,-0.06407679,0.031218102,0.060644943,-0.010295486,0.024557382,0.009799902,0.0013986942,0.0009807592,0.021086205,0.08707618,0.064951934,-0.009448118,-0.002726024,-0.0060387226,0.0024223335,0.00733218,-0.0008015644,0.031145135,0.031604476,-0.022156695,0.025464691,0.008718067,-0.07340712,0.05243386,-0.08382143,0.056322265,0.055872884,-0.0015058442,0.026618019,0.0371594,-0.013107337,0.03972288,-0.009832696,0.031233702,-0.029884553,-0.037519883,0.01989602,-0.017343873,0.002318036,0.04490504,0.007386489,-0.00078260916,0.038456153,0.0027013074,0.035899833,0.039786372,0.00256188,0.0054186294,-0.01028249,-0.04017023,-0.0016554773,0.019862117,-0.0018359779,-0.002186979,0.0744466,-0.08962719,0.066499434,-0.04495668,0.025363263,-0.010298831,-0.057154525,0.00037313096,0.038653713,-0.04042062,-0.016780484,-0.036773667,0.048917968,-0.03872165,-0.04407358,0.028733779,0.05157578,0.029092818,-0.032128733,-0.0024686668,0.0080024535,-0.013909855,-0.03594549,-0.046004377,0.02712332,0.008876457,0.004367771,0.08545508,-0.007326547,0.044752628,-0.031440713,0.019137885,0.015050401,0.005506109,-0.034229003,0.010828871,0.004979465,0.04357079,0.01745135,-0.042370487,0.050348002,0.024289368,-0.05237352,0.0059393616,0.025994673,0.029746614,-0.07622738,0.030874694,0.035276663,0.04354274,0.0032924921,-0.006598384,0.0029075432,0.025164349,0.020215848,-0.02679776,0.015472744,0.037628457,-0.032686874,0.026891809,-0.0024163167,0.047237545,0.031661768,0.04181428,0.030995043,-0.004544903,-0.043051194,0.03820656,-0.029649898,-0.032755286,0.0060476596,-0.06125646,-0.0145451715,0.04718339,-0.017158233,0.03353469,-0.0745639,0.048585046,-0.037155863,0.033392493,0.0812374,-0.035696734,0.02873944,-0.029860513,0.001170896,-0.039714098,0.038241833,0.025355825,-0.040546883,-0.01373344,0.029082606,-0.011334254,0.029462684,0.005790943,0.06914549,-0.005246965,0.0010990921,-0.039140254,-0.03461169,-0.011260185,0.015767528,0.024110666,0.030716708,0.00084148033,-0.012850981,0.0024103199,0.020600332,-0.047463637,-0.03575545,0.01284333,0.014507335,0.03756727,-0.049502995,0.035135295,-0.0033503326,-0.030027851,-0.018501358,0.05768358,0.028319104,0.013095986,-0.059937716,-0.004296044,0.024954475,0.016818022,0.016785929,-0.011760003,-0.026475342,-0.040737566,-0.035419997,0.04347997,0.005909515,0.019084366,0.032955103,0.03786826,0.03203779,-0.054572042,-0.007672327,0.025468638,0.019543128,-0.0022764073,-0.027997188,0.0046662446,0.06802447,0.0684008,-0.029201921,-0.013059629,0.013874647,0.010988672,-0.005819348,0.0821103,0.0035499462,-0.030206919,0.0050693727,-0.00032083783,-0.04625364,0.03675009,-0.03776102,0.023149658,-0.010720407,0.040978704,-0.026294323,-0.0015089726,0.028697228,0.030869782,0.0074482607,-0.02012954,0.045066774,0.023037963,0.06080147,-0.08314965,0.021504637,-0.035587043,-0.03708155,-0.001608629,-0.037395954,-0.06424274,-0.005222189,-0.06892726,0.019829065,0.02706876,0.010122614,-0.04412492,-0.0016974623,-0.025153061,0.048409577,-0.025952363,0.03298281,0.017105976,-0.012893484,0.07444806,-0.0040843817,0.04073085,-0.0047270795,0.00578289,0.024682036,0.039185114,0.068816975,-0.0028214436,-0.022493271,0.047668654,0.009304322,0.013956015,-0.028924815,-0.038390756,-0.023529086,-0.011086732,-0.013572718,0.00022862975,-0.032196146,-0.00094174483,-0.026491765,0.010754195,-0.009294257,-0.006428405,0.0029474401,0.052892216,0.04004221,-0.005345726,0.053598627,-0.025914904,0.07767153,0.043127615,0.031036822,0.0054157996,-0.008686387,0.018102922,0.018872136,-0.009593033,-0.01406603,0.0050175074,-0.010198376,-0.02378752,0.03156428,0.025452362,0.018585071,-0.03447186,0.03993772,0.019788412,-0.01794343,-0.0644592,-0.041522093,-0.031175544,-0.050511245,-0.031927038,-0.023919651,-0.0011299503,-0.011223997,0.022820879,-0.01210401,0.053401344,-0.10027015,0.014083807,-0.075878575,0.005236268,0.025621083,-0.03164234,-0.014629335,-0.025239414,0.0045014657,-0.01595086,0.05904465,0.00050403306,-0.0057871095,-0.0037716988,-0.012911704,-0.0034479892,0.013641775,-0.009599338,-0.010141702,0.0049495376,-0.010872533,-0.027850091,0.06078634,-0.031925812,0.0059057893,0.04253325,-0.034246113,-0.08859345,-0.0006240475,0.021717133,0.048104092,-0.0026683358,0.030745191,0.013502827,0.022574512,-0.058360495,0.004356887,-0.045980334,-0.026851146,0.00071020075,0.025964003,0.03968766,-0.010503252,-0.067950755,-0.00013776442,0.03325637,-0.028533252,0.03559441,0.028730487,0.047508582,0.048105035,0.054408494,0.0010775547,-0.09114977,-0.027616048,-0.044938616,-0.00840539,-0.03630883,0.056244675,0.012052492,0.018749496,-0.02510699,0.002461491,0.018836029,0.010160588,0.013534938,-0.0008405935,-0.04571158,-0.031296674,0.04713681,0.033454105,0.0009369322,0.0055860137,0.06974068,-0.003922263,-0.07084377,-0.020173043,-0.0874086,-0.029293213,0.019141596,-0.003378789,0.018690076,0.010513455,-0.023892362,-0.0031690556,0.05675615,-0.018124305,-0.023529738,-0.014852769,-0.0010990644,0.030716008,0.028085595,-0.005724954,-0.002857491,-0.101100795,0.012326203,-0.019073348,-0.06698724,0.0591896,-0.00776554,-0.024119508,-0.01781338,0.017841848,-0.016014867,0.020499859,0.032199718,0.017151212,0.025068255,0.029103903,0.013017362,-0.08015258,0.00087926077,-0.007309923,-0.02266009,-0.010472637,0.04093316,0.01285107,-0.0031586315,0.009319697,0.017196795,-0.0058148354,0.0077656037,0.005791268,0.046877574,-0.024968667,0.05730452,-0.009811088,0.026689913,0.07369646,0.019027304,-0.024565108,0.011383105,0.017950134,0.010658721,-0.004616251,0.0008929355,-0.06998799,-0.029088773,0.028208168,0.027518004,-0.0220565,0.02540536,-0.0036668177,-0.044266395,-0.05814733,-0.026319608,-0.042216476,0.046367016,0.018030997,0.03378283,-0.06495623,0.04045057,-0.01492244,0.06403036,0.019719083,0.019959394,-0.02639189,0.007578047]	Keywords: multi-head attention, queries, keys, values, learned projections\nKey Objects: Queries, Keys, Values, Multi-head Attention\nRefers to Images: None\nHypothetical Questions:\n- Why is it advantageous to project queries, keys, and values into different dimensions?\n- What is the purpose of the learned projections used in multi-head attention?\n- How does concatenating the outputs of the individual attention heads contribute to the model's overall performance?\n---\nSummary:\nTo enhance attention capabilities, the Transformer architecture employs multi-head attention, where queries, keys, and values are projected into different dimensions using learned projections, and the outputs are concatenated and projected back to the original dimension.\nOriginal Text:\nInstead of simply applying a single attention function, Transformer uses multi-head attention, where the D$\\_{m}$ GLYPH dimensional original queries, keys and values are projected into D$\\_{k}$ , D$\\_{k}$ and D$\\_{v}$ dimensions, respectively, with H di GLYPH erent sets of learned projections. For each of the projected queries, keys and values, and output is computed with attention according to Eq. (1). The model then concatenates all the outputs and projects them back to a D$\\_{m}$ GLYPH dimensional representation.  \n$$M u l t i H e a d A t t { \\mathcal { N } } ( Q, K, V ) = C o n a t { \\mathrm { c o a d } } { \\mathrm { h e a d } } ( 1, \\dots, h e a d _ { H } ) W ^ { O },$$  \n$$w h e r e \\, h a d _ { i } = A t t e n t i o n ( Q W ^ { O }, K W ^ { H }, V W ^ { I } ).$$  \n$^{1}$if not stated otherwise, we use row-major notations throughout this survey (e.g., the i -th row in Q is the query q$\\_{i}$ ) and all the vectors are row vectors by default.  \nFig. 1. Overview of vanilla Transformer architecture\nContextualized Text:\nInstead of applying a single attention function, the Transformer utilizes multi-head attention. In this approach, the original queries, keys, and values are projected into different dimensions using learned projections.  Each of these projected sets then undergoes an attention calculation (Eq. (1)), and the resulting outputs are concatenated and projected back to the original dimension.	{"tags": ["architecture", "attention", "transformer"], "doc_id": "121fda60-1994-4388-a82d-526129b3bde4", "summary": "To enhance attention capabilities, the Transformer architecture employs multi-head attention, where queries, keys, and values are projected into different dimensions using learned projections, and the outputs are concatenated and projected back to the original dimension.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["multi-head attention", "queries", "keys", "values", "learned projections"], "key_objects": ["Queries", "Keys", "Values", "Multi-head Attention"], "contextual_text": "Instead of applying a single attention function, the Transformer utilizes multi-head attention. In this approach, the original queries, keys, and values are projected into different dimensions using learned projections.  Each of these projected sets then undergoes an attention calculation (Eq. (1)), and the resulting outputs are concatenated and projected back to the original dimension.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.1 Vanilla Transformer"}, "hypothetical_questions": ["Why is it advantageous to project queries, keys, and values into different dimensions?", "What is the purpose of the learned projections used in multi-head attention?", "How does concatenating the outputs of the individual attention heads contribute to the model's overall performance?"]}
7a97f798-8aa4-452a-8aaa-862c64853369	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.036372613,-0.00084725145,0.0254102,0.08936845,-0.024787404,0.047299203,-0.02553556,0.05493069,0.048247516,-0.023202255,-0.011722104,0.035915073,0.034104608,-0.022822298,0.0089678615,0.032340188,0.037007272,0.084163435,-0.009504538,-0.02838293,0.01834189,0.016001707,0.025385385,-0.013342536,-0.0043960414,0.01787724,0.032213885,-0.05552122,-0.048262127,0.02029433,-0.016939621,-0.0062405923,0.010678671,0.013301837,0.04874083,-0.04064739,-0.00016850984,-0.048058193,-0.04739104,0.008744484,-0.009228402,0.07606633,-0.050137505,-0.008687685,-0.02584209,-0.0437954,-0.10656192,-0.020917613,0.02014434,-0.028067484,-0.01804622,0.026517648,0.01693136,0.001795376,-0.04544353,-0.018635953,-0.03160522,-0.065889336,-0.008184788,-0.10559317,-0.018422872,0.003586546,0.016415661,0.0070910524,-0.029622938,-0.010938069,0.017136062,0.0062034815,0.026740821,0.16185732,-0.02300633,0.024999294,0.018983396,-0.026363654,0.102005094,0.047701903,-0.02996044,-0.025933836,-0.07513719,-0.011841024,0.015407983,0.0749048,-0.0048119356,-0.021043666,0.09066282,0.006719821,-0.04234161,-0.016856732,0.06537755,-0.056886055,0.021895936,0.008502685,0.043292437,0.04793421,-0.003338603,-0.060424153,-0.03068893,-0.027717948,0.0005969559,-0.03644987,0.021208663,0.028880276,0.07477665,0.10043295,0.010265562,0.004541208,0.0013964312,-0.016849805,-0.008722623,0.03505586,-0.062154252,-0.00032401478,0.0029582258,0.011159537,0.025273766,0.04695252,-0.0140522625,0.014311639,0.022405138,0.011666499,0.020416828,-0.021660604,-0.0032119716,-0.036259696,0.05448012,0.06884764,-0.025472067,0.0073497933,-0.047689922,0.00017960992,0.084112436,0.007557234,-0.03728774,0.01814621,-0.015240061,0.009469524,0.036921468,-0.0024348223,0.0068480032,0.021642053,0.057595994,-0.068173885,-0.02284524,0.01263418,-0.028614981,0.054462604,-0.057891116,0.020149684,-0.058008432,0.05120829,0.032254465,0.00019071001,0.011469692,0.019571334,-0.017368065,0.023274587,0.006768847,-0.05248917,0.00014542855,-0.029787786,-0.05181725,-0.04971577,0.015557469,0.07588632,-0.004913169,0.089613944,0.0021979942,0.0011482225,0.01729109,0.042100307,-0.025829265,-0.005431204,-0.03714501,-0.03596584,0.022708014,0.025173971,-0.009521872,-0.009747664,-0.011136732,-0.037713982,-0.021991834,0.022637026,-0.0047335904,0.032826144,-0.042120386,0.016596798,0.0239139,0.03123167,0.016197637,-0.0590399,0.037584733,-0.010192773,-0.044920437,-0.018059362,0.0276504,0.022879915,0.0388559,-0.037009068,-0.06692735,-0.016903877,0.022038478,0.030396687,0.0023336548,0.0071323975,0.0025997069,-0.041834816,-0.003019022,0.023744963,-0.020452917,0.03169196,0.024869585,-0.055371195,-0.02545245,-0.014750655,0.015790442,0.009934524,-0.020130035,0.0062513687,0.015175516,-0.017221237,-0.044315968,0.022599991,-0.056870025,-0.007945885,-0.061400343,0.029387355,-0.072035536,0.0146431355,0.012497169,0.032659065,0.016856287,0.020470351,-0.045136143,-0.003018898,-0.031713363,0.0670153,-0.007223482,0.022860646,-0.038066227,0.019740887,0.040322706,0.02884416,-0.015653647,0.05644221,0.009213339,-0.027506385,0.016888998,0.049285043,0.022272758,0.05087479,-0.01162005,0.060449768,0.05563266,-0.0058007445,-0.007063674,-0.07166823,-0.028947363,0.011353926,0.027714059,0.0040243044,-0.024489118,-0.03533542,-0.016220247,0.030054512,0.013662902,0.03524141,0.026001126,0.06732513,-0.05914431,-0.02872672,-0.018557591,0.0023906166,0.048192844,-0.026622502,0.023147574,-0.03484554,-0.019385166,-0.026168669,0.008474577,-0.024401762,-0.010974638,-0.030824458,0.014799619,-0.008145158,0.015979743,-0.01377028,0.0012720624,-0.06371039,-0.071070015,-0.032469694,-0.04972,-0.033554897,0.0216912,-0.010660314,-0.0010655732,0.023050938,-0.026986476,-0.035433467,-0.0059747295,0.0069475565,0.10547336,-0.05352429,-0.0026164555,0.011196021,-0.02199491,0.015399797,-0.022314917,0.041089483,0.081133656,0.03672456,0.04851554,0.006263007,0.04115744,0.0205953,0.061285634,0.054870266,0.047338426,-0.03449587,0.010903873,0.013267077,0.036375422,0.007267456,0.045348734,0.07718482,0.07151136,-0.04513544,0.031605247,-0.025121601,-0.05150122,0.07023433,-0.043841507,0.021913271,0.056501474,0.02298305,-0.011696,0.027131043,0.03476374,0.058594078,-0.023743369,-0.020989075,-0.05385577,-0.023340754,0.03148347,0.01691606,-0.00787822,0.038675405,0.0016805868,0.026729533,0.030245721,-0.013154128,-0.015216658,-0.0061203777,0.03075647,-0.005152749,-0.0061112107,-0.02045594,-0.027000906,0.061014883,0.003847013,-0.038660333,0.05407365,-0.07136698,0.04037531,-0.028421026,0.030348094,-0.02362612,-0.046450343,0.016095938,-0.013261928,-0.031463027,-0.031566005,-0.02901309,0.033089295,-0.0080746,-0.046978462,0.04555528,0.02457596,0.019407302,-0.015376182,-0.02168471,-0.03027745,-0.0024342658,-0.050623506,-0.03806618,0.0032069974,0.041625306,-0.024998456,0.089505345,0.0034762768,0.032573085,-0.052247573,-0.012139528,-0.0011344934,0.0015886979,-0.03059791,0.0003267062,0.01264284,0.049999636,-0.0027325866,-0.016302109,0.017295731,0.054326624,-0.06132761,0.015928492,0.059749622,0.030960338,-0.04784407,0.040509198,0.03895284,0.023331331,0.049188245,-0.009437606,0.010944904,0.009982427,0.025512353,-0.011819786,-0.0197267,0.06499946,-0.026458645,0.0045470265,0.022200406,-0.009056486,0.024531681,0.07458466,0.0063199233,-0.012858912,-0.006020409,0.043566402,0.015372241,-0.017497072,0.020466356,-0.010236612,0.015139494,0.04581822,0.04622749,0.016758,-0.0519156,0.09116061,-0.040520635,0.039221283,0.050111756,-0.012507594,0.041160323,0.019386044,-0.0077477437,-0.012358838,0.032166045,0.05346715,-0.020201389,0.02128637,0.04549045,-0.0010000255,-0.0047809323,-0.00026819765,0.04375834,-0.0097676255,-0.008505547,-0.012295125,-0.07740386,-0.048831146,0.027088482,0.010493876,0.017472532,-0.01173378,0.03606359,0.009507196,-6.5910186e-05,-0.025432158,-0.019478098,0.036954653,-0.016916763,0.011491538,-0.020393917,0.047872983,0.0016476135,0.0057366644,-0.029993745,0.016682608,0.013946455,-0.015253633,-0.08037735,-0.032613557,0.02246141,0.056200333,-0.026856676,0.0019333655,-0.034353588,0.0035164377,-0.027867043,0.058321923,-0.012492437,0.033474457,0.033613075,0.049472395,0.024791604,-0.031473394,-0.011036658,0.049817916,0.015558995,0.018724298,-0.00156563,-0.030632295,0.07767438,0.06639382,-0.037981205,0.011657286,0.01158964,-0.0015739044,-0.01641499,0.06466875,0.026764017,-0.025786657,-0.023881378,0.039237604,-0.023766395,0.0517653,0.017108556,-0.015951047,-0.00213373,0.015191787,0.009463802,0.028256355,0.0022918538,0.068871014,-0.01469835,-0.013957683,-0.032755014,0.0056089456,0.014043281,-0.053097546,0.03560494,-0.06117669,-0.035811648,0.025664072,0.004128211,-0.07557087,0.0016453854,-0.055809606,0.018401163,0.014984672,-0.030727234,-0.037964616,-0.015686532,0.021543378,0.081857905,-0.055228967,0.043505862,-0.0010884883,0.003784064,0.028259821,0.02812399,0.027797716,-0.002098699,0.019240739,0.022170557,0.04299274,0.03779715,0.0116774915,0.008019579,0.05556263,0.023220599,-0.0009303919,-0.045221966,-0.026246509,0.001306279,-0.031661708,-0.02074649,-0.03818719,-0.04906511,0.03813369,-0.021623395,0.039204016,-0.03144695,0.0060065985,-0.043609798,0.062199168,0.053316906,0.009941832,-5.7329307e-06,-0.035847735,0.011061634,0.06362255,-5.3519703e-05,0.010389344,0.029596163,0.03335872,-0.001496595,-0.022549316,-0.013014862,-0.0158423,-0.013521298,-0.032182258,-0.031113574,0.033261433,0.02340629,-0.010941626,0.054771848,0.040246047,-0.024083767,0.0109888995,-0.097361244,-0.034402866,-0.026742324,0.001327737,0.041788008,0.018627957,0.00500781,-0.0010231166,-0.038428187,0.08047927,-0.06366166,-0.0075394395,-0.025750637,0.06641784,0.019848334,-0.049423683,-0.032041926,0.015124244,0.010448626,-0.0076123998,0.071138434,0.026198393,-0.0042748405,0.027004005,0.0020468202,0.036932945,0.014374281,0.013642237,-0.06424161,0.03719436,0.01838522,-0.01959224,0.057804774,0.036423363,0.0007549356,0.01714218,-0.064956345,-0.07851809,0.011350418,0.039292265,0.0032591976,0.018031118,-0.012002761,0.018832838,0.03977277,-0.090394504,-0.04040477,-0.05437034,-0.005463086,0.038601216,0.07211731,0.05212807,-0.040192254,-0.065758534,-0.014471237,0.005051044,-0.06653526,0.030160062,0.021276146,0.030112391,0.04386664,0.05281806,-0.040516265,-0.068405665,-0.04334117,-0.018381022,-0.01239171,-0.03617101,0.063471,-0.002813392,-0.012330986,-0.047822904,0.017176408,0.014167238,0.04468717,0.024117826,-0.01196328,-0.003252786,-0.029260164,0.025190523,0.04737819,-0.024981393,-0.009214645,0.07726784,0.029946072,-0.05592059,-0.059780285,-0.024706569,-0.007569145,0.043948367,0.019507123,0.014585351,0.014095254,0.008100372,-0.0091426745,0.05895225,0.0027618,-0.041326452,-0.028076941,0.024111768,0.03232504,0.052826617,-0.002045146,-0.00234001,-0.09049706,0.04316504,0.0034569008,-0.019255059,0.03823357,0.009866131,-0.026805649,-0.00763003,0.022937397,-0.018315747,0.041109007,0.00462692,-0.007896525,-0.02964126,0.0021158895,0.0037718297,-0.04043583,-0.024942178,-0.012373815,-0.01349116,0.015911516,0.00980972,-0.011037137,0.011406008,-0.0044392766,0.0015359222,0.022413991,0.022203254,0.008788334,0.021745808,-0.04118936,0.028274467,-0.020064047,0.041856013,0.036063652,0.031333867,-0.015755309,-0.036116075,0.013430183,-0.0006911631,0.020693079,0.013698706,-0.008466304,-0.045105934,0.011487434,-0.010892533,0.012413897,0.012297392,-0.019532355,-0.027522385,-0.026465796,-0.0108934,-0.0137827825,0.062393297,0.0007611843,0.026340956,-0.06519216,0.02027047,-0.04384622,0.048943274,-0.011755237,0.024068011,0.053723205,0.010552579]	Image title: Transformer Architecture\nTags: transformer, neural-network, NLP, self-attention, architecture, encoder-decoder\nKey objects: Encoder Stack, Decoder Stack, Multi-Head Attention, Feed-Forward Network, Positional Encoding, Token Embedding, Linear Layer, Softmax Layer\n---\nSummary:\nThis diagram illustrates the encoder-decoder architecture of a Transformer model. It showcases the flow of data through multiple encoder and decoder layers, highlighting the use of self-attention mechanisms and feed-forward networks.\nFull description:\nThe diagram depicts the encoder-decoder structure of a Transformer model. On the left side, 'Inputs' are first transformed by a 'Token Embedding' layer and then combined with 'Positional Encoding'. This combined result is fed into an 'Nx' stack of encoder layers. Each encoder layer includes a 'Multi-Head Attention' sub-layer followed by an 'Add & Norm' layer and a 'Feed-Forward Network'.  On the right side, the diagram shows the decoder. The decoder starts with '(Masked) Multi-Head Attention' which attends to the previous decoder outputs, followed by an 'Add & Norm' layer.  The output of the decoder goes through a 'Multi-Head Attention' mechanism which attends to the encoder outputs, then to a 'Feed-Forward Network', a 'Linear' layer and finally a 'Softmax' layer to produce 'Output Probabilities'.\nText found in image:\n- Inputs\n- Token Embedding\n- Positional Encoding\n- Multi-Head Attention\n- Add & Norm\n- Feed-Forward Network\n- Outputs\n- Linear\n- Softmax\n- Output Probabilities\n- (Masked) Multi-Head Attention\n- Nx	{"tags": ["transformer", "neural-network", "NLP", "self-attention", "architecture", "encoder-decoder"], "title": "Transformer Architecture", "doc_id": "7a97f798-8aa4-452a-8aaa-862c64853369", "source": "./images/a-survey-to-transformers/image_1.png", "summary": "This diagram illustrates the encoder-decoder architecture of a Transformer model. It showcases the flow of data through multiple encoder and decoder layers, highlighting the use of self-attention mechanisms and feed-forward networks.", "doc_type": "image", "key_objects": ["Encoder Stack", "Decoder Stack", "Multi-Head Attention", "Feed-Forward Network", "Positional Encoding", "Token Embedding", "Linear Layer", "Softmax Layer"], "parent_doc_id": "94a2f5d5-d379-4ab5-aa00-9d89bda364e3", "text_in_image": ["Inputs", "Token Embedding", "Positional Encoding", "Multi-Head Attention", "Add & Norm", "Feed-Forward Network", "Outputs", "Linear", "Softmax", "Output Probabilities", "(Masked) Multi-Head Attention", "Nx"], "contextual_description": "The diagram depicts the encoder-decoder structure of a Transformer model. On the left side, 'Inputs' are first transformed by a 'Token Embedding' layer and then combined with 'Positional Encoding'. This combined result is fed into an 'Nx' stack of encoder layers. Each encoder layer includes a 'Multi-Head Attention' sub-layer followed by an 'Add & Norm' layer and a 'Feed-Forward Network'.  On the right side, the diagram shows the decoder. The decoder starts with '(Masked) Multi-Head Attention' which attends to the previous decoder outputs, followed by an 'Add & Norm' layer.  The output of the decoder goes through a 'Multi-Head Attention' mechanism which attends to the encoder outputs, then to a 'Feed-Forward Network', a 'Linear' layer and finally a 'Softmax' layer to produce 'Output Probabilities'."}
647680ad-93e2-4763-95a6-fbdaed1ece29	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.014290243,0.01850962,0.029762888,0.05001058,-0.008108317,0.07463851,-0.022303917,0.06946391,0.020574566,-0.013710591,-0.012311952,-0.0034841388,0.048032712,0.018896205,-0.036509585,0.07384332,0.032872654,0.05104195,0.019108234,-0.014481447,0.042647056,0.03662852,0.028364489,-0.01732783,-0.0013422245,0.023979427,0.024475237,0.0033450471,-0.014435123,0.06620508,0.03712984,-0.037805226,0.021294618,-0.021048777,0.00762319,-0.04774401,-0.02216629,-0.026408516,0.008274386,-0.007059472,-0.0066565783,0.07341026,-0.08917835,0.0042622066,0.0063561266,-0.05270294,-0.07226589,-0.031311616,-0.0071584904,-0.03023232,-0.018150734,0.00024995286,-0.040323418,-0.009596067,-0.034492273,-0.0087091625,-0.025794687,-0.084167086,0.01691938,-0.09933006,0.012085636,-0.006001145,-0.027429152,0.020968556,0.012782591,-0.0029728534,0.020329898,0.039549626,0.053502314,0.13013773,0.0014988775,-0.014991067,0.020319592,-0.03525865,0.09525498,0.052570526,-0.05167535,0.016618162,-0.048610974,-0.054850172,-0.011643484,0.11450316,0.015065279,-0.016594658,0.06879755,-0.013302099,-0.027862333,0.0054122885,0.074410014,-0.018112643,-0.008553912,0.06890623,0.013070521,0.053834848,0.015077141,-0.077515334,-0.01799075,-0.015752096,0.011781249,-0.030459773,0.0008346266,0.0018603526,0.099563986,0.10318746,0.019773383,-0.0046397713,-0.017558306,-0.027264955,-0.0102289915,0.029295482,-0.051078197,0.020422276,-0.06964492,0.029239045,-0.014242348,0.017678456,0.004046192,-0.013759205,0.017358597,-0.029406143,0.005325921,-0.016689792,-0.025030034,0.03518412,0.072763614,0.03523832,0.029757574,-0.016480524,-0.009323176,0.005577803,0.056428287,0.012989152,-0.05480176,0.016334152,-0.007056183,0.030871192,0.05780042,-0.022914177,-0.008260719,0.043787483,0.0036915035,-0.049101315,-0.033389024,0.020743698,-0.059789184,0.043110825,-0.06715784,0.045426123,-0.041701328,0.05635489,0.02866789,-0.018907905,0.053249966,0.0016704582,-0.02771888,-0.0023796319,0.02849762,-0.022583669,-0.0057300106,-0.029179404,-0.060154516,-0.044586327,-0.042677462,0.04332165,0.007244515,0.100336865,-0.022730304,0.030576147,0.025275072,0.027028473,-0.047929887,-0.007549373,-0.01955138,-0.018408468,-0.0035419124,0.023035863,0.011110586,-0.025291227,0.00036655468,-0.013293111,0.014668964,0.03424221,0.012586209,0.04688869,-0.040108696,0.032994166,0.03978594,0.03033238,0.019605406,-0.02016481,0.0018868914,0.00946252,-0.022085438,0.011256054,0.031931963,0.044717595,0.03715731,-0.06918831,-0.082028314,-0.031006647,-0.036323823,0.0050931107,-0.0056407973,-0.050102346,0.06367139,-0.017235141,0.019653192,-0.015036551,0.00563884,0.048233323,0.029572712,-0.058751814,-0.021756098,0.03539038,0.049177982,-0.035433937,0.0185006,-0.0034804444,-0.008933627,-0.024045553,-0.028780436,0.004983101,-0.029435394,0.045366235,-0.011751291,0.038489535,-0.04918156,0.030304985,-0.006134578,-0.022947198,0.012257954,0.020622872,-0.049551513,0.025658464,-0.009444337,0.008097181,-0.0027666502,0.022069685,-0.03164343,0.024807852,0.057279058,0.03662515,-0.02492321,0.080191925,0.024452638,0.018727718,0.0016327045,0.06631605,0.017298238,0.0214409,-0.03422132,0.04931587,0.05695035,-0.0050310064,0.007106248,-0.053839527,-0.033590853,-0.0042224214,0.007819872,0.027021602,0.004311494,-0.008126787,0.009106843,0.007077954,-0.0122659085,0.018849779,0.025189074,0.05658039,-0.058921125,-0.047753263,0.010292314,0.048403013,0.03696958,-0.025218941,-0.031838316,0.01211128,0.003886771,-0.024740338,0.0053600585,-0.023480374,0.0033847238,0.0052465787,0.006940235,-0.014611695,0.004357025,-0.0078411205,0.0091413045,-0.015953429,-0.018114768,-0.001018547,-0.030199746,-0.047219437,-0.008706827,0.0042794803,-0.018920835,-0.024456726,0.0022591236,-0.032087475,-0.047394406,0.002019448,0.13525192,-0.030517653,0.00034620758,-0.005463203,0.019405622,-0.0066750273,-0.05294229,0.063860245,0.07888037,-0.0049746456,0.04680542,-0.01943774,0.0024620106,-0.02343744,0.069302894,0.07349717,0.06321816,-0.042447437,-0.036117,0.003805472,0.013063126,0.033673458,0.014528918,0.041025057,0.044635788,-0.010829842,-0.0049229097,0.003587101,-0.030652761,0.040740333,-0.068309665,0.050211463,0.06307825,0.04262273,0.001292373,-0.013258835,-0.024548266,0.017543815,-0.018836273,0.032598846,-0.011013096,-0.053502213,0.010740409,-0.012263929,-0.005653262,0.038993616,0.0015704292,0.006290995,0.030738894,0.024887785,0.017508453,0.034824505,0.012002613,-0.017072048,0.007884151,-0.018656734,-0.008430095,0.037491016,0.0043208627,-0.014614573,0.09546128,-0.05052565,0.038811944,-0.054598358,-0.004467018,-0.032314435,-0.06181339,0.0069275373,0.069238134,-0.07771775,-0.027352424,-0.032328423,0.05062814,-0.009211299,-0.018314416,0.050956756,0.015231276,0.011508009,-0.008889651,0.010991082,-0.003031047,0.006915701,-0.011516259,-0.048064105,0.025489494,-0.0068433057,0.0037453738,0.10382373,0.03756351,0.05306874,-0.038302712,-0.016767966,-0.0022826747,-0.02660893,-0.036296923,-0.02702657,0.007717603,0.04739948,0.020893604,-0.0083598215,0.01360391,0.042622995,-0.05040747,0.006604047,0.0358148,0.021505838,-0.077514954,0.048586044,0.02811172,-0.011401181,0.011777708,-0.034422453,-0.0061795614,0.0025061877,-0.010038815,-0.007980911,-0.0025183852,0.035892863,-0.047485694,-0.0121606765,0.0112829255,0.0088707,0.00020887666,0.04328836,0.013188232,0.0070224362,-0.048320763,0.015042823,-0.0026628857,-0.03211401,-0.0012875603,-0.05404483,-0.007882157,0.07376382,0.00020028176,0.016214693,-0.064610966,0.068128206,-0.042098127,-0.005048927,0.05970118,-0.028722363,0.0009244772,-0.010542964,0.01708397,-0.021219885,-0.0052554556,0.03483824,-0.07125248,-0.010871327,0.044444676,-0.018674908,0.036328707,0.016130164,0.024791928,0.0042786347,-0.017543716,-0.031272937,-0.03361898,-0.024494948,0.0029835387,0.024013178,0.041927002,-0.02699437,0.017726954,-0.0071587535,-0.008221458,-0.023985533,-0.03255179,-0.013804677,0.007207071,0.01316188,-0.04925687,0.035727326,0.049317036,0.012380118,0.0064273104,0.010095826,0.0055829487,-0.0044789696,-0.043606613,-0.010951301,0.034730542,0.035381306,-0.03208691,0.013083848,-0.0063808,-0.043179028,-0.01221149,0.024354653,0.015437617,0.05017422,0.03230257,-0.0074615926,0.030378306,-0.042615112,0.007655865,-0.0044026524,0.016968742,-0.028367503,-0.0318218,-0.03713443,0.05740509,0.047160316,-0.016430354,0.0016721106,0.009412282,0.014238965,-0.00821172,0.07446355,0.032921154,-0.014021678,0.0029654119,0.007500659,-0.040172994,0.054304183,-0.013960481,-0.01065423,0.014483047,0.018625623,-0.009486414,0.008141273,0.009252082,0.049441367,0.031362955,9.151871e-05,0.03041127,-0.0044328016,0.023327904,-0.041076597,0.034183633,-0.017906494,-0.057375573,0.0040568076,-0.026031164,-0.06246222,-0.005960252,-0.06669312,0.031082964,0.0040250826,0.0085436115,-0.04293016,-0.034380015,-0.03355322,0.08764507,-0.05969045,0.0698691,-0.0068399864,-0.018252041,0.05587413,0.00025372574,0.017163355,-0.008800951,4.4869237e-05,0.016496215,0.025083154,0.0690524,0.026956076,0.021493973,0.057783656,-0.0106409,0.0027451885,-0.025889123,-0.03654333,-0.01829504,-0.0072195185,-0.030307615,-0.014901463,-0.03487812,0.0068289707,-0.0019510586,-0.025041018,-0.051031046,0.033313096,-0.026794398,0.054159854,0.045254894,0.0034328408,0.02330475,-0.013951596,0.061952457,0.042445604,0.015628045,0.025418235,-0.01753665,0.008982132,-0.03726876,0.009452086,0.01700707,0.014025046,-0.01710618,-0.04029868,0.020402797,0.025500404,0.005763553,-0.031164395,0.04363729,0.028372869,-0.00063692644,-0.029970054,-0.034662195,-0.026310215,-0.02980797,-0.0005022892,0.013569247,0.000970313,-0.050784275,0.039984357,0.006647644,0.072596766,-0.06102658,-0.026913635,-0.06348579,0.0435683,-0.004422997,-0.033589445,-0.017092492,-0.011722129,0.02617409,0.008048068,0.025174469,0.027024008,-0.013655658,-0.026322251,-0.0070818556,-0.016007908,0.039423354,0.038925502,-0.011206484,-0.0036349215,0.009225929,-0.03419526,0.07653356,-0.06903506,-0.003981886,0.016260434,-0.020396292,-0.09224457,0.0035382467,0.049796,0.060979165,0.008937601,-0.01566285,-0.039696693,0.034078438,-0.076015286,-0.01013517,-0.049008105,-0.041335274,-0.010803715,0.03839135,0.026352296,-0.0010816109,-0.062483728,-0.022129875,0.039546415,-0.07322329,0.037379544,0.036640044,0.063411996,0.07039017,0.038791627,-0.023159131,-0.05373429,-0.009710949,-0.038104538,0.00026527245,-0.079411454,0.07141863,0.0022959022,0.009496007,-0.012845325,0.014148633,0.006173638,-0.00041279657,0.0270319,-0.03652993,-0.020332433,-0.026356611,0.03863886,0.022537706,0.042995088,-0.005274403,0.09599702,0.012691848,-0.037057977,-0.02513071,-0.025991865,-0.025619304,0.037264433,0.016504241,-0.0037825438,-0.0016000378,0.030218886,-0.01553674,0.062168334,0.006384563,-0.06925746,-0.035879232,-0.0014301656,0.02616829,0.018740674,-0.033793293,-0.014593774,-0.0908986,0.034531526,-0.020739298,-0.07781515,0.05809898,-0.0010390495,-0.03529889,-0.030507157,0.004295155,-0.0059086964,0.03626929,0.022455726,0.010648371,0.026783917,-0.016813071,0.0058286428,-0.08766649,0.017056972,-0.039524928,-0.019894691,-0.013812928,0.03804516,-0.01695494,-0.013671968,-0.030792508,0.012812675,-0.007850054,0.010823829,0.004430908,0.049978748,-0.032453638,0.01694453,-0.0140945725,0.06146243,0.08538965,-0.015731433,-0.043500792,0.0031614886,0.01182613,0.015335971,0.0054521253,-0.025582122,-0.05830614,-0.042759422,0.024001516,0.024096997,-0.017943801,0.011815689,0.0028295687,-0.051931012,-0.042054463,-0.04574679,0.021856183,0.017944174,0.02634784,0.042377964,-0.061171092,0.029587353,-0.010430731,0.07875748,-0.012066805,0.014114681,-0.033244636,0.018070433]	Keywords: cross-attention, position-wise FFN, encoder, decoder\nKey Objects: Cross-attention, Position-wise FFN, Queries, Keys, Values\nRefers to Images: None\nHypothetical Questions:\n- How does cross-attention facilitate interaction between the encoder and decoder?\n- What is the purpose of applying the position-wise FFN to each position independently?\n- Why is the intermediate dimension (D_f) of the FFN typically larger than the model dimension (D_m)?\n---\nSummary:\nIn the Transformer architecture, cross-attention utilizes queries from the previous decoder layer and keys/values from the encoder layer, while the position-wise feed-forward network (FFN) is a fully connected module applied identically to each position.\nOriginal Text:\n-  Cross-attention . The queries are projected from the outputs of the previous (decoder) layer, whereas the keys and values are projected using the outputs of the encoder.\n- 2.1.2 Position-wise FFN . The position-wise FFN 3 is a fully connected feed-forward module that operates separately and identically on each position  \n$$F F N ( H ^ { \\prime } ) = R e L U ( H ^ { \\prime } W ^ { 1 } + b ^ { 1 } ) W ^ { 2 } + b ^ { 2 },$$  \n$^{2}$This term seems to be borrowed from the causal system , where the output depends on past and current inputs but not future inputs.  \n$^{3}$The parameters are shared across different positions, thus the position-wise FFN can also be understood as two convolution layers with kernel size of 1.  \nwhere H ' is the outputs of previous layer, and W 1 $\\_{R}$ D$\\_{m}$  D$\\_{f}$ , W 2 $\\_{R}$ D$\\_{f}$  D$\\_{m}$ , b 1 $\\_{R}$ D$\\_{f}$ , b 2 $\\_{R}$ D$\\_{m}$ are trainable parameters. Typically the intermediate dimension D$\\_{f}$ of the FFN is set to be larger than D$\\_{m}$ .\nContextualized Text:\nWithin the Transformer model, a crucial component is cross-attention, which distinguishes itself by sourcing queries from the outputs of the preceding decoder layer and utilizing keys and values projected from the encoder layer. Alongside this, the position-wise FFN is introduced as a fully connected feed-forward module applied identically to each position to process information.	{"tags": ["architecture", "attention", "FFN", "transformer"], "doc_id": "647680ad-93e2-4763-95a6-fbdaed1ece29", "summary": "In the Transformer architecture, cross-attention utilizes queries from the previous decoder layer and keys/values from the encoder layer, while the position-wise feed-forward network (FFN) is a fully connected module applied identically to each position.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["cross-attention", "position-wise FFN", "encoder", "decoder"], "key_objects": ["Cross-attention", "Position-wise FFN", "Queries", "Keys", "Values"], "contextual_text": "Within the Transformer model, a crucial component is cross-attention, which distinguishes itself by sourcing queries from the outputs of the preceding decoder layer and utilizing keys and values projected from the encoder layer. Alongside this, the position-wise FFN is introduced as a fully connected feed-forward module applied identically to each position to process information.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.1 Vanilla Transformer"}, "hypothetical_questions": ["How does cross-attention facilitate interaction between the encoder and decoder?", "What is the purpose of applying the position-wise FFN to each position independently?", "Why is the intermediate dimension (D_f) of the FFN typically larger than the model dimension (D_m)?"]}
3f6e312c-fe14-4573-800b-49a22d6e2cf6	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.019228859,0.02516958,0.05372385,0.03368705,-5.107657e-05,0.028095772,0.0036357965,0.02263898,0.04498904,-0.032275848,-0.01148323,0.009376735,0.013337753,0.012061744,-0.041788545,0.051109016,0.043512892,0.09576238,0.00886684,-0.04153194,0.054389615,0.0028937818,0.036736958,-0.037904937,0.019650225,0.04866121,0.043108124,0.022437926,-0.0025408918,0.016000945,0.0033631818,-0.02228673,0.020381708,0.0078012035,0.0004944941,-0.014226816,0.02113643,-0.034240812,8.9863104e-05,-0.02554738,0.00024244013,0.05847606,-0.07300651,-0.027771058,0.034343846,-0.052044526,-0.10615943,-0.01975673,0.0019031261,-0.029958745,0.0056652166,0.0049766186,-0.021899773,-0.027640294,-0.029672137,-0.015581455,-0.0019865267,-0.025511269,-0.004388806,-0.05156953,-0.03854772,0.028215213,-0.03199854,0.020932095,0.006713393,-0.018924361,0.047508873,0.036085002,0.043251473,0.11435683,-0.015827302,0.038272463,0.01589614,-0.04418484,0.10201632,0.033844367,-0.03662362,-0.031436272,-0.061701342,-0.04387191,-0.00019697691,0.07574018,-0.00759769,-0.03414599,0.06360596,-0.0014051037,-0.030572467,-0.026502933,0.06463664,-0.025881043,-0.03136421,-0.012964944,0.0497168,0.07342292,0.016842833,-0.04123229,-0.0031651554,-0.033181008,-0.00039625858,-0.03941514,0.040112536,0.025856558,0.07953585,0.09296018,0.046283204,-0.033379328,-0.035494205,0.0040359725,0.024511522,0.05472466,-0.047927897,0.007949822,-0.06979078,0.034791104,-0.06738562,0.020951116,-0.022116724,0.023523824,0.0049444376,0.0021111432,0.016545992,0.0033141784,0.0038391892,0.035359923,0.05363083,0.04896919,0.017199893,-0.03766166,-0.03466729,0.0034652567,0.03244766,0.0017995163,-0.040918108,0.023168834,-0.017296704,0.0019193611,0.07725452,0.015623115,-0.00080650434,0.013379981,0.015248222,-0.06082587,-0.03161591,0.015555027,-0.059474673,0.0085889185,-0.025165407,0.042929336,-0.05262006,0.033821784,0.025645286,-0.03228979,0.049834568,0.03705543,-0.024129063,-0.010601112,-0.03399566,-0.014891637,-0.0025402163,-0.017790793,-0.05535788,-0.031032719,0.0062687458,0.09042425,0.043322958,0.10007989,-0.0020543362,-0.0021477425,0.05102565,0.011515399,-0.06862123,0.0050498415,-0.03555201,-0.0043415613,0.045079224,0.0059710173,0.00541102,-0.02861314,0.0039549475,0.036871605,-0.011378071,0.03826511,-0.014008556,0.009626952,-0.042608883,0.0016942956,0.011200907,0.10116213,-0.035547007,-0.028068934,0.02010623,0.0017275233,-0.016388971,0.0016155347,0.029652819,0.025851829,0.013973748,-0.04273143,-0.03575975,-0.010514876,0.007069265,-0.02315663,0.013727728,-0.03485728,0.052909646,-0.021830415,-0.00962227,0.020250373,-0.02927144,0.046543434,-0.009459688,0.0007561698,-0.033726774,0.0368585,0.031918284,-0.032321334,-0.016030954,0.025652444,0.024487862,-0.040686138,-0.044371765,-0.015285068,-0.021905512,0.05236915,-0.05721357,0.019118542,-0.050560236,0.0034042478,-0.009780301,-0.002855628,0.038536288,0.030057574,-0.07697146,0.030608963,-0.009571721,0.00477518,0.0155119905,-0.030905923,0.00039385463,0.0072056265,0.08100613,0.05769079,-0.0365789,0.071671516,0.0053617307,-0.024785094,0.022654716,0.103808925,0.015143186,0.027450336,-0.032504354,0.0076003186,0.06955633,-0.012043882,-0.017175173,-0.022481529,0.029978493,0.01407844,0.03792362,-0.013567942,-0.023398554,-0.033488266,0.008955069,-0.0053093676,-0.018569492,-0.0015038784,-0.02319266,0.026658004,-0.05947666,-0.046126578,0.010510908,0.050132066,0.037513584,-0.017298738,-0.028993731,0.014628467,0.0143442005,-0.0051667173,0.019278165,0.027858306,-0.021085214,0.018998863,-0.0005588784,-0.013938637,0.009080593,0.00012405105,-0.05169611,-0.03482519,-0.016302202,-0.008621177,-0.0020210857,-0.061167777,0.0033996413,0.031501852,-0.05003932,-0.0015023671,0.007876172,0.0300632,-0.008985829,0.02948193,0.10352968,-0.04090486,0.01640227,0.013217132,0.018481215,0.038065404,-0.05794268,0.031229509,0.03846161,0.016361162,0.019315701,-0.036694866,-0.018725174,-0.00042829054,0.06256252,0.1034542,0.08753365,-0.05172351,0.01800528,0.017617723,0.012123493,0.037473306,0.0034886077,0.055602115,0.015443115,-0.021766966,-0.017062163,0.012338475,-0.0768338,0.0687678,-0.051015425,0.06246235,0.042986024,-0.009374702,0.011489204,0.0064600776,0.040359516,0.046947442,-0.0033857175,-0.0102453455,-0.021166561,-0.0612059,-0.025383562,0.0063318177,-0.0182288,-0.0058377967,0.009175628,-0.00012196897,0.05536359,0.02621931,0.02087348,0.0076168682,0.016736036,-0.008649375,0.01127506,-0.039035793,0.0060455035,0.036440607,0.038983114,-0.03316756,0.08449063,-0.048685662,0.05276884,-0.014498155,-0.015672814,0.0020797367,-0.04431643,0.02508065,0.019767342,-0.051198106,-0.020309284,-0.050931294,0.051456537,-0.025310196,-0.043568637,0.04444972,0.04743911,0.023470234,-0.009946605,-0.029684495,-0.012280154,-0.0034374737,-0.036843307,-0.01934043,0.02253071,0.004028316,0.022084361,0.097491354,-0.022426408,0.04352538,-0.046999652,-0.0043474585,0.008123177,0.00020328758,-0.0065315184,0.012739599,0.008740452,0.045146793,-0.0030384907,-0.01172028,0.0511013,0.045970995,-0.050012577,0.014863751,0.05273502,0.027311344,-0.055119537,0.02370824,0.032873556,0.03596388,0.04531036,-0.04767695,-0.022432243,0.024258606,-0.0007091338,-0.02083848,0.008118025,0.008532275,-0.034185942,-0.024403812,0.014923418,-0.012731625,-0.0027416698,0.074902594,0.031763833,-0.0176639,-0.06655609,0.020622153,-0.008012497,-0.047938924,-0.004833979,-0.009903363,-0.03580864,0.05411348,-0.00064504857,0.025344647,-0.07276008,0.11521246,-0.06596156,0.011081593,0.04319978,-0.024743509,0.0657296,0.027562154,0.020351939,-0.033457406,0.039771143,-0.031340457,0.016449418,0.027524162,-0.018299665,-0.016088614,-0.009071156,-0.009901221,0.015840491,-0.000808018,-0.0014379238,-0.006565345,-0.015579449,-0.0028330933,-0.008966119,-0.022038551,-0.011215252,-0.012325295,0.01340275,-0.014375564,0.004429764,-0.01300061,-0.029505027,-0.0058419984,0.00789765,-0.036184754,-0.0142290015,0.0696071,0.024721852,0.01663757,0.010743328,0.008258161,0.015416404,-0.024821404,-0.07460359,-0.017935516,0.038031634,0.037756752,-0.025355777,0.031874113,-0.00436665,0.00900717,-0.053563792,0.030256579,0.016463857,0.042704456,0.031400472,-0.020136522,0.03887241,-0.036180165,-0.003214456,0.005771891,-0.0008787061,-0.024375292,-0.0061705047,-0.024292067,0.10452378,0.030520583,-0.01190215,0.036971774,0.032600593,0.0063323313,-0.03186274,0.06677214,0.027878959,-0.03585927,-0.014999675,-0.004302691,-0.020134553,0.06293071,0.00543069,0.011084777,0.036341723,0.02670081,-0.057696894,0.013186445,0.0040047993,0.025657386,-0.01535544,0.012243503,0.04578805,0.063516594,0.008854769,-0.07862885,0.014706121,-0.045103088,-0.006508315,0.0121531375,-0.06135388,-0.030280508,0.019691076,-0.038834814,-8.6461274e-05,0.023010641,0.001469866,-0.032072064,0.00011391944,-0.020809485,0.05075234,-0.06634456,0.033926677,0.00933425,-0.0073347255,0.07911068,-0.017957611,-0.005287793,-0.009303868,0.028950235,0.020873146,0.01569464,0.07223639,-0.011348315,-0.026378194,0.03886323,-0.024800515,-0.023921695,-0.03350897,-0.040240373,-0.036072355,-0.044620134,-0.023935564,-0.03218388,-0.047449064,0.025925692,-0.037844934,-0.0038986388,-0.017557966,0.052171316,-0.033435997,0.042293258,0.028148996,0.020527417,0.021356711,-0.029232439,0.04960984,0.0030580733,0.046850458,9.013291e-05,0.0068796896,-0.0020236608,0.0068073343,-0.0068897167,0.05423609,-0.013397405,-0.024946751,-0.04579229,0.020468965,0.0014287306,-0.014636727,0.01724728,0.0296362,0.041904777,0.014356866,-0.046547513,-0.054614693,-0.036914393,-0.051252984,-0.02579059,-0.0026099512,0.060195122,0.010736579,0.048273064,-0.06864868,0.061715484,-0.1201348,0.009326471,0.0037536812,-0.0045961,-0.026291845,-0.025125334,-0.025346246,-0.023354013,-0.019854141,-0.013261429,0.0005708069,0.027770733,-0.018368572,-0.005396377,-0.03920189,0.0049264017,0.030744866,-0.007878822,-0.030591903,0.0077120624,0.032573044,-0.030646482,0.07056634,-0.012947253,-0.018618304,0.015105422,-0.019079223,-0.07504479,-0.046962418,0.058414552,0.024153033,-0.0036155577,0.022963652,0.019873526,0.04753419,-0.04650912,-0.023491783,-0.056314863,-0.040670004,0.009756112,-0.0026859094,0.01820962,0.032864682,-0.07025733,-0.03621136,0.0040842085,-0.004112553,0.03604745,0.02337382,0.049825877,0.037358943,0.061701506,-0.04547303,-0.046336997,-0.050475765,-0.048164297,0.037177533,0.007980404,0.060464922,0.014300321,0.002426156,-0.016086541,-0.0056674383,-0.016110744,-0.00472693,0.0035913505,-0.082120165,0.006708259,-0.005012544,0.015451834,0.027254913,-0.00024810803,-0.039913185,0.051195815,0.009441945,-0.058989722,-0.02933552,-0.024527844,-0.03315087,0.011299214,0.060754523,-0.012295907,-0.019597584,0.02186891,0.012623413,0.0509295,0.007227237,-0.068677545,-0.02997561,0.027550023,0.021228049,0.022259781,0.0012031066,-0.027959213,-0.07926991,0.019746386,-0.01901271,-0.05293621,0.015868776,0.032807205,-0.006421991,-0.020268675,-0.004655647,-0.029319461,0.005141123,0.038012743,0.022975158,0.021846741,0.00086273503,0.0040536756,-0.03335256,-0.045033738,-0.03526757,0.016009765,0.028091595,0.050366983,-0.00956479,0.032068647,-0.028954906,0.007657545,-0.0059110452,0.0005857841,-0.0060381847,0.062423777,-0.007617579,0.056936495,-0.030647537,-0.0003357254,0.08583533,0.026186213,-0.027862865,-0.0018652879,0.035499603,0.00936961,0.0016139689,-0.014345706,-0.052265186,-0.0033580516,-0.0042811455,0.03686306,0.027721137,0.037719633,0.0010937451,-0.058161575,-0.054225516,-0.05898819,-0.019761853,0.029091371,0.05788157,0.04158191,-0.05521335,0.040588748,-0.032202058,0.071451075,0.0051199826,-0.013753854,0.05656933,0.0053123604]	Keywords: residual connection, layer normalization, Transformer, encoder block\nKey Objects: residual connection, layer normalization, encoder block, H', H\nRefers to Images: None\nHypothetical Questions:\n- Why is a residual connection necessary for training deep Transformer models?\n- What is the purpose of Layer Normalization in the Transformer architecture?\n- How do the equations for H' and H demonstrate the application of residual connections and Layer Normalization?\n---\nSummary:\nTo facilitate the training of deeper Transformer models, a residual connection is used around each module, followed by Layer Normalization, as exemplified by the equations for calculating H' and H within an encoder block.\nOriginal Text:\n2.1.3 Residual Connection and Normalization. In order to build a deep model, Transformer employs a residual connection [49] around each module, followed by Layer Normalization [4]. For instance, each Transformer encoder block may be written as  \n$$H ^ { \\prime } = \\, \\, L a y e r N o r m ( S e l f A t t e n t i o n ( X ) + X )$$  \n$$H = \\, \\, L a y e r N o r m ( F F N ( H ) ^ { \\prime } + H ^ { \\prime } ),$$  \nwhere SelfAttention() denotes self attention module and LayerNorm() denotes the layer normalization operation.  \n- 2.1.4 Position Encodings. Since Transformer doesn't introduce recurrence or convolution, it is ignorant of positional information (especially for the encoder). Thus additional positional representation (Detailed discussion in Sec. 5.1) is needed to model the ordering of tokens.\nContextualized Text:\nIn the vanilla Transformer architecture, deep models are built by employing a residual connection around each module, followed by Layer Normalization. This process, illustrated by equations for calculating H' and H, helps to train deeper networks effectively.	{"tags": ["architecture", "deep-learning", "normalization"], "doc_id": "3f6e312c-fe14-4573-800b-49a22d6e2cf6", "summary": "To facilitate the training of deeper Transformer models, a residual connection is used around each module, followed by Layer Normalization, as exemplified by the equations for calculating H' and H within an encoder block.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["residual connection", "layer normalization", "Transformer", "encoder block"], "key_objects": ["residual connection", "layer normalization", "encoder block", "H'", "H"], "contextual_text": "In the vanilla Transformer architecture, deep models are built by employing a residual connection around each module, followed by Layer Normalization. This process, illustrated by equations for calculating H' and H, helps to train deeper networks effectively.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.1 Vanilla Transformer"}, "hypothetical_questions": ["Why is a residual connection necessary for training deep Transformer models?", "What is the purpose of Layer Normalization in the Transformer architecture?", "How do the equations for H' and H demonstrate the application of residual connections and Layer Normalization?"]}
02e43c44-3cfb-4ac3-afdd-fbc4a8d146a0	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017597344,-0.028839087,0.028509,0.046450704,-0.015673235,0.09814811,-0.013451876,0.056759764,0.037186753,-0.014733171,-0.010349224,0.0077182925,0.020536482,-0.029299885,-0.022251531,0.0415522,0.043699145,0.09281882,0.007656944,-0.016314572,0.041718975,0.0042783003,0.04062269,-0.0010569007,0.0073108557,0.0235945,0.031296246,-0.013581164,-0.00025638,0.007412675,-0.02365333,-0.0275445,0.017831774,0.01761184,-0.012437305,0.033557825,0.0037485743,-0.03223631,-0.044523913,0.010514026,-0.029274553,0.046417776,-0.057943106,0.003187216,-0.021168804,-0.04445577,-0.12783106,-0.025893088,0.009864902,0.0026593865,-0.027297478,0.03356132,-0.02926188,0.023741186,0.0029428843,-0.0146222925,-0.02118,-0.051134534,-0.01329528,-0.06431839,0.0035055503,0.015791547,-0.028790029,0.04298088,-0.006801892,-0.012385491,-0.03356744,0.03191742,-0.0016455812,0.11165187,-0.021564394,-0.0038655621,0.012172355,-0.044004895,0.12188608,0.06467491,-0.0078507755,-0.00683646,-0.054779947,-0.027492594,0.008306583,0.06267791,-0.0315131,-0.053605646,0.090942636,0.0020756128,-0.044910863,-0.0038074134,0.098646075,-0.049827527,0.01402332,0.034070745,0.0012662492,0.06999461,-0.036165074,-0.095208734,-0.013104133,-0.054097902,0.0127531495,-0.029526716,0.049088202,-0.03596837,0.06877833,0.061600972,0.04183945,-0.014468592,-0.020658083,-0.019377416,0.018110039,0.03819694,-0.03151638,0.014365607,-0.009798744,0.0028600493,0.0020508463,0.010872983,-0.040304184,0.027968064,0.035207417,-0.007257014,0.020581305,-0.0032993702,0.018381402,0.025346447,0.062464185,0.058348592,0.016472943,-0.0044933325,-0.023071554,0.0020584667,0.03691593,-0.0042942264,-0.02846028,0.019208966,-0.008820075,0.015219468,0.05354739,0.008138778,0.0062797186,-0.01665579,0.027285606,-0.07476993,-0.014461663,0.028341442,-0.01657352,0.028926441,-0.057831638,0.025853131,-0.043029744,0.07257158,0.018349336,0.0032121136,-0.008389325,0.026551759,-0.026742721,0.011537797,-0.007804828,-0.03418413,-0.017551294,-0.007069671,-0.076653674,-0.04223422,-0.03501643,0.049266294,0.01939794,0.053408794,-0.022447804,0.022432394,0.029045919,-0.020990314,-0.0642611,0.013632673,-0.029611733,-0.033079874,0.023625188,0.05044404,0.010109176,0.011972461,-0.0050121564,-0.05271417,0.008315206,-0.007911788,0.0072743953,0.03387005,-0.023581766,0.046471775,0.009293412,0.052145876,-0.03710578,-0.02203793,0.056702722,-0.02273344,-0.006274312,-0.017693035,0.068193875,0.028871486,0.041349024,-0.0390596,-0.051649794,-0.0012923364,-0.015189599,-0.017772809,7.995444e-05,-0.029932037,0.032532018,-0.010167915,0.01267922,-0.017183235,-0.058647648,0.03363171,-0.0047171144,-0.05271115,-0.05900864,0.01915282,-0.005874072,-0.0022381414,-0.058472622,0.031345967,0.011817162,-0.014759177,-0.05454397,-0.021943405,-0.033990327,-0.0016473683,-0.04478326,0.016381582,-0.056836527,0.018451922,0.015732987,-0.036566783,0.025825128,0.008881935,-0.022609526,0.017240783,0.024427503,0.03462676,0.016125211,0.035275117,0.0036025306,0.04238713,0.035062734,0.02487179,-0.0449921,0.07301483,0.051280133,-0.021678854,0.0072232494,0.05656158,0.018915767,0.05019406,-0.0045614894,0.05281108,0.06764498,-0.008800688,-0.0056519047,-0.037233096,-0.011292592,0.021541582,-0.0006627657,-0.017196838,-0.016791092,-0.024448426,-0.016128995,0.028553255,0.0020453236,0.009800954,0.035734978,0.02950621,0.013819524,-0.011988703,-0.011929038,0.028623842,-0.0015590937,-0.018296411,0.0047815987,-0.030674037,-0.005785365,-0.00023711624,0.056574196,-0.04942991,-0.010428901,-0.017612714,0.017604638,0.0062840795,-0.008907573,0.007449829,0.0051637036,-0.06111818,-0.0836057,-0.008725216,0.009558728,-0.02628487,0.016965888,-0.0023114418,-0.020052152,0.010546751,-0.015825478,-0.039154883,0.00066587504,-0.0050782645,0.10808467,-0.014227093,-0.02918158,-0.026655156,-0.014402701,0.01642776,-0.06767957,0.032249607,0.029899482,-0.007642652,0.040416572,-0.021517908,0.003869685,0.040010814,0.07383015,0.07690947,0.051546764,-0.020438675,-0.04721453,0.009886535,0.0050336528,-0.0042398414,-0.008194101,0.044658694,0.024371374,-0.046538677,0.039607234,0.02409475,-0.08515425,0.022625333,-0.07050892,0.042760286,0.054619506,0.025394581,-0.021999985,0.044401605,0.02043591,0.045398444,-0.018697044,-0.02489701,-0.023976022,-0.0378964,-0.014675633,-0.026721956,-0.0030820246,0.045233503,0.013686747,0.0124900555,0.008258074,-0.034929335,-0.03542518,0.019201828,0.061327998,0.01487383,-0.0014904354,-0.04041143,-0.00012807728,0.015988205,0.039542932,-0.031939752,0.040550884,-0.05590981,0.05303701,-0.010405995,0.022282507,-0.019812677,-0.063268594,0.023995157,0.031247033,-0.033819538,-0.031476166,-0.023479458,0.05067215,-0.031525325,-0.018034218,-0.01975475,0.038870525,0.0036804306,-0.043106064,-0.003678621,-0.006267423,-0.03501381,-0.015508298,-0.033166695,-0.0138694,-0.004310743,0.0066005345,0.053337697,-0.0040514325,0.059746757,-0.058963884,0.0010300755,0.05828924,0.017965175,-0.007419604,0.007730656,-0.0034372078,0.03253934,0.007614356,-0.017590562,0.0024367813,0.025132058,-0.04918371,0.076745674,0.033981133,0.018820986,-0.0433157,0.050351452,0.057995275,0.018713092,0.062115345,-0.016978601,-0.006943223,-0.007314054,0.008081103,0.0030782018,0.019775476,0.035795238,-0.042441387,0.02679213,-0.034222875,0.022413997,-0.013890639,0.019646049,0.007548729,-0.05444812,-0.011999823,0.011997071,-0.028436836,-0.076045476,0.026242087,-0.06753293,-0.002599296,0.06324418,0.031793598,-0.016510123,-0.04668132,0.087448396,-0.0814612,0.030515473,0.057785828,-0.009755178,0.052485634,-0.018160323,-0.0076721655,0.01583787,0.005591523,0.037196353,-0.010040387,0.001430473,0.0450469,-0.012553721,0.046756003,-0.004203723,-0.012166958,-0.0066208113,0.013192909,0.011402849,-0.0140136415,-0.009181899,-0.021120202,0.019250393,0.024295095,-0.004373597,0.048206795,0.014500933,0.012410983,-0.006703683,-0.010907149,0.036694232,0.011353847,0.020943863,-0.019147648,0.020695105,-0.018485773,-0.014496312,-0.054459035,0.029684296,0.031957824,0.0007476811,-0.07431528,-0.04868232,0.007923649,0.022870116,-0.033882476,0.010562372,-0.009684883,-0.0332321,-0.019812535,0.019862514,0.029677222,0.0055709523,0.023134198,0.041188,0.02805452,-0.03016715,-0.0013797132,0.03684073,0.021222964,-0.06990079,-0.045905802,-0.039701134,0.066688046,0.04658152,-0.074706875,-0.015288002,0.012165811,-0.0047854264,-0.028985642,0.05931267,0.03142182,0.0005301426,-0.035731845,0.0074761724,-0.030146044,0.037679896,-0.004802254,-0.0055307783,0.01619034,0.03277434,-0.0015011012,0.041128013,0.063074306,0.0030426541,0.0026001483,-0.034044594,0.019957999,0.017934,0.059338506,-0.051878996,0.041487027,-0.028075133,-0.03972635,0.010061477,-0.016590668,-0.028875684,0.016120436,-0.031184126,0.0024363585,0.028948054,-0.014857544,0.023338374,-0.0060918187,-0.0018249742,0.10719297,-0.07905662,0.00930717,-0.043049507,0.01080659,0.053839773,0.023632769,-0.0046498943,-0.009486938,-0.028022936,0.01106286,0.004066898,0.06643717,0.006891941,-0.033681452,0.07091679,-0.03836944,0.059867628,-0.035533845,-0.053864952,-0.07017613,-0.01879801,0.005371975,0.00032255473,-0.023351567,0.0038632224,-0.010612137,0.018071694,0.01673603,0.036838997,-0.042408437,0.08065241,0.02955963,-0.012526265,-0.03797908,0.01004034,0.021034347,-0.008055566,0.0037942315,0.018991796,0.008075647,0.027748043,0.02082554,-0.040589605,0.023883082,0.014201369,-0.019051464,-0.044557396,-0.0148899285,0.04424999,-0.018081019,-0.02408972,0.029426567,0.065958254,0.0013155055,0.014534032,-0.08458268,-0.040360224,-0.041505035,-0.031058773,-0.028824579,-0.030011287,0.006227932,0.023781274,-0.054703172,0.07413432,-0.099211976,-0.0324057,-0.027767109,0.055341657,-0.012913959,-0.02757651,-0.012091038,0.006756437,-0.06742556,-0.046717368,0.05496066,0.05047201,0.009736712,0.0043046363,-0.0045736837,0.004306721,-0.018146615,0.004109059,-0.02900913,-0.0047496506,0.06471934,-0.013591331,0.08005113,0.011450896,-0.003742326,0.02420241,-0.013400146,-0.053796172,0.049552795,0.028291805,0.020940863,0.017015304,-0.010755229,0.041980512,0.043908164,-0.043118726,-0.07470269,-0.07823615,0.0015617514,-0.008276095,0.03579148,0.042251546,-0.031937078,-0.06673216,0.051586203,0.000402241,-0.06366287,0.011885239,0.014645541,0.070697896,0.05931125,0.028727211,0.00854379,-0.057028797,-0.019490357,-0.06119208,0.024984121,-0.04936152,0.073518716,0.015022536,-0.012598687,-0.023274606,0.0012179515,-0.0031128705,0.012551577,-0.0018668346,0.01428017,-0.025456434,-0.04871325,-0.014740935,0.021479774,-0.008554623,-0.024027597,0.050638888,0.06812246,-0.05024363,-0.036239117,-0.016638614,0.03524489,0.019794092,0.005500556,0.023028154,0.023980122,-0.023209946,-0.00026786502,0.023388695,0.017572757,-0.026366033,-0.015535774,0.03805688,0.020865418,0.030746467,-0.004559553,-0.039854173,-0.102441,0.03774284,0.002171103,-0.029293956,0.003298035,-0.009767342,-0.020781131,-0.022491517,-0.042227585,-0.00772121,-0.006591693,-0.0013418942,0.012178052,0.023949372,0.035915382,-0.028590398,-0.047576588,-0.024153812,-0.047333572,-0.02861704,-0.0016118529,0.028709529,-0.008477924,0.033305738,0.0066927224,0.00746527,0.007459955,0.0060380753,0.042240534,0.066173084,-0.04455672,0.042946875,-0.016963977,0.012102945,0.10495001,0.006860406,0.0077488804,-0.0027348215,0.03462482,0.02488925,-0.007017105,-0.012077419,-0.0072401143,-0.040362123,0.013442529,-0.0038661705,-0.029573604,0.019877752,-0.051659554,-0.04431112,-0.07207279,-0.021696063,0.032938093,0.07869008,0.024330938,-0.022970432,-0.06550136,0.034929767,-0.046971172,0.064689524,0.012749623,0.009460978,0.038524766,0.017618896]	Keywords: Transformer architecture, encoder-decoder, encoder-only, decoder-only, sequence-to-sequence, sequence generation\nKey Objects: Transformer architecture, encoder, decoder\nRefers to Images: None\nHypothetical Questions:\n- What are the core differences between the encoder-decoder, encoder-only, and decoder-only Transformer architectures?\n- How does the choice of architecture impact the type of problems the Transformer can effectively address?\n- Why would you choose to use only the decoder portion of the Transformer for a specific task?\n---\nSummary:\nThe Transformer architecture can be utilized in three configurations: encoder-decoder, encoder-only, and decoder-only, each suited for different tasks.\nOriginal Text:\n### 2.2 Model Usage  \nGenerally, the Transformer architecture can be used in three different ways:  \n-  Encoder-Decoder . The full Transformer architecture as introduced in Sec. 2.1 is used. This is typically used in sequence-to-sequence modeling (e.g., neural machine translation).\n-  Encoder only . Only the encoder is used and the outputs of the encoder are utilized as a representation for the input sequence. This is usually used for classification or sequence labeling problems.\n-  Decoder only . Only the decoder is used, where the encoder-decoder cross-attention module is also removed. This is typically used for sequence generation, such as language modeling.\nContextualized Text:\nThe Transformer architecture provides flexibility for various tasks and can be implemented in three primary ways: encoder-decoder, encoder-only, and decoder-only. The encoder-decoder configuration is complete, suitable for sequence-to-sequence tasks like neural machine translation.  Encoder-only models are used for classification and sequence labeling, while decoder-only models, which omit the cross-attention mechanism, are used for sequence generation tasks, such as language modeling.	{"tags": ["architecture", "NLP", "transformer", "modeling"], "doc_id": "02e43c44-3cfb-4ac3-afdd-fbc4a8d146a0", "summary": "The Transformer architecture can be utilized in three configurations: encoder-decoder, encoder-only, and decoder-only, each suited for different tasks.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["Transformer architecture", "encoder-decoder", "encoder-only", "decoder-only", "sequence-to-sequence", "sequence generation"], "key_objects": ["Transformer architecture", "encoder", "decoder"], "contextual_text": "The Transformer architecture provides flexibility for various tasks and can be implemented in three primary ways: encoder-decoder, encoder-only, and decoder-only. The encoder-decoder configuration is complete, suitable for sequence-to-sequence tasks like neural machine translation.  Encoder-only models are used for classification and sequence labeling, while decoder-only models, which omit the cross-attention mechanism, are used for sequence generation tasks, such as language modeling.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.2 Model Usage"}, "hypothetical_questions": ["What are the core differences between the encoder-decoder, encoder-only, and decoder-only Transformer architectures?", "How does the choice of architecture impact the type of problems the Transformer can effectively address?", "Why would you choose to use only the decoder portion of the Transformer for a specific task?"]}
52385e10-625c-41a6-bdd9-285d4ab72a95	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0054052677,-0.0011664345,0.027507437,0.054992236,-0.032667823,0.02795332,-0.003663106,0.065436244,0.028731229,-0.026808042,-0.02331919,0.031979527,-0.016336974,0.0012325323,-0.029205695,0.011588007,0.029455043,0.07798177,0.0011904238,-0.035412595,0.029597653,0.008765745,0.0057939403,-0.013905531,0.026910128,0.0532721,0.04983756,-0.03162883,-0.052566614,0.013849924,0.033770837,-0.08289105,0.00065322715,0.031325847,0.0091913445,-0.047435284,-0.008652407,-0.0798795,-0.014772404,0.031621847,-0.06395203,0.08262055,-0.079002045,-0.016886694,0.014703929,-0.069009714,-0.07481357,-0.017313585,0.007918166,-0.03679508,-0.019158885,0.01508238,0.013023463,-0.032364294,0.017855363,-5.868366e-05,-0.035671685,-0.078600146,-0.017176421,-0.07674888,-0.00011649757,-0.0012042514,-0.055338934,0.0038731692,0.013305782,0.0038375447,0.03681987,0.04282633,0.042303275,0.10833878,-0.0029728373,0.008743128,-0.008876983,-0.048118502,0.11498176,0.021870945,-0.04710308,-0.016335938,-0.05134116,-0.03533628,-0.04860762,0.078202695,-0.036119662,-0.02968516,0.07017294,-0.015768953,-0.018361703,-0.023300856,0.073895365,-0.03684489,0.013397407,0.011732408,0.033352945,0.030400878,0.002683873,-0.08075564,-0.0016685082,-0.028363686,0.037513826,-0.045458462,-0.0060861977,-0.0052377298,0.05864792,0.10682257,0.042975705,-0.040395036,-0.005763528,-0.00029881042,0.019190514,0.028874839,-0.020213164,0.00047170254,-0.046868067,0.04004622,-0.043792136,-0.009781382,0.012718935,0.0074960673,0.0018146149,0.0163011,-0.029859744,0.009348498,-0.011298663,0.018062003,0.032559093,0.029517356,0.017524738,0.007993051,-0.03185672,0.0026546458,0.025207493,0.012688813,-0.021526797,0.031769004,-0.012216358,0.01711457,0.0736725,0.0015579729,-0.039636504,0.013155125,0.0066824956,-0.048723876,-0.04268663,-0.00032850588,-0.04203404,0.028366392,-0.04472498,0.0368419,-0.036105666,0.04567012,0.028781094,0.014584872,0.02656874,-0.006889857,-0.00042490027,-0.0059386934,0.024632197,-0.052805625,-0.01756198,-0.008648578,-0.050631214,-0.057747442,-0.00522808,0.06370651,0.03546006,0.11338881,0.032882307,-0.012795074,-0.006459798,0.021365758,-0.029184671,0.0026680504,-0.015279387,-0.048648708,-0.009222891,0.0047112154,0.0004921827,-0.014429005,-0.01622262,-0.0409053,-0.03062227,0.020179546,-0.007951015,0.036258932,-0.034348264,0.027924992,0.04416876,0.037385963,-0.022431072,-0.026474843,0.03703353,-0.046988573,0.019700913,-0.038226195,0.047755968,0.031166334,0.03759514,-0.068137355,-0.012375428,-0.030637024,-0.0071232757,0.00025670385,-0.024316559,0.0013106498,0.05431893,0.013956004,0.008740455,-0.010296234,0.0038749832,0.02853869,0.023302969,0.013222395,-0.026174493,0.047081586,0.018350888,-0.014501283,0.024575213,0.016005604,0.018832058,-0.015651274,-0.005193846,0.011109773,-0.027854988,0.03469718,-0.05500956,0.033906255,-0.052776653,0.021463184,0.014884962,-0.029282762,0.0056917095,0.015112082,-0.06250322,0.009260865,-0.019551337,0.020202788,-0.007800085,0.003753075,0.01790504,0.014442194,0.034483656,0.04809023,-0.021202616,0.10144317,0.031333927,0.022011958,0.019937469,0.061218522,0.018639648,0.032703597,-0.023622394,0.060820673,0.013319692,-0.0025637443,0.0043276185,-0.067774475,0.0061228266,-0.0056234393,-0.0030658985,-0.023278177,-0.0081131,-0.008367098,-0.019575562,0.01637814,-0.014809702,-0.028554762,0.025043854,0.042386863,-0.022786317,0.00056056294,0.03640889,0.07531709,-0.0040105986,-0.005431202,-0.024853604,-0.00026155307,-0.022640275,0.012591682,0.016600763,-0.014860131,-0.037139326,-0.01167948,0.008277306,-0.01088211,0.013425928,-0.023134038,0.020928264,-0.022400282,-0.014586942,0.013897014,-0.0457975,-0.0625429,0.055141382,-0.022268286,-0.009760936,0.01372765,5.486301e-05,-0.024412079,0.0029541394,0.009049124,0.117468484,0.0021190539,0.00039345698,-0.006898308,0.045565784,0.011044498,-0.043054473,0.054510448,0.0130303,-0.0035398265,-0.010129791,-0.03364674,-0.025499858,-0.018037805,0.020928187,0.08267032,0.08179362,-0.021373011,-0.020501515,-0.015165914,0.00094812276,-0.0029226048,0.0025079744,0.0309032,0.058841407,-0.01274364,0.019359164,-0.021526262,-0.073202714,0.058153316,-0.09443912,0.061297216,0.06889525,0.018579267,-0.023830028,-0.01977037,0.014818671,0.01946747,0.015724007,0.01911792,-0.050324008,-0.054733645,-0.0027187732,-0.00687524,-0.03382315,0.05915264,0.0019819664,-0.016556107,0.008099185,0.02850237,0.030901022,0.039462376,0.0056486647,0.015108938,0.02791714,-0.013739679,-0.05484659,-0.014483048,0.00702286,-0.021816345,0.04176275,-0.040442146,0.045291685,-0.037909333,-0.00056235417,-0.02060671,-0.06985427,-0.002456249,0.068741985,-0.026779093,-0.02641893,-0.038137432,0.05587475,-0.054765597,-0.018302858,0.050049596,0.06625868,0.03371646,-0.016754262,-0.013435384,0.022210851,-0.018675167,-0.0052926815,-0.02809623,0.036729,-4.5792425e-05,0.011981153,0.09648323,-0.02849817,0.06318547,-0.036130603,-0.0102173295,-0.004958259,-0.008005699,-0.02514496,0.034079403,0.0154404985,0.04891625,-0.04173038,0.0059751202,0.04261993,0.037497234,-0.043147337,0.01303868,0.016760075,-0.023583971,-0.07516171,0.050952815,0.03219682,0.01195214,0.017629104,-0.023300672,-0.016724117,0.013603853,0.024846409,-0.030110499,-0.019070962,0.03447862,-0.04748737,-0.038316023,-0.033577107,0.06555166,0.006280245,0.03986785,0.01907785,-0.017171409,-0.012559969,0.03622653,0.021167297,-0.02365899,0.013763101,-0.016006991,-0.037220046,0.084722534,-0.004601475,-0.0116964,-0.08350748,0.07870547,-0.059677754,-0.020398377,0.061760142,-0.03978623,-0.013815514,-0.026485717,0.015639592,-0.0088636065,0.00696457,0.059299458,-0.021713011,-0.026089363,0.04446522,-0.0029371756,0.028276311,-0.019682392,0.015629595,-0.012799736,0.008266592,-0.0033237396,-0.07636416,0.010525087,-0.005661091,0.038176928,0.0052065956,-0.03706425,0.005202149,-0.02647458,-0.009933967,-0.04721878,-0.018708492,-0.02809929,0.037570816,0.02951747,-0.047838174,0.058662247,0.014382215,-0.03622138,-0.013109135,0.04917702,0.013907293,0.028880434,-0.06814765,-0.06314448,0.022637755,0.0047112876,-0.0011173773,0.0070247487,-0.02701266,0.013100783,-0.03409105,0.023357313,0.00907783,0.05864678,0.024125831,0.003249482,0.0028749814,-0.035939965,0.011883462,0.0046892003,0.020455586,-0.015932886,-0.05249202,-0.015660876,0.03860403,0.06433343,-0.021546494,-0.010181257,-0.00863146,0.015919672,-0.017357828,0.056025516,0.018318972,-0.0251863,-0.015771938,-0.012954189,-0.077670634,0.011169227,-0.016267812,-0.02568389,0.015893005,0.025171293,0.022894518,0.03490479,0.0706962,0.02914157,0.032592446,-0.0019048784,-0.024732118,0.0035114458,0.02993181,-0.066282086,-0.0076889596,-0.08159284,-0.031389512,0.004712723,-0.05022437,-0.06433809,0.014973613,-0.0682365,0.016436895,0.000773291,-0.018590638,-0.05612905,0.04568623,-0.05597602,0.06827682,-0.058850154,-0.00081378594,-0.015040685,-0.013309786,0.065009765,0.020764038,0.01130147,-0.013149476,-0.042138714,-0.0137690315,-0.030120479,0.07687407,0.02419029,0.027641432,0.039248284,-0.03911628,-0.031234965,-0.0262534,-0.06406118,-0.03251561,-0.0006169254,-0.017985713,0.020528838,-0.03289939,0.01382084,-0.024489507,-0.0076195425,-0.06188094,0.06609931,-0.045694478,0.07200464,0.044057593,0.013249756,0.034854326,-0.031169465,0.011099052,0.051023535,0.021313088,0.033593692,0.0139111485,-0.020393508,0.0048032408,0.027217837,0.032033846,-0.014690933,-0.040427838,-0.07572738,0.010896381,0.0044420036,-0.03733377,-0.035203304,0.06657131,0.040386338,0.03046854,-0.024002317,-0.038896292,-0.050647397,-0.010151126,-0.049815178,0.01196807,0.02670104,-0.01427904,0.0077331224,-0.041705806,0.055636447,-0.07865257,-0.031356726,-0.035785623,0.03567573,-0.0071755704,-0.039325587,-0.029313445,-0.027417766,-0.012438615,-0.046093613,0.028248249,0.065492965,-0.0083864415,0.010425102,-0.0056003314,-0.022021791,-0.00166465,0.021099906,-0.013594278,0.0037414192,0.00050800823,-0.027102858,0.058194775,-0.038992383,0.050948903,0.06261553,-0.06320178,-0.042841893,-0.0032920095,0.020794313,0.049929142,-0.035906836,-0.0032729285,-0.0005889275,0.007763693,-0.04118347,-0.020655192,-0.07025443,0.01906705,-0.04223952,-0.019633835,0.023171373,0.020476893,-0.037444945,-0.016420964,0.01424581,-0.054520838,0.00798434,0.043663952,0.05175622,0.041501924,0.040369406,-0.0080362195,-0.036406837,-0.040482417,-0.06831307,-0.0061054165,-0.05879399,0.04368367,-0.049091298,0.028459111,-0.01787634,0.02272267,0.027393132,0.010287475,0.028874135,-0.0043237535,-0.070637815,-0.021841327,0.032918807,-0.013228639,-0.0044577164,-0.01050398,0.0919658,0.019943152,-0.05750541,-0.020804092,-0.027123075,-0.02341972,0.002544279,0.010952193,0.0034574165,0.0078055086,0.03898872,-0.025990361,0.07048029,-0.0009185538,-0.035184737,-0.021612134,0.0070214267,0.026408123,0.04921807,-0.033549093,-0.02584767,-0.09288817,-0.009991944,0.009843606,-0.039991014,0.035859786,-0.031660415,-0.03409544,-0.0064139357,0.028186357,-0.04302899,0.007162546,0.008492946,0.030147137,0.030290075,0.021311851,-0.024424067,-0.04454246,-0.008489333,0.0054395944,-0.00857046,-0.023755338,0.041226998,0.028674608,0.011519758,-0.01334645,0.020714667,-0.01708889,0.00038695504,0.0253978,0.029164914,-0.045067757,0.048901293,-0.026468975,-0.020893345,0.07441537,-0.012012653,0.0032603622,0.008049549,0.040902786,0.004453608,0.0057119983,0.019671688,-0.062195737,-0.037099257,0.04650508,0.045382466,0.0083524585,0.044158373,0.0074279094,-0.045249287,-0.016589101,-0.030334387,-0.004365409,0.009479774,0.014245612,0.03848894,-0.07168487,0.004636591,-0.04390374,0.038983155,0.028356751,0.051065106,-0.0088712955,0.033665247]	Keywords: Transformer, self-attention, position-wise FFN, computational complexity, parameter requirements\nKey Objects: self-attention module, position-wise FFN, hidden dimension, input sequence length\nRefers to Images: None\nHypothetical Questions:\n- What does the notation 'O(T^2 * D)' represent in the context of self-attention complexity?\n- How does the relative importance of the hidden dimension (D) versus the sequence length (T) impact the Transformer's performance?\n- What does it mean for a module to be a 'bottleneck' in the Transformer architecture?\n---\nSummary:\nThis analysis explores the computational time and parameter requirements of the Transformer model by examining its core components: the self-attention module and the position-wise feed-forward network (FFN), as detailed in Table 1.\nOriginal Text:\n### 2.3 Model Analysis  \nTo illustrate the computation time and parameter requirements of the Transformer, we analyze the two core components of the Transformer (i.e., the self-attention module and the position-wise FFN) in Table 1. We assume that the hidden dimension D$\\_{m}$ of the model is D , and that the input sequence length is T . The intermediate dimension of FFN is set to 4D and the dimension of keys and values are set to D/H as in Vaswani et al. [137].  \nTable 1. Complexity and parameter counts of self-attention and position-wise FFN  \n|                   | Module              | Complexity   | #Parameters   |\n|-------------------|---------------------|--------------|---------------|\n| self-attention    | O ( T 2  D )       | 4D 2         | -             |\n| position-wise FFN | O ( T 1  D $^{2}$) | 8D 2         | -             |\nContextualized Text:\nTo understand the Transformer's performance, this analysis examines the computational complexity and parameter counts of its core components: the self-attention module and the position-wise feed-forward network (FFN). The analysis assumes a hidden dimension D and an input sequence length T, and the results are summarized in Table 1, which details the complexity and parameter counts for each module, as described by Vaswani et al.	{"tags": ["architecture", "NLP", "complexity", "performance"], "doc_id": "52385e10-625c-41a6-bdd9-285d4ab72a95", "summary": "This analysis explores the computational time and parameter requirements of the Transformer model by examining its core components: the self-attention module and the position-wise feed-forward network (FFN), as detailed in Table 1.", "doc_type": "text", "entities": ["Vaswani"], "keywords": ["Transformer", "self-attention", "position-wise FFN", "computational complexity", "parameter requirements"], "key_objects": ["self-attention module", "position-wise FFN", "hidden dimension", "input sequence length"], "contextual_text": "To understand the Transformer's performance, this analysis examines the computational complexity and parameter counts of its core components: the self-attention module and the position-wise feed-forward network (FFN). The analysis assumes a hidden dimension D and an input sequence length T, and the results are summarized in Table 1, which details the complexity and parameter counts for each module, as described by Vaswani et al.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.3 Model Analysis"}, "hypothetical_questions": ["What does the notation 'O(T^2 * D)' represent in the context of self-attention complexity?", "How does the relative importance of the hidden dimension (D) versus the sequence length (T) impact the Transformer's performance?", "What does it mean for a module to be a 'bottleneck' in the Transformer architecture?"]}
a0215f88-a7d0-41d9-b764-a72263c40428	abe8c200-bfa1-4355-947e-23ea618c310d	[0.004462131,-0.015222142,0.023431104,0.05440079,-0.052767668,0.04307148,-0.001776668,0.052598752,0.019820066,-0.028821371,0.006227206,0.039148606,-0.0008103531,-0.0041873325,-0.003523866,0.0069219354,0.02219716,0.052422885,-0.0072492156,-0.016286535,0.034685962,0.021537613,-0.00375683,-0.0019254388,-0.014477515,0.042229544,0.028746199,0.0027072087,-0.03684626,0.03208721,0.022701232,-0.05022593,0.025814656,0.014226809,0.038597845,-0.052458398,0.0075868997,-0.06613977,-0.00806627,-0.0069573997,-0.06094937,0.062264606,-0.07210134,-0.014378792,0.0057808245,-0.069067985,-0.072477184,-0.018003564,0.01988572,-0.02215047,-0.0069291606,-0.0058324393,-0.016797502,-0.049991135,0.009714464,0.008577653,-0.0018903197,-0.05856146,-0.01594528,-0.09344953,-0.0064925253,0.00090027606,-0.031852014,0.0063137324,0.013102199,-0.008096369,0.02577902,0.034896504,0.05385062,0.113170534,-0.005995413,0.007828564,-0.026516508,-0.08125873,0.13063988,0.029677443,-0.024253797,-0.0327314,-0.048805043,-0.008586831,-0.04364571,0.06081784,-0.031775374,-0.08181557,0.08102129,-0.031860944,-0.016681794,0.002269238,0.070030704,-0.047822397,-0.0043761237,0.010142088,0.00531966,0.037205778,-0.00924301,-0.07044678,-0.006030898,-0.024342375,0.0429752,-0.048955884,0.03549385,0.0016371928,0.08794896,0.10965186,0.029501988,-0.039101392,-0.001818154,-0.022350146,-0.007886975,0.03589167,-0.032702822,0.0029975988,-0.042050935,0.033567388,-0.008151776,0.0073181516,-0.015888823,0.0108263055,0.00849337,-0.0052915635,-0.007588371,-0.011864536,-1.7323271e-05,0.03114719,0.064070605,0.028060565,0.040574472,0.0014771835,-0.047880717,0.02092674,0.01869508,-0.0037114418,-0.04863353,0.014729851,-0.028491644,0.043226186,0.08566019,0.007832926,-0.026922485,0.029893832,0.004685996,-0.053310975,-0.046598066,0.039976377,-0.01651803,0.061082494,-0.037310556,0.059534706,-0.02907861,0.04958098,0.026399491,-0.00198943,0.057463452,-0.01473261,-0.03059294,-0.0049475473,-0.007889439,-0.055719916,-0.024672069,-0.06229476,-0.04363029,-0.059839003,0.014630335,0.054679748,0.053287923,0.10514632,0.0047948593,-0.021407593,0.019465774,0.015037477,-0.036195487,-0.0026076993,-0.009656717,-0.061002534,-0.0010159202,0.009483972,-8.232481e-05,-0.018369636,-0.004864996,-0.021363778,-0.009338889,0.05526966,-0.015183493,0.026783846,-0.06101326,0.023987914,0.034477852,0.04464041,-0.023554739,-0.03921052,0.030986289,-0.033447582,0.0062639974,-0.008087712,0.01902177,0.04607648,0.0469749,-0.06806482,-0.0108895125,-0.048489295,-0.01654149,0.016517688,-0.010839393,-0.004358532,0.030664476,-0.013984731,0.014142896,-0.023511266,-0.0010129816,0.015766378,-0.00456526,0.007708329,-0.030638412,0.03502812,0.014690074,0.0034865085,0.004394143,-0.0019066125,0.014065391,-0.009286701,-0.036334552,0.0070280028,-0.033533435,0.02537418,-0.032255013,0.036597636,-0.04010242,0.021486912,0.014764866,-0.018219754,0.018439734,0.033165473,-0.05859794,0.03774457,-0.056583494,0.022561502,0.008788797,-0.0016274861,-0.024009207,0.025922747,0.034982085,0.0753778,-0.018300073,0.095400624,0.010894269,0.013441298,0.008085895,0.053069603,0.020702308,0.030451018,-0.022582458,0.07033292,0.011407958,0.002266402,0.010205045,-0.059922114,0.018223867,-0.0011485122,0.027263362,0.019095358,0.012732387,-0.0037893234,-0.024627505,0.037911642,-0.014694266,-0.015742341,0.03816147,0.037203863,-0.06615113,-0.0012271329,0.02463981,0.054762967,0.004939041,-0.0053533316,-0.029418398,0.009208397,-0.0031453585,-0.024399761,0.024829749,-0.03768635,-0.019974992,-0.028601898,0.016451862,-0.015305956,0.01489381,-0.015102337,0.04943274,-0.012549746,-0.024629822,-0.0080850255,-0.01659883,-0.04136223,0.037929676,-0.003387337,-0.031307124,-0.025084825,0.0009425527,-0.028035246,-0.0051412345,-0.0029373455,0.0961074,-0.019752664,-0.0077618835,0.020665767,0.022547893,0.035437502,-0.06466097,0.0477518,0.039855357,-0.0017332785,0.006293197,-0.019709034,-0.02047727,0.011863826,0.020781789,0.06868411,0.08892275,-0.01241706,-0.020477487,0.0023811047,0.015420438,-0.009335279,6.809819e-05,0.015658343,0.06020683,-0.021635385,0.0004713924,-0.016455382,-0.07660579,0.053805653,-0.06017005,0.07172842,0.06657083,0.011492131,-0.0073900707,-0.014732186,-1.7690345e-05,0.030027661,0.0036041078,0.0058960826,-0.032565862,-0.03640544,-0.0053084744,-0.025348479,-0.009468502,0.06288313,0.013820797,0.0018637055,-0.017474918,0.021902397,0.022747427,0.020384261,0.0032688782,0.022026667,0.03602258,-0.0144836875,-0.0027026972,0.009551318,0.0011695542,-0.01813211,0.07878383,-0.079333276,0.0683202,-0.024358904,0.0012832433,-0.0093431575,-0.050368402,0.02785387,0.06254248,-0.033076115,0.0016486116,-0.049818315,0.07145525,-0.057398416,-0.03812169,0.048933897,0.057343684,0.032923717,-0.029552579,-0.014625642,-0.0024842606,-0.00039534597,-0.022424467,-0.06547476,0.027523922,0.0116597265,0.027967213,0.093475066,-0.00061099156,0.054825634,-0.0214514,0.008379884,0.02015914,0.0059871576,-0.05276088,0.008457527,0.022118974,0.063651755,-0.0032285035,0.0050387573,0.03624139,0.010169196,-0.054053936,0.016021626,0.03446179,0.008930258,-0.071739346,0.048912402,0.021259183,0.0008548307,0.02371591,-0.041666444,-0.0011459936,0.0026796013,0.046604704,-0.013574279,-0.02083981,0.016197748,-0.04118494,-0.022829598,-0.017326664,0.030360196,0.014446334,0.06315108,0.033951644,-0.0014822341,-0.04587212,0.031786285,0.015610604,-0.056796383,0.016767323,-0.0335827,-0.008637784,0.060563635,-0.008817527,0.0040084934,-0.056787815,0.089875385,-0.02508751,0.019713081,0.054256234,-0.02886189,-0.0032364293,-0.019531721,-0.012440726,-0.026952347,0.034486275,0.029690312,-0.012818915,-0.03203089,0.041519884,-0.015580991,0.027313761,-0.0039782603,0.039023265,-0.01787472,-0.0010136091,-0.0101527115,-0.058129653,0.003263837,-6.702622e-05,0.029473778,0.022817992,-0.021010427,0.01619755,-0.037397023,-0.02676178,-0.045149032,-0.01061625,-0.011596465,0.014783859,0.031643324,-0.032748703,0.05835496,0.013240552,-0.040565096,-0.010680033,0.044101972,0.040829167,-0.0096026445,-0.03723651,-0.04549637,0.04077254,0.011038313,0.03175199,0.02211494,-0.024989087,-0.018514669,-0.010007266,0.01340126,0.001992057,0.044034887,0.017873242,0.020777661,0.025142338,-0.033447754,-0.0007415353,0.021653265,0.032025818,-0.0180169,-0.046987057,0.0065170736,0.042945806,0.05873066,-0.047018986,-0.009711598,-0.005034028,0.011335354,-0.024556873,0.06907427,0.0135517,-0.008472799,0.025231779,0.008536097,-0.07456192,0.011502699,-0.027094597,0.007583375,-0.0038445648,0.035253115,0.024054402,0.0122373635,0.037669647,0.04283371,0.034892824,-0.004030515,0.0074615683,0.014590238,0.030239094,-0.07377243,-0.02462145,-0.049509887,-0.019359056,0.0243564,-0.057543296,-0.057023037,0.020148207,-0.078397006,0.046804745,0.003095175,-0.012623989,-0.036546215,0.027980287,-0.034637615,0.06227422,-0.072862916,0.0014011484,0.037045356,-0.028693888,0.065114215,0.0011712742,0.01850137,-0.027200226,-0.021986254,-0.006406454,-0.0048741247,0.09045756,0.005732653,0.030808195,0.0525207,-0.021511586,-0.01289049,-0.023469694,-0.06835153,-0.057661768,-0.013618489,-0.02478807,-0.0030064941,-0.015737554,-0.022457018,0.027649067,-0.009401793,-0.04869005,0.030900382,-0.03284285,0.06582252,0.054151967,0.00522824,0.027254868,-0.027167007,0.012643102,0.04428931,0.042146392,0.052315414,0.006410359,0.006017054,0.0021950935,0.008089292,-0.0015309671,-0.0137542365,-0.066856205,-0.055587176,-0.0011636585,0.011293759,-0.015658062,-0.030397031,0.045105476,0.05483156,0.0115841245,-0.024955172,-0.06391586,-0.06325959,-0.009506658,-0.025080882,0.0207822,0.022350844,-0.022946686,0.049627375,-0.0017495468,0.0801548,-0.113682546,-0.05583816,-0.05800415,0.058011595,-0.024961764,-0.016674902,-0.015383566,-0.030771438,-0.019891208,-0.04598145,0.0660019,0.06089815,-0.017827174,-0.0053585675,-0.013837276,-0.00908552,-0.0072638807,-0.023930717,-0.031022362,0.0023369074,-0.02629224,-0.024300106,0.05289239,-0.028488223,0.008119999,0.031312775,-0.021244377,-0.06792759,-0.0074760765,0.01748166,0.03672934,-0.019352281,0.00571356,-0.019252857,0.0346461,-0.04512773,0.0045988485,-0.06253389,0.019632006,-0.026072416,0.020765675,0.04635974,0.019740567,-0.018654864,-0.012112543,-0.009556657,-0.04847878,0.03139537,0.044322994,0.021933954,0.09269002,0.03376715,-0.05655721,-0.05013059,-0.01911329,-0.048900187,-0.006928329,-0.027590193,0.05269171,-0.025315613,0.030449566,-0.043945074,0.0061888727,0.02011995,0.016565904,0.036810152,-0.0010719278,-0.058054067,-0.040000923,0.02240807,-0.014889493,-0.0019904661,-0.016294194,0.07674081,0.013171028,-0.06320407,-0.021034157,-0.049693797,-0.041262813,-0.022396358,0.0032382554,0.0025505556,0.025518091,0.0045293104,-0.016811807,0.055203363,-0.013630741,-0.063370846,-0.037586186,0.007328106,0.055824246,0.041146968,0.010736414,-0.017270487,-0.10008199,0.0050486047,0.010902275,-0.031478852,0.037225023,-0.011328024,-0.028724242,-0.0141857695,0.017048612,-0.01637031,0.04201487,-0.027096312,0.043772668,0.02049837,-0.02649015,-0.008064575,-0.060256045,-0.028817002,-0.016570644,-0.029675296,-0.018615942,0.055622753,0.014833519,-0.009653802,-0.002998582,-0.003988262,0.0065036453,0.020632695,-0.0073959683,0.035290014,-0.032454226,0.033394396,-0.01886054,-0.021193076,0.07621007,0.00040848882,-0.00931433,0.032676592,0.015791569,-0.0155743165,0.016044317,0.004709772,-0.05294604,-0.028323082,0.013142467,0.038072303,0.015265544,0.051593196,0.0074304873,-0.046531312,-0.05575707,-0.052710883,-0.000768282,0.015648177,0.031806152,0.024291769,-0.06295107,0.03648287,-0.046527382,0.03576017,0.021231938,0.050041072,-0.02867466,0.002629232]	Keywords: self-attention, position-wise FFN, complexity, hidden dimension, sequence length\nKey Objects: self-attention, position-wise FFN, hidden dimension, sequence length\nRefers to Images: None\nHypothetical Questions:\n- How does the choice of hidden dimension 'D' affect the overall complexity of the Transformer?\n- Why does the attention distribution matrix become a limiting factor for long sequences?\n- What strategies can be used to improve the long-sequence compatibility of self-attention?\n---\nSummary:\nThe complexity of self-attention is O(TD) and the complexity of position-wise FFN is O(TD), with the relative importance of hidden dimension 'D' versus sequence length 'T' influencing which component acts as the bottleneck.\nOriginal Text:\n| self-attention    | O ( T 2  D )       | 4D 2         | -             |\n| position-wise FFN | O ( T 1  D $^{2}$) | 8D 2         | -             |  \nWhen the input sequences are short, the hidden dimension D dominates the complexity of self-attention and position-wise FFN. The bottleneck of Transformer thus lies in FFN. However, as the input sequences grow longer, the sequence length T gradually dominates the complexity of these modules, in which case self-attention becomes the bottleneck of Transformer. Furthermore, the computation of self-attention requires that a T  T attention distribution matrix is stored, which makes the computation of Transformer infeasible for long-sequence scenarios (e.g., long text documents and pixel-level modeling of high-resolution images). One shall see that the goal of increasing the efficiency of Transformer generally leads to the long-sequence compatibility\nContextualized Text:\nThe complexity of self-attention in a Transformer model is O(TD), while the complexity of position-wise FFN is O(TD). When dealing with short input sequences, the hidden dimension 'D' is the dominant factor influencing the complexity of both components. As input sequences become longer, the sequence length 'T' gradually dominates, making self-attention the bottleneck. The computation of self-attention necessitates storing a T  T attention distribution matrix, potentially rendering Transformer infeasible for long sequences.	{"tags": ["architecture", "NLP", "transformer", "complexity"], "doc_id": "a0215f88-a7d0-41d9-b764-a72263c40428", "summary": "The complexity of self-attention is O(TD) and the complexity of position-wise FFN is O(TD), with the relative importance of hidden dimension 'D' versus sequence length 'T' influencing which component acts as the bottleneck.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["self-attention", "position-wise FFN", "complexity", "hidden dimension", "sequence length"], "key_objects": ["self-attention", "position-wise FFN", "hidden dimension", "sequence length"], "contextual_text": "The complexity of self-attention in a Transformer model is O(TD), while the complexity of position-wise FFN is O(TD). When dealing with short input sequences, the hidden dimension 'D' is the dominant factor influencing the complexity of both components. As input sequences become longer, the sequence length 'T' gradually dominates, making self-attention the bottleneck. The computation of self-attention necessitates storing a T  T attention distribution matrix, potentially rendering Transformer infeasible for long sequences.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.3 Model Analysis"}, "hypothetical_questions": ["How does the choice of hidden dimension 'D' affect the overall complexity of the Transformer?", "Why does the attention distribution matrix become a limiting factor for long sequences?", "What strategies can be used to improve the long-sequence compatibility of self-attention?"]}
aabf053b-c6a3-4b0e-8c8c-c346ac31e0d6	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.014115434,-0.012294364,0.0027282329,0.023586433,-0.021737775,0.05687148,-0.0029029134,0.036525138,0.016236776,-0.05404781,0.0024088658,-0.022596277,0.0020352402,-0.02322101,-0.0070633227,0.011211539,0.015288883,0.086819254,0.004720442,-0.025674319,0.036531772,0.040103246,0.036815096,-0.009813544,-0.019821543,0.041429702,0.04535182,-0.006858167,0.0012187095,0.015748352,0.02257921,-0.059866272,0.061177444,0.012157397,0.05281791,-0.02410312,0.000749815,-0.07014514,-0.0055713006,-0.010818877,-0.016457694,0.015829561,-0.027715934,-0.009037102,-0.008883075,-0.0593894,-0.07845434,-0.0056314045,0.02358318,-0.018155834,-0.02919548,0.041216034,-0.022158561,-0.022008155,0.011014601,0.014014539,0.014405525,-0.06949379,0.022398349,-0.10380211,-0.022074262,0.029425453,-0.013582788,-0.019656252,0.032502137,-0.027922666,-0.00015322155,0.031934936,0.049149986,0.12742797,0.0020154454,-0.020477194,0.01754085,-0.039987147,0.119783945,0.046024673,-0.012285431,-0.031945612,-0.062252168,0.0006383973,-0.012426381,0.06822026,-0.0927184,-0.020505892,0.080199786,-0.014470518,-0.030861512,-0.0035912176,0.06379183,-0.053822283,0.014666395,0.0089620855,-0.02157231,0.074984394,-0.022343526,-0.07627116,-0.017228466,-0.027104592,0.033046726,-0.06737955,0.041236684,-0.008706983,0.0859486,0.098915584,0.045287453,-0.017513685,-0.01500234,-0.054117154,0.010777436,-0.0024590627,0.011639191,0.0049218275,-0.036511153,0.0144554395,-0.01822082,0.039054886,-0.05361576,0.022253804,0.0042922758,-0.023128727,0.03383372,-0.023161981,-0.01728484,0.027149215,0.07214581,0.045834385,0.04440305,-0.028925713,-0.0428068,0.022462891,0.036988553,-0.01597929,-0.031069234,-0.0009521822,-0.013404426,0.014100027,0.07838183,0.00482201,-0.008986948,0.022236416,0.01586082,-0.057376046,-0.043500066,0.017618699,0.008679527,0.03143972,-0.0743086,0.006603188,-0.034293603,0.046132088,0.031196102,-0.027387809,0.032124177,0.008143517,-0.031057542,-0.017933521,0.0038913651,-0.079480335,0.027626898,-0.04692149,-0.027473114,-0.058475725,-0.009090538,0.05309395,0.036314867,0.14002524,0.00800088,-0.004560507,0.011412688,-0.0037480728,-0.021833684,-0.0053704423,-0.01900141,-0.04669961,-0.0018870131,0.00996171,0.01270217,-0.03809232,-0.0030248747,0.0024970653,-0.009105643,0.055619247,0.017669298,0.025509201,-0.01710234,0.028708834,-0.016044248,0.050039858,-0.016349882,-0.013403163,0.015595912,-0.024705265,0.024580367,0.021343399,0.026462162,0.026349824,0.05217384,-0.06498104,-0.013742902,-0.032225598,-0.011374139,-0.011592526,0.0013338411,-0.016932664,0.016590636,-0.0137027735,0.014820968,-0.045753893,0.009744081,0.041108254,-0.025777401,-0.009085796,-0.022303294,0.05324543,-0.002383243,0.032644954,-0.024384119,0.015994513,0.02958404,-0.038050737,-0.046710335,-0.008699623,-0.057687465,0.026960313,-0.028993782,0.038983796,-0.017056186,0.030050326,0.01684337,-0.024352835,0.023870734,0.02511738,-0.03350721,0.024041802,-0.02150427,0.018647432,0.013368337,0.0006781417,-0.013239754,0.03981703,0.050739605,0.07399962,0.0040003527,0.103680275,-0.013528926,-0.015709056,0.0028149546,0.0402912,0.014843388,0.020529345,-0.02827729,0.03741624,0.03493809,0.0038789797,-0.022563769,-0.04534772,-0.0058488026,0.0051639187,0.05995939,-0.006907566,0.019278152,-0.015915245,-0.009098562,0.02004408,0.016907034,0.0069976933,0.014505238,0.0021665331,-0.02984279,0.022378149,0.04319699,0.052681517,0.020196564,-0.00822492,0.0007709533,0.040848356,0.03458229,-0.0009937227,0.025455669,-0.021088578,-0.012321747,-0.039849807,0.03624827,0.0007799226,0.011021902,-0.029802507,0.032308422,-0.038135763,-0.061775576,0.004960765,-0.013708083,0.00070394727,0.02435478,-0.030899225,-0.006033346,0.0071300524,0.00011490009,-0.02691776,-0.022418752,0.034464337,0.12112368,-0.035868052,-0.028288491,0.04178249,-0.0032106705,0.028201023,-0.06780983,0.022775777,0.02560854,-0.0016126519,0.00038487982,0.015747631,-0.031443667,-0.014286471,0.03653256,0.080049895,0.065414466,-0.003681219,-0.0022148103,0.008097194,0.040340543,0.01935062,0.0042905877,0.037259888,0.03712029,-0.0067338687,0.019008754,-0.03261285,-0.07799144,0.06454153,-0.061234485,0.035135187,0.070488416,-0.0068354164,-0.018268967,-0.0067804246,-0.012701971,0.031961985,0.03799555,-0.0035168363,-0.0013607605,-0.034725774,0.036953285,0.01577572,-0.015756272,0.053791896,-0.008303693,-0.009102598,0.018970905,0.0061073243,0.010368386,0.029554678,0.01902151,0.038827825,-0.018333558,-0.010115589,-0.018742874,-0.011052448,0.011224715,0.007224097,0.054486454,-0.09665727,0.09987548,-0.054948803,0.02781229,-0.009480981,-0.049188662,0.031988055,0.0443885,-0.015775204,-0.0037689954,-0.02760499,0.06308663,-0.039584104,-0.03771202,0.046251506,0.052137453,0.021402176,-0.027545214,-0.013202452,0.021119326,0.0005900406,-0.039011687,-0.03905911,0.040641062,-0.0148902815,0.017405558,0.06490572,-0.05295634,0.056888107,-0.036207963,0.022313166,0.0011763282,0.024595177,-0.050552584,-0.008509324,0.027849605,0.04191863,0.009943657,-0.018505469,0.03411294,0.03288423,-0.061302178,0.03272282,0.021395963,0.017537173,-0.095088,0.041979898,0.035317995,0.034432467,0.034264695,-0.0102513805,-0.0074101975,0.018602224,0.02930339,-0.024216354,-0.020163845,0.03328885,-0.059018247,-0.027920103,-0.028326984,0.0636362,0.030079065,0.03104493,0.03689566,-0.003374249,-0.054261703,0.05107084,-0.020453002,-0.054722372,0.026258666,-0.05298069,-0.011391775,0.044971783,-0.005451081,0.02087378,-0.048507333,0.07494732,-0.033623647,0.01024327,0.04981869,-0.037186224,0.028621228,-0.021698223,-0.0016027715,-0.05471552,0.029135372,0.046189565,-0.017787458,-0.0113602625,0.02577924,0.014068955,0.011800349,-0.004734424,0.08462166,0.0040090936,0.013004728,-0.026264291,-0.065527506,0.0034034937,0.013515393,0.04000647,0.035074368,-0.020092275,-0.009371148,-0.031677242,-0.010545423,-0.052238043,-0.037475742,0.0083001,-0.012498302,0.034439083,-0.021787237,0.047619965,-0.031893063,-0.030921146,-0.016119944,0.07163714,0.04854329,-0.002621946,-0.06895207,-0.017855553,0.003312645,0.03174983,0.004221272,-0.0066039707,0.0029890647,-0.035022043,-0.03560101,0.014598133,-0.03186354,0.049841706,-0.02115995,0.034823515,0.037694603,-0.045126297,0.019336672,0.037612494,0.041802343,-0.014569527,-0.018208865,0.0019779676,0.046917737,0.05185747,-0.021575354,-0.0123156095,-0.0008710227,-0.022134079,-0.0030441687,0.051172197,0.005957979,-0.03869261,0.037148736,-0.022392953,-0.06939586,0.046375643,-0.025512777,0.03788255,-0.024295885,0.034782216,-0.01355021,0.008351577,0.011052367,0.0021960954,-0.022937525,0.014575919,0.015287546,0.008218017,0.049680762,-0.08533381,0.04455339,-0.030757865,-0.040043287,0.014774183,-0.049676158,-0.06748822,0.017032197,-0.06415572,0.021756196,0.033258315,-0.004225928,-0.030219238,0.022890367,-0.01828903,0.079105236,-0.0593376,0.037670113,-0.007581691,0.011928132,0.08692321,0.02357058,0.0075261365,0.002109809,-0.012535974,-0.042037163,0.02517994,0.06948682,-0.0013807704,-0.013325585,0.056781057,0.008159573,-0.000506994,0.004361048,-0.06925958,-0.034641843,-0.03460324,-0.0030125966,0.015640358,-0.021184925,0.007987393,0.0023263986,0.01385654,-0.0016738342,-0.003957585,0.029524714,0.037689947,0.013804978,-0.0050343247,0.042909227,-0.01127472,0.05025937,0.013359746,0.058931436,0.008064267,-0.0041722273,0.04072638,0.0048062154,0.0028679722,0.0023503636,-0.020164886,-0.009036824,-0.042728998,-0.007218671,0.020111607,-0.0004989124,-0.040786643,0.029671598,0.02386799,0.02816387,-0.036224354,-0.059488222,-0.025367513,-0.016877104,-0.053173434,-0.035490017,0.018414622,-0.01739525,0.035007965,-0.030408742,0.06783505,-0.10094452,0.014182242,-0.037014924,0.02101232,-0.024178814,-0.028125994,-0.019370858,-0.02388432,0.020452533,-0.03856407,0.08900491,0.041135825,-0.034720577,0.034762464,-0.019027082,-0.0018442164,-0.027821524,0.0024071415,-0.015030522,-0.008951316,-0.007950198,-0.012944897,0.071394496,-0.033886682,0.022771934,0.025531722,-0.005390305,-0.083647266,0.0072775288,-0.014099337,0.020239301,-0.007950094,0.005788729,0.03166493,0.060361687,-0.03934841,-0.014782024,-0.051396493,-0.003153338,-0.020284286,0.028431354,0.04349542,-0.008028606,-0.038051862,-0.04033184,0.029567141,-0.011322091,0.019099748,0.052976735,0.05449346,0.06692596,0.036464807,-0.021635849,-0.07355314,0.00082143326,-0.045650657,0.020973757,-0.022706976,0.029239764,-0.025202436,-0.026794646,-0.014838639,0.0031743662,-0.025040485,-0.00091571425,0.015269562,0.007909786,-0.075972006,-0.04517731,0.006390272,0.00264214,-0.02292083,-0.018791277,0.08536262,0.01242431,-0.041019887,-0.045754988,-0.07766383,-0.024971955,0.0037871725,-0.013073454,-0.0053438493,0.021623073,-0.03239563,0.002708641,0.060999885,0.004275528,-0.026553111,-0.033383783,0.042750843,0.011617,0.046765063,0.024836477,-0.015271881,-0.069560684,0.025975425,-0.008427545,-0.060823817,0.061677862,-0.024341213,-0.025811754,0.00764352,0.02040075,-0.01830108,0.023055134,-0.014768311,0.00962709,0.02284038,-0.013199537,-0.02937192,-0.0627869,-0.041260365,-0.024182625,-0.008473767,-0.04021639,0.049804572,0.0010215426,-0.0042563756,0.031807143,-0.00751935,0.009098976,-0.00362631,0.009439428,0.060250476,-0.03098465,0.049239334,0.008224342,0.0073805093,0.07360289,-0.0186386,-0.0031795665,0.021344885,0.00076889765,0.016823418,0.033693407,0.011898557,-0.030413,-0.049582146,0.033635933,0.026737884,0.010862897,0.034158017,-0.008369851,-0.03630001,-0.06381987,-0.030209487,-0.003483376,0.034457117,0.023257934,0.0131396605,-0.049610704,0.042195253,-0.015325402,0.038870603,-0.01598192,0.02675036,-0.03495019,0.037609592]	Keywords: attention mechanism, multi-head mechanism, prototype, memory compression, low-rank self-attention\nKey Objects: Attention Mechanism, Multi-Head Mechanism, Prototype, Memory\nRefers to Images: ./images/a-survey-to-transformers/image_3.png\nHypothetical Questions:\n- How do prototype and memory compression techniques reduce the size of the attention matrix?\n- What is meant by the 'low-rank property' of self-attention, and how is it captured?\n- Why might it be beneficial to supplement standard attention with prior attention distributions?\n---\nSummary:\nSeveral approaches improve upon the standard attention mechanism in Transformers, including methods that reduce the size of the attention matrix and explore alternative multi-head mechanisms.\nOriginal Text:\nFig. 3. Taxonomy of Transformers  \n  \n- (3) Prototype and Memory Compression . This class of methods reduces the number of queries or key-value memory pairs to reduce the size of the attention matrix.\n- (4) Low-rank Self-Attention . This line of work capture the low-rank property of self-attention.\n- (5) Attention with Prior . The line of research explores supplementing or substituting standard attention with prior attention distributions.\n- (6) Improved Multi-Head Mechanism . The line of studies explores different alternative multi-head mechanisms.  \nWe will describe these attention variants at length in the rest of this section.\nContextualized Text:\nTo address the challenges of complexity and lack of structural prior in standard self-attention, several improvement strategies have been developed. These include prototype and memory compression to reduce the size of the attention matrix, low-rank self-attention to capture inherent low-rank properties, attention with prior distributions, and investigations into improved multi-head mechanisms. These variations will be examined in more detail throughout the remainder of this section.	{"tags": ["architecture", "NLP", "attention", "transformer"], "doc_id": "aabf053b-c6a3-4b0e-8c8c-c346ac31e0d6", "summary": "Several approaches improve upon the standard attention mechanism in Transformers, including methods that reduce the size of the attention matrix and explore alternative multi-head mechanisms.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["attention mechanism", "multi-head mechanism", "prototype", "memory compression", "low-rank self-attention"], "key_objects": ["Attention Mechanism", "Multi-Head Mechanism", "Prototype", "Memory"], "contextual_text": "To address the challenges of complexity and lack of structural prior in standard self-attention, several improvement strategies have been developed. These include prototype and memory compression to reduce the size of the attention matrix, low-rank self-attention to capture inherent low-rank properties, attention with prior distributions, and investigations into improved multi-head mechanisms. These variations will be examined in more detail throughout the remainder of this section.", "mentioned_images": ["./images/a-survey-to-transformers/image_3.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION"}, "hypothetical_questions": ["How do prototype and memory compression techniques reduce the size of the attention matrix?", "What is meant by the 'low-rank property' of self-attention, and how is it captured?", "Why might it be beneficial to supplement standard attention with prior attention distributions?"]}
2a23e58f-c063-4e5d-9ea4-50e9ca4f2875	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.013730172,-0.0032776424,0.0067031416,0.07135737,-0.019099776,0.030580075,0.015296561,0.054381877,0.021785695,-0.032855086,0.024606954,0.02905542,0.0149551965,-0.004391128,-0.023210824,0.027171329,0.016884547,0.05538501,-0.019949446,-0.014583891,0.018587995,0.023766993,-0.0018279998,-0.010763876,-0.028693408,0.047767103,0.026212284,-0.0042511066,-0.007379045,0.009757196,0.027879562,-0.046527103,0.016911376,0.005453964,0.013845554,-0.008309488,0.015927099,-0.06956833,-0.016909594,0.00028797486,-0.04657345,0.06269604,-0.065758124,-0.026558116,8.178165e-05,-0.048127662,-0.07204486,-0.020713354,0.029427718,-0.032006897,-0.02633253,0.018948076,-0.0045425375,-0.024184207,-0.017249674,-0.00030097307,0.0029807256,-0.04128875,0.0046803625,-0.11220923,-0.043752383,-0.00678202,-0.032082323,-0.009998467,0.046502206,0.004939832,0.041439906,0.0040687663,0.042679448,0.13714309,-0.0056059514,-0.0025861727,-0.016988767,-0.05704011,0.11132645,0.0555694,-0.028902682,-0.07529099,-0.057006393,-0.026063962,-0.05884227,0.09977957,-0.017241541,-0.061387055,0.088686705,-0.0021493086,-0.034837108,0.011190352,0.05257297,-0.042088617,-0.02910117,0.008072598,-0.0056588347,0.040296413,-0.016011078,-0.073746435,-0.00202388,-0.0014792247,0.053662583,-0.052459236,0.02683861,-0.010080998,0.06677342,0.09288895,0.01436689,-0.03681042,0.002585275,-0.03096558,-0.028958779,0.020362245,-0.038596306,-0.0004490279,-0.030692795,0.03299988,0.0041227825,0.026393732,-0.05489094,0.011754954,0.000523026,-0.036131587,0.010706094,-0.018924743,0.013130482,0.018172802,0.0743996,0.04462926,0.028704174,0.0028495805,-0.046658453,0.03827555,0.031747855,-0.0061623575,-0.051533543,0.0147370985,-0.021135863,0.026336672,0.058237728,-0.012706495,-0.00021609786,0.032334376,0.020899,-0.042024877,-0.04430711,0.06451639,-0.0017363823,0.021791106,-0.055447284,0.040294304,-0.028929899,0.0143482005,0.047004588,0.004425652,0.03584988,-0.015148662,-0.01809187,0.0029284817,-0.015699416,-0.048553422,0.0024180396,-0.07189727,-0.033806905,-0.06630122,0.018848801,0.03903295,0.04860086,0.123082966,-0.0042433087,-0.0032440359,0.021367501,0.007618039,-0.032675248,0.010084333,-0.022763286,-0.06659358,0.004218255,0.0053563556,0.014792843,-0.029380765,0.0061040875,-0.021775862,0.0119238915,0.016699461,-0.0256522,0.027850391,-0.03128138,0.03395295,-0.0035366872,0.05640047,-0.010715799,-0.03850817,0.04419677,-0.02449546,0.0053482107,-0.035639346,0.014054605,0.055907633,0.043579426,-0.03751425,-0.0077378075,-0.04581005,0.013555852,-0.0023917642,-0.004092072,-0.033455852,0.022331541,0.008690955,-0.0095385825,-0.032324042,-0.018418113,0.006606543,-0.021862859,0.008970412,-0.03686869,0.069258176,0.020791473,-0.003218121,-0.005211686,0.01549703,0.04164284,-0.0027436346,-0.061014492,0.020904599,-0.013780506,0.020551097,-0.025864854,0.038456813,-0.04886422,0.00010034663,0.027352666,-0.040581316,0.023519728,0.034984287,-0.032947022,0.050806765,-0.040107697,0.019300546,-0.0068632513,0.00986289,-0.034226216,0.039386604,0.05197944,0.058498763,-0.017642347,0.09482998,-0.010014943,0.0098032635,-0.03321076,0.03230181,0.042172216,0.03922175,-0.008777857,0.062157385,0.027500458,0.0039858297,0.005085377,-0.061223358,0.029264499,0.016756238,0.028427342,-0.013252828,0.022048302,-0.010499419,-0.005961189,0.011070651,-0.009428979,-0.0061731585,0.027116789,0.020417297,-0.052682426,0.00058639795,0.02443551,0.039871704,0.015870033,-0.031121707,-0.028275305,0.02672514,-0.00543906,-0.020548588,0.034879092,-0.03371698,-0.026912084,-0.00669862,0.006447534,-0.012828903,0.028907089,0.001107636,0.00856054,-0.028825495,-0.06936392,0.00070745905,0.010851554,-0.06281324,0.022759395,0.0017226968,-0.028950827,0.0065415706,-0.0089208,-0.014827672,-0.02933014,0.00097307155,0.08882208,-0.019938221,-0.012216801,0.019586073,0.0052777394,0.044884935,-0.057534575,0.011699069,0.018030634,0.007440198,-0.0008554629,-0.0029062435,-0.022030978,-0.0038426807,0.048868097,0.07020457,0.07999261,-0.031521935,-0.02286609,0.02913227,0.0401741,0.014398673,0.017468885,0.023609694,0.034765467,-0.012528417,0.026084645,0.014433502,-0.11167287,0.06835363,-0.07815207,0.04401777,0.06099959,0.021254057,-0.003324714,-0.015868185,0.00084903376,0.0130622545,0.015343148,-0.00987401,-0.00096068194,-0.07965481,-0.00023454726,-0.009326918,-0.027510203,0.07225525,-0.012114413,0.004013438,0.013169347,0.017949238,0.009420629,0.022329438,-0.0011334736,0.031426635,0.0026758807,-0.03561944,0.022213949,0.021966638,0.010272883,0.0056282366,0.08658729,-0.09378284,0.08081687,-0.0070665265,-0.0036504334,-0.0060979496,-0.03694104,0.0058052833,0.04644046,-0.03599543,-0.011924938,-0.026710378,0.06895578,-0.043001197,-0.026640236,0.032128755,0.057457272,0.04286106,-0.02063066,-0.026266553,-0.0066259005,0.0026051044,-0.023578942,-0.049622215,0.033076525,0.030527862,0.015089129,0.09855779,-0.010649265,0.04193919,-0.014079732,0.010798236,0.03675266,-0.0020372556,-0.032125954,-0.009531116,0.0017712313,0.03334546,0.019015737,-0.0034462886,0.023826975,0.026286665,-0.058441393,0.026028877,0.045694854,0.022092108,-0.1020159,0.03466134,0.030392561,0.01961583,0.02596863,-0.033828884,-0.032509256,0.016964203,0.0053089713,-0.028022934,-0.041265134,0.0096876,-0.05443387,-0.006111226,-0.02091745,0.03250595,0.029966034,0.07696237,0.007864174,0.03823971,-0.033424266,0.045056596,0.0044726986,-0.057435628,0.0287285,-0.019646544,-0.031133644,0.030525334,0.02094251,-0.00394209,-0.07494933,0.06100682,-0.05599682,0.011290192,0.03895104,-0.035068873,0.011138727,-0.027439542,-0.01603188,-0.03425829,0.016798273,0.04735259,-0.027094545,-0.030929156,0.042780135,-0.0053961473,0.009248727,-0.035495285,0.0007873305,-0.0090999035,-0.004185183,-0.001597487,-0.068802044,0.009974533,0.014229087,0.0074789138,0.020081013,0.0027008778,0.023043409,-0.036518887,-0.030757938,-0.039270047,-0.03276071,0.020823078,0.0084153535,0.052985843,-0.00712058,0.05033793,-0.029797493,-0.04392598,0.0048581064,0.021555008,0.03900105,-0.014757333,-0.07022684,-0.022741418,0.029475173,0.014905629,0.0154916085,-0.049408786,-0.029033903,-0.0055055325,-0.024310537,0.024930203,0.011260925,0.073968664,0.012208558,0.022118406,0.039450664,-0.031089673,0.028915212,0.024795184,0.010443894,-0.0120538715,-0.03495447,-0.028053164,0.06819616,0.064011686,-0.027533269,0.0075007374,-0.022004236,0.018022908,-0.050592918,0.06530305,0.016030736,-0.021071995,0.016059248,-0.01106788,-0.061620872,0.006310099,-0.01396764,0.03094793,0.018889122,0.059738867,0.010215344,0.01946455,0.026427608,-0.011013822,0.046655573,-0.01608741,-0.003135745,0.016421668,0.023278628,-0.08760172,0.02224387,-0.02828752,-0.011628778,0.0029192462,-0.052967824,-0.06715239,0.021734258,-0.057144854,0.04266782,-0.014118073,-0.029117068,-0.012929174,0.030421752,-0.03724343,0.09279338,-0.061733417,0.00017308055,0.019958505,-0.01918024,0.10859213,0.03496028,-0.00013514153,-0.015039849,-0.00024241029,-0.020889577,0.0605981,0.097990885,0.009510626,-0.0074446965,0.079497926,-0.010813613,-0.025110694,-0.03606441,-0.05396722,-0.016533047,-0.033025358,-0.01982715,0.006838727,-0.048179846,-0.018604884,0.027658701,-0.014518139,-0.015107769,0.006207167,-0.016174586,0.0772124,0.037995633,0.011601163,0.028751576,-0.03094031,0.029196292,0.016498579,0.0419379,0.02093676,0.02227411,0.022820815,-0.0030842072,-0.004569039,-0.03416056,-0.002192461,-0.05199443,-0.018825332,-0.0034781634,0.018796856,-0.011135073,-0.030446848,0.020318054,0.02564434,0.029147403,-0.032693155,-0.048383072,-0.027940737,-0.025969004,-0.040173173,0.014024086,-0.0061903857,0.0056150234,0.02598414,-0.00018466245,0.06634323,-0.06474245,-0.07083264,-0.0140423495,0.044121113,-0.0009010121,-0.03620992,-0.010850336,-0.011583824,-0.007309327,-0.05295175,0.06074516,0.06326293,-0.030344265,-0.014741294,0.01653224,0.030173562,-0.012277468,-0.013134426,-0.012709472,-0.007354103,-0.021082073,-0.015498204,0.07799421,-0.022760883,0.009765756,0.014299823,-0.0034568082,-0.08997626,-0.028672505,0.024598947,0.048011776,-0.00075101724,-0.0032104293,0.0024951212,0.046191096,-0.0437644,-0.023168832,-0.041642327,0.020128662,0.016017986,0.036561597,0.045565233,0.010047312,-0.03948918,-0.0080326535,0.017244905,-0.047772296,0.01652277,0.039800726,0.009050538,0.08890726,0.0463255,-0.05996469,-0.056269933,-0.019069863,-0.04073443,0.010377301,-0.03541905,0.04470483,-0.04882779,-0.013716698,-0.027154088,0.016245127,0.0055294433,0.011951145,0.03412822,0.015094321,-0.04738631,-0.052947134,-0.00018867117,-0.018883737,-0.0023605658,0.0064322096,0.06093534,0.022054391,-0.04048507,-0.050519705,-0.059995215,-0.02360651,-0.019512286,-0.004080375,-0.0067038997,0.019774444,-0.0019625572,-0.005176753,0.05109444,0.04456049,-0.029863048,-0.041149475,0.055210065,0.063704245,0.03838936,0.013649395,-0.00077694777,-0.098794155,-0.010663891,-0.018678967,-0.06783649,0.015963119,-0.012905772,-0.022921192,-0.027972609,0.005756055,-0.0066443128,0.019329268,0.022350783,0.029471215,0.016930804,-0.007638144,-0.018582249,-0.06313087,-0.045192875,-0.012363203,-0.030777767,-0.008463779,0.042653717,-0.0084772995,-0.0143267205,-0.004104215,-0.0048668818,0.011400174,0.011827482,0.012571081,0.052949313,-0.011874729,0.0049715834,0.0054514683,-8.136447e-05,0.064076334,-0.023470195,0.0129004605,-0.0039020788,0.022068253,-0.012520503,0.025250047,-0.036836326,-0.053418968,-0.0474265,0.02076562,0.023614187,0.022813251,0.010416448,-0.009372197,-0.03925256,-0.067373544,-0.043190546,0.0061640595,-0.003190184,0.019435685,0.0126828,-0.044940438,0.024788791,-0.015891287,0.07506619,-0.009964978,0.03145373,-0.017834596,-0.011055239]	Keywords: self-attention, position-wise FFN, Transformer, efficiency, long-sequence compatibility\nKey Objects: self-attention, position-wise FFN, efficiency\nRefers to Images: None\nHypothetical Questions:\n- Why is self-attention a bottleneck in long-sequence scenarios?\n- What are the key areas of focus when optimizing Transformers for efficiency?\n- How do researchers aim to make self-attention more compatible with long sequences?\n---\nSummary:\nEfforts to improve Transformer efficiency often focus on enhancing the long-sequence compatibility of self-attention and optimizing the computation and parameter efficiency of position-wise FFN for typical use cases.\nOriginal Text:\nof self-attention, as well as the computation and parameter efficiency of position-wise FFN for ordinary settings.\nContextualized Text:\nTo make Transformers more efficient, especially when dealing with long sequences, researchers focus on improving both the long-sequence compatibility of self-attention and the computation and parameter efficiency of position-wise FFN for typical usage scenarios.	{"tags": ["architecture", "optimization", "NLP"], "doc_id": "2a23e58f-c063-4e5d-9ea4-50e9ca4f2875", "summary": "Efforts to improve Transformer efficiency often focus on enhancing the long-sequence compatibility of self-attention and optimizing the computation and parameter efficiency of position-wise FFN for typical use cases.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["self-attention", "position-wise FFN", "Transformer", "efficiency", "long-sequence compatibility"], "key_objects": ["self-attention", "position-wise FFN", "efficiency"], "contextual_text": "To make Transformers more efficient, especially when dealing with long sequences, researchers focus on improving both the long-sequence compatibility of self-attention and the computation and parameter efficiency of position-wise FFN for typical usage scenarios.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.3 Model Analysis"}, "hypothetical_questions": ["Why is self-attention a bottleneck in long-sequence scenarios?", "What are the key areas of focus when optimizing Transformers for efficiency?", "How do researchers aim to make self-attention more compatible with long sequences?"]}
2d2a6127-47b2-4284-835d-fe49ff42317b	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.011396486,-0.013986099,0.0134211965,0.04901428,-0.038794152,0.043043476,-0.0032391627,0.07273126,0.02440231,-0.024819592,0.01002824,0.029895576,-0.011097413,0.0028098274,-0.009859881,0.025668358,0.013854802,0.08999534,-0.02632928,-0.037801422,0.024499174,0.046228938,0.016137442,-0.02167377,-0.020804316,0.033130694,0.054832116,-0.04201546,-0.03317143,0.0038545693,0.0041898983,-0.028707853,0.029503528,-0.0025856616,0.04106357,-0.03945842,0.00032591794,-0.055769503,-0.018896475,-0.020799888,-0.054123882,0.038525324,-0.059941392,-0.038746037,0.015508087,-0.050087906,-0.11536068,-0.023846688,0.009547405,-0.037391916,-0.021895759,0.01009592,-0.013102944,-0.039413054,-0.0058771926,0.0032869591,0.0036306481,-0.039722342,-0.031191798,-0.0905562,-0.045878526,-0.00677438,-0.032610495,-0.015647031,0.001643701,-0.00028399302,0.040875796,0.021159658,0.047167197,0.11145894,-0.040103417,0.00310966,-0.024426414,-0.03134723,0.12351006,0.018685456,-0.029476363,-0.04197591,-0.05551747,0.0060080835,-0.046275217,0.10036568,-0.03531792,-0.053205077,0.08706163,0.012111282,-0.017815886,-0.019942794,0.059573215,-0.021373995,-0.015382751,-0.004227117,-0.003509586,0.0544403,-0.010960731,-0.08986408,0.0036202907,-0.0020212724,0.023149244,-0.068761885,0.035338487,-0.0014754889,0.06556153,0.105360135,0.05144602,-0.050351273,-0.021496795,-0.018476853,0.016455611,0.01280389,-0.039253954,-0.0060740425,-0.028762963,0.040959135,0.003366862,0.027358096,-0.033913758,0.030145725,0.0490271,-0.022982273,0.006886423,0.023489231,-0.004769947,0.033895146,0.04550156,0.045044634,-0.0015362488,-0.0467225,-0.03478849,-0.008074484,0.024206938,-0.0031131422,-0.020050066,0.005478356,-0.020538399,0.008101289,0.08714178,0.008694112,-0.01989614,0.01859675,0.018250445,-0.06792008,-0.039679784,0.04580624,-0.013377574,0.056287266,-0.039503016,0.04108686,-0.028138325,0.045154274,0.02521449,-0.004052985,0.033474218,-0.015678925,-0.012091918,0.006246941,0.011244741,-0.04654008,-0.022011885,-0.06287137,-0.03409696,-0.08160062,0.025103942,0.029501203,0.053351585,0.09357009,0.008395112,-0.0050239507,0.009220552,0.0066021504,-0.032910243,0.0070730094,-0.027084382,-0.04962194,0.023266822,-0.002989422,-0.0077080894,-0.021553515,0.019328838,-0.047459446,0.0134503925,0.030641185,-0.0060829227,0.033395026,-0.06956539,0.03535072,0.009421734,0.04046323,-0.041126795,-0.058736805,0.014726531,-0.024192352,0.0026353835,-0.039448056,0.034868743,0.06187303,0.041755985,-0.08998758,-0.0023920608,-0.05867898,-0.024803381,-0.014379369,-0.008877205,-0.032353695,0.029944109,0.0008827026,-0.014657149,0.008955942,-0.01735648,-0.00016497406,-0.007845778,0.011847624,-0.029017795,0.044294342,0.019828418,-0.018905507,0.0033560686,0.013765261,0.01964245,-0.033617757,-0.04010784,-0.043489017,-0.031272493,0.018044014,-0.03561012,0.011945746,-0.042486146,0.021885674,0.015371815,-0.0030323153,0.039219093,0.0455201,-0.054238543,0.025025614,-0.03036798,0.04633169,0.019135488,0.016029757,-0.0051175873,0.00782072,0.014822871,0.029003274,-0.0069362517,0.09443919,0.015392548,0.014589228,-0.008222773,0.06523762,0.012478107,0.04167655,-0.0016345627,0.069664516,0.0010503134,0.021187503,0.014531119,-0.047654886,9.493647e-05,0.0029327678,0.00934756,0.012105179,0.016070375,-0.013104998,-0.0046100137,0.035736132,-0.019104723,-0.033212468,-0.023479596,0.04020292,-0.06352019,0.018458681,-4.326305e-05,0.06321065,-0.015507961,-0.031820714,-0.0015255801,0.0046132486,0.017885553,-0.030215679,0.05075477,-0.038653716,-0.036451146,0.0050636237,0.008691912,-0.012770937,0.02031304,0.0004992668,0.034075957,-0.023024663,-0.034296907,-0.009384868,-0.01778728,-0.06812206,0.025691018,-0.04014762,-0.032439567,0.025163952,-0.010986354,-0.016380215,0.032245997,-0.0185765,0.13314568,0.015228965,-0.013189255,0.036189932,0.018006848,0.048610494,-0.078629114,0.025771212,0.04609318,0.008536467,0.010132294,-0.024865415,-0.0387487,-0.0014162696,0.05125452,0.075329185,0.046724416,-0.0313981,-0.031563986,0.028839415,0.042910162,0.01990122,-0.0022793547,0.060989834,0.02378776,-0.034427784,-0.02945383,-0.043052994,-0.0767208,0.059518464,-0.08647501,0.04863082,0.061732378,-0.017900644,0.020849649,-0.013845498,0.009920199,0.034817953,-0.016278066,0.0009480658,-0.01636281,-0.053934965,-0.017290711,-0.015370813,-0.01095504,0.065770246,0.011846626,0.0031965796,0.0027687352,0.0020678323,0.033982508,0.017814916,-0.0007756836,0.025878971,-0.02796982,-0.04001877,-0.012885865,-0.009820999,0.019334318,-0.017196579,0.0494549,-0.049607772,0.059438992,-0.030202162,0.016259097,-0.026800627,-0.0651594,0.01856798,0.03760353,-0.03713528,-0.0046520657,-0.030281588,0.06575477,-0.057771306,-0.0395678,0.061113678,0.015279155,0.054669168,-0.00051904185,-0.019863883,0.013345375,-0.004736458,-0.056133095,-0.062901,0.020137722,0.0049864394,0.01490992,0.10472067,-0.004483088,0.04505553,-0.01951966,-0.002645504,-0.002413602,-0.012636132,-0.027571421,0.02749389,-0.00035445296,0.036283568,0.009492479,-0.025638228,0.027698154,7.661069e-06,-0.059647195,0.01606028,0.06996262,0.022024235,-0.07368482,0.044756874,0.028411517,0.020611558,0.032648973,-0.021600759,0.0046721324,-0.039489362,0.021875052,-0.0027051817,-0.014970404,0.019971857,-0.04287845,0.007076019,-0.0214712,0.04465353,0.02506806,0.057202023,0.013026507,-0.002529201,-0.020951493,0.03250615,0.028673679,-0.041501906,0.00824674,-0.027509926,-0.0020975582,0.06329934,-0.00034944795,-0.026756454,-0.06237471,0.06548249,-0.04196374,-0.007849158,0.05862125,-0.024146112,-0.013138297,-0.03991877,-0.005585758,-0.03552827,0.04356335,0.028903339,-0.027089769,-0.018519785,0.04370972,-0.031130532,0.011505472,-0.009276842,0.029498687,0.021733845,0.021105636,-0.011428335,-0.025592035,-0.008114505,0.012312693,0.011199508,0.008754305,-0.014685055,0.033749282,-0.009817826,-0.010321201,-0.029973347,-0.041490875,0.003978312,-0.0025777104,-0.0031234948,-0.06354109,0.03289551,-0.0059746443,-0.04075646,-0.04007457,0.03425819,0.02887585,0.013758712,-0.04607682,-0.040397972,0.030210538,0.024107454,0.018943433,0.02021112,0.0016965643,-0.0024109718,-0.02938561,0.031472806,-0.013189675,0.057965953,0.019694164,-0.0028672079,0.037429113,-0.051265985,-0.026858417,0.00423267,0.020140337,-0.026567144,-0.030019822,-0.025563268,0.057924055,0.062395923,-0.02829311,0.012525324,0.012206076,0.019977298,-0.034768376,0.06778285,0.026388777,-0.049940918,-0.010469397,0.013716679,-0.09830418,0.016235,-0.011860353,0.009628323,-0.0498679,0.0321198,0.020471508,0.046241216,0.015296679,-0.033408385,0.013715232,0.0031610534,0.009887325,-0.009550637,0.036633603,-0.077965036,-0.02120592,-0.05484398,-0.004511269,0.002092369,-0.034874428,-0.051785875,0.004601138,-0.0632959,0.030670073,0.013061752,0.013597181,-0.05138664,0.013234915,-0.020596096,0.056086265,-0.06170449,-0.0066327914,0.04416403,0.00024151686,0.049569704,0.002186305,0.048539482,-0.004181985,0.0026634056,-0.0039392575,0.046168726,0.09368697,0.037965678,0.022402572,0.059856307,-0.023529781,-0.014433968,-0.056176834,-0.06726385,-0.013309973,-0.020752499,-0.03126801,0.0024293235,-0.055022374,0.011873314,0.012152703,-0.015155049,-0.006539305,0.01087671,0.0006715095,0.050160877,0.036072295,0.037323758,0.022716895,-0.04842274,0.00870248,0.044438075,0.017582223,0.007779265,0.030057237,0.014904138,0.031894863,0.023002231,0.0018257771,-0.017415632,-0.037644994,-0.041889057,0.0007394621,0.01227268,-0.0048909206,-0.015151405,0.032042958,0.08257193,0.022765212,-0.015766166,-0.05933364,-0.0005566181,-0.026411045,-0.026818378,0.019021483,0.006778877,0.0006491402,0.046903506,-0.030634763,0.081642844,-0.071192704,-0.044788867,-0.025989594,0.055018805,0.009193449,-0.021704597,-0.030244758,-0.009202471,-0.015435718,-0.044747047,0.06295752,0.042806264,-0.0025402594,0.0036915261,0.01661128,-0.006249379,-0.03148479,-0.025083454,-0.04173692,0.0036339494,-0.013458185,-0.014473405,0.040275328,-0.024761606,0.029772792,0.07438514,-0.03633773,-0.046869416,0.003114159,-0.01020339,0.04179398,-0.01417329,0.0013298696,0.018550282,0.043832578,-0.044490103,-0.014484577,-0.071066275,0.004834817,-0.023896161,0.0005408805,0.033218544,0.04404925,-0.016554557,-0.00776527,0.025408588,-0.053301778,-0.013075014,0.035935026,0.028681137,0.06540314,0.02113645,-0.04304746,-0.06305231,-0.022215499,-0.044930547,-0.0007435084,-0.052359995,0.07005056,-0.021724941,-0.0072443755,-0.02128925,-0.00166574,0.021490846,-0.0198808,0.0321533,0.019977773,-0.047569364,-0.025154721,0.0020077012,-0.013730798,-0.012055331,0.0052325907,0.070841834,0.026222289,-0.059820294,-0.029221602,-0.07315304,-0.030266708,0.028961256,-0.015844652,0.0014321712,0.026227204,-0.0037269718,-0.037188098,0.03781803,0.02399764,-0.025974989,-0.066022076,0.027516635,0.029323254,0.028670063,-0.004804578,-0.027525937,-0.13499232,0.0058278847,0.019892633,-0.058966458,0.058063827,-0.020428726,-0.026667777,-0.013836187,0.025352301,-0.034384627,0.03096572,-6.1172155e-05,0.05248922,0.03616201,0.00034953648,-0.0043862425,-0.020815356,-0.0330698,-0.00623543,-0.014335607,-0.049694993,0.032825638,0.007038419,-0.0139624635,-0.016925255,-0.009695867,-0.002568668,0.0038271751,0.0017779273,0.03301242,-0.047483668,0.019386759,-0.026883896,-0.0035607351,0.04215344,0.013494501,0.0053261993,0.020572014,0.006675379,-0.031023055,0.031575985,-0.018292606,-0.022399943,-0.04174627,0.05141188,0.0549475,0.01164573,0.012951671,0.011688411,-0.032781083,-0.03769145,-0.009239067,0.018022714,0.021952197,0.025307752,0.03754572,-0.045544427,0.01712995,-0.052015588,0.036112107,0.009244483,0.029671622,-0.05136396,0.006707797]	Keywords: self-attention, variable-length inputs, pairwise relations, Transformer\nKey Objects: Self-Attention, Inputs\nRefers to Images: None\nHypothetical Questions:\n- How does the dynamic generation of weights in self-attention contribute to handling variable-length inputs?\n- What does it mean for self-attention to have the same maximum path length as fully connected layers?\n- In what ways is self-attention more efficient than fully connected layers in terms of parameter usage?\n---\nSummary:\nSelf-attention, a key component of the Transformer architecture, functions as a flexible layer where weights are dynamically generated based on pairwise relationships within the inputs, offering advantages in handling variable-length inputs.\nOriginal Text:\n### 2.4 Comparing Transformer to Other Network Types  \n2.4.1 Analysis of Self-Attention . As a central piece of Transformer, self-attention comes with a flexible mechanism to deal with variable-length inputs. It can be understood as a fully connected layer where the weights are dynamically generated from pairwise relations from inputs. Table 2 compares the complexity, sequential operations, and maximum path length 4 of self-attention with three commonly used layer types. We summarize the advantages of self-attention as follows:  \n- (1) It has the same maximum path length as fully connected layers, making it suitable for long-range dependencies modeling. Compared to fully connected layers, it is more parameterefficient and more flexible in handling variable-length inputs.\nContextualized Text:\nWithin the Transformer model, self-attention serves as a flexible mechanism for processing variable-length inputs. It can be conceptualized as a fully connected layer where the weights are dynamically generated, based on pairwise relationships between the inputs. This allows for adaptability and efficiency in handling sequences of varying lengths.	{"tags": ["architecture", "NLP", "transformer", "self-attention"], "doc_id": "2d2a6127-47b2-4284-835d-fe49ff42317b", "summary": "Self-attention, a key component of the Transformer architecture, functions as a flexible layer where weights are dynamically generated based on pairwise relationships within the inputs, offering advantages in handling variable-length inputs.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["self-attention", "variable-length inputs", "pairwise relations", "Transformer"], "key_objects": ["Self-Attention", "Inputs"], "contextual_text": "Within the Transformer model, self-attention serves as a flexible mechanism for processing variable-length inputs. It can be conceptualized as a fully connected layer where the weights are dynamically generated, based on pairwise relationships between the inputs. This allows for adaptability and efficiency in handling sequences of varying lengths.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.4 Comparing Transformer to Other Network Types"}, "hypothetical_questions": ["How does the dynamic generation of weights in self-attention contribute to handling variable-length inputs?", "What does it mean for self-attention to have the same maximum path length as fully connected layers?", "In what ways is self-attention more efficient than fully connected layers in terms of parameter usage?"]}
584f9d5d-3b55-42f5-bfc7-39ac2c43e164	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02145658,0.02619254,0.023186268,0.02400749,-0.04710548,0.057505142,-0.0014751834,0.028923677,0.019432733,-0.010395791,0.01590313,0.026635151,-0.0071791774,-0.008574218,0.00289024,-0.026415126,0.0027746914,0.059568286,-0.003229661,-0.026477428,0.092615746,0.010673993,-0.0062454212,-0.0011481645,0.007102319,0.050708555,0.035339657,-0.017823476,0.009015783,0.027324611,0.01909123,-0.028530091,0.019104663,0.021530235,0.035732508,-0.042734206,0.03152388,-0.05153426,-0.010932983,-0.033747062,-0.028547326,0.06685981,-0.06374078,-0.045490295,0.00998281,-0.049346663,-0.097949415,-0.026945813,0.0013783296,-0.010381541,-0.023698168,-0.028308792,0.009831411,-0.05186454,-0.048123904,0.012057427,0.0046882145,-0.036925554,0.02310511,-0.0807341,-0.037491884,0.019120961,-0.034719836,0.00861242,0.029850915,-0.0262154,0.0076707816,0.049349792,0.050743923,0.12669651,-0.04776125,0.015202389,-0.0509865,-0.053785063,0.10665369,0.038500015,-0.050500356,0.0047800806,-0.043128382,-0.0050449637,-0.018959202,0.074284494,-0.020460369,-0.0017977286,0.062655814,0.006035648,-0.03690269,0.033862714,0.017302861,0.009809211,0.009826998,0.019055713,0.030443901,0.075324975,-0.027356723,-0.04639169,-0.0037873632,-0.044713695,-0.028893748,-0.06278354,0.020658309,0.01546207,0.10646058,0.1464765,0.030682206,-0.030320644,0.0037106082,0.0027001947,0.023905016,0.03424287,-0.06806453,0.02427353,-0.041861404,0.036330424,-0.026171435,0.02740921,-0.06167231,-0.0004958405,0.023184963,-0.0324699,0.043933857,0.0026026573,-0.008193539,0.060519252,0.025768565,0.022028988,0.009532417,0.0036956267,-0.025981655,-0.013281069,0.036693413,-0.014732361,-0.032137834,0.036882345,-0.009832379,0.027632691,0.06622382,0.019361615,-0.0047423188,0.047147345,0.009945955,-0.032747585,-0.02766755,-0.0077568716,-0.0073753446,0.037681874,-0.049493082,0.08786302,-0.021375788,0.041532464,0.04283155,-0.016110724,0.070903786,0.0062809307,0.009861005,-0.0062831035,-0.008014035,-0.03703354,-0.027430728,-0.06369457,-0.046999168,-0.059134264,0.008091628,0.04627737,0.02469881,0.11162617,0.023564156,-0.04988957,0.006173839,0.050485127,-0.034097347,0.0087315645,0.016278904,-0.06426682,-0.009775063,0.007223213,0.024145177,-0.030564588,0.017692601,-0.0053553456,-0.0097493725,0.06915565,-0.025929114,0.061716493,-0.077928975,0.017829625,-0.0032388452,0.057640065,0.006367875,-0.07112097,-0.0057190727,-0.004843386,0.008994945,-0.034775518,0.06046376,0.041306384,0.020845544,-0.06360309,-0.011374629,-0.039503835,0.013009345,6.141662e-05,-0.030752638,-0.03390134,0.036441743,-0.018040746,-0.024877865,-0.0130624855,-0.006912516,-0.00479883,0.018674605,0.021646524,0.0027512934,0.028518172,0.04372381,0.008993949,-0.022490941,-0.009103197,0.03906745,0.0011482752,-0.009507853,-0.016796688,-0.035737734,0.048458908,-0.009952041,0.040381774,-0.015812607,0.061299328,0.043964595,-0.028017845,-0.021634655,0.050852373,-0.0496876,0.07047062,-0.05651259,-0.0066847983,0.02714183,0.020730514,0.0011267294,0.0051431153,0.03168987,0.05831494,-0.011231209,0.09082624,0.027695714,0.009354439,0.015175593,0.059179846,0.018288797,0.034755617,-0.024283906,0.050843295,0.030452017,-0.02338787,-0.022914024,-0.021975193,0.01923574,0.03294827,0.029201388,-0.024313565,-0.005989155,-0.00797134,0.0021447078,0.0121587515,0.019375393,-0.0377273,0.023130722,0.027365945,-0.045878846,-0.0017961301,0.020268776,0.029273117,0.010044916,-0.008375467,0.0022198341,0.018513657,0.020862322,-0.008830172,0.03953835,-0.011540901,0.03667263,-0.01289388,0.00043703394,-0.026972149,0.03358844,0.01173664,0.013459145,-0.01618931,-0.013967784,-0.015709756,-0.03228896,-0.02698944,0.039937794,-0.031855885,-0.0070501585,-0.052609503,0.011674864,-0.016048554,0.034123916,0.015287421,0.08050469,-0.019968217,0.017990492,0.040875085,0.021333426,0.0630829,-0.041354466,0.01516383,0.016440436,0.004753112,0.0021086135,-0.037511494,-0.015926946,0.010156038,0.03220585,0.05446349,0.06318074,-0.017613634,0.006137937,0.016282894,0.053573027,0.0024306034,0.0075895423,0.0513631,0.020389533,-0.033082817,0.02946363,-0.021962669,-0.09948205,0.049847666,-0.07191342,0.03996167,0.046070743,-0.012032627,0.023104578,-0.0071047503,0.015794659,0.012757317,-0.034261525,0.024907243,-0.046375155,-0.063695416,-0.0072474987,-0.020289715,-0.021602169,0.011395856,0.04959935,-0.030833676,0.0085451985,0.012280729,0.0074726823,0.010705466,-0.011609615,0.009128872,0.011532476,-0.010626473,0.0011893138,0.055661228,0.038247257,-0.017662024,0.07165953,-0.08524497,0.06825246,-0.012316391,0.0064574536,-0.026316684,-0.054045476,0.0059918393,0.07726001,-0.011540549,-0.0073575303,-0.024444811,0.029930105,-0.09641106,-0.019126844,0.040937252,0.03712925,0.022022711,-0.042673502,-0.017972508,0.008281588,-0.014277048,-0.007870686,-0.021663021,0.01329211,0.0213034,0.023858126,0.07276024,-0.034045797,0.042130094,-0.04639203,0.007951591,0.01548661,-0.026179066,-0.03647999,0.015158841,0.023395412,0.033126704,0.01836791,0.017426107,0.029746294,0.030652577,-0.05161136,-0.0039523435,0.05966247,-0.00069935044,-0.057523802,0.0045612906,0.045157798,0.0028049257,0.03377976,-0.021150667,0.024786867,-0.028308002,0.033522654,-0.041131735,-0.010737639,-0.012999185,-0.040865276,0.025314134,-0.014860148,0.03399031,0.008584141,0.055948935,0.011080283,-0.006260263,-0.05704333,0.022746043,0.01746381,-0.0399098,-0.017895838,-0.058267467,-0.023001513,0.04901884,-0.013602956,-0.019432964,-0.03558331,0.07378948,-0.032477636,-0.0088986475,0.06235347,-0.026304234,0.037919827,0.001110859,-0.02004309,-0.016463501,0.026404528,0.004215905,-0.047430407,-0.0207273,0.03350787,-0.0054733274,0.016579676,0.005968785,0.008617254,0.002699438,0.012770371,-0.02364462,-0.048750155,-0.0049409107,0.0111384895,0.018543778,0.012732722,-0.0077972375,0.002572668,-0.036777895,-0.00299417,-0.0050903936,-0.017331872,-0.033916697,0.011400923,0.020681731,-0.05716027,0.052817974,0.018671336,-0.023545833,0.0114876665,0.025100878,0.033123232,0.008859596,-0.053511277,-0.014441924,0.04360754,-0.0033677865,0.023020195,0.022344086,-0.007993093,0.016084902,-0.014053586,0.04817768,0.0010443538,-0.01540949,-0.016854644,0.0054093716,-0.0049343524,-0.030187234,-0.009206463,0.0020038951,0.040462155,0.0037383803,-0.076249376,-0.00043026457,0.060853682,0.08409644,-0.035725284,0.016104115,0.015168277,0.041523933,-0.00036962144,0.045561112,0.009531016,-0.02211253,0.0041247536,0.011406433,-0.010094328,0.039214484,0.025618525,0.0024517816,0.01152763,0.03731432,0.027135001,0.03348539,0.025163097,0.0133118555,-0.018253112,-0.01760148,-0.011516273,0.060190648,0.06096218,-0.12298629,0.0032112468,-0.057657454,-0.04210975,0.004227605,-0.0693746,-0.056014918,-0.023647722,-0.018525267,0.03862145,0.010966411,-0.009382344,-0.070309006,0.02584631,-0.06364206,0.05614142,-0.045527242,0.033059172,0.051298935,-0.014929458,0.059047986,0.021516126,0.010743962,-0.012229261,0.023259321,0.026819346,0.03225908,0.06376883,0.0330181,-0.0011979985,0.05865137,0.0014374848,-0.00031657887,-0.02294642,-0.030594133,-0.031762805,0.029159728,-0.01322432,-0.023241993,-0.022703497,0.053175595,-0.017386414,-0.008154045,0.0041128416,0.067653485,-0.011076436,0.05882527,0.06503922,0.02979074,0.025547488,-0.021690467,-0.0015399475,0.019724973,0.025818523,0.027286,0.023981636,-0.03190791,-0.02845024,0.021394063,0.022385232,-0.011085456,-0.04561488,-0.05564332,0.0036093104,0.005995947,-0.0029931806,-0.027675876,0.033449896,0.02551786,0.0047452664,0.015500361,-0.074341305,-0.028596187,-0.022806307,-0.016984023,0.0367486,0.024928434,-0.020634396,0.0438232,-0.037845597,0.048425786,-0.09818892,-0.06665554,-0.06501603,0.0042930082,-0.007188677,-0.006616636,-0.054385807,0.004267386,-0.015951464,-0.022538776,0.03770584,0.023061415,-0.024803711,0.0019709808,-0.011708075,0.010294655,0.02216888,-0.0047179637,-0.03354197,0.0042163725,0.010511778,-0.03358335,0.061572175,-0.02339663,-0.002300857,0.035503685,0.025633808,-0.05091739,-0.018801814,0.020212768,-0.016726501,0.0019391254,0.013646114,0.004026586,0.039186567,-0.10388966,0.0034602822,-0.07293582,-0.010407735,0.0036047467,0.033981644,0.06675369,0.013156294,-0.033288006,-0.030717866,0.012748327,-0.04626291,0.0154276,0.02584252,0.03552527,0.06094787,0.029453559,-0.05250206,-0.016705386,-0.003866119,-0.04485908,0.0138025675,-0.055939607,0.049849562,-0.008314009,-0.01775726,-0.032504402,-0.018479357,0.012283302,-0.0051595457,0.040086597,-0.022983536,0.00015153087,-0.034425236,0.022809569,0.007979857,-0.0037684657,-0.0042389235,0.0520532,-0.014638111,-0.043170426,-0.054662168,-0.04218086,-0.093967356,0.020731995,0.0011255251,-0.028677216,-0.008582285,0.030093731,-0.03674902,0.055714294,0.006281296,-0.06864011,-0.047327284,0.009699929,0.046535645,0.04961991,-0.00014229416,-0.014201011,-0.122623466,0.0115559045,-0.013917433,-0.06366826,0.018895969,0.019562557,0.006915146,-0.025885615,0.0169484,-0.002273135,0.0005303412,-0.015411711,0.030433174,-0.00070074346,0.015385448,-0.016326105,-0.052866984,-0.025851797,-0.028174901,-0.00955153,-0.0075301095,0.007892783,0.037305493,-0.019356871,-0.029776134,-0.019561825,-0.014111577,0.045378923,-0.04703425,0.03628186,-0.018944757,0.032392733,-0.03140886,0.0062542195,0.08818944,-0.024023894,-0.017366875,0.034466397,-0.0063144425,0.0003085681,0.014049319,-0.005810293,-0.050764892,-0.022715857,-0.011627455,0.05111111,0.0054273936,0.01955896,0.023538632,-0.05154111,-0.0427007,-0.02922257,-0.02311804,0.052461017,0.030716443,0.024971249,-0.032588456,0.01593358,-0.058924727,0.019085756,0.07926816,-0.010019372,-0.006484888,-0.010788073]	Keywords: receptive field, long-range dependencies, parallelization, convolutional layers, recurrent layers, self-attention\nKey Objects: receptive field, self-attention, long-range dependencies\nRefers to Images: None\nHypothetical Questions:\n- Why do convolutional layers typically require stacking many layers to achieve a global receptive field?\n- How does self-attentions constant path length contribute to its ability to model long-range dependencies?\n- In what ways does self-attention's architecture make it more amenable to parallelization compared to recurrent networks?\n---\nSummary:\nConvolutional layers require deep networks to achieve a global receptive field, whereas self-attention's constant maximum path length allows it to model long-range dependencies with fewer layers. Self-attention also outperforms recurrent layers due to its constant sequential operations and path length, enabling better parallelization and long-range modeling.\nOriginal Text:\n- (2) Due to the limited receptive field of convolutional layers, one typically needs to stack a deep network to have a global receptive field. On the other hand, the constant maximum path length enables self-attention to model long-range dependencies with a constant number of layers.\n- (3) The constant sequential operations and maximum path length make self-attention more parallelizable and better at long-range modeling than recurrent layers.  \nTable 2. Per-layer complexity, minimum number of sequential operations and maximum path lengths for different layer types. T is the sequence length, D is the representation dimension and K is the kernel size of convolutions [137].  \n| Layer Type      | Complexity           | Sequential Operations   |\n|-----------------|----------------------|-------------------------|\n| Per Layer       | (O'T$^{2}$ D)       | (O'1)                   |\n| Self-Attention  | (O'T$^{2}$ D)       | (O'1)                   |\nContextualized Text:\nConvolutional layers have a limited receptive field, meaning they need to be stacked in deep networks to capture global information. In contrast, self-attention's consistent maximum path length allows it to effectively model long-range dependencies using fewer layers. Additionally, self-attentions consistent sequential operations and maximum path length make it more parallelizable and better suited for long-range dependencies than recurrent layers.	{"tags": ["architecture", "comparison", "NLP", "transformer"], "doc_id": "584f9d5d-3b55-42f5-bfc7-39ac2c43e164", "summary": "Convolutional layers require deep networks to achieve a global receptive field, whereas self-attention's constant maximum path length allows it to model long-range dependencies with fewer layers. Self-attention also outperforms recurrent layers due to its constant sequential operations and path length, enabling better parallelization and long-range modeling.", "doc_type": "text", "entities": ["Transformer", "Convolutional Layers", "Recurrent Layers"], "keywords": ["receptive field", "long-range dependencies", "parallelization", "convolutional layers", "recurrent layers", "self-attention"], "key_objects": ["receptive field", "self-attention", "long-range dependencies"], "contextual_text": "Convolutional layers have a limited receptive field, meaning they need to be stacked in deep networks to capture global information. In contrast, self-attention's consistent maximum path length allows it to effectively model long-range dependencies using fewer layers. Additionally, self-attentions consistent sequential operations and maximum path length make it more parallelizable and better suited for long-range dependencies than recurrent layers.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.4 Comparing Transformer to Other Network Types"}, "hypothetical_questions": ["Why do convolutional layers typically require stacking many layers to achieve a global receptive field?", "How does self-attentions constant path length contribute to its ability to model long-range dependencies?", "In what ways does self-attention's architecture make it more amenable to parallelization compared to recurrent networks?"]}
e34e6ac8-b609-4905-b98c-397adfe988b7	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0117922025,-0.0050696773,-0.0019746765,0.022756385,-0.053417582,0.044811394,0.017071113,0.04844687,0.040131614,-0.0261315,-0.011754308,0.04043496,-0.00046407705,-0.009975026,0.006555475,0.0012006486,0.008636536,0.05456073,0.011123234,-0.034550816,0.060377732,0.021311587,-0.026348468,0.014110929,0.054772038,0.053349387,0.046227593,-0.043745704,-0.015754264,0.03476874,0.005783643,-0.028333172,0.033071067,0.044141576,0.017311629,-0.055119522,0.009613398,-0.04184238,-0.03586699,-0.023148702,-0.051915105,0.038962737,-0.06215616,-0.018608904,0.00200526,-0.028086703,-0.09201647,-0.053907853,-0.014473701,-0.024820432,-0.025848579,0.003919757,-0.0013444381,-0.043970067,-0.029463042,0.018698106,-0.023718907,-0.032836083,-0.013411649,-0.10050227,-0.03485985,0.018826379,-0.052626513,-0.0023331174,0.00019038064,-0.0102087315,0.026738344,0.06086792,0.03087807,0.10547761,0.003192935,0.012101131,-0.03631075,-0.053181484,0.15081158,0.03321879,-0.06569531,-0.014162631,-0.07701913,-0.013739072,-0.022450391,0.07344486,-0.05578774,-0.016697882,0.076522686,0.013119737,-0.024806334,-0.0067698807,0.060523264,-0.02384966,0.0247249,-0.004987246,-0.0035288332,0.087110676,0.0054817237,-0.08683953,0.019464273,-0.065574996,0.014664905,-0.013388598,0.021224283,0.0011366201,0.07698323,0.12434189,0.05685124,-0.02773059,-0.019538637,-0.016757278,0.021546995,-0.0054576313,-0.054608915,0.009273321,-0.033627782,0.051738374,-0.006684287,0.0053427964,-0.0344386,0.012077941,0.029480394,-0.0003362469,0.03904434,0.0011251908,-0.039507248,0.023679357,0.030501587,0.0153691005,-0.022309953,0.007356221,-0.008473612,0.0032181093,0.036056433,0.013527091,-0.055064324,0.03404745,0.0030880375,0.022819111,0.06277955,0.034015678,-0.013694304,0.012970882,-0.0013062871,-0.038084973,-0.04270222,-0.014826089,0.013708881,0.020028925,-0.048098266,0.034676857,-0.011145753,0.07697443,0.03752266,-0.027121985,0.050724957,0.012862151,-0.0056401337,-0.008943149,-0.0092287725,-0.038972672,-0.013317255,-0.052955806,-0.055430047,-0.053462513,-0.026354106,0.053466845,0.05215473,0.08217672,0.042773888,-0.008827032,-0.014502057,0.014868967,-0.038291205,-0.012094035,-0.0036125295,-0.062093247,-0.009757247,-0.0036344614,0.028179985,-0.013540759,0.0013676271,-0.012141023,0.0108057335,0.07543476,0.004795336,0.04181982,-0.038270343,0.037823007,-0.024963766,0.057294615,-0.025369817,-0.034800135,0.0016524551,-0.010283407,0.021952415,-0.016806407,0.04206905,0.034028027,0.01693817,-0.07290137,-0.0374053,-0.036164343,-0.007103105,-0.027309103,-0.008990928,-0.017883973,0.09171004,0.009056966,-0.030069876,0.010981266,-0.035742003,0.0024254948,0.024858894,0.031313207,-0.0064027472,0.017045697,0.020287955,-0.014172057,-0.023174265,0.005474078,0.03857052,0.00051827345,-0.03292798,-0.039647974,-0.022235146,0.005138881,-0.028893227,0.010234878,-0.059346378,0.063799724,0.056587826,-0.018759368,0.008473038,0.024520714,-0.041933384,0.07213676,-0.03312551,0.033530228,0.029593242,0.014906313,0.01277366,0.034637332,0.01717751,0.061089195,-0.022989545,0.1020163,0.035176888,-0.0026795184,0.014418876,0.0719601,0.024745284,0.023857897,-0.0022987719,0.045100283,0.031091519,0.019738548,-0.030555056,0.004994054,0.026687017,0.008844006,0.031207303,0.007759587,-0.022536807,-0.016948344,-0.018274,0.034604065,-0.00017067317,-0.032008268,0.0016940065,0.028997175,-0.048817895,-0.0035822452,-0.015230937,0.03949822,0.007760122,-0.045723245,-0.002756741,0.0069540218,0.014899001,-0.0047191833,0.009295398,-0.0070877867,0.010060912,-0.024214026,-0.026601112,-0.010201461,0.007833838,-0.0026116527,-0.014171426,-0.043480136,-0.035125203,0.0014385489,0.011110216,-0.026171215,0.0484214,-0.034198195,-0.018390365,-0.036268976,0.0037225876,-0.026708575,0.035331473,0.018398937,0.10623903,0.008585525,0.0383938,0.013570384,0.010512893,0.035483163,-0.064206734,0.023363475,0.016330676,0.00028468092,0.012931653,-0.041108347,-0.0030514502,0.025381347,0.04757825,0.08681653,0.05403967,-0.02579984,-0.016617676,0.015614127,0.049470298,-0.03971235,-0.002499888,0.06285835,0.032713342,-0.03139295,-0.010153131,-0.039166793,-0.090576276,0.07315567,-0.061513275,0.062273618,0.06363045,-0.0007296455,0.014481317,-0.0076017925,-0.009525585,0.045274984,-0.015471406,0.029304305,-0.012887969,-0.07032604,-0.03174626,-0.016248377,-0.021418517,0.03967145,0.012509197,0.012453062,0.024060328,0.015952913,-0.027776448,0.029528715,0.0074573043,0.051050816,0.008896042,-0.020801606,-0.01495711,0.008819923,0.028656673,-0.0021672633,0.060277347,-0.030902369,0.060179427,-0.020055287,-0.0021786236,-0.021383489,-0.057575036,0.0140995355,0.035828847,-0.048728045,-0.007763585,-0.0265365,0.054580677,-0.08481645,-0.042228334,0.032661166,0.00023583205,0.0076819393,-0.028817054,-0.01927431,0.004749976,-0.008731249,0.016237192,-0.03143086,-0.013335785,0.004147271,0.01343312,0.03325574,-0.04269699,0.047020078,-0.037985545,0.031240478,0.015930945,-0.0031567905,-0.04712571,0.01244195,0.012410496,0.014879261,0.0079082195,-0.006388229,0.053570036,0.052168563,-0.04933925,0.0060698134,0.05402279,-0.0057830284,-0.0440837,-0.008312911,0.021287937,-0.009033057,0.021643303,-0.022266272,0.020644095,-0.034038283,0.029706845,-0.051077344,-0.009166832,0.0133935,-0.04443016,0.016859077,-0.046926696,0.076644264,-0.00047245447,0.019780071,0.019964924,0.013041623,-0.03808387,0.020152936,0.015871724,-0.040661547,-0.023553746,-0.03908483,-0.03732981,0.051209,0.037282355,-0.05032531,-0.074509375,0.058518197,-0.032530468,0.013559203,0.07810156,-0.01394474,0.023604583,-0.010036008,0.012270372,-4.9701805e-05,0.019180834,0.009973622,-0.01782546,-0.024482545,0.042469967,-0.002714758,0.033499867,-0.00685162,-0.0038409221,-0.010135858,0.0047027613,-0.024597356,-0.040792342,0.02366082,-0.014917392,0.027980026,0.018299138,-0.008592828,0.023649458,0.001669279,-0.011639543,0.00571797,-0.03184923,-0.024962485,0.016408307,0.03818285,-0.036616776,0.033604205,-0.016688412,0.000630839,0.016210083,0.03145635,0.05257143,0.0009572813,-0.044616368,-0.036981918,0.028745973,-0.021887537,-0.01127754,-0.008882802,-0.0025198527,-0.022146765,-0.014781824,0.030740812,-0.009768617,0.022237891,-0.008167961,0.017772377,0.002813656,-0.037502553,-0.016191894,0.029124802,0.040007535,-0.025606666,-0.06475924,0.0002557863,0.051975686,0.031850606,-0.05544831,0.018575419,-0.0024856734,0.0075102565,0.001222111,0.034714524,0.021896716,-0.015726423,-0.014181558,-0.01465178,-0.03354009,0.030209102,0.0035444852,-0.002161415,-0.011947223,0.017844716,0.020305965,0.026333382,0.043254104,-0.025704583,-0.004077736,-0.0141618615,-0.028125202,0.013450123,0.052895557,-0.08810204,0.0089528095,-0.08386514,-0.0075304904,-0.006528102,-0.07265946,-0.037240192,-0.021643186,-0.022963326,0.025487121,0.042358484,-0.008415631,-0.052833065,0.028033188,-0.038665514,0.07933049,-0.06594407,0.0026217205,0.029072585,-0.003470629,0.05513703,0.020420967,0.035555862,-0.01035292,-0.025865829,0.007231855,0.004362629,0.060992204,0.03917158,-0.039415568,0.06361784,-0.037480067,-0.016874043,-0.00042102006,-0.07359551,-0.0040026875,-0.013042309,-0.00021063082,-0.054473747,-0.026079258,0.037983343,-0.022363706,-0.064093284,-0.017208971,0.031249752,-0.034474157,0.042905994,0.07301699,0.018513534,0.019269798,-0.043106534,0.010840714,0.003144234,0.009982858,0.011698469,0.032514118,-0.005931836,-0.0047864867,0.01476453,0.02099721,-0.00066327257,-0.014486713,-0.07871043,-0.006457921,-0.007047795,-0.059646998,-0.038816735,0.052316573,0.0607574,0.023290522,-0.0013877953,-0.0888525,-0.013289592,0.0031058649,-0.0069528916,0.030494241,0.0114772525,0.008143335,0.052819733,-0.05327267,0.07944437,-0.09654857,-0.047349297,-0.0646073,0.044906512,0.01653198,-0.0019442325,-0.07099302,-0.05136732,-0.035794873,-0.049983446,0.037089903,0.03883535,-0.005622818,-0.008381293,-0.027681908,-0.04411434,0.038560428,-0.026934074,-0.051245812,0.0034098532,0.023521148,-0.023837108,0.05620094,-0.025056213,0.015607217,0.056846168,0.0027910322,-0.03146996,-0.010380612,-0.023051424,-0.014597538,0.008102614,0.012902787,0.01006426,0.008030373,-0.06333379,0.0018992574,-0.08204902,0.008787633,-0.03100155,0.006374002,0.043381415,0.042325247,-0.03575233,-0.04060566,-0.0062316237,-0.06773422,-0.0030618527,0.014962564,0.04527743,0.046794288,0.004780471,-0.04649641,-0.042799074,-0.0070948205,-0.03348821,0.01154718,-0.030573634,0.05529653,-0.021888724,-0.06275638,0.007910334,-0.002174168,0.018188672,-0.015508987,-0.004158955,0.0025236448,0.0010228138,-0.04154393,0.011789269,0.0045952713,-0.011975175,-0.0054300604,0.056210406,-7.151289e-05,-0.046158254,-0.059408505,-0.021236733,-0.034512315,0.013830866,-0.016558608,-0.021716384,0.012199829,0.010266174,-0.05417715,0.039954334,0.014230108,-0.059260786,-0.055707835,0.013700191,-0.0022184164,0.03641639,-0.005030604,-0.026660025,-0.09639976,0.01283902,0.018193245,-0.038350623,0.08026383,0.007114126,-0.0057061245,-0.01575297,0.031333216,-0.02397389,0.017903762,-2.4424224e-05,0.042719178,0.017308109,0.009089258,-0.02937229,-0.017244589,-0.027164523,0.013017077,0.0028984346,-0.030788373,0.015852403,0.038230743,0.03947654,-0.009861927,-0.005665297,-0.0026326838,0.047824714,0.019900048,0.043144625,-0.017862404,0.06573918,-0.0019716402,-0.004851643,0.08299858,-0.020594662,0.013269431,0.044932716,-0.013087825,-0.022759128,0.037305962,0.00063482876,-0.042758007,0.004483066,0.01998006,0.08119212,0.020385843,-0.016617721,0.018727293,-0.042479813,-0.05050138,-0.02855931,-0.008872066,0.067928076,0.036990877,0.03708961,-0.063490674,0.010342158,-0.04433657,0.023979457,0.04321901,0.051511727,-0.045377836,0.0151099125]	Keywords: complexity, sequential operations, path length, self-attention, fully connected, convolutional, recurrent\nKey Objects: self-attention, fully connected layers, convolutional layers, recurrent layers\nRefers to Images: None\nHypothetical Questions:\n- What does the notation 'O' represent in the complexity values?\n- How does the maximum path length of self-attention contribute to its ability to model long-range dependencies?\n- Why is recurrent layer complexity lower than other layer types?\n---\nSummary:\nThis table compares the per-layer complexity, minimum sequential operations, and maximum path lengths for various layer types, including self-attention, fully connected, convolutional, and recurrent layers.\nOriginal Text:\n| Per Layer       | (O'T$^{2}$ D)       | (O'1)                   |\n| Self-Attention  | (O'T$^{2}$ D)       | (O'1)                   |\n| Fully Connected | (O'T$^{2}$ D$^{2}$) | (O'1)                   |\n| Convolutional   | (O'KTD$^{2}$)      | (O'1)                   |\n| Recurrent       | (O'TD$^{2}$)        | (O'T)                   |  \n2.4.2 In Terms of Inductive Bias. Transformer is often compared against convolutional and recurrent networks. Convolutional networks are known to impose the inductive biases of translation invariance and locality with shared local kernel functions. Similarly, recurrent networks carry the inductive biases of temporal invariance and locality via their Markovian structure [9]. On the other hand, the Transformer architecture makes few assumptions about structural information of data. This makes Transformer a universal and flexible architecture. As a side effect, the lack of structural bias makes Transformer prone to overfitting for small-scale data.\nContextualized Text:\nTable 2 compares the per-layer complexity, minimum sequential operations, and maximum path lengths of different layer types, such as self-attention, fully connected layers, convolutional layers, and recurrent layers. The table illustrates that each layer type exhibits different characteristics in terms of computational complexity and operational sequence.	{"tags": ["architecture", "comparison", "performance", "complexity"], "doc_id": "e34e6ac8-b609-4905-b98c-397adfe988b7", "summary": "This table compares the per-layer complexity, minimum sequential operations, and maximum path lengths for various layer types, including self-attention, fully connected, convolutional, and recurrent layers.", "doc_type": "text", "entities": [], "keywords": ["complexity", "sequential operations", "path length", "self-attention", "fully connected", "convolutional", "recurrent"], "key_objects": ["self-attention", "fully connected layers", "convolutional layers", "recurrent layers"], "contextual_text": "Table 2 compares the per-layer complexity, minimum sequential operations, and maximum path lengths of different layer types, such as self-attention, fully connected layers, convolutional layers, and recurrent layers. The table illustrates that each layer type exhibits different characteristics in terms of computational complexity and operational sequence.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.4 Comparing Transformer to Other Network Types"}, "hypothetical_questions": ["What does the notation 'O' represent in the complexity values?", "How does the maximum path length of self-attention contribute to its ability to model long-range dependencies?", "Why is recurrent layer complexity lower than other layer types?"]}
aa31128e-af9e-4666-947c-f92296fa4119	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0103339385,-0.004690246,0.031166796,0.035932414,0.0052197976,0.074087664,-0.016369907,0.0619646,0.024287336,-0.019465785,-0.016439436,0.023041217,0.021681802,-0.014929485,-0.0036437693,0.041553196,0.009258887,0.08114849,0.031036273,-0.022108676,0.08490017,0.028256549,-0.012746359,-0.0123646995,0.0068958434,0.038355693,0.052890766,-0.019469753,0.011109182,0.0023595032,0.0011448062,-0.032638606,0.030340215,-0.017353065,0.029286398,0.0015166495,0.0070018745,-0.008928213,-0.006062855,-0.005361212,-0.019215202,0.022737857,-0.03322768,0.0155797275,-0.03134487,-0.04067053,-0.13930078,-0.030571768,0.0058245156,-0.02869811,-0.039618034,0.0063019167,-0.053524163,-0.032891616,-0.04728533,-0.040887598,0.009849864,-0.04926772,0.014016246,-0.09264115,-0.04020482,0.018121758,-0.035687894,-0.028720519,0.013036184,-0.03585961,0.014439234,0.045385614,0.060113735,0.13252082,-0.007535412,0.0065949457,-0.013038727,-0.0538182,0.10188318,0.023621295,-0.04747712,-0.032683283,-0.055677522,-0.018667202,0.008656828,0.083561614,-0.008452588,-0.031426545,0.062477067,0.0024447867,-0.058156405,-0.036523953,0.09200798,-0.027645852,0.014723526,-0.0018271729,0.032139037,0.056061737,-0.028065197,-0.077394925,0.0071657253,-0.012420725,0.014170929,-0.04425976,0.0531199,-0.028693534,0.06938086,0.092132695,0.025010072,-0.020608822,-0.010773791,-0.024215268,0.03532441,0.026409827,-0.022142358,0.035298213,-0.021342559,0.020300258,-0.016369108,0.010452409,-0.035411805,0.018924763,0.016477793,-0.016735591,0.037616815,-0.027252018,-0.03162621,0.0033034512,0.06220764,0.07896212,0.013765003,-0.04460742,0.012743949,-0.028757052,0.014111714,-0.015111075,0.0005495596,0.011096152,0.02642061,0.03350831,0.09253527,-0.01865341,-0.010733277,0.022373,0.018061463,-0.06732428,-0.03606284,0.019292714,-0.007413891,0.07542289,-0.004573869,0.04588282,-0.03169964,0.06600329,0.04942258,-0.013400484,0.04995038,0.0019299384,0.01146011,0.00054319965,-0.019209081,-0.032762665,-0.024483396,-0.05713724,-0.028188324,-0.049991783,-0.042510625,0.03500189,0.023436014,0.12934531,0.013875518,-0.016924681,0.041934125,0.044939324,-0.042306412,0.013265145,-0.008157878,-0.053876378,0.020755596,0.008270222,0.024066495,0.015167734,0.0014733516,-0.007458437,-0.013273835,0.038592026,0.008338069,0.06842623,-0.032581072,0.037500516,0.010025344,0.02919836,-0.03405054,-0.026580654,0.0043437025,-0.023734994,-0.012464954,0.041472454,0.07022914,0.052682485,0.01955989,-0.08325097,-0.014073105,-0.034342762,0.0053201937,0.013697458,0.004389318,-0.03485548,0.00959345,-0.019253204,-0.011704863,-0.034839246,-0.0039055992,0.033509597,0.025434555,-0.03976927,-0.030470852,0.043157544,0.049876053,-0.0074923714,0.017523844,0.0031538147,0.028529042,0.010383307,-0.04542924,-0.016304735,0.005083805,-0.027198544,-0.04278334,0.008771872,-0.011106966,-0.020508183,0.008095693,-0.008020925,0.027402308,0.016769411,-0.047305483,0.042119626,0.008430517,0.053861443,0.026871407,0.034557812,0.03288429,0.027517699,0.047116686,0.0409371,-0.051746987,0.07540842,-0.009295239,-0.0032034707,0.05745723,0.06681173,0.043788675,0.042837285,0.034337126,0.037209745,0.060719725,-0.025318671,0.033441797,-0.024713498,0.020198194,0.015513027,0.008704318,-0.0019741561,-0.045152165,-0.0027356367,0.010999959,0.030066801,0.011683971,-0.018228235,0.0060661514,0.040662326,-0.019499792,-0.022601912,0.043729343,0.015032753,-0.01781286,-0.030122304,-0.012130067,0.01083566,0.030220097,-0.015827334,0.058977798,-0.037174355,-0.01382794,0.037529387,0.021368787,-0.010713175,0.03230629,-0.036864787,-0.014902517,-0.039539017,-0.038402744,-0.004992891,0.008054093,-0.036226686,0.013283982,-0.00385501,-0.024298508,0.022330478,-0.009412388,0.020558335,-0.008519567,0.02464439,0.103521995,-0.038247533,-0.014809911,-0.0071975305,-0.005877965,0.0076085585,-0.056932844,0.014355656,0.07462244,0.036529742,0.046790063,-0.0074525718,-0.030858833,0.017118672,0.02479544,0.07829039,0.07934382,-0.045250762,0.018799592,0.052148614,0.008971461,-0.013779886,-0.036602423,0.042739715,0.03791124,-0.0037908205,0.0071275127,-0.029608704,-0.07909261,0.050832424,-0.07367598,0.048637055,0.041088793,0.0036857857,0.007517082,0.043466095,0.030694004,0.06728957,0.0029960473,-0.0006342993,-0.009611552,-0.068131715,-0.0014372087,-0.006496791,0.01634835,0.029354036,-0.009572735,0.0296783,0.006915028,-0.014493508,0.0013489756,0.061989725,0.047946297,0.03751782,0.0004042336,-0.04161736,0.04529662,0.007394225,0.013616071,0.012964799,0.023669098,-0.022364233,0.031553145,-0.014750547,0.04477144,-0.01783051,-0.053601235,-0.007509162,0.0222402,-0.035906266,0.0023332885,-0.02729985,0.07084338,-0.032239184,-0.02245492,-0.005570946,0.0391707,0.004779637,-0.033318933,-0.080982745,0.010987067,0.020046521,-0.0588019,-0.03616076,0.002017084,0.013784698,-0.018365745,0.050855845,0.004074384,0.030329054,-0.034291618,0.03491028,-0.016590213,-0.0035145162,-0.037021067,-0.004843754,0.023148037,0.038072363,-0.026816558,-0.012113554,0.033276066,-0.0063051074,-0.06083651,0.041934643,0.04085174,0.0009911761,-0.06545965,-0.016854012,0.05400093,0.0007985904,0.04479388,-0.016085684,-0.016626803,0.009974621,-0.013384942,-0.012392882,-0.026289402,0.022811264,-0.04130976,0.024078252,-0.013708623,0.011682133,0.023064738,0.033638857,0.01482888,0.0047136815,-0.030180287,0.029058168,0.0060391296,-0.024662713,0.022502415,-0.053173114,-0.0015929954,0.052469537,0.04405261,0.025937812,-0.09772917,0.079102874,-0.058518898,0.038934227,0.08292297,-0.024033401,0.016895683,-0.0020009372,0.025998605,-0.013897516,0.014877909,0.019153466,-0.0062298104,-0.023310445,0.005292106,-0.020581942,-0.030417139,-0.013208491,0.004942957,0.0061611244,0.020474322,-0.009883127,0.020615056,-0.01671939,0.017479103,0.046110038,-0.003993676,-0.0073654037,0.022327613,-0.005187924,-0.013164098,-0.04282759,-0.023414537,-0.001183337,-0.031960502,-0.028279025,-0.061789356,0.04224527,-0.053043015,-0.012922328,-0.0190081,0.040376738,0.018332139,-0.027733674,-0.02450659,-0.04789421,0.035716504,0.05951616,0.0011138425,0.0122844195,-0.01429581,-0.008920837,-0.045873486,0.00791641,-0.016521556,0.033046823,0.03308965,0.0074573136,0.047875933,-0.040790528,-0.017943935,-0.023624523,0.017697493,-0.051321145,0.0066483407,-0.03400235,0.058339186,0.04820895,-0.03463862,0.031601228,-0.011523628,0.023217106,-0.0024976665,0.035869524,0.008485992,-0.017882837,-0.0037617385,-0.036478754,-0.036628693,-0.008181782,0.00304879,-0.050384726,0.0037761359,0.04078598,-0.016047304,0.039644923,0.008014063,-0.0012506188,-0.013272096,-0.0044256616,0.04555091,0.030913437,0.08072511,-0.05976592,0.023067579,-0.008502128,-0.009449256,0.01122469,-0.041402385,-0.028540213,0.0046299063,-0.046989482,0.0066892323,0.029866299,0.045303382,-0.026471414,0.029496947,-0.024184741,0.06855556,-0.07689727,0.0040664314,0.03421231,-0.016191533,0.06820655,0.018543053,-0.01803934,-0.01608061,0.0010261526,0.0085078785,0.01833361,0.080447875,0.03511362,-0.0073711583,0.053353615,0.03336769,0.008590816,-0.024444504,-0.027886016,-0.025131647,-0.0401598,-0.032545302,0.0117612975,-0.00801342,0.03241439,-0.021900902,-0.018434204,0.016960014,-0.012741529,-0.04930366,0.048531033,0.06498948,0.0017436106,0.03623572,-0.013576076,0.025596207,0.046749912,0.02672049,0.046310708,0.003355771,0.03441449,0.023327978,-0.023125395,0.02261801,-0.02656984,-0.0021583252,-0.033371117,0.003918175,0.025282128,-0.035407968,-0.0142323,0.002601079,0.032873902,0.015698351,-0.0041416604,-0.08960131,-0.029235693,-0.04383357,-0.029605545,-0.021220146,0.0025967564,-0.010550141,0.04460289,-0.07263412,0.054914292,-0.08461708,-0.033933476,-0.009545653,0.0262263,-0.01912038,-0.04831076,-0.029328609,-0.01776308,-0.026507892,-0.027624082,0.050917707,0.04456228,-0.021648908,0.015233922,-0.021216469,-0.0029031918,-0.023293598,-0.01688603,-0.031886287,0.017978394,0.013817681,-0.034674883,0.0905294,-0.014507595,0.010958512,0.036379933,-0.013507228,-0.07471056,-0.009825542,-0.008303497,0.029679377,0.024226168,-0.012376841,0.050732337,0.032422278,-0.06955006,-0.032183677,-0.08572126,0.0012111086,0.0060267234,0.041195158,0.045532685,0.0065874737,-0.05218906,0.013957328,-0.018552683,-0.05730906,0.006739095,0.018990591,0.050208773,0.07672107,0.018534416,-0.054327473,-0.09109043,0.017019587,-0.0033394897,0.021341546,-0.040248066,0.07085986,0.008481698,0.028174022,-0.02451437,-0.013235081,-0.022764312,-0.009059129,0.01098504,-0.004797854,-0.0029373863,-0.053067002,0.011665368,0.054298043,-0.014501677,0.031607572,0.068511486,0.03484858,-0.06244408,-0.005445876,-0.03573532,-0.057494015,0.014179182,0.022537751,0.031314913,0.022425978,-0.020378537,-0.014708906,0.03741543,-0.01124895,0.027309056,-0.0460252,0.05049132,0.023932008,0.029843533,-0.03299579,-0.0015009464,-0.094807684,0.016555829,-0.0059642945,-0.05001272,0.050250236,0.028548628,-0.0103060575,-0.029552784,-0.00031045638,-0.034591835,0.029926475,0.017752869,-0.018678576,0.034526102,0.029973816,-0.031876173,-0.049376223,-0.01779824,-0.03552996,0.014626179,-0.049228884,0.029427376,0.0055031246,0.016786097,0.01737516,-0.0034020718,-0.019689983,0.012765951,0.0037968024,0.049470745,-0.064945474,0.053453617,-0.012094745,0.031663243,0.078147635,-0.004065906,-0.015418611,-0.015193645,0.024037737,-0.012084353,0.02162563,-0.013707861,-0.03432698,-0.012364277,0.032455746,0.013913244,-0.020952992,-0.00789991,-0.01795484,-0.047609795,-0.028701669,0.0037363595,-0.017715566,0.06361655,-0.0059335073,0.024922706,-0.06651173,0.012992573,-0.07554654,0.055080798,-0.009269459,-0.029392982,0.029638644,4.6549976e-05]	Keywords: Graph Neural Networks, GNNs, message passing, Transformer, directed graph\nKey Objects: Transformer, Graph Neural Networks, message passing\nRefers to Images: None\nHypothetical Questions:\n- How does the lack of structural bias in Transformers contrast with the inductive biases found in convolutional or recurrent networks?\n- What is a 'complete directed graph' in the context of the Transformer architecture?\n- If Transformers don't rely on structure, how do they learn relationships between inputs?\n---\nSummary:\nTransformer architectures can be conceptualized as a type of Graph Neural Network (GNN) operating on a complete directed graph, distinguished by their lack of pre-defined structural assumptions about the data.\nOriginal Text:\nAnother closely related network type is Graph Neural Networks (GNNs) with message passing [149]. Transformer can be viewed as a GNN defined over a complete directed graph (with self-loop) where each input is a node in the graph. The key difference between Transformer and GNNs is that Transformer introduces no prior knowledge over how input data are structured - the message passing process in Transformer solely depends on similarity measures over the content.  \n$^{4}$The maximum length of the paths forward and backward signals have to traverse to get from any input position to arbitrary output position. Shorter length implies a better potential for learning long-range dependencies.\nContextualized Text:\nWhen comparing different neural network architectures, the Transformer can be understood as a specialized type of Graph Neural Network (GNN).  Specifically, its a GNN that functions over a complete directed graph where each input represents a node. Unlike other networks, the Transformers message passing process relies entirely on content similarity, without assuming any prior knowledge about the datas structure.	{"tags": ["architecture", "deep-learning", "GNN", "NLP"], "doc_id": "aa31128e-af9e-4666-947c-f92296fa4119", "summary": "Transformer architectures can be conceptualized as a type of Graph Neural Network (GNN) operating on a complete directed graph, distinguished by their lack of pre-defined structural assumptions about the data.", "doc_type": "text", "entities": ["Graph Neural Networks", "GNNs", "Transformer"], "keywords": ["Graph Neural Networks", "GNNs", "message passing", "Transformer", "directed graph"], "key_objects": ["Transformer", "Graph Neural Networks", "message passing"], "contextual_text": "When comparing different neural network architectures, the Transformer can be understood as a specialized type of Graph Neural Network (GNN).  Specifically, its a GNN that functions over a complete directed graph where each input represents a node. Unlike other networks, the Transformers message passing process relies entirely on content similarity, without assuming any prior knowledge about the datas structure.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "2 BACKGROUND", "h3": "2.4 Comparing Transformer to Other Network Types"}, "hypothetical_questions": ["How does the lack of structural bias in Transformers contrast with the inductive biases found in convolutional or recurrent networks?", "What is a 'complete directed graph' in the context of the Transformer architecture?", "If Transformers don't rely on structure, how do they learn relationships between inputs?"]}
28fd33f3-d034-4457-8995-9c910410d4b9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.05716027,0.024667656,0.028394425,0.061020292,-0.032778308,0.019155463,0.0013416909,0.048332006,0.0605414,0.0064420234,-0.01286809,-0.005842465,0.008497732,-0.014823944,-0.06015738,0.016724303,0.06813932,0.05991443,-0.0003529989,-0.05661551,0.03134016,-0.010360081,0.01235232,-0.03044075,0.038807657,0.030727625,0.025548372,-0.023179743,0.032708243,-0.018491153,-0.030034285,-0.032701794,0.03920592,0.007878424,0.043746993,-0.017761888,0.011211265,-0.018390313,-0.0025079267,-0.004023198,0.0041405223,0.027360106,-0.026667187,0.020745017,-0.012919734,-0.05086226,-0.10251839,0.014360748,0.024041943,-7.598581e-05,-0.01690051,0.007588452,-0.05445054,-0.0007569032,-0.034573272,-0.05442998,-0.013896978,-0.052277002,-0.013399697,-0.10199439,-0.033681653,0.040268496,-0.008209832,-0.004488205,-0.013759054,0.0075716646,0.024485134,0.064745635,0.043026067,0.16631022,-0.014169158,-0.026576214,-0.002278689,-0.018290373,0.11387228,0.043391906,-0.028267773,-0.016143642,-0.033541393,0.0025885438,0.011527703,0.060342487,-0.0740044,-0.050799035,0.082100965,0.0045738737,-0.059172507,0.0074184295,0.05241642,-0.066467464,0.0061389687,-0.03564474,0.0066852937,0.09440985,-0.030080922,-0.036367837,-0.044904694,-0.03227709,-0.017471183,-0.055992823,0.009070181,-0.01682396,0.08339224,0.080663145,0.042662285,-0.015166313,0.021128044,-0.016539764,0.010690201,0.0002698415,-0.027539061,0.02193066,0.02447036,0.051514294,-0.006875775,0.07262451,-0.045979787,0.021429442,-0.025112266,-0.0103817135,0.0046797176,-0.012469502,-0.025773749,0.030048156,0.040141705,0.08036811,0.0064697783,-0.050189387,-0.022795746,-0.018170206,0.047307696,-0.0007925246,-0.011894966,0.01763239,-0.011792768,0.027093248,0.09050663,0.0051317196,0.003286622,-0.032357626,0.023958404,-0.038266852,-0.031227287,0.04146808,-0.029364396,0.04382523,-0.054940484,-0.0036565615,-0.008435394,0.05202913,0.033633262,-0.023223974,0.044424187,0.0065133404,-0.04103111,0.0006715265,-0.0034191364,-0.050652657,0.0120994765,-0.031032346,-0.062411554,-0.059934877,0.017233526,0.09225644,0.029115263,0.08327953,-0.007113747,0.015332412,0.017787237,0.016751206,-0.009649846,-0.014564902,-0.02470323,-0.041065454,0.0106588155,0.008992453,0.010118221,-0.03706085,0.0038897751,-0.020841718,-0.0099018775,0.059551522,-0.0020333128,0.025512073,-0.016258666,0.059432473,0.002942384,0.060268007,0.02556399,-0.02042292,-0.0038738488,-0.033377785,-0.038513865,0.006417833,0.026744261,0.037608217,0.05496792,-0.05767621,-0.03611086,-0.040896915,-0.007840656,0.007764125,-0.0009755085,-0.0029094024,0.023977892,-0.032101493,0.014238242,-0.009694193,-0.012920968,0.01630007,0.012279525,-0.020675577,-0.01822689,0.025789764,0.048249137,0.008313339,-0.014138527,0.0530678,0.0070895236,-0.03752807,-0.03459536,-0.023987565,-0.032234553,0.008294853,-0.07377025,0.0007340244,-0.0317045,0.020238671,-0.013135568,-0.014929376,0.031076858,0.04777301,-0.046782255,0.02375449,0.017842289,0.052710045,0.034530796,-0.0006162425,0.022620425,0.013612504,0.0029647269,0.05479056,-0.014774815,0.057240415,-0.0024509158,0.031132266,0.020456987,0.06394997,0.023631994,0.027825773,0.005253986,0.03484809,0.036028504,-0.010672426,-0.02450814,-0.011428944,-0.009550794,0.0074043046,0.02662768,-0.02824641,-0.021003012,0.007507893,0.0032797654,0.042879548,0.035509124,0.0037547955,-0.010174562,0.025609465,-0.035066273,-0.013952679,-0.004911548,0.036469307,0.022989364,-0.057284266,0.004689028,-0.009310747,-0.014416332,-0.019253258,0.02183615,0.008902093,-0.023287361,-0.03512446,0.046366595,0.019221889,0.04773509,-0.019020854,0.04660355,-0.05559667,-0.08558869,-0.025499726,-0.005641844,-0.014407635,0.02802634,-0.010409716,0.023074374,0.0026269248,-0.013363921,-0.020621698,-0.052546166,0.018585078,0.11878501,-0.01909449,0.029898,0.008160505,-0.0017058898,0.016192736,-0.018094145,0.027471818,0.060152784,0.01810651,0.0038031656,0.012418618,-0.0035836522,0.030036977,0.025069969,0.10969955,0.056744058,-0.044153918,-0.002427146,0.05794515,0.062117755,0.027773656,-0.024474993,0.019281063,0.072564624,-0.005999663,0.050908033,-0.073338605,-0.06494237,0.08340237,-0.08945875,-0.0041717654,0.066493906,-0.015188563,-0.019763075,0.015743673,0.02202629,0.079319894,0.009146356,-0.012666984,-0.024911687,-0.031810217,-0.0018089606,-0.02594823,-0.0010276352,0.029076457,-0.043065786,-0.007923354,-0.007886501,-0.04026299,-0.0012446266,-0.018302478,0.036274847,0.0035974816,-0.0017565629,-0.048154116,-0.007370752,0.015715525,-0.008408186,-0.03407541,0.05765714,-0.07735193,0.11019929,-0.0031695266,0.06669965,0.006282231,-0.08997039,0.02676015,0.01856356,0.004549124,-0.04839049,-0.02396283,0.073668994,-0.02605493,-0.03445305,0.028823633,0.029246602,0.043422293,-0.020075988,-0.033859186,-0.0022827843,0.030631967,-0.020689521,-0.046145745,0.001981979,-4.3751324e-06,-0.013158906,0.04084801,-0.029672863,0.05227481,-0.03965975,0.04635286,-0.00025779844,0.031304225,-0.03668183,-0.025733516,0.012499199,0.058581952,0.002723877,0.0027330376,0.012769421,0.06453771,-0.050215926,0.03714784,0.0615659,0.015171798,-0.041920315,0.013198465,0.028283311,0.0392178,0.037803233,-0.017624678,0.018364416,-0.017069045,0.03574197,-0.006095272,-0.03300147,0.05300825,-0.025600212,0.0013082772,-0.028970635,0.025709333,0.036421806,0.022672735,0.008807534,-0.007844272,-0.037133705,0.043447588,0.04977696,-0.0322919,-0.0020714558,-0.04730957,-0.00045623162,0.040737566,0.037020978,0.049056306,-0.036717337,0.058130912,-0.060686607,0.029233962,0.048537407,-0.022080438,0.014463629,-0.0054729376,0.009652336,-0.03650552,-0.015746672,0.04448665,-0.017930333,-0.013664938,0.07041446,-0.008052147,0.00885969,-0.049477827,0.047845285,-0.05033568,-0.012080972,-0.04775156,-0.051497456,-0.056027662,0.0162766,0.027879337,0.008027884,-0.022809455,-0.0064567593,-0.015800046,-0.008674144,-0.034190208,-0.034101933,0.01163366,-0.03515619,-0.01563533,-0.029208906,0.008620141,-0.014948844,-0.02345526,-0.0317484,0.07523575,0.040002353,0.010091291,-0.031595215,-0.03494085,0.01479787,0.015464169,-0.01495466,-0.035091504,-7.324211e-05,-0.02379778,-0.02444056,0.03148403,0.031310566,0.044836182,0.024952063,0.042088524,0.0057347696,-0.033108838,-0.00013392551,-0.0073862285,0.039452918,-0.005678268,0.05247143,0.022865534,0.049882565,0.030250411,-0.029638547,0.02662325,0.0107171275,-0.0410463,-0.026283288,0.054938477,0.042330846,0.012912098,0.0118375225,-0.008716205,-0.06686624,0.024456136,0.00029212897,-0.03525391,0.012855995,0.041228805,-0.023049096,0.025729217,0.03184755,-0.017831147,-0.0360878,0.0029824332,-0.025746508,0.038725458,0.05452192,-0.039939422,0.03330084,-0.071165316,-0.028905757,-0.006837079,-0.0565786,-0.053523906,0.004554443,-0.061777614,-0.0113406265,0.0036021876,-0.0017992388,-0.017603626,-0.014682893,0.0022254037,0.07377959,-0.052941848,0.021316575,-0.0062317955,0.03680127,0.03834373,0.05643265,0.04084217,-0.040771276,0.0043260064,0.004783269,0.07890338,0.029263224,0.029571457,0.0046866345,0.05473481,0.015134143,-0.012185046,-0.00026056325,-0.037496984,-0.013097331,-0.010039368,-0.038614243,-0.030362923,-0.040802147,0.07076907,-0.041655965,0.02738038,0.0094676055,-0.038736846,-0.0044143456,0.02710414,0.06817209,0.0050767614,0.049869463,-0.06371174,0.009587613,0.06354735,0.0055660387,-1.1236136e-05,0.030426634,0.02279266,0.03725943,0.006600468,0.018307466,-0.014213067,-0.015926285,0.0022469691,0.005240815,-0.024537366,0.0011778979,0.019417463,-0.007637135,0.030716252,0.009915211,-0.03218377,-0.06974261,-0.015986966,0.024754943,-0.019008646,-0.01631645,-0.0017508097,0.0038017207,0.008307052,-0.03407094,0.05037065,-0.05707521,-0.043717057,-0.016566286,0.04452125,-0.0067375577,-0.04246122,-0.0143208355,-0.019490289,-0.021981066,-0.03397112,0.07642863,0.029990284,-0.0128240185,0.018717878,-0.02941594,0.018857777,-8.106578e-05,-0.02188813,-0.06001199,0.025162647,0.014527299,-0.018236987,0.058288604,-0.0076939007,0.012652953,-0.0048846365,-0.05921136,-0.0485105,-0.007878867,-0.00389268,0.015102052,0.041938286,-0.0027257514,0.046392128,0.052389454,-0.055350505,-0.022213787,-0.040137995,0.007569541,0.0038147827,0.04035356,0.06157436,-0.0026211408,-0.059334002,0.04296155,-0.0069408696,-0.031955868,0.004172463,0.019947957,0.031877063,0.07457551,0.03520295,-0.028400442,-0.10077746,0.0088880025,-0.024955578,0.020371662,-0.04794326,0.045034867,-0.009313722,-0.040657185,-0.035697944,0.026255932,0.017827066,-0.034529027,0.022333093,0.038285427,0.019978674,-0.012195607,0.0097312555,0.034598295,-0.07612712,-0.0027388786,0.027085407,0.0441448,-0.041770738,-0.030313157,-0.032584053,-0.0013823007,0.014283235,0.017622981,0.0023702325,0.014564737,0.0012094575,0.015100185,0.062159628,0.011120886,-0.010085389,-0.038417596,0.011861399,-0.001690056,0.034238227,-0.0052667796,-0.03383982,-0.06433431,0.04223731,0.0073339627,-0.05142387,0.061140593,-0.015058798,-0.034442455,0.0071993982,0.047772687,0.0007514079,0.05813198,-0.020551484,-0.0075887395,-0.024922268,-0.014956183,0.0023451291,-0.033355623,-0.0017907737,0.0122357635,0.010968976,0.012982915,0.022002926,-0.015392222,-0.0021789074,0.017835017,0.014144326,0.02870812,-0.0012464464,-0.0007644532,0.030638859,-0.021298986,0.051403556,-0.013701828,0.029313486,0.06612285,0.0090899095,0.01209131,-0.026817564,0.012177916,-0.03170396,0.006222892,0.008926589,-0.030839693,-0.034347177,-0.025027668,-0.013777046,0.07095894,0.0134253,-0.04440601,-0.020639822,-0.043040123,-0.013079183,-0.024521964,0.04707764,0.020066226,0.03033494,-0.06336795,0.02381455,-0.018933324,0.039409228,-0.027024364,-0.0036318738,-0.03975968,-0.00889724]	Image title: Sparse Transformer Architecture Tree\nTags: transformer, sparse, architecture, routing, prototype, memory, attention\nKey objects: Sparse, Attention, Routing, Prototype, Memory, Attention, Levels, Architectures\n---\nSummary:\nThis diagram depicts a hierarchical tree structure illustrating the evolution and relationships of various sparse transformer architectures. It categorizes these architectures based on different levels of abstraction, starting from the foundational 'sparse' concept and branching out to specific implementations across different levels.\nFull description:\nThe diagram presents a tree-like structure showing different sparse transformer architectures. The root of the tree is labeled 'Sparse', which branches into sub-categories 'Attention', 'Routing', 'Prototype', and 'Memory'. 'Attention' further splits into sub-architectures like 'Starformer,' 'Longformer,' and others, each marked with a reference number. 'Routing' branches out to variations like 'Adaptive Routing.'  'Prototype' includes architectures such as '[...]'. 'Memory' has sub-branches depicting different memory-based transformers. The tree visually organizes the landscape of sparse transformer designs based on their underlying mechanisms and functionality. Each branch represents a line of architectural developments.\nText found in image:\n- Sparse\n- Attention\n- Routing\n- Prototype\n- Memory\n- Attention\n- Levels\n- Architectures\n- Sparse Transformer Architecture Tree\n- Starformer [14]\n- Longformer [13]\n- Routing [10]\n- Prototype [24]\n- Memory [15]\n- Sparse Transformer Architecture Tree\n- Attention\n- Levels\n- Architectures\n- Sparse\n- Routing\n- Prototype\n- Memory\n- Attention\n- Levels\n- Architectures	{"tags": ["transformer", "sparse", "architecture", "routing", "prototype", "memory", "attention"], "title": "Sparse Transformer Architecture Tree", "doc_id": "28fd33f3-d034-4457-8995-9c910410d4b9", "source": "./images/a-survey-to-transformers/image_3.png", "summary": "This diagram depicts a hierarchical tree structure illustrating the evolution and relationships of various sparse transformer architectures. It categorizes these architectures based on different levels of abstraction, starting from the foundational 'sparse' concept and branching out to specific implementations across different levels.", "doc_type": "image", "key_objects": ["Sparse", "Attention", "Routing", "Prototype", "Memory", "Attention", "Levels", "Architectures"], "parent_doc_id": "aabf053b-c6a3-4b0e-8c8c-c346ac31e0d6", "text_in_image": ["Sparse", "Attention", "Routing", "Prototype", "Memory", "Attention", "Levels", "Architectures", "Sparse Transformer Architecture Tree", "Starformer [14]", "Longformer [13]", "Routing [10]", "Prototype [24]", "Memory [15]", "Sparse Transformer Architecture Tree", "Attention", "Levels", "Architectures", "Sparse", "Routing", "Prototype", "Memory", "Attention", "Levels", "Architectures"], "contextual_description": "The diagram presents a tree-like structure showing different sparse transformer architectures. The root of the tree is labeled 'Sparse', which branches into sub-categories 'Attention', 'Routing', 'Prototype', and 'Memory'. 'Attention' further splits into sub-architectures like 'Starformer,' 'Longformer,' and others, each marked with a reference number. 'Routing' branches out to variations like 'Adaptive Routing.'  'Prototype' includes architectures such as '[...]'. 'Memory' has sub-branches depicting different memory-based transformers. The tree visually organizes the landscape of sparse transformer designs based on their underlying mechanisms and functionality. Each branch represents a line of architectural developments."}
a15ddce5-2631-44db-8950-9cc957003396	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.022039741,-0.00507004,0.034507375,0.0353351,0.017172933,0.05152218,-0.020609235,0.04422445,0.0566353,-0.02242142,-0.018359153,0.0036071131,-0.009289524,0.018673385,-0.027576415,0.027795857,0.0060884273,0.06565792,0.0017757961,-0.023990933,0.077631205,0.010154962,0.045231324,-0.022713589,0.023595301,0.009124567,0.016985506,-0.016156418,0.0143642565,0.01840193,-0.0471896,-0.033772856,0.052636463,-7.1565264e-05,-0.0058720196,0.028693125,0.03345949,-0.019756652,-0.03329295,-0.023871083,-0.050154928,0.037129376,-0.06631014,0.05225339,-0.07101155,-0.016881883,-0.12476414,-0.035937294,0.031942423,0.0029841538,-0.041112218,-0.010869455,-0.04372451,0.0068255058,0.034683373,-0.029234044,-0.030975409,-0.037773337,-0.0051689297,-0.077261984,-0.0045604045,-0.008886824,-0.043508895,0.004985677,0.017665552,-0.030425739,0.044538554,0.0480097,0.028581358,0.1165736,-0.03489451,-0.033726517,-0.0046642106,-0.01811968,0.12067065,0.041865733,-0.06525243,-0.049604252,-0.022411546,-0.006734084,-0.052915752,0.07560095,-0.05629726,0.017317148,0.10364806,0.03330384,-0.0573344,-0.056717116,0.083515875,-0.040440574,-0.004427584,0.0044428213,-0.028610578,0.0396573,-0.016427951,-0.046107814,0.007621227,-0.05274413,0.040487383,-0.0152106825,0.037990395,-0.023287982,0.038830988,0.04457888,0.038096778,-0.027113955,0.01143813,-0.027794523,0.03590601,0.016167639,-0.0102631515,0.0066322424,0.00783511,0.023602989,0.020726882,-0.006797532,-0.060081482,0.0036102503,0.0069677993,0.0017541122,0.004183996,-0.019821953,-0.020620832,0.040824503,0.05882985,0.055486068,-0.010727001,-0.026712246,-0.014171096,-0.009409214,0.0059598912,-0.011745956,0.006838543,0.02344281,-0.009447288,0.011082426,0.07406679,0.02737861,0.002537953,-0.012255523,0.015599612,-0.05948479,-0.00662111,0.03202762,-0.022596192,0.049317673,-0.036131628,-0.011780011,-0.00052773877,0.053212535,0.042605855,-0.036900204,0.016001167,-0.06068304,-0.04353274,0.0077685495,-0.031379577,-0.052628487,0.007514242,-0.016805371,-0.05124587,-0.070434295,-0.0529368,0.009950995,0.015076083,0.060807645,0.04300544,0.01768675,0.004985801,-0.020435158,-0.045340475,-0.0032508473,0.00938658,-0.0394364,0.011755849,0.015038976,-0.020695433,-0.03260593,-0.028998908,-0.009717274,0.015047547,0.004986757,0.011859962,0.06462608,-0.0091223465,0.017913338,-0.0063201557,0.050368737,-0.041839425,0.021446204,0.015636073,-0.031569503,-0.025766283,0.011263938,0.060958248,0.026403503,0.055991225,-0.0582202,-0.04294814,0.024315089,-0.057741713,0.0034885842,0.01626206,-0.031773448,-0.0069905403,-0.026463728,0.01851615,-0.0017928897,-0.014499359,0.030243786,-0.02211916,-0.05458658,-0.06274891,0.066572726,0.03558594,0.011016061,-0.041813727,0.059679314,0.04787465,0.005637447,-0.0073362365,-0.009454967,-0.0031023175,0.004872761,-0.027091859,-0.02462812,-0.05487447,-0.009282055,-0.022290215,-0.043227367,0.048619337,0.018112116,-0.04650296,0.033648796,0.015157875,0.016612045,-0.005348186,-0.0064998297,0.038856387,0.03589344,0.013258006,0.058452822,-0.06271251,0.056952737,0.025308782,-0.010546443,0.0064409347,0.023504043,-0.00032308564,0.031137066,-0.0070935655,0.061488934,0.050730642,0.011265451,0.012069396,-0.049596403,-0.0030613914,0.020472378,0.031766217,-0.004811401,-0.014166838,0.01991582,0.00028049853,0.04264287,0.009050762,-0.016245933,-0.0027694928,0.015813874,0.02046836,-0.013947329,0.004363776,0.045193896,0.042501945,-0.035381924,-0.014441712,0.02434747,0.005431977,0.02271221,0.047006268,-0.010695518,-0.021209164,-0.012651474,0.01137386,0.016418159,-0.023416134,-0.013430764,0.013539028,-0.036026724,-0.09190497,-0.019521756,0.019001756,-0.030505674,0.011845425,0.003875009,0.0074947095,0.0220605,-0.010392522,-0.0006138231,-0.02995084,0.028983716,0.112569325,-0.060318515,-0.02798419,-0.027344711,-0.0096583525,-0.018373225,-0.0537733,-0.017429411,0.009454758,-0.014248243,0.053084623,-0.027469404,-0.047814302,0.021657443,0.038047574,0.092635185,0.065289654,-0.069244824,0.0014153634,-0.015585988,0.04166638,-0.031122042,-0.038832832,0.04735114,0.043530922,-0.021800954,0.0020182566,-0.004443624,-0.06394857,0.02519933,-0.105045795,0.03437232,0.06893664,0.02179581,-0.0240944,0.018542074,0.0027740237,0.055979956,0.018857077,-0.0061304336,-0.0014332616,-0.042555492,-0.023334375,-0.010500877,0.033368167,0.04745741,-0.02075935,0.014755829,0.0036534031,-0.00052455376,0.0064632646,0.010486968,0.03316771,0.012250478,0.013136705,-0.0361707,0.0070565543,-0.009691351,0.017884603,-0.030595783,0.05495109,0.013291989,0.051109053,-0.019931933,0.029691763,0.030456536,-0.05948845,-0.021442434,0.020219073,-0.026219351,-0.0052866223,0.00069282734,0.06032215,-0.0020034264,8.566932e-05,0.028205931,0.022975005,-0.018291492,-0.06359433,-0.03144408,-0.0030481808,-0.0011639076,-0.049279068,-0.06066305,0.0077953693,0.045421213,0.0045228614,0.04540326,-0.0058775772,0.021017881,-0.010535806,-0.007669415,0.02352876,0.029846115,-0.013771777,0.012277024,0.004645097,0.03199313,-0.01046852,-0.032490417,0.023679622,0.041358862,-0.042198066,0.07178457,0.047314752,0.0096111875,-0.07169211,0.00096438895,0.050674353,0.043767057,0.018673137,-0.012170462,-0.05237443,-0.00033217293,-0.032141387,0.013815639,0.0005281762,0.03115729,-0.06462263,0.009933313,-0.032929648,0.044508666,-0.011212339,0.044030577,0.0411257,-0.024270575,-0.014803888,0.005703259,-0.017655369,-0.041232415,0.031307008,-0.07170684,-0.019291976,0.057590704,0.024866957,0.0027608832,-0.027601464,0.08067706,-0.052328065,0.04729476,0.11221341,0.016290572,0.068671964,0.0092059495,0.021878175,-0.0044656224,-0.010969459,0.062609926,0.011563231,-0.021163415,-0.0070173997,0.021251755,0.0019190945,-0.021847533,0.02711149,-0.014487534,0.014349423,-0.0036826134,-0.018191386,-0.021350862,-0.032269157,0.029418735,0.03563275,0.01648312,0.008354554,-0.024415905,0.004294697,-0.016881822,-0.0046966705,0.031065907,0.01563816,0.0037865678,-0.041042775,-0.015903717,-0.030038316,0.015402708,-0.04480533,0.09982415,0.04096911,0.02496105,-0.027096076,-0.034458388,0.018715795,0.046926126,-0.021230176,-0.031651497,-0.02900299,-0.02128296,-0.036183517,0.014333386,0.031043712,0.048230767,0.03736629,0.07043062,0.01676919,-0.048666943,-0.02437779,0.012731674,0.009053854,-0.06133909,0.003734661,-0.009973634,0.06580763,0.008101482,-0.009144937,0.012893716,0.0056454646,-0.031651184,-0.007891017,0.06556158,0.03274412,0.02652945,-0.029813828,-0.031639066,-0.030024076,0.027466873,-0.016115313,-0.01978464,0.033473775,0.072943516,-0.009052474,0.016376182,0.040782,-0.035167836,-0.021220956,-0.016520048,0.08409278,0.049117904,0.09710578,-0.007864961,0.052555162,-0.041435145,-0.043876827,0.01015999,-0.033328153,-0.02862854,0.035094287,-0.036229845,0.012913835,-0.0150941275,-0.0064284876,0.03870104,-0.012483367,0.002545172,0.07458223,-0.07208625,-0.004172251,-0.034345046,0.014511432,0.07755904,-0.0046708663,0.01140551,-0.0131966965,-0.07769048,-0.012244644,0.057636816,0.062094726,0.016702134,-0.06486427,0.04974807,-0.033340156,0.051957518,0.02132376,-0.049785282,-0.04402681,-0.0011545675,0.01954185,0.020071471,-0.015215279,-0.007113726,-0.025550285,0.0026298314,0.0027867744,0.022889163,-0.01657935,0.047540586,0.015887598,-0.018046046,0.013355942,-0.037588473,0.03768358,0.037456654,-0.027487447,0.031189932,-0.006245688,0.019196516,0.014853141,-0.011526106,0.016024161,0.033891015,0.008842665,-0.04534479,0.009275093,0.0183422,-0.069520295,-0.018121762,-0.025228966,0.019717677,0.03305748,-0.03082844,-0.042386673,-0.008974088,0.009578994,-0.013229829,-0.031413164,-0.005614157,0.013192648,0.015640462,-0.07465719,0.0667587,-0.052589502,-0.006604403,-0.019784389,0.038183913,-0.011284535,-0.019491665,0.0052246912,-0.020763664,-0.034121618,-0.07948925,0.05452603,0.0605861,0.010061446,0.0022798285,-0.002369605,-0.0010864333,-0.01658963,-0.006497363,-0.018307151,0.0014009252,0.008306098,-0.024897395,0.05701397,0.0065645436,0.004738308,0.007973626,-0.027341796,-0.047409657,0.0070810807,0.000763274,0.051661186,0.044798013,-0.015951047,0.07576205,0.048903298,-0.08033398,-0.039693706,-0.04598978,-0.017455012,-0.048399508,0.005360232,0.08151445,-0.032970447,-0.084353514,0.038763598,-0.031718604,-0.049837314,-0.015108315,0.0030507448,0.06856444,0.013485643,0.01148517,0.006165935,-0.06597968,0.013356504,-0.055623274,0.02873288,-0.015852392,0.052120704,0.02510165,0.016277043,-0.007907055,0.010837341,0.029851085,0.013510995,0.015728027,0.014020206,-0.014717077,-0.04753121,-0.020086538,0.05318011,-0.044758424,0.00066503993,0.07086578,0.0410101,-0.028123457,0.01580803,-0.044357445,0.035640005,0.016377281,0.018149115,0.030718701,0.0076989434,-0.0354547,0.016553715,0.032479357,0.0034156542,-0.037813026,-0.0033155263,0.0332615,-0.002813526,-0.0007917745,-0.033187408,-0.00079996564,-0.054652534,0.035594337,0.0037956096,-0.08948346,0.035422444,-0.022124384,-0.02680951,-0.020514682,0.04276069,-0.03837949,-0.0066759875,0.01688475,0.02781351,-0.019744173,-0.0083876625,0.019252636,-0.0016566209,0.019243756,-0.009492941,-0.0071062874,0.035532128,0.047943607,-0.0019797687,0.0018659491,0.024543658,-0.0037803396,0.009734573,-0.047869097,0.05048376,0.063211106,-0.05723512,0.038988736,-0.0058022393,0.0015055133,0.07122516,-0.0019210341,0.021362692,-0.034781355,0.055991676,0.039426778,0.019700538,-0.02523082,-0.03398373,-0.090479314,-0.018032767,0.019760014,0.008099166,0.00020219716,-0.011548967,-0.025390612,-0.058710467,-0.027509656,-0.0003890319,0.048253555,0.017877871,0.008330268,-0.051318113,0.048311617,-0.030516014,0.04429057,-0.0442254,-0.0054335175,0.00077714643,-0.019835966]	Keywords: Transformer models, architecture modification, pre-training methods, applications, categorization\nKey Objects: Transformer models, categorization\nRefers to Images: ./images/a-survey-to-transformers/image_2.png\nHypothetical Questions:\n- What are the three main categories used to classify Transformer variants?\n- How does Fig. 2 help in understanding the different types of Transformer models?\n- Why is it useful to categorize Transformer models based on these three perspectives?\n---\nSummary:\nNumerous Transformer models have emerged, categorized by modifications to the architecture, pre-training approaches, and applications, as illustrated in Fig. 2.\nOriginal Text:\n## 3 TAXONOMY OF TRANSFORMERS  \nA wide variety of models have been proposed so far based on the vanilla Transformer from three perspectives: types of architecture modification, pre-training methods, and applications. Fig. 2 gives an illustrations of our categorization of Transformer variants.  \nFig. 2. Categorization of Transformer variants.  \n  \nFig. 3 illustrates our taxonomy and some representative models.\nContextualized Text:\nThis survey categorizes the wide variety of Transformer models developed. These models are grouped based on three perspectives: modifications to their architecture, the methods used for pre-training, and their specific applications, with Fig. 2 illustrating this categorization.	{"tags": ["architecture", "survey", "models"], "doc_id": "a15ddce5-2631-44db-8950-9cc957003396", "summary": "Numerous Transformer models have emerged, categorized by modifications to the architecture, pre-training approaches, and applications, as illustrated in Fig. 2.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["Transformer models", "architecture modification", "pre-training methods", "applications", "categorization"], "key_objects": ["Transformer models", "categorization"], "contextual_text": "This survey categorizes the wide variety of Transformer models developed. These models are grouped based on three perspectives: modifications to their architecture, the methods used for pre-training, and their specific applications, with Fig. 2 illustrating this categorization.", "mentioned_images": ["./images/a-survey-to-transformers/image_2.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "3 TAXONOMY OF TRANSFORMERS"}, "hypothetical_questions": ["What are the three main categories used to classify Transformer variants?", "How does Fig. 2 help in understanding the different types of Transformer models?", "Why is it useful to categorize Transformer models based on these three perspectives?"]}
58eeb957-3ec1-47cd-9e00-e6ee4b59afb3	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.043875355,0.015554956,0.02806099,0.06790374,-0.029762918,0.049956813,-0.012732525,0.031531263,0.020347347,-0.014379155,-0.011390348,0.030905353,0.029581167,-0.011682284,-0.0075847674,0.05759649,0.04955581,0.057679325,-0.00036658847,-0.025497366,-0.0027627847,0.013207259,0.016542146,0.003767923,-0.007826154,0.026944054,0.022863273,-0.022465106,-0.048568416,0.011857221,0.0213906,-0.045959834,0.007246044,0.02068291,0.05162157,-0.01979002,0.0023646723,-0.024313724,-0.027300049,0.010593402,-0.02583345,0.05221163,-0.088988714,0.010456674,-0.032062612,-0.046157245,-0.094750576,-0.04521054,0.011648486,-0.021895712,-0.009651622,0.02492075,0.0005129303,-0.017368307,-0.08421435,-0.024046144,-0.043661725,-0.044500597,-0.02557647,-0.08794073,-0.017697215,-0.0016239798,0.01234048,-0.009286087,0.008998227,-0.0038650641,0.03621851,0.006526187,0.02608102,0.15643279,-0.023532905,0.0071464134,0.0039018919,-0.0048206686,0.08939795,0.04822702,-0.058400013,-0.03270143,-0.062291358,-0.033817317,-0.00094762084,0.066274464,-0.039840207,-0.030415867,0.08430128,0.05126437,-0.04562437,0.0015945875,0.043985076,-0.03732531,-0.0114175435,0.0046499437,0.049296614,0.052210703,-0.0135002965,-0.034835845,-0.049593788,-0.010203047,-0.028310483,-0.034512922,0.015379021,0.009985332,0.06891672,0.10444861,0.005650226,0.017219882,-0.011957158,0.023199411,-0.013751892,0.03226119,-0.06724531,0.015841857,-0.0072946483,0.052457903,0.0040953257,0.050201505,-0.02258476,0.01039281,0.018749848,0.026335452,0.0355553,-0.01046082,0.0054406533,-0.005013,0.03346143,0.015090491,-0.030199548,-0.025271228,-0.06441201,-0.012708985,0.05445015,0.012681576,-0.0125010405,0.05786318,-0.058095925,-0.0021205894,0.062313944,0.013871154,0.004506178,0.0041575893,0.049510553,-0.050707813,-0.04573755,-0.0035162708,-0.029785141,0.04047908,-0.06829927,0.002561541,-0.052903797,0.026790809,0.07163287,-0.015234625,0.044431314,-0.04063231,-0.028771212,-0.0024529463,-0.028522102,-0.052490644,-0.00981671,-0.036210172,-0.038806673,-0.05952718,0.04721806,0.054508846,0.04371724,0.087391086,-0.004210904,0.011005295,0.013997698,0.023708273,-0.047984976,-0.0026048122,-0.012223625,-0.042977218,-0.0062185437,0.031880453,-0.009305004,-0.051732622,-0.00014241334,-0.04324535,0.013816352,0.053701878,0.013302305,0.029930992,-0.03811603,0.00767834,0.03513076,0.04056331,0.0063856114,-0.035564393,0.03633685,0.004486962,-0.020021955,-0.016562834,0.048360303,0.04515795,0.04412296,-0.033542503,-0.070363685,-0.02521812,0.01742605,-0.005679517,0.023463205,-0.010251235,0.016155543,-0.025939535,0.02132455,-0.0046784016,-0.023456754,0.023439923,0.018189952,-0.022822356,0.004250329,-0.0023328613,0.028799303,-0.023758188,0.0057068174,0.015980389,0.02528738,-0.039137494,-0.027336594,0.0041093784,-0.03238647,0.0057389527,-0.021321468,0.013631757,-0.05337501,-0.010064892,0.009882567,0.0106422305,0.01503553,0.009418248,-0.07067547,-0.007207442,-0.029598853,0.05766762,-0.0137003325,0.031073494,-0.009369641,0.016831074,0.03423932,0.019160602,-0.034026377,0.070816815,0.0058523137,0.013672798,0.023947109,0.04394852,-0.010576638,0.07287667,-0.029372862,0.042298473,0.05454375,0.0049817706,-0.003325035,-0.034368422,-0.014131267,0.041497733,0.013227725,0.0063656736,-0.005125972,-0.034916688,0.002386247,0.04552264,0.017199928,-0.009735125,0.009621589,0.046215303,-0.10472671,-0.012763427,0.00058720156,0.024283977,0.046916705,-0.028107086,0.009518663,0.005785806,-0.029668981,-0.0018379265,-0.008295855,-0.007275781,0.0015104822,0.0016802474,0.015988888,-0.0043467977,0.04163007,0.014877179,0.0033142834,-0.018707043,-0.061895803,-0.031953882,-0.0688048,0.0019370576,0.037090324,-0.033004485,-0.024069477,0.027634209,-0.026805239,-0.021005673,-0.007187166,0.016757559,0.07393311,-0.04114941,-0.0048247776,-0.004375364,0.000108752785,0.028678522,-0.009324438,0.029047538,0.054006886,0.01074126,0.03717497,0.016817337,0.025156507,0.013439217,0.053314824,0.0849775,0.046781696,-0.031750914,0.046525836,0.02973637,0.028679287,-0.0064308792,0.010273409,0.06714406,0.061274964,-0.0023766072,0.025935369,-0.038361475,-0.06689763,0.078200534,-0.06537943,0.035027247,0.049377248,0.034890544,-0.016795376,-0.011919609,0.008437145,0.076349005,-0.0034699787,0.019729963,-0.034596257,-0.019796489,-0.008264339,0.024386453,0.0033568975,0.05381268,-0.018889206,0.029247612,0.028264059,-0.009807007,-0.012740376,0.0036692766,0.02226158,-0.012340983,0.01364588,-0.056628942,0.0025644668,0.022195842,0.0041712373,-0.04684656,0.07165568,-0.056403052,0.054074526,0.003240475,0.016961461,-0.0156556,-0.0420177,0.00055167725,-0.01656433,-0.051371496,-0.0319473,-0.04998329,0.011573693,-0.0069820997,-0.012686795,0.037059274,0.0473658,0.0071050017,0.0024541754,-0.025748178,-0.011591074,0.014448935,-0.06311928,0.0005178176,0.012027047,0.04584559,-0.04207862,0.08714176,0.0003641982,0.0389308,-0.00609906,0.008180486,-0.0007607504,-0.022052784,-0.026650548,-0.036272455,0.013440641,0.073764175,-0.001877587,-0.025338951,0.020642411,0.082642704,-0.056612067,-0.0058290237,0.06351129,0.012689815,-0.043818537,0.03623827,0.05513305,-0.019576104,0.068386085,0.005667596,0.009779487,-0.03015547,0.014993735,-0.02488065,-0.034336045,0.05430926,-0.02092235,0.010344154,0.008059613,-0.017935459,0.037823502,0.05544894,0.0043412475,-0.00024346652,-0.044159252,0.026629578,0.020012992,0.012384422,0.021928936,-0.030679114,0.02789484,0.05894017,0.01609105,-0.0010170267,-0.05240809,0.069627054,-0.053670015,0.007333851,0.061663143,-0.038903493,0.016773535,0.009851479,-0.002048465,-0.05912108,0.039896026,0.06757121,-0.03420755,-0.012247784,0.036538653,-0.033103682,-0.007677027,-0.012921812,0.031770427,-0.012069354,0.013844899,-0.036706083,-0.09472603,-0.0559735,0.008049089,0.009196207,-0.0005708017,-0.028942117,-0.008237412,0.0013351877,0.011277892,-0.017067878,-0.033630252,0.03921831,-0.0001572122,0.022405503,-0.040497687,0.037792947,0.018707512,-0.008818108,-0.0047280523,0.013618418,0.02908896,-0.005325665,-0.047782376,-0.029367292,0.03540506,0.059918303,-0.046942204,0.0024339773,-0.009351052,0.020080207,-0.056457292,0.041126065,0.0021137067,0.04104079,0.023800636,0.041350003,-0.004902136,-0.03500424,-0.050476696,0.020916985,0.010402165,0.017097475,-0.005189451,-0.040901978,0.04953321,0.062893055,-0.03336601,0.01950475,0.030292662,0.0061610183,-0.0068678213,0.075589105,0.06490772,-0.04223715,0.0011923944,-0.014885233,-0.03926579,-0.0012491914,0.0016154569,-0.009586887,0.008726828,0.023848293,-0.007510288,0.052458003,0.004698757,0.06346273,-0.025328944,-0.004860322,0.00319653,-0.0010041288,-0.0020749872,-0.0504761,0.047382846,-0.049287926,-0.053011063,0.020260183,-0.012602077,-0.053534087,-0.008840739,-0.05729656,0.01063833,0.010536448,-0.033728536,-0.017524188,-0.019063765,-0.01480363,0.09965706,-0.06493484,0.0516166,0.025363252,0.02125476,0.03089225,0.04572952,0.017135365,-0.045439813,0.028284598,0.04116291,0.054992303,0.03371788,0.03752362,-0.01980181,0.017561648,0.0006834939,0.0012246832,-0.042067423,-0.033636954,0.018206611,-0.0041745347,-0.0037106338,-0.03827894,-0.057329312,0.062121492,-0.045935106,0.01326931,-0.007950509,0.008690364,-0.03389033,0.06750441,0.089164294,0.01714828,-0.009284828,-0.037922718,0.049466338,0.050198324,0.007360049,0.023388274,0.024987217,0.0077837445,0.0044720923,-1.7209773e-05,-0.030991374,-0.0014926107,-0.004175407,-0.05352636,-0.025240352,0.01364794,0.016591715,-0.017188143,0.057326484,0.060950283,0.007629135,-0.004553521,-0.09074127,-0.028703917,-0.019177707,-0.01995305,0.02092951,-0.00017067239,0.004790487,-0.003200387,-0.051664878,0.082225524,-0.055168163,-0.011585979,-0.043286964,0.05912617,0.02480193,-0.06821548,-0.02310763,0.019155016,-0.00801239,0.01877922,0.06362215,0.020937055,-0.010300472,-0.012014287,0.033803664,-0.004526411,0.020552034,0.020654414,-0.034988686,0.0343292,0.04794549,-0.019947078,0.062067162,-0.014909085,-0.006406169,0.009950241,-0.031767365,-0.07486905,-0.013638818,0.004641345,0.00084646075,0.04847111,0.0074782344,0.040970787,0.051146746,-0.115840174,-0.024096182,-0.01963398,-0.019262718,0.008615085,0.042295214,0.038784586,0.011343643,-0.061631277,-0.01712185,0.0153278075,-0.051929053,0.01812321,0.004088757,0.017379645,0.06149192,0.038919434,-0.03523591,-0.100789584,-0.02680926,-0.026798064,0.0050709727,-0.040235486,0.07150613,-0.011045962,-0.03412576,-0.03779614,-0.0041315206,0.008361177,-0.027377494,0.04353079,-0.014058446,0.023775421,0.0026568505,0.053626124,0.05404593,-0.030586628,0.014456256,0.057436652,0.039882038,-0.058220897,-0.037860975,-0.016982852,-0.018749258,0.056466166,0.034685098,0.021449048,0.014906561,0.019326948,0.0008914595,0.073400065,0.007273203,-0.04691065,-0.03370962,0.031741843,0.03262186,0.042338558,-0.021455573,-0.00979577,-0.08419207,0.026908593,-0.003100089,-0.011775565,0.049741115,-0.005945177,-0.011497172,-0.016722288,0.035945967,0.0010585089,0.03924206,0.012932976,-0.001893254,-0.007830747,0.0026550216,0.03845819,-0.044446472,0.0010395482,-0.0070050796,-0.035654683,0.059878346,0.00834609,-0.008322899,-0.011909112,-0.030715737,0.0022297348,0.035838246,0.0139100505,-0.008278632,0.058996305,-0.04329518,0.03304656,-0.003006819,0.042180818,0.050349005,0.018775415,0.004634904,-0.023949524,-0.012594374,-0.009597529,0.020632992,0.0053383945,-0.011120083,-0.035920333,0.004524947,0.019482825,0.018726857,0.006795699,0.021395361,-0.047894128,-0.032423623,-0.011035761,-0.010435633,0.03385681,0.017514728,0.033940613,-0.070206165,0.0245223,-0.034477897,0.048461985,-0.016683508,0.02684304,0.01739179,-0.01733153]	Image title: Diagram of a Transformer Architecture\nTags: transformer, neural-network, self-attention, architecture\nKey objects: Multi-Head Attention, Add & Norm, Feed-Forward Network, Positional Encoding, Token Embedding, Inputs\n---\nSummary:\nThis diagram illustrates a single layer of a Transformer architecture. It shows how the input embeddings, enhanced with positional encoding, are processed through a series of layers, including Multi-Head Attention and Feed-Forward Networks, with Add & Norm operations in between.\nFull description:\nThe diagram depicts a single layer within a Transformer architecture. It starts with 'Inputs' which pass through a 'Token Embedding' layer, followed by the addition of 'Positional Encoding'. The combined output is then processed through a 'Multi-Head Attention' layer, followed by an 'Add & Norm' operation. Subsequently, the result is fed into a 'Feed-Forward Network' before going to another 'Add & Norm'.  This pattern repeats, signified by the 'x' indicating that the layer is repeated multiple times.\nText found in image:\n- Inputs\n- Token Embedding\n- Positional Encoding\n- Multi-Head Attention\n- Add & Norm\n- Feed-Forward Network	{"tags": ["transformer", "neural-network", "self-attention", "architecture"], "title": "Diagram of a Transformer Architecture", "doc_id": "58eeb957-3ec1-47cd-9e00-e6ee4b59afb3", "source": "./images/a-survey-to-transformers/image_2.png", "summary": "This diagram illustrates a single layer of a Transformer architecture. It shows how the input embeddings, enhanced with positional encoding, are processed through a series of layers, including Multi-Head Attention and Feed-Forward Networks, with Add & Norm operations in between.", "doc_type": "image", "key_objects": ["Multi-Head Attention", "Add & Norm", "Feed-Forward Network", "Positional Encoding", "Token Embedding", "Inputs"], "parent_doc_id": "a15ddce5-2631-44db-8950-9cc957003396", "text_in_image": ["Inputs", "Token Embedding", "Positional Encoding", "Multi-Head Attention", "Add & Norm", "Feed-Forward Network"], "contextual_description": "The diagram depicts a single layer within a Transformer architecture. It starts with 'Inputs' which pass through a 'Token Embedding' layer, followed by the addition of 'Positional Encoding'. The combined output is then processed through a 'Multi-Head Attention' layer, followed by an 'Add & Norm' operation. Subsequently, the result is fed into a 'Feed-Forward Network' before going to another 'Add & Norm'.  This pattern repeats, signified by the 'x' indicating that the layer is repeated multiple times."}
7ba7bc78-0dcf-4e3d-9ce9-0feed3256f87	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.026882486,0.008325626,0.040920693,0.02344169,0.014587118,0.03580794,-0.01471142,0.028357469,0.047829334,-0.026054135,0.004538445,0.017063936,0.026504816,0.019175045,-0.024082221,0.03280626,0.00048917794,0.06328,0.016321179,-0.032380506,0.064919405,0.011421901,0.04502308,-0.022029366,0.017992344,0.014094184,0.046281468,0.005875062,0.01490161,0.01693545,-0.02459071,-0.049810555,0.056026164,0.0018659517,-0.014385667,0.017292855,0.017154438,-0.032632075,-0.016008537,-0.023994315,-0.039217763,0.048137307,-0.0635607,0.029997112,-0.060905706,-0.033138998,-0.10689597,-0.033196084,0.015056491,0.0013749042,-0.034748144,-0.014098652,-0.01852129,-0.010204676,0.023286,-0.008024198,-0.024557846,-0.055135723,0.016462792,-0.08619467,-0.022615023,0.007132374,-0.049736384,-0.0020813816,0.013613524,-0.03708853,0.026877766,0.049846184,0.011333398,0.120218165,-0.006560874,-0.03525383,0.0057467273,-0.0071037672,0.11269734,0.048267756,-0.05563452,-0.044391442,-0.041915312,-0.0061741434,-0.083893485,0.089329444,-0.07004902,0.00023673939,0.10416218,0.0278252,-0.0546922,-0.060901497,0.08416586,-0.038788095,0.016057387,-0.002889722,-0.028169299,0.029716544,-0.021240752,-0.05896828,-0.008354036,-0.05437516,0.035910197,-0.05635545,0.012359017,-0.013008198,0.058747392,0.049978454,0.037654452,-0.020525502,0.009087481,-0.045209825,0.04314917,0.02568669,-0.023100412,-0.0034721014,0.01115577,0.015366974,0.0013261909,-0.013822885,-0.04121403,-0.01690435,0.0038077792,-0.00011622618,-0.006324995,0.001658135,-0.03557396,0.028660333,0.06959601,0.05339921,0.00066657807,-0.0066598174,-0.027755622,-0.01623563,0.009410316,-0.012115608,-0.0025959753,0.019907944,-0.029155368,0.016992245,0.097345434,0.028887123,0.025472546,-0.012210481,0.016574217,-0.032828033,-0.027427265,0.04008515,-0.028868267,0.048875544,-0.032783326,-0.0078009,-0.0106670465,0.039874732,0.0433229,-0.029723283,0.031041414,-0.027930154,-0.016606469,-0.025604479,-0.013523246,-0.07423831,0.008864621,-0.019436773,-0.040910996,-0.085568175,-0.0613903,0.016495597,0.01284842,0.07595946,0.03230052,0.018105464,0.040064454,-0.014759327,-0.060343172,-0.011906509,-0.0015250541,-0.044113412,0.024348542,-0.011829205,-0.002195354,-0.0486598,-0.031544674,-0.016269082,0.023380646,-0.007956295,0.018284965,0.068848655,-0.011395396,0.020764474,0.006742452,0.06932554,-0.044582203,0.019434102,0.030601123,-0.0485482,0.021438248,0.0065985387,0.063179426,0.024429852,0.047778398,-0.061702203,-0.06432798,0.0064441124,-0.05279823,-0.007531339,0.013976698,-0.040091418,-0.0032488657,-0.020773467,0.011896959,0.008165435,-0.026329579,0.018073494,-0.00226241,-0.042714205,-0.02452853,0.066124365,0.02366064,-0.004127421,-0.0138947135,0.040174972,0.04292901,-0.027886406,-0.022592999,-0.008685552,-0.0072315102,0.029978761,-0.009561029,-0.0141031435,-0.046041902,-0.03053202,-0.0033479757,-0.03915545,0.08985072,0.03157905,-0.04593398,0.01877063,0.0041868356,0.009556003,-0.01222528,-0.007213793,0.035124026,0.046433304,0.037749927,0.021689145,-0.043529347,0.057690505,0.010819479,-0.001791097,0.0023054462,0.031156886,-0.0028137707,0.029288383,-0.017745655,0.051143724,0.038082514,0.023385448,6.0917042e-05,-0.024873747,0.01243004,-0.017616251,0.04391714,0.0020970008,-0.014148147,-0.006689283,0.011109194,0.040004496,0.0065168724,-0.015170868,-0.010244315,0.021528827,0.0022893497,-0.012474887,0.005343347,0.05470522,0.023124948,-0.03815771,-0.026643727,0.025425803,0.0008865888,0.04200412,0.041215137,-0.014675243,-0.008547228,-0.025796141,0.015954789,0.025211165,0.0009784064,-0.030953597,0.027957642,-0.035348926,-0.10491341,-0.03704478,0.011819785,-0.0340703,0.025375798,-0.028508997,-0.0025582402,0.040956087,-0.0011444618,-0.009872956,0.00075633783,0.026524087,0.08580435,-0.011583875,-0.03192636,-0.012004695,-0.017943623,-0.025014019,-0.058069203,0.014504002,0.013071705,-0.007453226,0.022834674,-0.037144206,-0.05628979,0.011435409,0.051248744,0.099872164,0.08933822,-0.0743688,0.0051847794,0.00079886196,0.019203922,-0.039031398,-0.04131503,0.038763024,0.037029877,-0.00033551402,0.007383316,-0.017458094,-0.057911847,0.04860368,-0.11370172,0.051348787,0.068040445,0.012905687,-0.04777358,0.016481012,-0.021209206,0.043546993,0.0029644088,-0.01603054,0.018813282,-0.048742127,-0.037499398,-0.008038311,0.029249992,0.04411193,-0.023408277,0.022184871,0.0058795693,0.009228304,0.0041946694,0.011036107,0.014656672,0.02159979,0.0072808173,-0.04354959,0.01070986,-0.03883754,0.03698453,0.01225859,0.07706996,0.0065046307,0.03373632,-0.040822033,0.0254327,0.03143374,-0.072694056,-0.017941551,0.00612703,-0.017033458,-0.0217809,-0.023599153,0.065313205,0.0016124957,-0.026782246,0.006883852,0.044897098,0.013552761,-0.029883038,-0.024359796,0.012251629,0.008521192,-0.048138957,-0.03654768,-0.005119519,0.013812338,0.019015536,0.035198893,-0.005044528,0.036326464,-0.03569155,-0.01343005,0.02415911,0.008871963,-0.017097289,0.022804765,0.025106268,0.020724267,-0.0030286792,-0.046714153,0.021081308,0.044950206,-0.036653765,0.066653065,0.01923395,0.01024242,-0.08226126,-0.0012705729,0.059608944,0.040953282,0.018554306,0.008869075,-0.0734997,0.0069969757,-0.03365365,0.0064142635,-0.009361851,0.04801239,-0.065731145,0.00498374,-0.03796402,0.04545379,0.00062132836,0.037452903,0.021398807,-0.014290672,-0.015931172,0.01273152,-0.011920078,-0.02797253,0.023008669,-0.05921404,-0.040723994,0.038539708,0.049923435,0.00827412,-0.06117276,0.066647924,-0.05177369,0.039296377,0.09014958,0.010676535,0.029484017,-0.018317934,0.03981095,-0.015662597,0.0053679887,0.0666251,0.013725144,-0.0047370037,-0.008045878,0.020618096,-0.0025633045,-0.0059221797,0.01365153,0.003983154,0.0004443295,0.012330581,-0.020362534,-0.027349025,-0.04110068,0.044472832,0.00793097,0.012299553,0.01324456,-0.043094877,-0.021341778,-0.028125323,-0.002190006,0.021012807,0.0061404584,0.021458192,-0.033623576,-0.016748404,-0.014696567,-0.008083284,-0.04413697,0.10790951,0.052260965,0.038788587,-0.04955761,-0.023388473,0.009003197,0.05738451,-0.034678135,-0.018302906,-0.023684913,-0.03403913,-0.040860094,0.025137974,0.032834888,0.031751465,0.042484023,0.04894691,0.02711584,-0.062811464,0.00037697144,0.02371515,-0.025013464,-0.06408954,-0.014692462,0.010361994,0.06417548,-0.0028806604,-0.043900274,0.0029594942,-0.025462124,-0.008692138,0.0018685942,0.06577313,0.04718028,0.010978343,-0.02592854,-0.037537012,-0.041033704,0.017251119,-0.0137273455,-0.03442699,0.02508344,0.07304252,-0.019428542,0.03926163,0.031208703,-0.033834793,-0.008972811,-0.03174956,0.06865208,0.03637692,0.082504876,-0.0062733106,0.08109511,-0.037941437,-0.011274248,0.018256651,-0.0055008433,-0.027371714,0.038641084,-0.046825565,0.0041399524,-0.018597443,0.001531913,0.015842136,0.0039204345,-0.020440556,0.073100895,-0.06357773,0.010975907,-0.05005963,0.0018530844,0.08864361,-0.022783486,-0.017384369,-0.01899854,-0.054332074,0.00096988585,0.05386721,0.053052634,0.02586304,-0.04815761,0.058923304,-0.030681266,0.055358376,0.017937474,-0.05010754,-0.036433782,-0.003407159,-0.0089399805,0.0028196238,-0.023812199,0.0061830124,-0.02208057,0.011629054,-0.0070335693,-0.0028718892,-0.008856925,0.05089747,0.04094696,-0.03183067,0.03674058,-0.026113197,0.013925374,0.04568912,-0.0039449492,0.003765033,0.029884953,0.0023908392,0.007945501,-0.023185246,0.036782335,-0.014383909,-0.009678715,-0.027729126,0.018199435,0.03906165,-0.06983415,-0.027054707,-0.023488184,0.022279613,0.055045176,-0.028427275,-0.039192248,-0.0038400723,-0.0015243184,-0.027929127,-0.014577403,0.011997154,0.0086830435,0.02527064,-0.044639718,0.0930446,-0.0801303,-0.010860729,-0.035736393,0.026063163,-0.024962384,-0.009399451,-0.020088347,-0.013555427,-0.036455028,-0.060137145,0.03332675,0.06112784,-0.017120741,0.003906906,-0.013670862,-0.011788944,-0.014336656,-0.0038045049,-0.0077396864,-0.0075470856,-0.0052281916,-0.01899242,0.06375728,-0.019945428,0.0005322133,0.028450819,-0.03748345,-0.08066359,-0.008474106,-0.0049316557,0.043842223,-0.0031770905,0.000101358186,0.032147117,0.039498445,-0.073178455,-0.036450088,-0.039131783,-0.0041130204,-0.035868954,-0.013406769,0.08734484,0.0069619175,-0.086308226,0.03578997,-0.036846,-0.03956814,-0.0068960846,0.014455628,0.05691641,0.032365553,0.00427444,0.0068762284,-0.07520675,0.0014890102,-0.035393305,0.029499358,-0.026579766,0.042921375,0.00029960438,-0.0069901594,-0.017893247,0.0071377074,-0.0057053384,0.006817391,0.017782664,0.031450704,-0.015677484,-0.049045365,-0.0038455236,0.034696423,-0.028849253,-0.013368446,0.039537746,0.031880185,-0.046090897,-0.00611054,-0.054706164,-0.0053659664,0.0022421514,0.013044846,0.01961213,0.034823023,-0.044927746,0.009540188,0.04934855,-0.014315653,-0.02966481,-0.016482595,0.022525424,0.017049383,0.01575916,0.0068538636,0.007657979,-0.09274377,0.016735528,0.006226488,-0.09766156,0.02717693,-0.0142825935,-0.022231614,-0.016673727,0.035750877,-0.027847312,-0.016746579,0.021876551,0.0138058625,0.004567539,0.0009484166,-0.0023228442,-0.01344823,0.025369845,-0.023161098,0.022670576,0.012942793,0.06275764,-0.015665406,6.649701e-05,0.03091943,0.011434044,0.002033217,-0.027597366,0.037966285,0.051504295,-0.065958425,0.029776117,-0.016543945,0.0010206162,0.050971422,-0.016664388,0.026138414,-0.032756865,0.051100634,0.026898835,-0.006699911,-0.004329385,-0.05676738,-0.10116951,0.00041266574,0.016902762,-0.006158732,-0.019255646,0.0116897905,-0.024942746,-0.0432261,-0.025244879,0.014968536,0.04817422,0.0018590912,0.01709611,-0.017570414,0.046217162,-0.030624907,0.026902169,-0.0546576,-0.025351698,0.022857985,-0.026030108]	Keywords: Transformer variants, architecture modifications, pre-training methods, applications, taxonomy\nKey Objects: Transformer, architecture, variants\nRefers to Images: ./images/a-survey-to-transformers/image_2.png\nHypothetical Questions:\n- What are the three main ways Transformer models have been modified?\n- Why is the attention module considered the key component of the Transformer?\n- Besides architecture modifications, what other areas of Transformer research are covered in this survey?\n---\nSummary:\nThis survey categorizes Transformer variants based on architecture modifications, pre-training methods, and applications, with a focus on architecture modifications.\nOriginal Text:\nFig. 2. Categorization of Transformer variants.  \n  \nFig. 3 illustrates our taxonomy and some representative models.  \nIn this survey, we focus on reviewing the works on architecture modifications. Since the attention module is the key component of Transformer, we solely describe the attention-related variants in Sec. 4 and introduce the other module-level variants in Sec. 5. Then Sec. 6 describes the other architecture-level variants. Finally, we briefly review the works on pre-training in Sec. 7 and applications in Sec. 8. There are some comprehensive surveys on the latter two categories of work, such as pre-trained models (PTMs) [100] and visual Transformers[47, 64].\nContextualized Text:\nThis survey organizes different types of Transformer models into categories based on how they have been modified. The focus will be on reviewing changes made to the Transformer's architecture, with particular attention given to modifications of the attention module, as it is the key component of the Transformer.	{"tags": ["survey", "architecture", "NLP", "deep-learning"], "doc_id": "7ba7bc78-0dcf-4e3d-9ce9-0feed3256f87", "summary": "This survey categorizes Transformer variants based on architecture modifications, pre-training methods, and applications, with a focus on architecture modifications.", "doc_type": "text", "entities": ["PTMs", "visual Transformers"], "keywords": ["Transformer variants", "architecture modifications", "pre-training methods", "applications", "taxonomy"], "key_objects": ["Transformer", "architecture", "variants"], "contextual_text": "This survey organizes different types of Transformer models into categories based on how they have been modified. The focus will be on reviewing changes made to the Transformer's architecture, with particular attention given to modifications of the attention module, as it is the key component of the Transformer.", "mentioned_images": ["./images/a-survey-to-transformers/image_2.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "3 TAXONOMY OF TRANSFORMERS"}, "hypothetical_questions": ["What are the three main ways Transformer models have been modified?", "Why is the attention module considered the key component of the Transformer?", "Besides architecture modifications, what other areas of Transformer research are covered in this survey?"]}
38a78818-cd09-44e0-94b0-e94b3b30eaa5	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.050148286,0.007295211,0.023914916,0.08397775,-0.015819632,0.06293795,-0.02883774,0.049423777,0.039776046,-0.015148932,-0.024643362,0.030584095,0.012074874,-0.040842045,0.0034684169,0.03350541,0.04113349,0.07829257,0.019168414,-0.03396742,0.021563172,0.0314979,0.007689867,-0.012592954,0.010463953,0.009613797,0.045050934,-0.048062887,-0.040353965,0.022232695,0.0052013933,-0.013671421,0.035558984,0.024010116,0.067269206,-0.024982274,0.0023791292,-0.03504945,-0.018171756,0.0043744626,-0.030639414,0.06644094,-0.060364198,-0.00011884835,-0.025250023,-0.05416909,-0.10256074,-0.017473,0.018273233,-0.037400097,-0.017549714,0.016426621,-0.008838465,0.0017469977,-0.042658277,-0.03742249,-0.04135391,-0.067543946,-0.019595668,-0.10707107,-0.026871964,0.0041922205,0.0006714181,0.016937932,-0.012123885,-0.012513033,0.028299518,-0.01566538,0.02659053,0.15877207,-0.010130026,0.03326289,0.030571427,-0.024380589,0.11031589,0.05685511,-0.05743453,-0.016830549,-0.067038715,-0.021329438,0.0144172115,0.07574509,-0.03207951,-0.03950535,0.09050586,0.029026337,-0.043482274,-0.021820761,0.042751364,-0.05732434,-0.0028575603,-0.0129317595,0.038904175,0.02820851,-0.01108398,-0.033370912,-0.04871486,-0.017558923,-0.017447306,-0.021852806,-0.014518443,0.007313489,0.063682884,0.09998761,0.030090872,-0.0039548385,-0.016144264,-0.0016247955,-0.012373891,0.026467003,-0.06791896,-0.015239141,-0.0027278147,0.03286076,0.0075461837,0.037280757,-0.008380827,0.0034890787,0.013614511,0.031344395,0.02600727,-0.014866279,-0.015273324,-0.02752096,0.041095354,0.056751136,-0.034447018,-0.0015822971,-0.044870798,-0.011606054,0.061450385,0.00691004,-0.028817482,0.026556594,-0.02216336,-0.015814804,0.035777558,-0.00941133,0.013175046,0.0034488477,0.041256927,-0.06820783,-0.041317888,0.01532452,-0.032177776,0.055776205,-0.05508444,0.012305483,-0.061119128,0.0394019,0.050465606,-0.0140724955,0.027948229,-0.0018734209,-0.032464184,0.017086277,-0.025650209,-0.05555736,-0.011361847,-0.04375814,-0.06296012,-0.05600425,0.03469704,0.077935085,0.026180085,0.0928405,0.011678018,-0.0072003263,0.025699185,0.013138158,-0.029625753,-0.0010114029,-0.0220943,-0.02546791,-0.005255969,0.03923149,-0.021010958,-0.016153676,-0.013495891,-0.022279913,-0.00551541,0.050459635,0.00044554926,0.037092365,-0.034689143,0.037966706,0.01961712,0.03424226,0.010374492,-0.0454986,0.026657019,0.007892069,-0.026300173,-0.012386951,0.052197274,0.027279109,0.054021426,-0.027777847,-0.07439722,-0.029929932,0.032978877,0.01613077,0.0317596,0.0069931624,0.0077860346,-0.02389246,0.014638095,0.009311143,-0.026847534,0.03428309,-0.0025177577,-0.043580975,-0.0061173197,-0.011598014,0.02637645,-0.0008454437,-0.007154094,0.021568766,0.011230772,-0.04321476,-0.039254855,0.0053137997,-0.06186433,-0.013276537,-0.054488156,0.018263552,-0.06455985,0.009459131,0.006903623,0.0038967398,0.014694519,0.0036284507,-0.05219276,-0.004980657,-0.010985921,0.07352385,0.0053195707,0.037457444,-0.031216847,0.021615574,0.029186558,0.034036677,-0.025060477,0.061401546,-0.011611754,-0.016785808,0.032771084,0.05064858,0.031472974,0.04664469,0.00428107,0.033561423,0.039548974,-0.0013329453,-0.021658087,-0.056105826,-0.014235384,0.030061863,0.018897664,-0.011292433,-0.0044719283,-0.009434424,0.0019766851,0.03259802,0.023628535,0.022431375,0.026262008,0.05385809,-0.06729284,-0.040314183,-0.009973479,0.002087494,0.035237923,-0.027254362,0.03513721,-0.02775409,-0.030423725,-0.034260605,0.011945041,-0.0082435105,-0.009060983,-0.02470757,0.011644751,0.010490827,0.012935302,-0.027010078,0.004700621,-0.07370705,-0.07871277,-0.03194259,-0.037760466,-0.03156503,0.03996549,-0.024358882,-0.005109943,0.029210875,-0.027316445,-0.010060461,-0.018104043,0.00752558,0.08894912,-0.05290929,0.02511187,0.0009825425,0.0062986882,0.015205432,-0.026993083,0.0067567728,0.06660808,0.018639615,0.039814726,0.009873609,0.035346575,0.017397396,0.060493547,0.06837232,0.04099697,-0.026862446,0.044732694,0.03128403,0.040689398,0.00991363,0.03333057,0.054078266,0.0707158,-0.035123445,0.030502997,-0.012794873,-0.046590045,0.07752082,-0.054805364,0.028322175,0.06273152,0.0136917615,-0.02126895,0.014851898,0.028440451,0.08112076,-0.028534997,-0.013901619,-0.04818479,-0.013659033,-0.013788211,0.0074058413,0.0030981642,0.062801905,-0.013002207,0.0012486604,0.035592627,-0.017770123,0.004928713,-0.012470356,0.015594343,0.011938298,-0.014756577,-0.01845321,-0.020361809,0.03852508,0.010422362,-0.03695613,0.053831052,-0.06826963,0.07220135,-0.018464943,0.056440357,-0.012343845,-0.030795798,0.003727395,-0.01576346,-0.019783285,-0.04967253,-0.04028887,0.03429938,-0.016848912,-0.02105714,0.038760602,0.03539832,-0.00027089388,-0.00464574,-0.003982814,-0.0183432,-0.0111252945,-0.058669426,-0.05323878,-0.007884933,0.05265558,-0.019046959,0.09946482,0.0043990174,0.03570887,-0.029795082,-0.006491374,0.0051328484,0.0047004516,-0.027282137,-0.019356102,0.008364185,0.058131862,0.0020017752,-0.007518264,0.033054218,0.07238655,-0.05008805,0.012487215,0.05419311,-0.013235641,-0.043247607,0.05433209,0.05708835,-0.008942124,0.045930486,-0.010684462,0.023017561,0.012714888,0.0042864643,-0.008719383,-0.047479983,0.06629258,-0.015010335,0.004607474,0.019714747,-0.017355932,0.019366015,0.031970844,0.010172986,0.016801821,-0.008578205,0.054782167,0.021878093,-0.0062625697,0.028585782,-0.04028614,0.0218724,0.05641306,0.021745745,0.03890038,-0.0611406,0.07367729,-0.038124453,0.030052433,0.03696638,-0.035495613,0.022933453,0.028222064,-0.002155085,-0.023498423,0.063987084,0.049713463,-0.024173679,0.0047855377,0.03296568,-0.020409886,0.0050289994,-0.0165798,0.03448189,-0.009703999,-0.026806187,-0.024736045,-0.08797984,-0.050186843,0.014023518,0.028827693,0.015909517,-0.013902077,0.010852301,-0.0060212356,-0.014069469,-0.021651521,-0.013723475,0.028907483,-0.0034817732,0.021527352,-0.01974877,0.039934147,0.002872681,-0.02253805,-0.030520525,0.01893347,0.015310863,-0.025834478,-0.03268507,-0.030440437,0.033086855,0.049724586,-0.030806784,-0.0059687146,-0.019430922,0.0044096005,-0.04878053,0.048042815,-0.005277932,0.02523726,0.047060143,0.054204952,0.013321844,-0.036171492,-0.025875863,0.019021181,0.013165306,0.0014708902,0.012750825,-0.010815962,0.053326163,0.06300574,-0.030688768,0.003522307,0.014851092,-0.02902103,-0.0077118683,0.048851125,0.031460714,-0.029544678,-0.04501042,0.007405026,-0.04497865,0.03703094,-0.020466415,-0.007827184,-0.024256734,0.03411108,-0.0056377323,0.008916405,0.008582239,0.08265457,-0.011352354,-0.011589439,-0.017627748,-0.0020215893,0.023811152,-0.06055892,0.049540024,-0.04498994,-0.050279368,-0.0015840817,0.008193362,-0.0476623,-0.023078686,-0.03681823,0.034315225,0.029082794,-0.022873485,-0.02280505,-0.037370436,0.017288676,0.07384608,-0.07021198,0.039671842,0.017820653,0.050446432,0.024423672,0.014537017,-0.0013939497,0.0036821123,0.01013396,0.026622415,0.06956979,0.024977366,0.0017683577,0.029247804,0.036538914,-0.0056632096,0.009381414,-0.021925086,-0.044728946,0.012314155,-0.06085248,-0.014286547,-0.030835096,-0.045030385,0.043654796,-0.036145125,0.027139967,-0.025314968,-0.014336826,-0.024450216,0.065844245,0.044213068,0.022403177,0.037548285,-0.03684012,0.028511632,0.08445564,0.028734853,0.027745198,0.009590648,0.01258644,-0.013769801,-0.0133533375,-0.009470401,-0.00038946274,-0.0114702,-0.038137436,-0.0039337906,0.012942254,0.021653602,0.008391132,0.037542548,0.038478114,-0.024513237,-0.0021474648,-0.08876838,-0.04222501,-0.005970287,-0.0009928014,0.019903826,0.01670776,0.011015291,0.0054442193,-0.054276038,0.08661674,-0.062599175,-0.014369813,-0.03448442,0.06848115,0.0144048445,-0.06204286,-0.020180117,0.0020462594,0.005995524,-0.0033650848,0.07814244,0.066378854,-0.025565738,0.0025006325,-0.01931259,0.01361522,0.029120423,0.026402934,-0.041190777,0.045643028,0.004848161,-0.016157353,0.08193523,0.026832333,-0.0020983187,0.02921541,-0.06073016,-0.057431802,-0.022000395,0.025317764,0.013490829,0.055827603,0.014710212,0.038747575,0.029679637,-0.09845591,-0.009262859,-0.009842457,-0.007150979,0.017347336,0.05200225,0.057403166,-0.014712988,-0.04774815,0.011376738,-0.015111904,-0.09480564,0.022999898,0.01278937,0.031868033,0.058723122,0.057270117,-0.046840034,-0.07312488,-0.041536372,-0.060171545,-0.016917033,-0.029922433,0.047226697,0.0025579378,0.0074738576,-0.037024625,0.018370267,0.040582757,0.013019315,0.03377075,0.0023053752,0.0048511303,-0.0361366,0.052359097,0.053426854,-0.030240417,0.01539638,0.05060785,0.032474916,-0.06404113,-0.031036703,-0.05065614,0.011670758,0.06933928,0.027258681,0.04495891,0.022009715,0.022380237,0.00449689,0.08643655,-0.004229862,-0.052781302,-0.022514887,0.012108911,0.033243824,0.04540755,-0.0031448076,0.0056013158,-0.071192764,0.04589089,0.013181735,-0.014695483,0.05748513,0.0024344744,-0.029962724,-0.009079318,0.029075507,-0.023037937,0.041819587,0.012870335,0.0029952002,-0.0120688705,0.011317637,-0.006450909,-0.051851124,-0.016255423,0.004157954,-0.011667877,0.039748862,0.0175526,-0.009309038,0.006579616,-0.0012193833,0.030098593,0.026006198,-0.0067986767,0.013524378,0.037631903,-0.020854576,0.02781462,0.0005833033,0.0346638,0.044253785,0.050407708,-0.004889064,-0.027775502,0.010938774,-0.0073968717,0.0034301297,0.011347218,-0.027149739,-0.04624274,0.009056195,0.021554008,0.0378096,0.007668594,-0.007834242,-0.03635339,-0.031589277,-0.014465706,-0.029717626,0.05161434,0.040623244,0.028590914,-0.04862255,-0.004251726,-0.013573754,0.014352911,0.02501741,0.033101235,0.027454298,0.021632625]	Image title: Transformer Model Architecture\nTags: transformer, neural-network, NLP, self-attention, architecture, encoder-decoder\nKey objects: Token Embedding, Positional Encoding, Multi-Head Attention, Feed-Forward Network, Add & Norm, Nx\n---\nSummary:\nThis diagram illustrates a simplified view of a Transformer model architecture. It shows the flow of data through multiple layers, including token embedding, positional encoding, multi-head attention, and feed-forward networks. The architecture highlights various modules and layers involved in processing input sequences.\nFull description:\nThe diagram depicts a stack of identical layers (Nx) processing input data. The process begins with 'Inputs' which are first converted to 'Token Embedding' vectors. These embeddings are then combined with 'Positional Encoding'. The combined information then flows into a 'Multi-Head Attention' layer.  The output from 'Multi-Head Attention' is added and normalized, represented by 'Add & Norm'.  This result is then passed through a 'Feed-Forward Network' before being added and normalized again. The output of the last layer is the final output.\nText found in image:\n- Inputs\n- Token Embedding\n- Positional Encoding\n- Multi-Head Attention\n- Add & Norm\n- Feed-Forward Network\n- Nx	{"tags": ["transformer", "neural-network", "NLP", "self-attention", "architecture", "encoder-decoder"], "title": "Transformer Model Architecture", "doc_id": "38a78818-cd09-44e0-94b0-e94b3b30eaa5", "source": "./images/a-survey-to-transformers/image_2.png", "summary": "This diagram illustrates a simplified view of a Transformer model architecture. It shows the flow of data through multiple layers, including token embedding, positional encoding, multi-head attention, and feed-forward networks. The architecture highlights various modules and layers involved in processing input sequences.", "doc_type": "image", "key_objects": ["Token Embedding", "Positional Encoding", "Multi-Head Attention", "Feed-Forward Network", "Add & Norm", "Nx"], "parent_doc_id": "7ba7bc78-0dcf-4e3d-9ce9-0feed3256f87", "text_in_image": ["Inputs", "Token Embedding", "Positional Encoding", "Multi-Head Attention", "Add & Norm", "Feed-Forward Network", "Nx"], "contextual_description": "The diagram depicts a stack of identical layers (Nx) processing input data. The process begins with 'Inputs' which are first converted to 'Token Embedding' vectors. These embeddings are then combined with 'Positional Encoding'. The combined information then flows into a 'Multi-Head Attention' layer.  The output from 'Multi-Head Attention' is added and normalized, represented by 'Add & Norm'.  This result is then passed through a 'Feed-Forward Network' before being added and normalized again. The output of the last layer is the final output."}
ab3cdc79-1823-4c5f-ac9c-273b9946be37	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0042544296,0.00821747,0.007540877,0.019243782,-0.027738672,0.023883171,-0.0044548977,0.057270974,0.031535145,-0.016835906,0.0073690247,0.016131502,-0.009583287,-0.0014511426,-0.005735906,0.004358379,0.018506601,0.058988877,0.00048579773,-0.0450928,0.027218143,0.03511852,-0.0195962,-0.0129511105,-0.0026919774,0.037154425,0.03077974,-0.0095648635,-0.01626137,0.0070963404,0.027608424,-0.04579958,0.050375983,0.004844294,0.05306951,-0.028300758,0.015029244,-0.075788505,0.011213409,-0.0008117298,-0.034228437,0.05268585,-0.05548983,-0.011711034,-0.0070133065,-0.08582612,-0.079312645,-0.02730279,0.016432399,-0.009813854,-0.0046258355,-0.0020163606,-0.0068271086,-0.040478963,-0.0105737,-0.008291639,0.006873669,-0.043599986,-0.0049594366,-0.10510365,-0.016397286,0.005828994,-0.02637563,0.00032018698,0.029084614,-0.0058692843,0.04131335,0.023923466,0.040852588,0.117848605,0.01757532,0.00888292,-0.029719837,-0.07985766,0.12678245,0.02890313,-0.023343539,-0.040953215,-0.048502367,-0.0091475565,-0.028884783,0.07403383,-0.060824998,-0.057393488,0.06935409,-0.028598897,-5.9208334e-05,0.01573139,0.061280433,-0.04052777,-0.0018783191,-0.0046780147,0.013976873,0.07496149,-0.031844635,-0.068396874,-0.005297472,-0.026575817,0.030549524,-0.08731527,0.026443478,0.012605954,0.0873483,0.10347881,0.04955096,-0.03262236,-0.011768924,-0.033659313,0.012909909,0.007552808,-0.01800203,0.00065149914,-0.03417179,-0.0024328532,-0.020786986,0.022530906,-0.030411182,0.01952869,-0.0015900482,-0.014036347,0.0042665964,-0.028477574,0.00094625313,0.039531816,0.08369797,0.03454069,0.03824667,-0.006544916,-0.037144445,0.012639099,0.022871511,-0.034082476,-0.019591348,0.022933751,-0.011219673,0.009163159,0.090164974,0.030375818,-0.012848256,-0.0020581682,0.019700099,-0.034599207,-0.050011404,0.0131004285,-0.007958024,0.028058795,-0.021723185,0.033879206,-0.025726773,0.06614881,0.03283497,-0.024869805,0.045776255,-0.00038466917,-0.024465442,-0.035511028,-0.01949297,-0.061702404,-0.0032523607,-0.046606448,-0.020684488,-0.03739103,0.030992797,0.08478024,0.06334581,0.11755959,0.024085797,-0.013336087,0.021720788,0.017764132,-0.036848918,0.011903825,-0.0062091756,-0.047057558,-0.013717554,0.009870892,-0.011342425,-0.03339496,-0.0015267369,-0.01698495,-0.0023196966,0.08538321,-0.005327752,0.01811953,-0.042907607,0.026634838,0.008113166,0.057647534,-0.04007735,-0.00745473,0.020094145,-0.039603412,0.039621275,0.00051107054,0.041413773,0.045670398,0.036827575,-0.0962538,0.0054734154,-0.045899883,-0.00899783,0.009785001,-0.03790841,-0.006398929,0.029688528,0.0034218945,0.0074079684,-0.019020943,-0.012652322,0.03612274,-0.016266007,0.00678877,-0.035733055,0.053044427,0.0028045073,-0.018817525,0.0074897334,0.014730319,0.032364585,-0.026701324,-0.022754326,0.004147132,-0.02147362,0.03636988,-0.03386518,0.03202513,-0.03361992,0.022039147,-0.010362046,-0.013848678,0.018600406,0.022524202,-0.06039654,0.042151384,-0.03706293,0.018990578,0.011509265,0.0111379465,-0.01802665,0.01829389,0.028696261,0.058321245,-0.005873532,0.102970846,0.0067150136,0.009952491,0.020490846,0.086455435,0.02710714,0.038126092,-0.009636868,0.051892336,0.018751446,-0.013773983,0.007156447,-0.04463307,0.02164622,0.0051427903,0.05122412,-0.04403591,0.029217565,-0.019175412,0.00028369774,0.037910663,0.005699451,-0.036315814,0.03248249,0.018246742,-0.049708758,0.017473307,0.038127393,0.022960385,-0.016249822,-0.027131287,-0.05497011,0.030282235,-0.0033541124,-0.02176874,0.02297509,-0.01843718,-0.027299799,-0.017187407,0.012958004,-0.00703924,0.01729443,-0.021420276,0.022200238,-0.019927511,-0.035064388,0.0002800834,-0.01597189,-0.020965513,0.036015708,-0.03592329,-0.023291912,0.013032527,0.014436507,-0.0038174742,0.012163173,0.0036874462,0.06343228,-0.0022678254,-0.01893375,0.023560278,0.051051006,0.05014695,-0.060008053,0.035892993,0.02600044,0.020266218,-0.010091574,-0.03133485,-0.031306986,0.020253552,0.025438955,0.09615856,0.101534925,0.008189768,0.0026970601,-0.0098386835,0.028915375,0.0016298094,0.0054771025,0.007187007,0.02269742,0.007000176,0.018348116,-0.034492813,-0.10250971,0.030429095,-0.07837526,0.0819606,0.05507031,-0.0005882543,0.014697059,-0.0008593714,-0.0070838975,0.03938404,0.017177118,-0.015106224,-0.012770059,-0.030803302,0.007258074,-0.026418112,-0.01731721,0.06955318,0.003877067,-0.020050453,0.007356819,0.030859781,0.022715913,0.027207814,0.02466788,0.038709074,0.025026591,-0.009816814,0.002547979,-0.0077328067,0.006990104,-0.028757188,0.059052162,-0.07291411,0.08648466,-0.021278821,0.025539028,-0.0046552913,-0.087633215,0.023178114,0.060945094,-0.0029280728,0.014441851,-0.04805295,0.081402324,-0.0419666,-0.05297534,0.044989932,0.052134052,0.030506503,-0.028739087,0.012953871,0.014624242,-0.008028616,-0.031096278,-0.046196796,0.007942104,0.012044077,-0.008129564,0.08451192,-0.03427719,0.06211635,-0.056495685,0.0047939033,0.010073772,0.006761037,-0.041450802,-0.0024898453,0.032266762,0.049177922,-0.00019575025,-0.016005436,0.028353382,0.034970727,-0.05377114,0.022222603,0.03838139,0.010988611,-0.08269369,0.029844595,0.017640198,0.0036536632,0.0024356276,-0.048982363,0.019909035,0.0025589431,0.027110353,-0.038183436,-0.023202963,0.013317967,-0.052705787,-0.048482966,-0.029189,0.06089246,0.02389028,0.06372185,0.031891737,-0.00834417,-0.04666043,0.028047591,-0.02635705,-0.06652326,0.026214268,-0.024235355,-0.015596321,0.04721845,-0.01478628,0.023042116,-0.048656207,0.096034646,-0.042540684,-0.0004459507,0.070452504,-0.04392814,-0.01420343,-0.040761303,-0.00018230255,-0.039318584,0.0055460846,0.01988889,-0.027121358,-0.014609412,0.010533062,-0.011824974,0.013077794,-0.0110097155,0.048436455,0.01227027,-0.008432598,-0.0327214,-0.06343924,0.020875461,0.010847943,0.035589036,0.023359593,-0.020791102,0.027839575,-0.0213139,-0.01111458,-0.04366289,-0.020131677,0.0035522056,-0.0024272208,0.041371934,-0.026772564,0.045739543,-0.0091566965,-0.052589618,0.0015995246,0.05850882,0.03620602,0.0012138153,-0.049375728,-0.05677142,0.01850212,-0.00424346,0.017301522,0.018229565,0.013262528,-0.040956616,-0.02674596,0.032039788,-0.020298593,0.05056353,-0.0039023429,0.0117714135,0.024615686,-0.04291048,-0.017517604,0.0083252415,0.024432523,-0.01649688,-0.039535172,0.0115608275,0.031072713,0.04465727,-0.043218706,0.009994505,0.003544782,0.010839685,-0.01731778,0.039240856,0.043431666,-0.011867539,0.03083184,0.014482661,-0.0691912,0.0144223375,-0.031735715,0.0064005605,-0.0073588593,0.035144504,0.008156194,0.02335495,0.05242831,0.012129053,0.003666681,0.02081638,-0.015916742,0.036999572,0.027815232,-0.10003921,-0.023544846,-0.04041664,-0.019425007,0.007830899,-0.052613273,-0.055809617,0.0065663457,-0.05195653,0.029706499,0.009675256,-0.020859603,-0.04140068,0.041244455,-0.04123163,0.03907093,-0.052957866,0.01859601,0.029884225,0.003697374,0.07018086,0.02217992,0.0116633605,-0.021726396,-0.018922152,-0.042397078,0.004987262,0.08593431,0.008768932,0.028842272,0.033761982,-0.017267575,0.009741679,-0.021795023,-0.06246917,-0.06805425,-0.008921161,-0.01955337,-0.0009264394,-0.027095197,0.0112993475,0.00075213396,0.006495524,-0.01654603,0.0056415633,-0.002158791,0.03189206,0.049096987,0.010919559,0.048454713,-0.032807145,0.04314973,0.052672584,0.055350147,0.010102107,0.03405551,0.03938438,0.01543275,0.015550079,0.039408464,-0.025114717,-0.039107785,-0.047392633,-0.0023419124,0.002101311,-0.022412978,-0.009603495,0.01866636,0.0038123624,0.036206737,-0.012582737,-0.0553381,-0.04117462,0.001292325,-0.018202797,0.019295514,0.03033116,-0.0154955955,0.040471125,-0.025990829,0.055373505,-0.114072464,-0.03828221,-0.014487466,0.04780799,-0.021874439,-0.03347719,-0.015326067,-0.010503162,-0.041243583,-0.03148803,0.06737864,0.015550616,-0.022427268,0.019987913,0.007336397,0.005035745,-0.020245185,0.010770194,-0.0025285215,-0.0037369833,-0.009122709,-0.025809452,0.0725954,-0.056405824,0.018147985,0.06429359,-0.045426175,-0.061803468,-0.011700156,0.0029498897,0.053248163,-0.015126315,0.021756848,0.008157146,0.06327776,-0.030809237,0.015902316,-0.057721637,0.010590909,-0.026176091,0.037728507,0.052401006,0.009806008,-0.019806279,-0.0071541504,0.0011307934,-0.020660777,0.03195577,0.057557896,0.0512242,0.07273961,0.03035933,-0.05557329,-0.027774094,-0.01572707,-0.037997417,0.035633508,-0.016917283,0.043734927,-0.026470596,0.0008127406,-0.0050218,0.0041633435,0.030359799,-0.0054951287,0.052621722,-0.0063876254,-0.06109709,-0.066479966,0.018404733,0.011950119,0.0067970767,-0.029455727,0.043213297,0.003882843,-0.057446994,-0.017683623,-0.053319063,-0.060913295,-0.00983627,-0.012026021,-0.012172366,-0.013482704,-0.008371545,-0.015597481,0.07850611,0.008491073,-0.061712,-0.045130014,0.037429444,0.0326727,0.03230834,-0.011531853,-0.0037225208,-0.0666023,0.021612322,-0.0018832859,-0.0593969,0.02266503,-0.005816968,-0.005778688,-0.019202359,0.033141255,-0.015021771,0.018556822,-0.012896482,0.017381078,0.012242479,-0.026293673,-0.007615857,-0.09058309,-0.009803135,-0.036495633,0.005759034,-0.021250192,0.05549017,0.011065078,-0.019670818,0.0032681006,0.010602507,0.0078078667,0.02926248,-0.011282597,0.06591549,-0.036000118,0.037018362,-0.008468541,0.00056183327,0.081462756,-0.0029225964,0.0078038415,0.038149945,0.026399981,-0.023101708,0.030739214,-0.0018818908,-0.02512957,-0.0389049,0.026721386,0.025982004,0.02141399,0.027524788,-0.0013727291,-0.046062794,-0.033821657,-0.03845784,-0.012529357,0.05099001,0.042879436,0.02184438,-0.056690138,0.0137899695,-0.035168316,0.02407829,0.011339997,0.044092804,-0.022677004,-0.0020760763]	Keywords: self-attention, Transformer, structural bias, complexity, attention mechanism\nKey Objects: self-attention, attention mechanism\nRefers to Images: None\nHypothetical Questions:\n- Why does self-attention have a complexity of O(1<sup>T</sup>D)?\n- What does it mean for self-attention to lack 'structural bias'?\n- How do the different approaches to improving attention (e.g., sparse, linearized) try to address the identified challenges?\n---\nSummary:\nSelf-attention is crucial for Transformers, but its complexity and lack of structural bias present challenges, leading to research focused on improving the attention mechanism.\nOriginal Text:\n## 4 ATTENTION  \nSelf-attention plays an important role in Transformer, but there are two challenges in practical applications.  \n- (1) Complexity. As discussion in Sec. 2.3, the complexity of self-attention is O (1 $^{T}$ D ). Therefore, the attention module becomes a bottleneck when dealing with long sequences.\n- (2) Structural prior. Self-attention does no assume any structural bias over inputs. Even the order information is also needed to be learned from training data. Therefore, Transformer (w/o pre-training) is usually easy to overfit on small or moderate-size data.  \nThe improvements on attention mechanism can be divided into several directions:  \n- (1) Sparse Attention. This line of work introduces sparsity bias into the attention mechanism, leading to reduced complexity.\n- (2) Linearized Attention. This line of work disentangles the attention matrix with kernel feature maps. The attention is then computed in reversed order to achieve linear complexity.  \nFig. 3. Taxonomy of Transformers\nContextualized Text:\nIn Transformers, self-attention plays a vital role, but its computational complexity (O(1<sup>T</sup>D)) can create a bottleneck when processing long sequences. Additionally, self-attention lacks inherent structural bias, requiring training data to learn order information, which can lead to overfitting. To address these challenges, various improvements to the attention mechanism have been explored, including sparse attention, linearized attention, and others.	{"tags": ["NLP", "deep-learning", "architecture", "attention"], "doc_id": "ab3cdc79-1823-4c5f-ac9c-273b9946be37", "summary": "Self-attention is crucial for Transformers, but its complexity and lack of structural bias present challenges, leading to research focused on improving the attention mechanism.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["self-attention", "Transformer", "structural bias", "complexity", "attention mechanism"], "key_objects": ["self-attention", "attention mechanism"], "contextual_text": "In Transformers, self-attention plays a vital role, but its computational complexity (O(1<sup>T</sup>D)) can create a bottleneck when processing long sequences. Additionally, self-attention lacks inherent structural bias, requiring training data to learn order information, which can lead to overfitting. To address these challenges, various improvements to the attention mechanism have been explored, including sparse attention, linearized attention, and others.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION"}, "hypothetical_questions": ["Why does self-attention have a complexity of O(1<sup>T</sup>D)?", "What does it mean for self-attention to lack 'structural bias'?", "How do the different approaches to improving attention (e.g., sparse, linearized) try to address the identified challenges?"]}
ece93dd4-60ac-42ed-b18c-93daff723e30	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.024701597,0.010731043,0.024956537,0.0569781,-0.011271934,0.056061417,-0.022000434,0.0747472,0.02388211,0.0037977756,-0.002592128,0.013894571,-0.00022680094,-0.00834246,-0.007031435,0.0133469,0.021331163,0.059975173,0.00431118,-0.03892637,0.013816749,0.029811548,-0.017771231,0.0034080117,-0.009962146,0.052738845,0.021657279,-0.016548954,-0.009521601,0.0002472845,0.014186964,-0.07727126,0.026765665,0.019364277,0.042548846,-0.022314293,0.029562572,-0.08380131,0.006029349,-0.009849919,-0.0067665633,0.057247117,-0.028096663,-0.033155993,-0.0049495758,-0.086463116,-0.1005779,-0.020667873,0.0321761,-0.031120747,0.015824921,-0.010850561,-0.018644648,-0.02167119,-0.012857142,-0.00589836,0.016886925,-0.06024196,0.006855118,-0.10886122,-0.02561238,0.014284625,-0.014555234,0.0051661525,0.025825579,-0.009650456,0.018994004,0.024779841,0.0268723,0.12666495,-0.0042699366,0.03063189,-0.019501328,-0.0799513,0.10886753,0.03763084,-0.0044139232,-0.026145179,-0.049809195,-0.02287218,0.004618892,0.07123347,-0.03573364,-0.042553212,0.06484236,-0.01608506,0.011938997,0.034409165,0.036516994,-0.025757192,-0.022092374,0.0017904473,0.017361248,0.05825004,-0.039221086,-0.05073645,0.0038973312,-0.020358434,0.0324679,-0.08754084,0.021996198,-0.0014289679,0.0924853,0.111676864,0.04498274,-0.04743428,-0.0030513,-0.030230831,-0.0053569847,0.018115362,-0.026276952,0.017956577,-0.05257667,-0.007010304,-0.039779335,0.02051956,-0.047013577,0.02865144,-0.021386152,-0.0044921343,0.011842224,-0.026606847,-0.01966443,0.030565731,0.06738072,0.033649318,0.041302167,-0.03985992,-0.02133636,0.02123086,0.050513245,-0.0045840023,-0.034469664,0.0313599,-0.026180666,0.025283925,0.08265402,-0.030647311,-0.010195767,0.03285795,0.007280132,-0.015413122,-0.047871694,0.008959229,-0.018969785,0.0201966,-0.061566133,0.04463293,-0.03522123,0.041546747,0.03975259,-0.004891993,0.062478583,0.019772902,-0.011820567,-0.011544351,-0.005550763,-0.050448533,0.011743072,-0.044242512,-0.053078998,-0.017731862,0.058858763,0.071038045,0.038203366,0.107009515,0.010663708,-0.023330487,0.029567217,0.00674984,-0.027446609,-0.006463919,-0.013527964,-0.018275637,-0.018118659,0.016539682,0.01581321,-0.00919635,0.022930587,-0.028673556,-0.003585228,0.082159646,-0.004111107,0.0037079896,-0.026479553,0.038392812,0.0058948216,0.04364377,-0.016717223,-0.034286518,-0.0008403693,-0.026012816,-0.0029770238,-0.016831474,0.038440656,0.050638657,0.040151164,-0.07058617,0.0030330592,-0.056296997,0.022797907,-0.0059460606,-0.03743659,-0.006906497,0.044423424,-0.030777726,0.019274686,-0.038812183,0.010289335,0.022205407,-0.007951847,0.03461707,-0.035003725,0.027049245,0.027472848,-0.013538753,-0.02009861,0.019233342,0.025138432,-0.031183867,-0.030053088,0.0079044625,-0.04112051,0.03661908,-0.04004302,0.046778366,-0.02036369,0.0048470944,-0.024535853,-0.019537663,0.008083578,0.022610104,-0.069408886,0.020960182,-0.03072036,0.02726403,0.0077727856,0.0022278642,-0.034133613,-0.0037199769,0.039726242,0.06359029,-0.002871205,0.08931611,-0.029612003,0.043025997,0.013898467,0.05307649,0.032011885,0.02909486,-0.013650879,0.04202867,0.03218123,-0.00043196898,-0.0059744837,-0.074464455,0.019153116,0.00094344444,0.03366431,-0.023144176,0.03745198,-0.007580775,0.02476212,0.058504805,-0.03138238,-0.007581116,0.026744684,0.023313759,-0.054053914,-0.016291648,0.01885994,0.050678037,0.014053005,-0.031096758,-0.03028281,0.013370541,0.031025788,-0.021687891,0.022291318,-0.019764643,-0.02515814,6.332616e-05,0.013083855,0.0063440553,0.0037108941,0.010250889,0.03506932,-0.0051870593,-0.06236483,0.02656595,-0.031611666,-0.04448772,0.021613186,-0.040293053,0.0035622611,0.003170833,0.013024028,-0.040123846,0.0099278875,-0.008746214,0.100975126,-0.006678194,-0.011862551,0.040040955,0.015342047,0.06253068,-0.042990252,0.0168061,0.028132202,0.01774609,-0.01574896,-0.023676232,-0.011398739,0.00040817092,0.026456423,0.07510587,0.092475586,0.009455741,-0.013586597,0.0076353005,0.03889885,-0.000609309,0.014834387,0.005836931,0.031126445,-0.0001494193,0.028655713,-0.02714536,-0.0928745,0.06707824,-0.07252079,0.058101796,0.058142204,-0.012269435,-0.003927574,0.005969978,-0.011916622,0.02404616,0.019670414,0.0063624084,-0.018528828,-0.036061645,0.0011762548,-0.011650145,-0.03441736,0.05460965,-0.01050695,-0.033071037,0.006746245,0.0054367403,0.013223996,0.015008562,0.013942426,0.038789082,0.013341398,-0.007698335,-0.023527808,-0.024222093,0.008096263,-0.025287589,0.06088081,-0.0782273,0.09233476,-0.047406532,0.015762627,-0.03004239,-0.07377978,0.03771575,0.051826965,-0.021613337,-0.012446668,-0.01761491,0.06136392,-0.05096745,-0.049392175,0.042260755,0.054417487,0.059040233,0.01243553,-0.025623368,0.004859166,0.0015934604,-0.03929173,-0.038576897,0.0016792283,0.009530379,0.0033096408,0.10406158,-0.03295148,0.07208669,-0.058008783,0.008901865,0.025828281,-0.0011735996,-0.014927185,-0.03212998,0.0136220055,0.07448898,0.025376495,0.008070598,0.028173737,0.024020834,-0.04272366,0.0065722526,0.026911477,0.033221584,-0.059159067,0.039425172,0.012069288,0.006376703,0.029075902,-0.03891631,-0.022922056,0.016929165,0.048190176,-0.02121888,-0.013697485,0.006422988,-0.04991452,-0.016616924,-0.033323303,0.038147245,0.013849905,0.031605013,0.022666363,-0.01989041,-0.039363164,0.02990792,-0.0014612755,-0.039530307,0.02238944,-0.024102226,-0.02660951,0.018151574,-0.008799215,0.0048708743,-0.059685245,0.07556125,-0.03382274,-0.025207529,0.07245885,-0.043769244,-0.008103331,-0.0072211656,0.019256445,-0.038898706,0.008702128,0.006085661,-0.01940464,-0.010129336,0.036827184,0.006902577,0.016486099,-0.021857494,0.064427026,0.0058055925,0.0076017394,-0.020455727,-0.04778779,0.030987084,0.0153573565,0.04619322,0.0317505,-0.014806183,-0.018321134,-0.007618025,-0.0045980997,-0.020391647,-0.018019455,-0.00072464364,0.0003164156,0.009767877,-0.020405473,0.038773242,-0.011316815,-0.041131448,-0.015694948,0.07292132,0.028771898,0.013437422,-0.070202515,-0.04667878,0.019781228,0.0023816328,0.0038362532,0.010030248,0.0047010235,-0.047837686,-0.020190049,0.0070965844,-0.037241284,0.03856035,-0.0033602903,0.017441068,0.010508495,-0.049355052,-0.027570726,-0.0035492424,0.045520328,-0.032760724,-0.03656528,0.0024030495,0.061186634,0.07200491,-0.00557218,0.0045239623,0.0025879608,-0.012248001,-0.0048626564,0.05980316,0.0029743467,-0.04159638,0.011338386,0.0011760335,-0.10203196,0.023361491,-0.013051264,0.005953691,-0.021437475,0.042467013,0.014575719,0.037706774,0.04929442,0.016494466,0.00019362646,0.016000245,0.015030246,0.06414447,0.052811984,-0.09309088,-0.010458874,-0.073361814,-0.03455835,0.010973943,-0.06248736,-0.056472063,0.017047625,-0.04382143,0.018616952,0.022659518,-0.037633646,-0.055017877,0.034579363,-0.05369296,0.059817888,-0.051896993,0.025073964,0.013289498,-0.0017731406,0.09939511,0.011596209,0.017599786,-0.041894376,-0.01160221,-0.011331264,0.031614404,0.070548885,0.00952117,0.0011180574,0.06337277,-0.024242258,-0.008889533,-0.05862918,-0.07137611,-0.033396456,0.00456773,-0.015083505,0.00887983,-0.019816829,0.008920858,0.005210588,0.0069563,0.01435584,0.030819532,0.01130916,0.022120157,0.02612209,0.020918258,0.040564694,-0.004571754,0.03424242,0.037517812,0.047322523,0.010745282,0.0023133303,0.031025102,-0.0008178359,-0.00132793,0.0018724152,-0.013304786,-0.021955157,-0.020122895,0.0040006335,-0.0019664539,0.017370883,-0.0044018026,0.009418829,0.024010897,0.041594125,-0.024461271,-0.040502198,-0.032079037,-0.013129557,-0.052758254,-0.025302174,0.02281807,-0.008098334,0.039027736,-0.018220251,0.06058758,-0.10335441,-0.04584561,-0.024892714,0.042796124,-0.015209893,-0.027989758,-0.034493264,0.00043347222,-0.020131279,-0.002966901,0.06274157,0.024111804,-0.04737817,0.03544625,-0.0034177748,-0.00083544553,-0.017859751,-0.023725724,-0.037432972,-0.016481241,-0.018313248,-0.015472427,0.07541786,-0.052080512,-0.009460484,0.050188184,-0.017014412,-0.073019005,0.008951709,0.023887478,0.06796876,0.003950949,0.005982006,0.010658061,0.042351715,-0.02758726,-0.005790842,-0.07587327,0.013483616,-0.0055587464,0.032605674,0.031214857,0.017390504,-0.02171401,-0.009485705,0.00097142125,-0.020378906,0.033765506,0.07020359,0.050081488,0.07456932,0.062398113,-0.054678097,-0.023812745,-0.009650284,-0.045572642,0.023470988,-0.061809253,0.048345163,0.013959205,0.0038905437,-0.013805569,-0.028699683,0.005302569,-0.02259681,0.024303332,-0.01651983,-0.059361935,-0.015404846,0.011039581,0.0063087596,-0.0009033753,-0.028380832,0.07243022,0.02763253,-0.054670893,-0.034819502,-0.067848444,-0.037666913,0.009755215,-0.005924336,-0.022099,-0.0053551067,-0.0076112007,0.0028269575,0.060295593,-0.014860009,-0.04967823,-0.06618433,0.029363927,0.021454303,0.043499224,0.013360297,-0.005513287,-0.07518703,0.018828806,-0.00032172594,-0.047216885,0.024170332,-0.010651044,-0.025854891,-0.023585197,0.017444683,-0.016294692,0.01276803,-0.00498034,0.020133814,0.02876176,-0.013878333,0.009528295,-0.059908077,-0.020653134,-0.016159065,-0.0154699655,-0.022551151,0.04963785,0.023028089,-0.016493564,0.013172731,0.025401259,0.032656003,0.021686956,6.7534033e-06,0.042860765,-0.059848476,0.023227083,-0.016236065,-0.012110837,0.08265933,-0.02640308,0.014773731,0.03512552,0.049681995,-0.017723506,0.046604294,0.008283898,-0.02270795,-0.045830727,0.019719325,0.04068178,-0.0076796697,0.02620112,-0.006329665,-0.0456236,-0.0611454,-0.023949493,-0.0033722469,0.042547207,0.022724472,0.03256746,-0.050836265,0.027798716,-0.03213052,0.03321009,0.031238368,0.0053555537,-0.01525822,0.021757068]	Keywords: sparse attention, self-attention, computation complexity, query-key pairs, structural bias\nKey Objects: query-key pairs, attention matrix\nRefers to Images: None\nHypothetical Questions:\n- Why is the standard self-attention mechanism computationally expensive?\n- What is the primary benefit of using sparse attention over standard self-attention?\n- How does limiting the number of query-key pairs reduce computational complexity in transformers?\n---\nSummary:\nStandard self-attention requires every token to attend to all others, which can be computationally expensive. Sparse attention addresses this by limiting the number of query-key pairs a query attends to, reducing complexity by incorporating structural bias.\nOriginal Text:\n### 4.1 Sparse Attention  \nIn the standard self-attention mechanism, every token needs to attend to all other tokens. However, it is observed that for the trained Transformers the learned attention matrix A is often very sparse across most data points [17]. Therefore, it is possible to reduce computation complexity by incorporating structural bias to limit the number of query-key pairs that each query attends to. Under this limitation, we just compute the similarity score of the query-key pairs according to pre-defined patterns  \n$$\\hat { A } _ { i j } = \\begin{cases} q _ { i k } T _ { j } & \\text {if token i attends to token j,} \\\\ - \\infty & \\text {if token i does not attend to token j,} \\end{cases}$$  \nwhere  A is un-normalized attention matrix. In implementation the - item is usually not stored in memory so as to decrease memory footprint.\nContextualized Text:\nIn the Transformer model, standard self-attention requires each token to attend to every other token, which can be computationally expensive. To reduce this complexity, sparse attention methods limit the number of query-key pairs a query attends to. This is achieved by incorporating structural bias, which computes similarity scores only for pre-defined patterns.	{"tags": ["architecture", "NLP", "transformer", "optimization"], "doc_id": "ece93dd4-60ac-42ed-b18c-93daff723e30", "summary": "Standard self-attention requires every token to attend to all others, which can be computationally expensive. Sparse attention addresses this by limiting the number of query-key pairs a query attends to, reducing complexity by incorporating structural bias.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["sparse attention", "self-attention", "computation complexity", "query-key pairs", "structural bias"], "key_objects": ["query-key pairs", "attention matrix"], "contextual_text": "In the Transformer model, standard self-attention requires each token to attend to every other token, which can be computationally expensive. To reduce this complexity, sparse attention methods limit the number of query-key pairs a query attends to. This is achieved by incorporating structural bias, which computes similarity scores only for pre-defined patterns.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["Why is the standard self-attention mechanism computationally expensive?", "What is the primary benefit of using sparse attention over standard self-attention?", "How does limiting the number of query-key pairs reduce computational complexity in transformers?"]}
3d11a7b8-2d4b-40b4-9abc-a837a754cbcf	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01100345,0.0124778,0.027381115,0.020881025,-0.020004444,0.05274439,-0.028084252,0.058017645,0.033458184,-0.008904702,0.003114397,0.0039445553,-0.0017388117,-0.02158899,0.011411813,0.010338732,0.032394845,0.054496028,0.010114066,-0.056413714,0.032126766,0.026660103,-0.01827786,0.03189164,0.025319355,0.059198752,0.0040934538,0.018350866,-0.008940353,-0.0015919125,0.029801631,-0.06949874,0.039755784,0.017632073,0.029417656,-0.047388714,0.0066049714,-0.083863355,0.03172102,-0.00438559,-0.028947338,0.031091183,-0.009478949,-0.00642057,0.012053778,-0.07729379,-0.076024674,-0.03834045,0.0104890885,-0.009541453,0.007480598,0.01659895,-0.01364225,-0.016791439,-0.031158807,-0.019153308,0.027607433,-0.05517424,0.0019109172,-0.101542056,-0.017675497,0.0139933685,-0.027453938,0.006927634,0.030390343,-0.011555354,0.017340656,0.025137987,0.045480475,0.14375895,0.01789019,0.035704542,-0.026351117,-0.083952524,0.10564137,0.032154016,-0.0030618703,0.0011353204,-0.07227488,0.0006484428,0.036910415,0.047013707,-0.04168831,-0.019063665,0.072156705,-0.033898,-0.014413929,0.016581737,0.04723937,-0.04761431,-0.0038944022,-0.012933734,0.0031072544,0.08840258,-0.041245144,-0.027169416,0.020883583,-0.012846778,0.03872519,-0.07056266,0.031044977,-0.011481572,0.09348071,0.114130005,0.02842788,-0.03333568,0.011292149,-0.040891517,-0.011000006,0.008146061,-0.015918655,0.0041502817,-0.06357807,0.01973307,-0.039111767,0.033880778,-0.047729705,0.0244209,-0.008356974,-0.021738326,0.030628514,-0.045221206,-0.0047855424,0.015767425,0.06571465,0.032890152,0.043654323,-0.03856284,-0.0035138803,0.021333532,0.06527733,0.018617874,-0.032338567,0.023283947,-0.008134235,0.027435575,0.056853812,-0.019141678,-0.033606485,0.015751235,-0.008515113,-0.027006231,-0.03989508,0.02997291,0.009374295,0.011654497,-0.058316477,0.036965948,-0.023056826,0.06227226,0.048652306,-0.019056212,0.048405707,-0.009566089,-0.023442507,-0.033965163,-0.025221417,-0.035328317,-0.0038047326,-0.045780327,-0.07528474,0.008206522,0.013359314,0.10390798,0.06250281,0.13682202,0.021093205,-0.023198785,-0.003446095,0.01205021,-0.028426778,-0.018884964,0.0042719156,-0.029883662,0.0015690357,0.015338945,0.016577648,-0.040013608,0.016057644,-0.020365726,-0.01131699,0.09522134,-0.0030674937,-0.004891126,-0.02376988,0.07225102,-0.020939512,0.06744671,-0.010088452,-0.016080588,-0.00092339795,-0.04792356,0.0046052,-0.010661379,0.047407113,0.040225524,0.032818567,-0.058138065,0.0018672217,-0.049099647,7.825189e-05,-0.0050136703,-0.042373743,0.0032768364,0.03393665,-0.016734533,0.021805897,-0.04627319,-0.0133274235,0.031101808,-0.015336741,0.015302821,-0.035819795,0.030518431,0.028661307,0.020408098,-0.020516934,0.01936511,0.01072902,-0.032646097,-0.027390309,0.0017756209,-0.057753645,0.04597102,-0.03627977,0.05611414,0.009366705,-0.0075980336,-0.025880266,-0.012072571,0.01303073,0.024108838,-0.04216212,-0.00042931485,-0.03814854,0.00083598774,0.038897295,-0.017811429,-0.016130937,0.013796086,0.03498988,0.077614605,-0.009618393,0.09206804,-0.025543455,0.017142186,0.02471338,0.05465092,0.028033854,-0.00012321935,-0.027013417,0.03974423,0.055084,-0.017285932,0.0082314005,-0.027185146,0.0056061093,0.0005440994,0.040233992,-0.029642425,0.02278997,-0.009231006,-0.03065154,0.064681605,-0.009725798,-0.005675267,0.054055993,0.04446773,-0.050942704,-0.0048077526,0.038810976,0.048227824,0.00023234016,-0.014362108,-0.06707851,0.016618557,0.008351629,-0.020972019,0.018382616,-0.0054815602,-0.013507513,-0.0007576808,-0.00197199,0.0065919585,0.032246545,-0.0129181435,0.041887127,-0.01250701,-0.009684144,0.014482454,-0.0033015392,-0.019709319,0.021509085,-0.030959249,-0.017102709,0.011018351,0.002235558,-0.049918976,0.0108805485,0.032115415,0.098565586,-0.0098720305,0.016583255,0.049622465,0.062033076,0.01543435,-0.03794587,0.053468253,0.015425226,0.021585183,-0.009574233,-0.027681546,-0.008854948,-0.003615336,0.040758267,0.0785167,0.12175045,0.015191878,-0.0004051066,0.022362165,0.03544345,0.027911993,0.0023520554,0.009273559,0.056417048,0.011178556,0.057313032,-0.021508737,-0.07185643,0.05957114,-0.06510683,0.04696994,0.0740053,-0.023683988,0.005852427,0.01930558,-0.009851926,0.027796606,0.03916479,-0.004656709,-0.030207373,-0.018377448,0.017168328,-0.02107186,-0.030851737,0.049790356,-0.019213887,-0.009098329,0.004735164,0.012972019,0.024666617,0.006430894,0.0050111497,0.030443674,0.008206284,0.0025178462,-0.012124136,0.011315649,-0.004963644,-0.024207633,0.052563302,-0.10631546,0.08489975,-0.05396126,0.036675826,-0.028560983,-0.056436747,0.04653792,0.058136575,-0.0055387695,0.00034887478,-0.030483672,0.060803674,-0.02905299,-0.036028653,0.039125584,0.05016531,0.030580109,0.00165452,-0.018317556,0.01174126,5.0217088e-05,-0.025942707,-0.06712639,0.021286726,-0.0077703195,-0.023123644,0.068719804,-0.01837194,0.04851799,-0.076548256,0.018292662,0.015839064,0.002190018,-0.02841708,-0.013865792,0.03300105,0.06531088,0.0016824027,0.025805367,0.023034094,0.0217903,-0.055274624,0.0010574621,0.017501296,0.02536337,-0.08480944,0.04371095,-0.02081689,-0.027123325,0.025501128,-0.052211706,-0.0015009816,-0.018647369,0.041654147,-0.047121786,0.007641548,-0.008157186,-0.05752632,-0.02050659,-0.011387674,0.046592914,0.043306176,0.010002852,0.029231457,-0.015815988,-0.03847427,0.03646747,-0.015602267,-0.039405674,0.01960438,-0.0025638535,-0.022042366,0.006617703,-0.021471126,0.01078864,-0.032090843,0.111634836,-0.013895339,-0.03375172,0.09325954,-0.050225027,-0.011027977,-0.016569719,0.037434313,-0.035913758,0.020398188,0.006827401,-0.016750053,0.006767819,0.03408928,0.017022338,0.016632788,-0.024416862,0.06629813,0.018104136,-0.0036821533,-0.042384706,-0.04860102,0.022857262,0.019054709,0.021833153,0.02780237,-0.023688402,-0.008876169,-0.0122364415,-0.030910255,-0.057555597,-0.013099786,-0.011757332,-0.020851785,0.023084728,0.006374594,0.047897976,-0.03148766,-0.03190463,-0.004260402,0.04776157,0.03284548,0.012789336,-0.047569968,-0.07806282,0.024865713,-0.004509145,0.024990361,0.0003382323,0.0027552883,-0.03255677,-0.0011063413,0.004634954,-0.032944303,0.045099452,-0.0131766945,0.0073739165,0.0063741524,-0.061040234,0.00458754,-0.02261324,0.04358936,-0.03688953,-0.019092593,0.015995374,0.045297455,0.045795187,0.0005396244,0.030920364,0.012545945,0.009185546,-0.01064113,0.053059306,0.0072734845,-0.05219508,0.009768824,0.008772252,-0.074886784,0.014309511,-0.015224568,-0.0011093266,-0.012127964,0.046634573,0.026911482,0.0053518456,0.056153636,0.0032183763,0.0056951363,0.013124674,0.027758721,0.04498114,0.033075593,-0.082557835,0.0047134813,-0.07452035,-0.013975433,0.014366129,-0.07658463,-0.049561184,-0.009623162,-0.05755001,0.016930593,-0.01834489,-0.037295327,-0.048940502,0.04674957,-0.049339492,0.07999217,-0.055819955,-0.005136326,0.0029621492,-0.0044359025,0.06345924,0.03079077,0.02521881,-0.020764537,-0.020370457,-0.0008754582,0.039796375,0.048371267,0.013947353,0.0146222655,0.069419965,-0.010714035,-0.0026178863,-0.03729773,-0.044724636,-0.041815847,0.0019491368,-0.022274125,0.016338825,-0.023032922,0.02425185,-0.016178822,-0.022377918,0.025427083,0.012859967,0.016102796,0.02613461,0.05224896,0.00096247485,0.04057976,-0.013359273,0.027502997,0.0046432503,0.07186597,0.01856407,0.0015640504,0.045427695,-0.013310444,0.02934326,0.02888941,-0.02310795,-0.022181077,-0.039560415,-0.028706757,-0.022601385,0.014261509,-0.024548704,0.020912679,0.0013636127,0.03149794,-0.020761678,-0.037792075,-0.0025742413,-0.015629316,-0.042055387,-0.006329158,0.035600577,-0.0039016723,0.06334216,-0.022458771,0.053454954,-0.09200878,-0.025190396,0.0034164225,0.012336239,-0.04689376,-0.014927951,-0.04070265,-0.0044353786,-0.021069907,-0.02998586,0.05213067,0.015625648,-0.022826925,0.03515207,-0.006796581,0.003869263,0.009510065,-0.026056083,-0.013951544,-0.002632754,-0.030318651,-0.011894859,0.07639126,-0.05768706,-0.021621948,0.034337923,-0.0077051413,-0.09053873,-0.026905298,0.024994817,0.034051858,-0.012501169,0.02821618,0.0074454104,-0.0032768094,-0.021604609,-0.010179636,-0.058202,0.018617367,-0.002839675,0.04331464,0.05204472,0.015775882,-0.020427035,-0.0014449471,-0.023707071,-0.010335669,0.04152141,0.070634075,0.0417092,0.09293329,0.043348774,-0.040067527,-0.021528112,-0.011951908,0.00045841822,0.01932475,-0.05322335,0.030930134,0.0023412576,0.01444212,0.0050820857,-0.018931549,0.013521761,-0.012592564,0.037806533,-0.0267223,-0.020108566,-0.033420004,0.02588801,0.023094913,0.0035914353,-0.018401071,0.07225535,0.021073252,-0.014257191,-0.043060064,-0.03343611,-0.022666633,0.009292394,0.0033019152,-0.005944294,-0.0047798823,-0.0149269495,0.0157194,0.034993708,-0.008742403,-0.039985202,-0.035050992,0.01316392,0.020562334,0.022409452,-0.00813065,-0.008037837,-0.063621,0.028714478,-0.0051665436,-0.03655797,0.03533425,-0.0076873284,-0.03555504,-0.014126705,0.017376283,-0.017931126,0.043732315,-0.00053173193,0.0064379442,0.0142091215,-0.033143554,-0.007188044,-0.10316992,-0.02292854,-0.013367248,-0.011072267,-0.02003316,0.04165261,0.020276435,0.0015943125,-0.000227414,0.015991062,0.0166694,0.024818923,-0.0012869862,0.034015678,-0.03684303,0.063469,-0.0069434238,0.01642995,0.060391083,-0.013320485,-0.023944143,0.028961565,0.018015305,-0.0195711,0.006108979,0.012739007,-0.03350707,-0.020179747,0.031730898,0.06900571,-0.016826943,0.035762616,-0.022798186,-0.026146172,-0.032587163,-0.033839323,0.0014808108,0.057811216,0.023125682,0.016055895,-0.060413584,0.027012708,-0.0018225246,0.04773972,0.0012929578,0.013882324,-0.04237208,0.034209844]	Keywords: sparse attention, query-key pairs, bipartite graph, position-based, content-based\nKey Objects: attention matrix, query, key\nRefers to Images: None\nHypothetical Questions:\n- How does representing attention as a graph help understand its behavior?\n- What's the trade-off between computational efficiency and expressiveness when using sparse attention?\n- What are the potential benefits of using content-based sparse attention compared to position-based?\n---\nSummary:\nSparse attention reduces computational complexity by limiting the connections between query and key tokens, which can be implemented by either pre-defined patterns or content-based similarity.\nOriginal Text:\nwhere  A is un-normalized attention matrix. In implementation the - item is usually not stored in memory so as to decrease memory footprint.  \nFrom another perspective, the standard attention can be regarded as a complete bipartite graph where each query receives information from all memory nodes and updates its representation. The sparse attention can be considered as a sparse graph where some of the connections between nodes are removed.  \nBased on the metrics of determining the sparse connection, we categorize these approaches into two classes: position-based and content-based sparse attention.  \n- 4.1.1 Position-based Sparse Attention . In position-based sparse attention, the attention matrix is limited according to some pre-defined patterns. Although these sparse patterns vary in different forms, we find that some of them can be decomposed into some atomic sparse patterns.\nContextualized Text:\nIn sparse attention, computation complexity is reduced by limiting the connections between query and key tokens. The un-normalized attention matrix, denoted as  A, is computed with predefined patterns or content-based similarity. Standard attention can be considered a complete bipartite graph where each query attends to all memory nodes, while sparse attention represents a sparse graph where some connections are removed.	{"tags": ["NLP", "attention", "transformers", "sparse"], "doc_id": "3d11a7b8-2d4b-40b4-9abc-a837a754cbcf", "summary": "Sparse attention reduces computational complexity by limiting the connections between query and key tokens, which can be implemented by either pre-defined patterns or content-based similarity.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["sparse attention", "query-key pairs", "bipartite graph", "position-based", "content-based"], "key_objects": ["attention matrix", "query", "key"], "contextual_text": "In sparse attention, computation complexity is reduced by limiting the connections between query and key tokens. The un-normalized attention matrix, denoted as  A, is computed with predefined patterns or content-based similarity. Standard attention can be considered a complete bipartite graph where each query attends to all memory nodes, while sparse attention represents a sparse graph where some connections are removed.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does representing attention as a graph help understand its behavior?", "What's the trade-off between computational efficiency and expressiveness when using sparse attention?", "What are the potential benefits of using content-based sparse attention compared to position-based?"]}
08329443-05d2-498f-9838-5e2f1c951c56	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.021719845,0.036271527,0.0051966244,0.018183246,-0.03093193,0.041239284,-0.026638042,0.05915511,0.048258778,-0.025564333,-0.010110355,-0.02523422,-0.015814556,-0.018520182,-0.010103311,0.012198126,-0.0031282874,0.068774864,0.0016920496,-0.06255007,0.059591066,0.028158054,0.015690055,-0.0047745546,-0.0013063154,0.043119714,-0.004895116,0.01572703,0.048306953,0.020594593,0.015399254,-0.0607237,0.020196803,-0.027706848,0.03471384,0.003168455,0.053862523,-0.09040947,0.047978472,0.0048185824,-0.025759595,0.0068584625,-0.041546904,-0.015773507,-0.012799241,-0.0630017,-0.1096858,-0.05425419,-0.019470543,0.018657431,-0.005539414,0.0024164254,0.004683139,-0.015097119,-0.01481739,-0.01945759,0.0013640594,-0.035946075,-0.003524379,-0.09726338,0.010282702,0.023619942,-0.042033616,-0.009274094,0.00093693595,-0.0054239607,0.006287641,0.043559935,0.045160104,0.12604779,-0.0060423785,-0.00030435948,-0.013876536,-0.049528614,0.093660295,0.042816244,-0.002094843,0.0038899835,-0.029473325,-0.017448623,0.0017800308,0.07413948,-0.0494376,-0.03738143,0.101753145,-0.026945999,-0.038498316,0.015177419,0.046436653,-0.035745114,-0.027028969,-0.021263117,-0.021926135,0.088645495,-0.079947494,-0.039548486,0.014283249,-0.019370543,0.0011182203,-0.10023245,0.007416682,0.042588983,0.069513135,0.06724387,0.050342664,-0.015027719,0.011904502,-0.036910996,-0.006186223,-0.009972323,-0.0138970185,0.023534102,-0.048554618,0.017687839,-0.022987159,0.04554369,-0.028214758,0.019071095,-0.027590077,-0.007525186,0.023896,-0.03510631,-0.015660368,0.030311812,0.07905194,0.020321334,0.045213174,-0.036122523,-0.02608328,0.031056827,0.050353136,-0.0036638384,-0.027932417,0.02859195,-0.04674684,0.024453923,0.116868235,0.006494917,-0.006038784,0.01759906,0.007875454,-0.022655753,-0.04787344,0.030355196,-0.0005200864,0.02820999,-0.06554475,0.046778493,-0.020205978,0.06717858,0.022918811,-0.023241334,0.051469408,-0.0080684,-0.02700252,-0.03385814,-0.020075636,-0.014809057,0.004866807,-0.05547125,-0.07012967,-0.028030666,-0.014743423,0.05528095,0.06033406,0.089917965,0.010718432,-0.011240996,-0.0075140945,0.034655523,-0.05197508,-0.005918871,0.005683118,-0.03047009,0.03372175,-0.0068283128,0.021467146,-0.055579625,-0.026520777,-0.025661442,-0.015686149,0.07082167,0.020628508,0.029063258,-0.015013029,0.041194364,-0.013599098,0.056237273,-0.01817322,-0.005115305,-0.015552159,-0.03674403,0.0073651522,0.0008871794,0.04088301,0.03708187,0.021634562,-0.04548352,-0.016740115,-0.042115275,-0.022122854,-0.03303107,-0.029431967,-0.011588136,0.0005657058,-0.043291897,0.012168488,-0.036179215,-0.046496917,0.01752372,-0.03435399,0.0018887566,-0.07342435,0.034997385,0.026151326,0.007119348,-0.05601017,0.008290948,0.00057667325,-0.020404186,-0.02414803,0.0044542723,-0.033846654,0.049109932,-0.025155474,0.023435902,-0.007640565,0.011099179,-0.02330891,0.009855082,0.020159615,0.04197362,-0.05609384,0.0052532037,-0.039637387,0.01653512,0.020576768,0.03312726,-0.019424357,0.04018128,0.029982453,0.051270857,-0.010690386,0.07839709,-0.0011081897,0.006584327,0.02171112,0.034846272,0.018088775,0.0033862346,-0.0033824162,0.060159773,0.050811738,-0.0109039005,-0.0047151265,-0.029974196,0.025994405,-0.0017453366,0.04510629,0.0011519974,0.020563988,-0.026615746,0.0032869293,0.049928978,-0.013315456,-0.019476095,0.0019057073,0.04119085,-0.040686864,-0.00084697374,0.03308528,0.056050073,0.008140776,-0.059920598,-0.05536406,0.008481892,0.055624142,0.01178809,-0.00066767295,-0.021190565,-0.031629637,0.0021423628,0.034326654,0.007883097,0.021276934,-0.0011694505,0.03966459,-0.007211195,-0.011913542,-0.0069885124,-0.00038346226,-0.017286332,0.012498606,-0.028203528,0.0104375575,0.032104205,-0.004718013,-0.008617275,0.018315885,-0.00030522933,0.09699834,-0.03299186,0.020032436,0.021417446,0.0068049105,0.0031673203,-0.021775346,0.012698702,0.05574814,0.019843979,-0.005896395,-0.027560765,-0.020256981,-0.0025574933,0.026592791,0.07850222,0.08386926,-0.014853952,-0.045703333,0.012188383,0.034484815,0.023220245,-0.020070026,0.009619095,0.048698418,0.017511873,0.028853282,-0.043871637,-0.08790981,0.07250647,-0.10403763,0.051147565,0.06792781,0.005312192,0.021979837,0.011583589,-0.0047745886,0.01464786,0.008517792,-0.019637167,-0.0014208159,-0.0010386208,0.017042734,-0.039046805,-0.04232651,0.042443983,-0.008517116,-0.0047954284,-0.034557544,0.007992711,0.03099236,0.0067791673,-0.021658298,0.014037353,0.014733746,-0.057460897,-0.037041504,0.020501612,-0.0072650746,-0.009328349,0.081113845,-0.09093001,0.08760374,-0.032649845,0.045694295,-0.023001436,-0.07312525,0.02583639,0.07273292,-0.033041656,0.015207569,-0.012311699,0.03664865,-0.014648897,-0.03463919,0.02806563,0.035049964,0.029991,-0.0095238555,-0.018154716,-0.006251295,0.021993034,-0.030902125,-0.063368544,0.026237601,0.022278009,-0.034395587,0.06549367,-0.023719199,0.05870576,-0.059411123,0.019318162,0.01387473,0.021879096,-0.02964867,0.00379896,0.0028704347,0.016635705,0.01506344,-0.00756284,-0.0063316263,0.051958997,-0.051026296,0.035824902,0.051266126,0.012421214,-0.07004393,-0.007575371,0.010152294,0.01442507,0.0115318,-0.06277412,-0.0048161666,-0.014121855,0.035393227,0.00022952828,0.007496431,0.039733,-0.07766331,0.0065001273,-0.035848837,0.0517905,0.01869713,0.04168211,0.017012464,-0.029648041,-0.044913918,0.02863738,-0.015533545,-0.047756806,0.020471668,-0.018425088,-0.02482524,0.0066842134,0.0093521625,0.027860219,-0.038831618,0.1011463,-0.008311854,-0.0019019626,0.09353355,-0.029490892,-8.142854e-05,-0.010893152,0.015234756,-0.05610307,0.016426729,0.023071496,-0.012228415,0.011950215,0.040235583,0.03960347,-0.0048637637,-0.055479247,0.01962545,-0.013827419,-0.021443238,-0.07354144,-0.012544763,0.01811667,-0.02506891,0.029410234,-0.00144019,0.0074155554,-0.022689378,-0.026566412,-0.032710463,-0.030540124,-0.026588095,-0.014257242,-0.011508756,-0.001604287,-0.0037281478,0.01433546,0.034818113,-0.008941296,-0.0106646465,0.08051425,0.06617568,0.01142013,-0.03966688,-0.061391424,-0.015703432,0.019514177,0.009327282,0.005623377,0.023728764,-0.042707477,-0.0057065072,0.039681494,-0.01096278,0.030393325,-0.0049462365,0.011244905,0.020509696,-0.0631488,-0.0042311857,0.016099077,0.021822099,-0.04685868,-0.023409331,-0.01112126,0.022720356,0.012138997,0.007120055,0.03580315,0.016875759,-0.017266048,-0.01037359,0.072941855,0.072353885,-0.023300083,-0.0072067278,0.010616577,-0.053341992,0.01943693,-0.0068940837,-0.041646555,0.03068223,0.0706143,0.029892983,0.03536401,0.083393246,-0.016624179,-0.022502998,-0.0007424396,0.025915693,0.024208156,0.03630583,-0.050452072,0.010801427,-0.07689572,0.01330222,-0.01686087,-0.051816836,-0.03722318,0.025592327,-0.074130766,0.0036712175,-0.03757147,-0.016963204,-0.017850848,0.02187407,-0.035155535,0.063286014,-0.058988445,-0.012748125,-0.019902412,0.016114155,0.08138559,0.016574845,0.038950615,-0.04633683,-0.025404796,-0.032030053,0.020368837,0.06294438,0.0313386,-0.012841804,0.060457814,-0.0098485015,0.02735292,-0.051278353,-0.06406212,-0.060917508,0.020596202,-0.033223193,-0.01352701,-0.032690194,0.041535445,-0.022762073,0.02608125,0.033271573,0.030723466,0.022785861,0.020483548,0.004429756,-0.012380525,0.050429676,-0.01494287,0.048588283,0.056323774,0.031686913,-0.016469529,0.013152871,0.030517953,-0.0034291642,0.013401412,0.014281129,-0.038459424,-0.044622015,-0.017533252,0.02247316,-0.0121624945,-0.009597681,0.001855664,-0.0381448,0.008281765,0.033065036,-0.03348342,-0.053377695,0.018012296,-0.010513172,-0.0501268,0.007632139,0.016683295,0.029182833,0.028039828,-0.03984339,0.034293182,-0.08346254,-0.019393431,0.01563284,-0.0042071305,-0.04551135,-0.010734444,-0.054973714,0.0053281724,0.005093918,-0.039996274,0.07136511,-0.0015802787,-0.043174766,0.02278136,0.007996599,0.0017279721,0.02015638,0.010363034,0.021373827,0.0070178136,-0.009881308,-0.020957809,0.0809535,-0.062937446,-0.010806557,0.011665048,-0.049472556,-0.10381568,-0.040412266,0.0027288462,0.07445001,-0.0040090675,0.028641948,-0.008690127,0.0056480016,-0.026481355,-0.021690536,-0.04849468,0.030391054,-0.024634363,0.04507883,0.05149426,-0.0018791193,-0.058825646,-0.031987507,0.01072084,-0.0073318738,0.06297814,0.04011182,0.053103846,0.08242126,0.044408537,-0.038772967,-0.056488015,-0.00081050623,-0.0033085844,0.004886982,-0.01988887,0.03274433,-0.012306295,0.007787628,0.015214674,0.00367013,0.023631629,-0.045390468,0.037069615,0.013517886,0.0059470884,-0.017423306,-0.009321358,0.031362202,-0.007520422,-0.023405772,0.038480595,0.0050953724,-0.023425104,-0.055347018,-0.046711758,-0.030219194,0.010689209,-0.018288808,-0.00781849,-0.00962948,0.000117080366,0.009122354,0.02788532,0.012405565,-0.03212686,-0.057755005,0.020424621,0.027037717,0.0036663923,-0.0260389,-0.022633454,-0.08817984,0.018749284,-0.01813123,-0.059732545,0.044155262,-0.017216112,-0.027217591,-0.014235025,0.01013451,-0.008543446,0.019386442,-0.007237068,0.0021199826,-0.002667508,-0.031007482,0.022719005,-0.06683257,0.014601905,-0.030525304,0.013894539,0.01830809,0.03444606,0.026588107,-0.002978515,-0.020036196,0.0039415467,0.0063225036,-0.0031157227,-0.0134466225,0.05474943,-0.039426085,0.04937733,-0.0050395955,0.021287965,0.059304062,-0.016517857,0.006198885,0.008788049,0.026625613,0.018202834,0.01798891,-0.0085940305,-0.03769031,-0.0586275,0.023629952,0.02619118,-0.017495673,0.024836779,-0.038766522,-0.019705042,-0.044146117,-0.024283556,0.0010239852,0.04859361,0.030719012,0.04162868,-0.07021182,0.034317113,-0.009296681,0.05136011,-0.033578128,0.020521725,-0.01853139,0.009678927]	Keywords: atomic sparse attention, sparse patterns, global attention, band attention\nKey Objects: Atomic Sparse Attention, Sparse Patterns, Band Attention, Global Attention\nRefers to Images: None\nHypothetical Questions:\n- What is the significance of categorizing sparse attention approaches?\n- How do extended sparse patterns contribute to the flexibility of transformer models?\n- Why are researchers interested in understanding how different atomic sparse patterns are combined?\n---\nSummary:\nTo categorize and understand sparse attention approaches, researchers identify atomic sparse patterns, describe how these patterns are combined in existing work, and introduce extended patterns for specific data types.\nOriginal Text:\nWe first identify some atomic sparse patterns and then describe how these patterns are composed in some existing work. Finally, we introduce some extended sparse patterns for specific data types.  \n- 4.1.1.1 Atomic Sparse Attention . There are mainly five types of atomic sparse attention patterns, as shown in Fig. 4.\n- (1) Global Attention . To alleviate the degradation of the ability to model the long-range dependencies in sparse attention, one can add some global nodes 5 as the hub for information propagation between nodes. These global nodes can attend all nodes in the sequence and the whole sequence attend to these global nodes, as illustrated in Fig. 4(a).\n- (2) Band Attention ( a.k.a sliding window attention or local attention ). Since most data come with a strong property of locality, it is natural to restrict each query to attend to its neighbor nodes. A widely adopted class of such sparse pattern is band attention, in which the attention matrix is a band matrix as illustrated in Fig. 4(b).\nContextualized Text:\nSparse attention mechanisms are often categorized by identifying fundamental patterns, like global and band attention. These basic patterns are then analyzed to understand how they are used in combination within existing transformer architectures. Researchers also develop extended patterns tailored to specific data types.	{"tags": ["architecture", "sparse attention", "NLP", "transformer"], "doc_id": "08329443-05d2-498f-9838-5e2f1c951c56", "summary": "To categorize and understand sparse attention approaches, researchers identify atomic sparse patterns, describe how these patterns are combined in existing work, and introduce extended patterns for specific data types.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["atomic sparse attention", "sparse patterns", "global attention", "band attention"], "key_objects": ["Atomic Sparse Attention", "Sparse Patterns", "Band Attention", "Global Attention"], "contextual_text": "Sparse attention mechanisms are often categorized by identifying fundamental patterns, like global and band attention. These basic patterns are then analyzed to understand how they are used in combination within existing transformer architectures. Researchers also develop extended patterns tailored to specific data types.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["What is the significance of categorizing sparse attention approaches?", "How do extended sparse patterns contribute to the flexibility of transformer models?", "Why are researchers interested in understanding how different atomic sparse patterns are combined?"]}
73c7f3ef-beaf-448f-83fb-21a1e7a0c1f3	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.024719492,0.02468671,0.025101002,0.031015307,-0.028460579,0.026049022,-0.008616642,0.057417285,0.002848641,-0.0324859,0.0067720255,-0.038398925,0.013806313,0.005433095,-0.025468886,0.002050337,-0.00094387366,0.07538258,0.024304383,-0.026616938,0.07628085,0.03478622,-0.03108627,-0.017042264,0.012600159,0.056411035,0.0033145796,-0.00019919877,0.00267794,0.01973474,0.055896193,-0.02259978,0.016689198,0.009556122,0.033002265,-0.051956624,0.009108203,-0.095122784,0.042241808,-0.014232476,0.010785456,0.03388077,-0.023022853,-0.04623082,0.04645217,-0.030314447,-0.084374286,-0.024832528,-0.03263282,-0.015267947,-0.017244782,-0.005571821,0.016873566,-0.012357622,-0.02176976,-0.013756617,0.019620344,0.013035183,0.01921714,-0.062533155,-0.015896887,0.0056159915,-0.01358191,0.009138097,0.015056246,0.0015155183,0.005217553,-0.008400055,0.04579111,0.1455899,-0.018356655,0.012652947,0.0054006055,-0.059386533,0.07581459,0.017567165,-0.020522576,0.021498311,-0.051913556,0.008356328,-0.013398642,0.078964435,-0.055383775,-0.02863599,0.06466153,0.0029716215,-0.057658803,-0.0048501207,0.035190355,-0.00905591,0.007760856,0.011388593,-0.0010337053,0.06282037,-0.043865442,-0.054124657,-0.023881536,-0.021330215,-0.029286271,-0.06662498,0.02677507,0.032184046,0.093186304,0.119189,0.028599078,0.0043520583,-0.019404275,-0.027150767,-0.01138185,0.02236979,-0.00021370343,0.040580723,-0.03908172,-0.009563887,-0.030944197,0.0065449863,-0.027256748,-0.006019079,0.010641075,0.017104138,0.0563322,-0.0562622,0.019342938,0.06204519,0.021075053,0.010358906,-0.00051234127,-0.027081344,-0.022375487,-0.0060786596,0.05566688,-0.007321281,-0.027445776,0.019906672,-0.017624859,0.026332814,0.047247965,0.017799005,-0.011895421,0.050540946,0.015817454,-0.0423787,-0.04780234,0.04707502,-0.00773397,0.03289112,-0.07662762,0.061055515,-0.028723828,0.03480123,0.03301342,-0.015458015,0.05238537,-0.0072018187,0.0024168543,-0.024375195,-0.028702702,-0.04709078,-0.008369156,-0.055019256,-0.05507688,-0.04139522,-0.0031391128,0.088743426,0.05341618,0.117598414,-0.0074141626,-0.012901644,-0.0014306854,0.050071932,-0.04483518,-0.0049964255,0.037539676,-0.06564168,0.02118452,0.006725791,0.018552518,-0.048963677,0.008963431,-0.024675425,-0.015587628,0.066771366,0.01449267,0.0477159,-0.033946328,-0.02964311,-0.019531196,0.076385856,-0.0051745498,-0.03470129,-0.038240936,0.002886666,0.015589019,-0.012541069,0.01259379,0.037750494,0.02092135,-0.05984216,-0.00971625,-0.031191615,-0.011096838,0.038582608,-0.074788764,-0.042114105,0.0019770337,-0.02843463,0.010249901,-0.0036398226,-0.045412835,0.018142952,0.019526279,0.01565543,-0.030076507,0.039995328,0.03266829,0.019383376,-0.014265793,0.012373169,0.031091508,-0.0010445745,-0.056639563,-0.015364199,-0.013759786,0.039164152,-0.010075131,0.029099787,-0.015907256,0.03221028,0.030427454,0.0044701393,0.010040123,0.020987038,0.012902273,0.05174029,-0.043268573,-0.021029977,0.027078038,0.02580001,-0.037508864,0.019638907,0.054123364,0.0506741,-0.023509456,0.084141076,-0.0125254905,0.00023226833,-0.00119466,0.040769387,-0.023620721,0.015583218,-0.0265606,0.057912145,0.032533176,0.010698557,-0.056032196,0.009029043,0.008634829,-0.0019756057,0.03723744,-0.024330597,-0.002188625,-0.016538769,0.015388574,0.00079961924,0.008939123,-0.026833944,0.048246104,0.025407266,-0.02800814,0.004791603,0.0622178,0.08400933,0.01955769,0.015152319,-0.051305257,0.04732241,0.030947858,-0.021792065,0.022186326,-0.016500631,0.040843025,-0.041771043,0.018488888,-0.018272297,0.033462822,0.014284208,0.058697235,-0.023749884,-0.0015931784,-0.016238451,-0.05618383,0.007637707,0.013285522,-0.030668182,0.01611877,-0.036332376,-0.017875388,-0.008796953,0.0034120062,0.021816887,0.09890888,-0.0004884968,-0.002536327,0.03752965,0.045633152,0.049013298,-0.057052165,0.033118468,0.0253167,0.007827172,-0.006151324,-0.04446632,-0.009185768,0.03002538,0.044780657,0.037395805,0.09582381,-0.022140423,-0.033206634,0.023987794,0.04741298,0.010408707,-0.0044276994,0.032501604,0.025110804,0.0010691461,0.011496065,-0.0036763358,-0.07644083,0.03128903,-0.07714446,0.04074662,0.04853107,-0.055016734,0.030824265,0.013925928,0.017133195,-0.010081959,0.0073421067,0.003634757,-0.021172678,-0.024501344,0.03460453,0.03862125,-0.0131343845,0.050395872,0.02582684,-0.025249435,0.0012639719,0.005397651,-0.023831729,0.06324205,0.021234723,-0.027040375,0.0031907612,-0.017371668,0.03276709,0.03486016,0.024154335,-0.035052005,0.071330905,-0.076946065,0.012709007,-0.038032413,0.008549392,-0.035323877,-0.04216346,0.003649273,0.02489258,-0.017244983,0.005612835,-0.012624888,0.033736397,-0.044918165,-0.019682815,0.025096333,0.03202082,-0.0049913586,-0.03543288,-0.04681332,-0.03048619,0.003384154,-0.030352017,-0.05380122,0.030390255,-0.0044599404,-0.0091701,0.09687347,-0.021840118,0.02909811,-0.035869606,0.047787394,-0.017397517,0.04031014,-0.06495387,0.00315143,0.0068338173,0.049341943,0.029815612,0.025664233,0.016709907,0.0020232378,-0.06556714,0.024186559,0.02191856,0.009789896,-0.08412349,0.029164014,0.032420766,0.044151098,0.011747411,-0.06488851,0.015169915,0.0060734204,0.033102,-0.034598943,0.003812281,-0.01414319,-0.036838874,0.02761667,-0.02978539,0.03367398,0.016573368,0.036700442,0.025734223,-0.029946992,-0.07114453,0.021293586,-0.014654016,-0.043017223,-0.028923852,-0.038686447,-0.0022641409,-0.003603357,-0.015114065,-0.02563801,-0.043282077,0.08328188,0.01636759,-0.011372364,0.09037207,0.0066860085,0.010294687,0.0023900107,-0.015005715,-0.032955185,0.010874006,0.034974553,-0.033474736,-0.018052232,0.046580102,-0.02679796,0.030442474,0.057499543,0.038643405,0.038100943,-0.023128223,-0.033462137,-0.06017421,0.000151268,0.028781794,0.044047255,-0.007442156,-0.03135674,0.019963967,-0.04724784,-0.018140065,-0.073473364,-0.031682737,-0.024723668,-0.020862563,0.0060494132,-0.026683861,0.033779725,-0.035456117,-0.02386738,-0.018918766,0.013522034,0.015374366,0.03309523,-0.08176504,-0.0052492297,0.050593242,0.03435685,-0.03762935,0.043904454,-0.012246798,-0.022908105,-0.010741185,0.0044698124,0.017663378,0.06252732,-0.057099964,-0.00437446,0.03659668,-0.04169978,0.0017742437,0.003021681,0.039445803,-0.0025952964,-0.05371261,-0.03391853,0.034756895,0.025690516,0.0140668135,0.06641759,0.0046825754,0.032559503,-0.040028848,0.039383974,0.04687159,-0.056591883,0.030989714,0.044454765,-0.0014132205,0.037595604,-0.014740859,-0.033454627,-0.025683155,0.009541155,0.034871187,0.016749194,0.035618737,0.028147025,0.013555844,-0.005165647,0.007708447,0.025761196,0.07195596,-0.10456777,0.00247572,-0.08649645,-0.008451747,-0.0038397626,-0.06444657,-0.07796231,-0.01122458,-0.042794846,0.024462167,-0.047461044,-0.023204178,-0.036861498,-0.0017862513,-0.021773843,0.024484687,-0.029316287,0.049515158,0.011582832,0.0020865106,0.07065925,0.03404866,0.006638116,0.009845877,0.010269181,0.0041861683,0.04748531,0.055565003,0.02948457,-0.059146665,0.06646624,0.017150957,0.017726036,-0.02560929,-0.049514275,-0.04525271,-0.0200569,-0.033623483,0.029991511,-0.021161174,0.005114417,-0.037682682,0.004947849,0.021101765,0.033191644,0.03919128,0.054918133,0.028949099,-0.0107815815,0.024670927,0.014521623,0.032554407,0.015183522,0.014045872,0.03828489,0.014709737,0.0003228774,-0.013469842,0.02018992,0.012548511,-0.022775397,0.020269291,-0.017948972,0.009112366,-0.011065103,0.019002555,0.010339521,-0.019086238,0.021654237,0.052869402,-0.030127507,-0.082571395,-0.018049335,-0.007498885,-0.031709,0.03818049,0.03800304,0.015005549,0.014980611,-0.024625622,0.019315,-0.0798763,-0.016834704,-0.038548935,-0.025822839,-0.046910126,0.0024206406,-0.02539362,0.0037566915,-0.008908357,-0.0017365362,0.057579055,0.00017531485,-0.022405725,0.03913495,0.0078685675,-0.023335597,-0.032796543,0.011015785,0.02032362,0.030800132,-0.01286366,-0.023506304,0.07759832,-0.042669628,-0.02148246,0.0016305044,0.0217052,-0.09027284,-0.03897087,0.016975202,-0.00021966718,-0.020847252,0.0021547987,0.007969997,0.007632374,-0.04605564,0.012550207,-0.038925752,0.03218634,-0.026662633,0.036252975,0.046929333,-0.007525179,-0.050533567,-0.020240724,0.036357883,-0.043921754,0.025931703,0.042632777,0.050858486,0.0655981,0.0305617,0.003866729,-0.05149224,-0.010529277,0.015591823,0.020064894,-0.024454076,0.051843308,-0.006530565,-0.020085197,-0.017837847,-0.014240535,0.002151328,-0.02080056,0.041416373,-0.029833293,-0.02387092,-0.03810035,0.0053832335,0.019785034,0.004609367,-0.055227354,0.080461405,-0.0020616518,0.0031420568,-0.051057395,-0.07508105,-0.0624365,0.014138268,0.024097916,-0.006214522,0.01801436,0.0011632399,0.007345285,0.014652448,-0.0113603845,-0.08046127,-0.049559783,0.029089011,0.044295263,0.039699428,-0.033868007,0.026445504,-0.095380224,0.036390275,-0.033343896,-0.087870926,0.03307524,0.021538273,-0.012561956,-0.030250818,-0.014227241,0.035374437,0.04048082,-0.009419723,-0.0020630902,0.03400502,-0.0058646365,0.019838668,-0.03803886,-0.0018465975,-0.01908658,-0.020250913,0.013915158,0.044530988,0.05848172,-0.0318476,-0.014350876,-0.0331106,-0.02999833,0.033538446,0.01776753,0.038681805,0.010004289,0.01976269,0.008571909,0.026972394,0.0588908,0.005700263,-0.007906754,-0.020859517,0.039586697,0.03352096,-0.05022548,-0.050861187,-0.04215896,-0.012144719,-0.033043694,0.0076752645,-0.013468548,0.031423673,-0.021491202,-0.032461885,-0.013379966,-0.03546358,0.018050257,0.047077868,0.037767358,0.01848245,-0.08947168,-0.003104462,0.009413734,0.07901388,0.015943091,-0.036039457,-0.0011936561,0.003898257]	Keywords: dilated attention, receptive field, dilation, strided attention\nKey Objects: attention window, dilation\nRefers to Images: ./images/a-survey-to-transformers/image_4.png\nHypothetical Questions:\n- How does dilated attention relate to dilated CNNs?\n- What is the advantage of using strided attention over standard dilated attention?\n- How does dilation affect the receptive field of the attention mechanism?\n---\nSummary:\nDilated attention, inspired by dilated convolutional neural networks, expands the receptive field of band attention without increasing computational cost by introducing gaps in the attention window.\nOriginal Text:\n- (3) Dilated Attention . Analogous to dilated CNNs [134], one can potentially increase the receptive field of the band attention without increasing computation complexity by using a dilated  \n$^{5}$In practice, these global nodes can be selected from the sequence (internal global nodes) or virtual nodes with trainable parameters (external global nodes).  \nFig. 4. Some representative atomic sparse attention patterns. The colored squares means corresponding attention scores are calculated and a blank square means the attention score is discarded.  \n  \nwindow with gaps of dilation w$\\_{d}$  1, as depicted in Fig. 4(c). This can be easily extended to strided attention , where the window size is not limited but the dilation w$\\_{d}$ is set to a large value.\nContextualized Text:\nTo broaden the receptive field of band attention without increasing computation, dilated attention can be implemented by introducing gaps in the attention window. This technique, inspired by dilated convolutional neural networks, allows for an expansion of the receptive field, and can be further extended to strided attention where dilation is set to a large value.	{"tags": ["attention", "sparse attention", "architecture"], "doc_id": "73c7f3ef-beaf-448f-83fb-21a1e7a0c1f3", "summary": "Dilated attention, inspired by dilated convolutional neural networks, expands the receptive field of band attention without increasing computational cost by introducing gaps in the attention window.", "doc_type": "text", "entities": ["CNNs"], "keywords": ["dilated attention", "receptive field", "dilation", "strided attention"], "key_objects": ["attention window", "dilation"], "contextual_text": "To broaden the receptive field of band attention without increasing computation, dilated attention can be implemented by introducing gaps in the attention window. This technique, inspired by dilated convolutional neural networks, allows for an expansion of the receptive field, and can be further extended to strided attention where dilation is set to a large value.", "mentioned_images": ["./images/a-survey-to-transformers/image_4.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does dilated attention relate to dilated CNNs?", "What is the advantage of using strided attention over standard dilated attention?", "How does dilation affect the receptive field of the attention mechanism?"]}
7ef6113b-1bb7-4c7e-8f46-8ff989b4bb9d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.035631444,0.043819472,0.01226313,0.002902674,-0.03393495,0.056338344,-0.014971324,0.018745327,0.050280597,-0.014362403,0.03638251,-0.019522205,-0.014403442,-0.024150223,0.01883042,0.07375841,0.048831973,0.07122929,-0.01455729,-0.06389231,0.042327847,0.038475774,0.0046523563,0.018148879,0.04546899,0.033099074,-0.028783996,-0.014441213,0.01671401,0.060401656,-0.04415585,-0.027956141,0.049627684,0.02317899,-0.017271876,-0.028980263,0.0008405617,-0.088268556,0.035577413,-0.014160219,-0.046436626,0.056365494,-0.06216792,0.025053624,0.042673994,-0.019520042,-0.09502833,-0.042137742,0.011978414,-0.017766383,-0.036183532,0.0154283205,0.0021123139,0.024954055,-0.038590617,-0.041109677,0.00555442,-0.032698937,0.00771157,-0.0057118344,0.047091283,0.03118194,0.01489768,-0.013332918,0.030046996,-0.04299062,0.032318696,0.01950972,0.045527235,0.14276332,-0.04003942,-0.01784673,0.042814095,-0.0077096233,0.12691143,0.012960642,0.0016060566,-6.8830705e-05,-0.026873358,-0.0008294406,0.017561467,0.008871049,-0.047776558,-0.008496467,0.097931825,0.047079127,-0.04523642,-0.039696272,0.032771572,-0.09783693,0.015664585,-0.047083862,0.016454982,0.0038159282,-0.051557086,0.00026563488,0.004446358,-0.06496354,-0.022554707,-0.03605657,-0.0074719125,-0.027296485,0.05554537,0.10231428,0.051923607,0.003586537,-0.036432557,-0.059027344,-0.026655613,-0.012838786,-0.015835445,-0.023140252,-0.0662846,0.023621807,0.009718545,0.021032374,-0.02167839,-0.04242024,-0.02561329,-0.0035799048,0.060783044,0.027508223,0.017073546,0.026893856,0.043710515,0.051943947,-0.031066544,-0.038404986,-0.019324778,-0.04245911,0.046805277,0.012522682,-0.054514088,0.046944782,-0.022317227,0.010764026,0.04595544,0.035670057,0.0005667349,-0.023009941,0.013402329,-0.021646146,-0.05502166,0.029632442,-0.0146589065,-0.008240614,-0.039939016,0.022772217,-0.022667948,0.07286456,0.042879254,-0.0012731031,0.09244107,-0.06533611,-0.017261453,-0.021751616,-0.03570746,0.021166228,-0.022326726,-0.043199852,-0.074288875,-0.028291153,0.0015987481,0.049192622,0.03735572,0.048813134,-0.00182055,0.020213593,-0.027424827,-0.02869603,-0.053683102,-0.03230407,0.04110257,0.019926423,0.05345127,-0.008875809,0.042036265,-0.004298898,0.03108941,0.0027267218,0.018405743,0.067463405,-0.0030208935,0.016392732,0.0420062,0.0127631575,-0.041623123,0.035162333,-0.015887232,-0.04302817,0.017206352,0.019397106,-0.0063053933,-0.017578142,0.0115839,0.037856396,0.059670594,0.005800137,-0.04441777,0.026211642,-0.018621162,-0.010956026,-0.010107259,-0.026109692,0.003843376,-0.034650523,0.029257147,0.02286179,-0.006416737,0.041275177,0.008735055,-0.0074100257,-0.038379356,0.031461213,-0.021698616,0.0013298762,-0.03512748,0.054806057,0.033347342,-0.017402623,0.0061378833,0.023599273,-0.025555313,0.08199899,0.011939133,-0.031725638,-0.084953874,-0.02055845,-0.010781173,-0.01114047,0.03504684,0.060815014,0.018625986,0.040120397,-0.025254203,-0.046377514,0.014121426,0.014029446,-0.008444984,0.0803822,-0.020571034,0.0246473,-0.004693742,0.064629585,0.039104983,-0.02437914,0.020225238,-0.0023798929,0.006691093,0.028362628,-0.013818798,0.037257034,0.06004025,-0.0024163136,-0.035816014,-0.027072337,-0.02566971,0.036460996,0.03194393,-0.023979357,0.02135295,-0.015047829,-0.038219757,0.03136435,-0.014080749,-0.013397573,0.021701068,0.0030167908,-0.078742355,0.020277161,0.036412388,0.07834429,0.025966285,-0.07195244,0.045240983,0.005270654,0.0032599522,0.057806984,0.0426172,-0.04774619,0.03609253,-0.030104533,0.022710584,0.026310265,0.036380146,0.049383894,-0.023121413,-0.03392212,-0.025832776,-0.021153318,-0.005757239,-0.0062080417,-0.024635948,0.028837416,0.026927527,-0.030502643,0.009587592,-0.062355895,0.018949112,-0.012234537,0.0691295,-0.0132681085,-0.013707884,0.005384119,-0.0043059606,-0.025582843,-0.036222994,0.020772839,0.009666906,0.030545961,-0.03878917,-0.051167328,-0.066780806,0.035339314,0.008789012,0.049046136,0.10087115,0.004294583,-0.023591759,0.038981065,0.048809424,0.005002901,-0.008851567,0.030562852,0.013299799,-0.006342578,-0.0013389692,-0.049845595,-0.062046647,0.048641663,-0.05958241,0.06008209,0.06972447,-0.04716519,0.0052857953,0.027623193,-0.039580338,-0.018726787,0.019426532,-0.0041657304,0.013505732,-0.05855032,0.021777773,0.02288169,0.031463426,0.070055015,-0.00044754546,0.018161016,-0.017448172,-0.021051027,-0.006333319,0.0064416104,-0.0020733208,-0.024182718,0.039348017,0.016308393,0.008135218,0.060257893,0.018153397,-0.012586818,0.030667096,-0.045806378,-0.02333972,-0.048761025,-0.00036903535,0.0020326122,-0.022422938,0.016190097,-0.039972864,0.048669778,-0.0006368944,-0.050297584,0.022138242,0.01361693,-0.006914821,0.061859157,0.00079406664,-0.0104825515,0.0015137107,0.0049313027,-0.036731668,0.0022291944,-0.014985547,-0.052594654,0.00084044645,0.019476313,-0.031366646,0.07280712,-0.036726464,0.05096962,-0.022445332,0.02390097,0.011844653,0.028403394,0.00427132,0.007369525,-0.029822042,0.012401139,0.06931376,-0.005437951,-0.02392743,0.052437518,-0.052142743,0.009800631,-0.0046316655,-0.013709511,-0.048220422,0.030987605,0.015917717,0.013887231,-0.0106807025,-0.01602989,-0.026079847,0.0070547243,-0.033663515,0.019682266,-0.037384328,0.007702744,-0.03639498,-0.047318198,-0.08086428,0.0065374747,0.09194226,0.013209855,0.022581356,-0.05839636,0.0035776778,0.04002543,-0.015443369,-0.0542338,0.049249075,-0.015521448,0.028938076,0.005659816,0.016518839,-0.022329114,-0.015003871,0.060993087,-0.071968555,0.036238614,0.070304364,-0.043255866,0.039232455,-0.019500185,0.007943789,-0.017030941,-0.008321786,0.03843832,-0.0117891645,-0.0053724847,0.0075430605,0.03383121,0.028266445,0.04084257,0.026543492,0.0051504746,0.01401218,-0.0749105,0.002853715,0.006180254,-0.027489783,-0.0114874765,0.013379816,0.017886447,0.01808641,-0.030798728,0.022173911,-0.013351138,0.022682028,0.011600785,0.01572154,0.026207915,-0.024110187,-0.0027231367,-0.005761043,0.00614003,0.0014461644,0.05368511,0.087715425,0.015613543,-0.07154189,-0.05946126,0.012471887,0.022480164,0.015983159,0.021392481,-0.00095097075,-0.0139137525,0.034796737,0.03231305,0.027803328,0.00800403,-0.052106366,0.024065094,0.011986651,-0.06048991,-0.0077671753,-0.01602296,-0.0020965205,-0.02057586,-0.008526929,-0.0048115607,0.021006541,-0.005485807,-0.014323291,0.025351636,-0.048372805,0.06290808,-0.06794089,0.0514787,0.0359901,0.033886194,-0.0042517297,0.04726832,-0.019416315,0.005417361,-0.035748255,-0.032765873,0.007874302,0.02430919,0.030785069,0.0053670136,0.059107244,-0.025666574,0.023716483,-0.0105075985,-0.027973069,0.0013315597,-0.018189799,-0.05803349,0.024362931,-0.094536126,-0.043054596,-0.009696589,-0.09132896,-0.082819246,-0.02353954,-0.0021407036,-0.03879247,-0.014828592,-0.009287685,-0.049703658,0.0324743,-0.010300366,0.07976825,-0.02409419,0.026112542,0.029952481,-0.013872118,0.102161765,0.042298123,0.030002361,-0.020999372,0.005084358,0.014425263,0.027279323,0.02244734,0.03694734,-0.00019687266,0.0775719,0.012369104,0.04415093,-0.028701987,-0.024234833,0.0177458,0.007624126,-0.004167814,-0.014174123,-0.025913335,0.020566544,-0.038102213,0.045668114,-0.003967909,-0.013641758,0.0574743,0.063336365,-0.0039384384,-0.02787434,0.016494587,0.0383084,0.016493365,-0.0060606604,0.044769287,0.08640996,0.026754728,0.039430242,-0.009115223,0.02929635,0.0023782647,0.011240201,0.050522033,-0.024284454,0.012686031,-0.034541793,-0.008444644,0.008961927,-0.0017548264,0.007813387,0.04523633,-0.04098804,-0.069693014,0.02847469,0.0037702774,0.002962086,0.015416398,-0.043118462,-0.0062680375,0.027690314,-0.009177049,0.049321137,-0.0594427,-0.031445283,-0.024411105,0.030900398,-0.08866913,-0.013131358,0.027876044,-0.035284955,-0.05620025,-0.003787552,0.013689072,0.027652964,-0.014680685,-0.03540474,-0.045039292,-0.02326688,-0.030751964,-0.024807671,-0.017556546,-0.010488873,0.0052035777,-0.01538945,0.05403251,0.0027496014,-0.009018476,-0.040819485,0.050408915,-0.054576688,-0.03662736,0.020959629,-0.033039067,0.022596752,-0.00016649284,0.0055364594,0.00957561,-0.06441316,0.01177503,-0.04631305,0.017287375,0.010347343,0.019422319,0.038696293,-0.045448933,-0.06640276,0.0013898582,-0.014965684,-0.047535878,0.026215728,0.04467892,0.0025793875,0.024623023,0.024229722,0.0050237356,-0.06150952,-0.037119642,-0.03691503,0.052626725,-0.0009827507,0.034558687,0.0016645034,-0.011574319,0.016746838,0.03174928,0.02817871,-0.003816661,-0.004732724,0.05683638,-0.014990658,-0.030129649,-0.026374713,-0.022557113,0.030903572,0.014995982,0.022725007,0.023545103,-0.020857029,-0.03597445,-0.06388126,-0.019701883,0.019328333,0.062441304,-0.0072566587,0.038023755,-0.02247437,-0.012959623,-0.04125548,0.0595776,-0.022268444,-0.037443582,0.01447555,0.03581356,0.043304883,0.02433829,0.010076776,-0.056131747,0.025522335,0.0034787473,-0.017767703,0.0022237734,0.013805136,-0.03463406,0.003979407,0.008483048,0.0076251454,0.08310362,-0.06261344,-0.010461087,-0.012234955,-0.023078937,-0.035470624,-0.013196921,0.0058309664,0.023347685,0.044498906,0.050193634,0.03210691,0.0003464532,-0.025677878,-0.027754074,-0.05781361,-0.011498127,-0.0023477615,0.016592251,0.040629346,-0.06098795,-0.008847107,-0.010493097,0.0132482,0.06805169,-0.01284099,-0.0076114833,-0.050081566,0.014993205,0.017299661,-0.024398651,-0.014170164,-0.05112813,-0.07571046,0.031127308,-0.02330431,-0.030981215,-0.016847745,0.00524978,-0.030219642,-0.0576109,-0.055831827,-0.023099657,0.032753475,0.022898221,-0.036939148,-0.040152736,-0.03099573,0.027129048,0.029313523,6.091075e-05,-0.0022719807,-0.02790157,-0.010944532]	Image title: Different types of Image Warping\nTags: image-processing, warping, distortion, transformation, pixels\nKey objects: Image, Global Warping, Band Warping, Dilated Warping, Random Warping, Block-Local Warping\n---\nSummary:\nThis figure illustrates five different methods for warping an image, showing how different approaches affect the spatial relationships between pixels. The techniques shown are global warping, band warping, dilated warping, random warping, and block-local warping.\nFull description:\nThe figure presents five image warping techniques. (a) 'global' warping shows a distorted image where the entire image undergoes a uniform transformation. (b) 'band' warping shows horizontal bands of pixels shifted. (c) 'dilated' warping shows diagonal lines through the image, suggesting a pixel-by-pixel alteration. (d) 'random' warping exhibits a scattering pattern, indicating that the image's pixels have been displaced randomly. (e) 'block local' warping shows squares being displaced suggesting a local distortion.\nText found in image:\n- (a) global\n- (b) band\n- (c) dilated\n- (d) random\n- (e) block local	{"tags": ["image-processing", "warping", "distortion", "transformation", "pixels"], "title": "Different types of Image Warping", "doc_id": "7ef6113b-1bb7-4c7e-8f46-8ff989b4bb9d", "source": "./images/a-survey-to-transformers/image_4.png", "summary": "This figure illustrates five different methods for warping an image, showing how different approaches affect the spatial relationships between pixels. The techniques shown are global warping, band warping, dilated warping, random warping, and block-local warping.", "doc_type": "image", "key_objects": ["Image", "Global Warping", "Band Warping", "Dilated Warping", "Random Warping", "Block-Local Warping"], "parent_doc_id": "73c7f3ef-beaf-448f-83fb-21a1e7a0c1f3", "text_in_image": ["(a) global", "(b) band", "(c) dilated", "(d) random", "(e) block local"], "contextual_description": "The figure presents five image warping techniques. (a) 'global' warping shows a distorted image where the entire image undergoes a uniform transformation. (b) 'band' warping shows horizontal bands of pixels shifted. (c) 'dilated' warping shows diagonal lines through the image, suggesting a pixel-by-pixel alteration. (d) 'random' warping exhibits a scattering pattern, indicating that the image's pixels have been displaced randomly. (e) 'block local' warping shows squares being displaced suggesting a local distortion."}
ea315172-8f2c-468a-9bd9-05fe35c985c3	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.023852162,0.0027106847,0.023633188,0.036924757,-0.057600457,0.0433917,-0.036398314,0.032726306,0.02964658,-0.0037060524,0.009013522,-0.0010247809,0.013268436,-0.014488396,-0.007037859,0.00012304747,0.029413875,0.08043605,-0.01905357,-0.06630948,0.06210299,0.036978807,-0.0032437332,0.00481846,-0.017897505,0.029916719,0.049506236,0.01961079,-0.009966928,0.0067671747,0.0013710967,-0.06268326,0.04511406,-0.0054205065,0.04387865,-0.03043918,0.0051679774,-0.062293585,0.035591982,-0.015092117,-0.03019331,0.03843705,-0.025333315,0.00880074,0.007905171,-0.052935835,-0.083205655,-0.022396712,0.0031390933,0.021342907,-0.023542723,0.012515147,-0.0016589927,-0.036740236,-0.0016935146,0.0009199192,0.0101856515,-0.035944007,-0.055508465,-0.08790118,-0.007253044,-0.025279772,-0.032759164,-0.01457109,0.033186983,-0.044386677,0.032668933,0.024521058,0.08686083,0.14650188,0.00787301,0.028773546,-0.007353069,-0.08338512,0.097566284,0.021678943,-0.027344886,-0.038784746,-0.05203937,-0.0006949954,-0.0068566953,0.05047265,-0.035736814,-0.016602568,0.08177675,-0.016308041,-0.04519926,0.02049312,0.03516209,-0.043075588,-0.01882703,-0.03450189,-0.024394222,0.066915736,-0.030287446,-0.021661278,-0.0070611653,-0.015534509,-0.014683014,-0.062187973,0.029711924,0.0006593432,0.06216739,0.11501601,0.02446277,-0.011545113,-0.03573171,-0.07263976,0.024150437,0.014654843,-0.012924796,-0.0079133995,-0.033819973,0.012781344,-0.0036884353,0.00049237296,-0.03868444,0.025692156,0.0064439946,-0.025640845,0.024465393,-0.040071424,-0.040603496,0.06661147,0.050328385,0.060977284,0.01924632,-0.050714117,-0.0009203228,0.007051463,0.013667298,0.026260117,-0.044543162,0.007961123,-0.049396027,0.026401764,0.10702099,-0.00080135436,-0.021126002,0.024163825,-0.0026643977,-0.023639167,-0.07619707,0.044809107,0.004448664,0.05858241,-0.07630707,0.04521505,-0.049743064,0.054455195,0.030235527,-0.0433742,0.01875382,-0.021082945,-0.017199254,-0.0239177,-0.0020603172,-0.052768443,-0.011074755,-0.06424135,-0.029636415,-0.016425105,-0.001726476,0.08081201,0.07561685,0.09943997,0.007559322,-0.017625824,0.006654973,0.009487556,-0.033051293,-0.014738023,0.026393251,-0.050601397,-0.017657382,0.013652132,0.014309834,-0.06684535,0.03327183,-0.037989534,0.014870478,0.06403299,-0.010246804,0.02670648,-0.041977,0.03122631,-0.036155976,0.068349935,-0.0028606376,-0.048152342,-0.006453831,-0.033169497,0.0016566231,0.018370185,0.013414885,0.052635353,0.022121469,-0.054578166,-0.0048337546,-0.054214377,-0.031590104,-0.0072003277,-0.026726905,-0.01703469,0.019115584,-0.008814544,0.017586477,-0.016092371,-0.025960192,0.002676779,-0.022669937,-0.0016096791,-0.056941323,0.01085687,0.013515354,0.022778613,-0.02491471,0.016835816,0.056469575,0.0015177019,-0.037636995,-0.013109348,-0.041024007,0.051639356,-0.002965445,0.031124247,-0.003603061,-0.035575744,-0.025655631,0.03479417,0.03426065,0.03618754,-0.037243128,0.0354226,-0.060443833,0.043059576,0.03594939,0.017728977,-0.028172439,0.04488369,0.033823602,0.07069009,0.0022586407,0.09749317,-0.018127494,0.013750224,0.016219161,0.04604171,0.032034323,0.00797403,-0.030759275,0.06218715,0.043489978,-0.024205463,-0.012206111,-0.031162685,0.012294694,-0.0024704007,0.020010024,-0.017643068,0.029373728,-0.00017886859,0.018638223,0.05083472,-0.014234431,-0.020747868,0.029773371,0.0012539551,-0.07059442,-0.025475293,0.036984447,0.02545449,-0.013554203,-0.04275624,-0.04147857,-0.019224942,0.021099018,-0.0077248327,0.0391496,-0.011303543,-0.035162244,-0.029074468,0.03625761,0.0071477266,9.597878e-05,0.034809485,0.00547767,0.011984096,-0.02646248,-0.0058773393,0.009210617,-0.028036345,0.027008705,-0.03388742,0.0011886422,0.020898534,-0.001762763,-0.014731603,-0.0051628677,0.013212301,0.099533886,-0.032570943,0.024306081,0.042693377,0.008648967,0.033719014,-0.022960221,0.036793187,0.049268726,0.0073398626,-0.018799763,-0.028964892,-0.0018082765,0.01596603,0.0023959263,0.09094882,0.110572815,-0.030340288,-0.02685608,-0.008511436,0.008014234,-0.003566994,0.00048550573,0.020375714,0.045982048,0.00043815278,0.04628163,-0.035843764,-0.069958024,0.09687609,-0.044846874,0.032927554,0.077620186,-0.006940312,0.012351465,0.01132855,-0.000555714,0.048882607,0.016339803,0.016623259,-0.025376415,-0.023583446,0.034397863,0.008158591,-0.011893454,0.01120606,-0.015810607,0.0038646846,-0.01465768,0.005164551,0.038576603,-0.0051897573,-0.009221083,0.034534123,0.0034960206,-0.034397505,-0.008745398,0.017736329,-0.023449061,-0.012572442,0.051872488,-0.094481915,0.08942936,-0.07267184,0.052969363,-0.020931486,-0.044765353,0.039158974,0.03869295,-0.03337442,-0.0077106063,-0.013090707,0.047471296,-0.0339561,-0.031917542,0.05630278,0.031565424,0.053128116,-0.006613249,-0.0011622856,-0.020188302,-0.013119576,-0.002949962,-0.0835052,0.029292691,0.0078108995,-0.013449983,0.076566964,-0.03722115,0.037049457,-0.060998537,0.019087594,0.020944098,0.03086443,0.0021639718,0.006488515,0.016379114,0.05076547,0.03250836,-0.021528604,0.017541002,0.021185368,-0.069203794,0.03894543,0.02312984,0.03799197,-0.091834955,-0.0018272412,0.024015002,-0.002117974,0.03102345,-0.05022146,-0.024388345,0.0013174908,3.4665074e-05,-0.023410335,-0.00048448745,-0.0028415304,-0.053226784,0.0073597315,-0.0077732084,0.0257043,0.032962035,0.024666246,0.033012904,-0.008899018,-0.03627754,0.043540224,-0.014364033,-0.056768183,0.0012667418,0.002828315,-0.030736974,0.034884315,0.019769104,0.0031893095,-0.04500905,0.09378548,-0.009444967,0.035878096,0.09111347,-0.02854656,-0.001307651,-0.041818906,0.027817491,-0.069766805,0.0406272,0.022836098,0.024746545,-0.0073459614,0.03825314,0.000506929,-0.0016219361,-0.0032429611,0.033342436,0.004732421,-0.007362187,-0.06486384,-0.048972473,7.002524e-05,0.019496761,0.05260099,0.018340617,-0.023646744,-0.007824392,-0.009294972,-0.004472187,-0.021202771,-0.005035908,-0.0025507247,0.0037939579,0.044015173,0.016165152,0.03989869,-0.019821744,-0.010659019,0.003591278,0.055403285,0.04468342,0.019112017,-0.050437745,-0.011419704,-0.0018979089,-0.0003329942,0.044101276,-0.028982716,0.0031557386,-0.004124692,-0.058859516,0.00088596094,-0.0007491727,0.048561726,-0.012620443,0.0024774333,0.010758223,-0.038960204,0.0039766724,-0.011216434,0.03483337,-0.02232351,-0.004338356,-0.014095602,0.05851865,0.034224816,-0.016519794,0.037067242,0.0054434524,0.04747992,-0.035259113,0.05099656,0.015572806,-0.055475567,0.0015039042,0.0060765543,-0.059023,-0.0034455927,0.0010348373,-0.021694269,-0.020864172,0.04103964,0.007880646,0.018715966,0.047407992,-0.015733983,-0.033982202,-0.0023526095,0.004082062,0.020487566,0.01872305,-0.054362547,0.0010854924,-0.024315197,-0.011043601,0.0009941072,-0.05961697,-0.039027065,-0.04352276,-0.06027065,-0.028666181,-0.016960984,-0.008058036,-0.02057754,0.01820513,-0.030331122,0.054560218,-0.048724085,-0.015304697,-0.01062395,0.010620616,0.08689534,0.031158978,0.03961012,0.0017679499,0.0049008243,-0.015416412,0.022799155,0.053626634,0.026596293,0.02372007,0.07920136,0.015864134,0.007170756,-0.05333457,-0.058640003,-0.06212622,-0.0016331292,0.0047432478,-0.020690188,-0.030628564,0.007936988,0.010779869,0.009228045,0.021870056,0.016253745,0.0123956315,0.051128,0.00052377867,0.0196306,0.06349967,0.0024873107,0.061274532,0.023268241,0.07201934,-0.029255498,0.035801154,0.004189345,-0.0026945232,0.030022092,-0.006759342,-0.01680612,-0.020891177,-0.04743005,-0.025532221,0.014157017,-0.002868161,-0.042439446,0.027096698,0.023221387,0.042340245,-0.011241371,-0.035837796,-0.01061866,0.017253675,-0.059137624,0.016659006,0.020576179,0.004511359,0.035782445,-0.059869543,0.03901369,-0.07023117,0.018483082,-0.004445311,-0.005393326,-0.06271475,0.0021374803,-0.023210699,-0.017251747,-0.019662013,-0.03482987,0.021546356,0.04257826,-0.036660466,0.020384403,-0.009351978,0.035144247,0.013587089,-0.047117483,-9.106299e-06,0.0008024252,0.011570043,-0.02679207,0.08008051,-0.02764857,-0.0020428787,0.029825982,0.017385373,-0.08247674,-0.030347837,0.030514818,0.04207357,-0.028571602,0.072290644,-0.00809382,0.011583407,-0.010327881,0.024803154,-0.07229242,-0.01838795,-0.056992725,0.04160035,0.02679792,0.0023018785,-0.041324366,0.0050395005,-0.031691637,-0.008351058,0.05022617,0.062177017,0.040024057,0.08063039,0.051308148,0.016216619,-0.02668472,-0.016935006,-0.021826563,0.030994749,-0.053343467,0.05655769,-0.039682847,-0.017012235,-0.000336421,0.015323749,-0.019970313,-0.018093556,0.021270426,-0.0049017784,-0.018394884,-0.020551875,0.006083705,0.0355374,-0.011015112,-0.01662039,0.049453266,0.044508096,-0.029093519,-0.016211882,-0.05959065,-0.03852243,0.004493958,-0.027096044,0.010754379,0.022615522,-0.042494163,0.02415551,0.027465755,-0.0073179123,-0.02656854,-0.03334012,2.3119135e-05,0.03170822,0.04794549,-0.024733663,-0.0065826024,-0.10055755,0.01558044,-0.012015487,-0.027879154,0.028688714,-0.015304406,-0.0271008,-0.034897048,0.019673359,-0.013002567,0.020961063,0.004480381,0.012318372,0.03082676,-0.0153741315,-0.0080142245,-0.070275865,0.004421197,-0.024609689,0.017133927,-0.022330744,0.045917872,0.030403275,-0.019208314,-0.014742099,0.0016260935,0.038034745,0.027965615,-0.0070782523,0.06373457,0.016008656,0.027224718,-0.024271172,0.02232176,0.07441295,-0.03387553,0.030558288,0.03297432,0.015760453,-0.0007562687,0.006867468,0.0034743112,-0.03414382,-0.067323335,0.020579046,0.05528548,-0.026574574,-0.004785063,-0.0031950464,-0.032733478,-0.018702267,-0.01561021,-0.023351599,0.057084788,0.001484465,0.029836474,-0.05073045,0.038952585,-0.04371926,0.022299828,-0.036800522,0.019424656,-0.05751787,0.0298632]	Keywords: random attention, non-local interactions, random graphs\nKey Objects: random graphs, edges, queries\nRefers to Images: None\nHypothetical Questions:\n- Why might random graphs have desirable spectral properties for attention mechanisms?\n- How does random attention compare to other sparse attention methods in terms of its ability to capture non-local dependencies?\n- What are the potential drawbacks of using random attention, and how might they be mitigated?\n---\nSummary:\nRandom Attention increases a model's ability to handle non-local interactions by randomly sampling edges for each query, drawing inspiration from the spectral properties of random graphs.\nOriginal Text:\n- (4) Random Attention . To increase the ability of non-local interactions, a few edges are randomly sampled for each query, as illustrated in Fig. 4(d). This is based on the observation that random graphs (e.g., Erdos'Renvi random graph) can have similar spectral properties with complete graphs that leads to a fast mixing time for random walking on graphs.\n- (5) Block Local Attention . This class of attention segments input sequence into several nonoverlapping query blocks, each of which is associated with a local memory block. All the queries in a query block attend to only the keys in the corresponding memory block. Fig. 4(e) depicts a commonly used case where the memory blocks are identical to their corresponding query blocks.\n- 4.1.1.2 Compound Sparse Attention . Existing sparse attentions are often composed of more than one of the above atomic patterns. Fig. 5 illustrates some representative compound sparse attention patterns.\nContextualized Text:\nTo enhance a Transformer model's ability to understand relationships between distant tokens, Random Attention randomly selects a few edges for each query. This method is inspired by the observation that random graphs, like Erdos'Renyi random graphs, can have spectral properties that facilitate efficient information flow.	{"tags": ["attention", "sparse attention", "graph theory"], "doc_id": "ea315172-8f2c-468a-9bd9-05fe35c985c3", "summary": "Random Attention increases a model's ability to handle non-local interactions by randomly sampling edges for each query, drawing inspiration from the spectral properties of random graphs.", "doc_type": "text", "entities": ["Erdos'Renyi random graph"], "keywords": ["random attention", "non-local interactions", "random graphs"], "key_objects": ["random graphs", "edges", "queries"], "contextual_text": "To enhance a Transformer model's ability to understand relationships between distant tokens, Random Attention randomly selects a few edges for each query. This method is inspired by the observation that random graphs, like Erdos'Renyi random graphs, can have spectral properties that facilitate efficient information flow.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["Why might random graphs have desirable spectral properties for attention mechanisms?", "How does random attention compare to other sparse attention methods in terms of its ability to capture non-local dependencies?", "What are the potential drawbacks of using random attention, and how might they be mitigated?"]}
4bdb059e-f562-46e3-a99d-8fa0d4f6ecc9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.020638257,0.0067477524,0.049748104,0.08176394,-0.028972771,0.038633354,-0.019567214,0.034098312,0.0038466316,0.0015422982,0.026926551,-0.033197306,0.002029102,-0.02366742,-0.03173355,0.02234794,0.04189985,0.061740316,-0.04024083,-0.0004675416,0.081410944,0.018165607,0.014579281,0.008696353,-0.0013045634,0.07399072,-0.00741444,-0.008675453,0.016858838,0.03492969,0.008551717,-0.034620408,0.022137169,0.005969544,0.06887671,-0.007342499,0.04961995,-0.05030043,0.03630608,0.014685551,-0.012220032,0.034001272,-0.02758555,-0.009152496,0.013614454,-0.049760126,-0.12772119,-0.013693876,-0.00778014,0.0020225656,-0.015111688,-0.00070007273,-0.0012499931,-0.03218041,-0.018734228,-0.019558504,0.011679988,-0.058121424,-0.010622974,-0.097483456,-0.049692344,-0.011520605,-0.045846265,0.019093286,0.030691989,0.0117406985,0.0009578584,0.036405638,0.074182585,0.120802164,-0.039431248,0.044028692,-0.0130789485,-0.020778352,0.1060558,0.035686355,0.00032674597,-0.025320433,-0.07686079,-0.0019209373,0.01197968,0.058249045,-0.051146436,-0.045211304,0.1082893,-0.037246883,-0.03302327,0.011349735,0.034339134,-0.019690689,0.0077745514,0.0022260067,0.008678433,0.06358108,-0.052951973,-0.036982402,-0.013784006,-0.038330525,0.0061752945,-0.049319033,0.0034186433,0.015013114,0.14291823,0.12667386,0.04941535,-0.023475997,-0.021397173,0.017989237,-0.030870866,-0.04601567,0.00882461,0.018589366,-0.056983456,0.06386736,-0.004762271,0.011237724,-0.06474335,0.010483642,-0.0061879023,-0.008894264,-0.014726025,-0.025685275,-0.040006593,0.04099401,0.02624547,0.05958381,0.016812379,-0.03692063,-0.027481161,-0.010430623,0.003269376,0.037365418,-0.0056084422,0.0362054,-0.038624138,0.013122149,0.10552454,-0.0034339435,-0.023878923,0.04132427,-0.019783696,-0.01941177,-0.05515012,0.03403469,0.017722799,0.02944789,-0.0800096,0.024976576,-0.020556962,0.04106211,0.02953651,0.013162247,0.032491688,-0.015398809,-0.012119269,0.007843753,0.026099902,-0.04218735,0.015140336,-0.02242616,-0.080753215,-0.020904096,-0.02382496,0.075387426,0.056940537,0.07979959,0.00922187,-0.0053819176,-9.343734e-06,0.008450591,-0.04739149,-0.017679803,-0.028790142,-0.034503654,0.0018109696,0.0084180245,0.020560967,-0.033761658,-0.0035964851,0.03257768,0.011164086,0.038985796,0.0007552438,0.036650445,-0.034188874,0.068254076,-0.0236679,0.081357434,0.006147686,-0.009190981,0.0427674,0.013829061,-0.009899011,0.021484,0.011938068,0.05734321,0.042781148,-0.082340635,0.009550504,-0.028260268,-0.0012865403,-0.024900533,0.00023022314,-0.010829854,0.013807618,-0.021860523,0.01042082,-0.027697025,-0.0017157904,-0.016301991,-0.023146175,-0.015781827,-0.049381774,0.04188016,0.03887697,0.02362409,-0.025745088,0.036931593,0.033096746,-0.01173876,-0.062165715,0.035544425,-0.040897734,0.005293039,-0.042315606,0.017096274,0.0021753826,-0.007353274,-0.024516897,0.00680139,0.024438113,0.032423295,-0.055309933,0.011772955,-0.045542922,0.026028443,0.005149164,0.044583384,0.0010072273,0.036161188,0.026947813,0.04904537,0.024450485,0.08536325,0.0005195746,-0.0016154445,0.01910275,0.05280164,0.03145677,0.009299454,-0.038981546,0.07233057,0.01775462,-0.0026158832,0.034615345,-0.028847286,-0.005733951,-0.008170454,0.049605668,-0.01837624,0.00801126,-0.031071413,0.013723691,0.019717274,0.006165668,-0.028758124,0.03521734,0.015998691,-0.053656206,0.03778023,0.055700332,0.03353397,0.014727977,-0.050935574,-0.07563712,-0.004046267,0.02812514,-0.009984044,0.01734938,-0.02236541,0.0017702212,-0.011141462,0.0255634,0.009138165,0.004878391,-0.0070399707,0.01911197,-0.009206319,-0.009113952,0.028556755,0.02370355,-0.01495711,0.03415334,-0.033726636,0.009617833,0.012271192,0.018719912,-0.0051822704,-0.0024793693,0.0017058528,0.10180265,-0.04519817,0.011984703,0.04941655,0.028425097,0.030352786,-0.024922147,0.002058375,0.048201002,0.023011787,-0.015492176,-0.038227152,-0.019251283,-0.0061653405,0.019169785,0.07448665,0.10232839,-0.008618424,-0.019764649,0.008368369,0.016401779,0.02829074,-0.023379412,-0.016934946,0.07265896,-0.013925375,0.03892335,0.0016386877,-0.07233211,0.07363532,-0.046083484,0.01869042,0.076407164,-0.0011951993,-0.00037320112,0.022349592,0.0067707733,0.011508419,0.021069687,0.00292042,-0.009218133,-0.005358227,-0.03234329,-0.017954208,-0.0071669533,0.045671474,-0.019768469,0.014084939,-0.033041097,0.00093462144,0.024977379,0.014183036,0.0033439097,0.029294781,0.019699093,-0.038838208,-0.018007942,0.04627657,0.024246635,-0.0028657664,0.08162846,-0.08359687,0.07691149,-0.03458907,0.02163752,-0.051592685,-0.045246914,0.026379228,0.028444588,0.00958683,-0.028650217,0.0021220972,0.030430665,-0.017402641,-0.04143723,0.06491566,0.065292686,0.029563833,0.01014117,-0.022247111,-0.005491006,-0.03588259,-0.07764932,-0.047024105,0.013558123,-0.00081046147,0.0038715284,0.071386255,-0.035822812,0.06524978,-0.021334922,0.048027392,0.0059256814,0.012727361,-0.032682054,0.016183319,0.032695234,0.053132515,-0.0030362674,-0.017840382,0.016918683,0.02591519,-0.042278707,0.04483451,0.04021994,0.06450466,-0.05688317,0.015127484,0.013536073,0.021962974,0.007301967,-0.018652266,0.0061392486,0.0046749576,0.017069343,-0.027718406,0.0058376524,0.017940948,-0.052277733,0.008875928,-0.049060754,0.009511714,0.006525376,0.04055297,0.038301095,-0.033904072,-0.08836879,0.041707043,0.0046141027,-0.03656872,0.054534767,-0.052513707,-0.016415102,0.012012918,-0.008750546,0.046144675,-0.013523433,0.11429422,-0.02627618,0.038053345,0.0823613,-0.052874327,0.014124727,-0.023027837,-0.021675233,-0.04647123,0.022521676,0.0402511,0.005954009,-0.011882217,0.03161731,0.009389194,0.021380996,-0.02805381,0.021359956,-0.0070384955,0.012523433,-0.05380441,-0.0459071,-0.007872444,-0.0045925584,0.060046006,-0.0052938582,-0.040187243,-0.005860862,-0.035802055,-0.02425938,-0.048251137,-2.3218604e-06,-0.028114231,-0.03136153,0.055536713,0.004163828,0.03323973,0.021053802,-0.054796077,-0.0116359815,0.029791407,0.044820927,-0.013307489,-0.040673845,-0.041811474,-0.034846935,-0.012711331,0.05011306,0.0051862025,-0.00077331497,-0.06027321,-0.0051182033,0.03383331,-0.0058045746,0.04635538,-0.0056882366,0.0071891285,0.008541566,-0.053423043,0.028916242,-0.013215952,0.03164143,-0.009832022,0.013769029,0.0059113284,0.03728859,0.017597489,-0.040323608,0.01789788,0.0038492237,-0.009120241,-0.015676318,0.022185542,0.04141997,-0.00654452,-0.006773273,0.02067064,-0.037788145,0.016265005,0.016049268,-0.02020713,0.021132838,0.06305104,-0.02347693,0.005867307,0.05447745,-0.00781923,-0.04947599,-0.027091783,-0.02321885,0.014809839,0.00891097,-0.06103417,0.03662191,-0.06315048,-0.030775048,0.011854979,-0.082194515,-0.0389448,-0.017146409,-0.06449708,-0.021570116,-0.02839887,0.005625805,-0.025341075,0.017946718,-0.04341122,0.05764956,-0.049871426,0.021486666,0.022638788,0.019087086,0.044218425,0.034815975,0.01351294,-0.026098039,-0.02366459,-0.022826785,0.005108186,0.08980604,0.022076294,0.0050884727,0.06319467,0.004743134,0.05228046,-0.038396217,-0.07139327,-0.037262395,0.00044549207,-0.021958372,-0.0050340937,-0.019838933,0.03600845,-0.022997674,0.01730516,0.0084711695,0.023459572,0.03336277,0.05394259,0.02247282,-0.018866017,0.043943692,0.0010686073,0.053060196,0.02982677,0.059418518,0.037846748,0.002042321,7.7711346e-05,-0.00088409364,0.0020186529,-0.014006415,-0.000607872,-0.022143885,-0.02520617,0.010154769,0.0011632174,0.015475138,0.015287535,-0.00509625,-0.0035523812,0.03741472,-0.016232617,-0.046798177,0.00014078991,-0.03356524,-0.02648413,-0.022762952,0.046974797,0.027435381,0.009697991,-0.04162147,0.014333978,-0.11123327,-0.037747722,-0.0074113687,0.015546745,-0.022371152,-0.032913126,-0.02504701,-0.01511381,-0.033678994,-0.028720386,0.06421357,0.020553326,-0.041553907,-0.014900637,-0.03751425,0.0037107058,0.015451872,-0.0046265903,-0.030023845,-0.019298034,-0.02540815,-0.023868801,0.037917305,-0.03887138,-0.0130184395,0.016176576,-0.03157541,-0.089999095,0.014288831,0.02163147,0.060683597,-0.003704605,0.045464862,-0.013699388,0.008868091,-0.031189358,0.033542186,-0.037421633,0.03375965,-0.005802442,0.016464418,0.03288135,-0.011885109,-0.046381645,0.010899316,0.0011706925,-0.008408886,0.017234601,0.03773311,0.032241013,0.070064,0.043311138,-0.036135785,-0.0788454,-0.0025020877,-0.02380882,0.021942358,-0.06498438,0.003098168,-0.0015165245,0.003441023,-0.019824233,0.00465256,0.026074199,-0.0045606135,-0.007816611,0.003061413,0.012989414,-0.032313384,0.015671335,-0.001757362,-0.00030793206,0.027974123,0.07307178,0.028300544,-0.022800338,-0.053937398,-0.05116327,-0.078771226,0.011294625,0.04038429,0.024187986,-0.049833175,0.0037762197,-0.007587388,0.03321779,0.023765618,-0.026510667,-0.0014083132,0.011196365,0.04841294,0.000317895,0.027883451,-0.006718494,-0.09115121,0.050491486,-0.0072930176,-0.048799686,0.06862103,-0.011205525,-0.036868006,-0.012682656,-0.0041242964,0.0077101886,0.03415055,-0.025691157,0.015569581,-0.027182508,-0.01678505,0.005056676,-0.03453938,-0.010476172,-0.014531079,0.0024395583,-0.018712228,0.02338905,0.020895716,-0.0332961,-0.023922306,0.0060659302,-0.03331595,0.014151131,-0.018259158,0.061721087,-0.020951224,0.057534266,-0.020861115,0.008062099,0.063983336,0.0107155135,0.011672666,0.030690968,0.0026587513,0.012903676,-0.020781258,-0.007167221,-0.05714398,-0.035494324,0.0043419865,0.012821863,0.028777553,-0.0026412166,-0.023257684,-0.032125294,-0.045956668,0.0056510447,0.0284697,0.019167345,0.021757703,0.021108905,-0.027178356,0.021301005,-0.057744652,0.041258413,0.0008592038,0.0023425214,-0.034630246,0.03829342]	Keywords: Star-Transformer, Longformer, band attention, global attention\nKey Objects: attention patterns, star-shaped graph, global nodes\nRefers to Images: ./images/a-survey-to-transformers/image_5.png\nHypothetical Questions:\n- How does the star-shaped graph in Star-Transformer improve attention?\n- What is the purpose of using global nodes in Longformer?\n- How do these compound attention patterns build upon the atomic sparse attention techniques?\n---\nSummary:\nStar-Transformer combines band attention and a global node, creating a star-shaped graph, while Longformer uses band attention with internal global nodes.\nOriginal Text:\nFig. 5. Some representative compound sparse attention patterns. The red boxes indicate sequence boundaries.  \n  \nStar-Transformer [43] uses a combination of band attention and global attention. Specifically, Star-Transformer just includes only a global node and a band attention with the width of 3, in which any pair of non-adjacent nodes are connected through a shared global node and adjacent nodes are connected directly with each other. This kind of sparse pattern forms a star-shaped graph among nodes. Longformer [10] uses a combination of band attention and internal global-node attention. The global nodes are chosen to be [CLS] token for classification and all question tokens\nContextualized Text:\nCompound sparse attention patterns often combine multiple atomic patterns. For example, Star-Transformer uses a combination of band attention and a single global node to form a star-shaped graph where non-adjacent nodes are connected through a global node. Longformer, on the other hand, combines band attention with internal global nodes to enhance performance.	{"tags": ["attention", "transformers", "architecture"], "doc_id": "4bdb059e-f562-46e3-a99d-8fa0d4f6ecc9", "summary": "Star-Transformer combines band attention and a global node, creating a star-shaped graph, while Longformer uses band attention with internal global nodes.", "doc_type": "text", "entities": ["Star-Transformer", "Longformer"], "keywords": ["Star-Transformer", "Longformer", "band attention", "global attention"], "key_objects": ["attention patterns", "star-shaped graph", "global nodes"], "contextual_text": "Compound sparse attention patterns often combine multiple atomic patterns. For example, Star-Transformer uses a combination of band attention and a single global node to form a star-shaped graph where non-adjacent nodes are connected through a global node. Longformer, on the other hand, combines band attention with internal global nodes to enhance performance.", "mentioned_images": ["./images/a-survey-to-transformers/image_5.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does the star-shaped graph in Star-Transformer improve attention?", "What is the purpose of using global nodes in Longformer?", "How do these compound attention patterns build upon the atomic sparse attention techniques?"]}
a6b71423-e314-4dbd-898e-e7aca11da3fb	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.04208598,0.016995234,0.05134389,0.13672134,-0.030494198,0.035748363,-0.029532664,-0.002289801,0.029922739,0.02566873,-0.024162978,0.006203302,0.033898603,-0.005541584,0.0299331,0.03276574,0.050918516,0.08559087,-0.0014183883,-0.02890217,0.04837548,0.0077687884,0.044051867,-0.025633806,0.05172174,0.046767402,0.010693219,-0.0059452467,-0.028094959,0.0025828877,0.015186889,-0.03641225,0.016885865,0.0016549286,0.027730992,-0.0146520585,0.027592717,0.009645064,0.0006400252,0.023515986,0.023477614,0.058657248,-0.055904098,-0.0005968479,-0.012182228,-0.00011135991,-0.1018579,0.0013178929,0.021760676,-0.017418485,-0.021505,0.013205197,0.005323837,-0.037522063,-0.09597435,-0.017506883,-0.036806013,-0.05958475,-0.02884647,-0.05753903,-0.03999084,-0.046352405,-0.025581408,-0.0022575487,0.033648472,0.014144256,0.01560187,0.018708967,0.02934588,0.1448327,-0.035008505,0.029705243,-0.03232425,-0.009701622,0.09618202,0.027515177,-0.065600745,-0.027683502,-0.079923235,0.0021291233,-0.012520731,0.058586013,-0.047606546,0.014244601,0.096760415,0.0042613912,-0.037163027,-0.007956533,0.04651121,-0.03503221,0.021268683,0.0005343921,0.0417504,0.036549322,-0.039894357,-0.037180815,-0.025253996,-0.059388854,0.016783219,-0.019492332,0.018861517,0.021729756,0.11226721,0.08776054,0.05525171,-0.0013583855,-0.0059315246,0.016650736,-0.029327184,-0.04823353,-0.053348616,0.025710955,-0.025488859,0.0466714,0.0026263902,0.016282534,-0.022197023,-0.004352127,0.0110034235,0.019304406,-0.022991145,0.0064905854,-0.01869068,0.034390792,-0.024150727,0.04281594,-0.027422005,-0.05662895,-0.013122866,-0.035419162,0.008017521,0.051265918,0.013244935,0.07701533,-0.02441962,-0.022551926,0.07883158,0.004014492,0.0024379585,-0.003414133,0.026770504,-0.014672469,-0.061980356,0.03399578,-0.015886465,0.023412196,-0.031179657,0.0069518792,-0.026133625,0.05609151,0.051347323,0.0112590175,0.066672266,-0.04763251,-0.0026815091,0.007041295,-0.018983725,-0.04258348,-0.023961965,0.0014115764,-0.05976114,-0.07745775,0.013165545,0.04242687,0.064498514,0.06994246,-0.0068210373,-0.019563917,-0.016789926,-0.019779602,-0.07899632,-0.010074452,-0.030476855,0.00030097974,0.02867211,-0.010167485,-0.0078110923,-0.010236897,0.005273296,0.018546905,0.0032174985,0.041359104,0.0523168,0.024726441,-0.026303845,0.0920676,0.027680183,0.058491647,0.035127006,-0.037399918,0.03862819,-0.014460228,-0.032730553,0.03308971,0.014263402,0.054126225,0.041037384,-0.05854267,-0.028194945,-0.007899977,0.012009317,-0.03585845,0.023221873,0.010560754,0.0026357607,-0.024808982,0.023066316,-0.0071015498,-0.020820143,-0.0030498158,-0.0067388183,-0.016696773,-0.03815794,0.014583381,0.029085476,-0.022755027,0.01864429,0.033742625,0.03178916,-0.029887289,-0.049543813,-0.0007917779,-0.0074687717,-0.04058997,-0.06752503,-0.009096278,-0.0152694965,0.016757155,-0.02315398,-0.021347553,0.02517425,0.027715856,-0.04829104,0.024718901,-0.023247017,0.025982924,-0.024267633,0.03411389,0.017636474,0.026211694,0.0027517653,0.0016443091,0.012012759,0.04768616,0.0033032447,-0.017695364,0.040061314,0.05046993,-0.004468937,0.05246463,-0.016847836,0.027175749,0.02863469,0.0036953085,0.030890351,0.0040655555,-0.03936812,0.005434454,0.02867411,-0.036429793,-0.010435641,-0.04855204,0.04145387,0.035999157,0.043315355,-0.004966818,0.030027127,0.022235116,-0.049245138,0.033450514,-0.02937964,0.04952917,0.02102639,-0.046162073,0.009104213,0.016144559,-0.019316193,-0.00080841256,-0.010090845,-0.012612005,0.0053421236,0.013750249,0.005373149,-0.004035318,0.020130405,-0.0015476086,0.0056733317,-0.0354374,-0.056651518,-0.022871686,-0.0055501815,0.029410368,0.0049842135,0.010138847,0.0038550158,-0.010934351,-0.0018696028,-0.018758802,-0.031693492,0.026973223,0.082330495,-0.018387457,0.018079216,0.027775161,0.03667711,0.03176368,-0.007766229,0.0043677734,0.048775718,0.060271032,0.014931224,-0.0035904094,-0.0015654095,0.027986389,0.03201675,0.09365775,0.06947768,-0.054678462,0.001525855,0.03630857,0.052637722,0.039278194,-0.07477302,0.061901566,0.05123222,-0.028514225,0.06027406,-0.03411246,-0.031570885,0.08170702,-0.038705286,-0.014443309,0.04621972,0.01949768,-0.028435975,0.03411311,-0.00018086341,0.015559058,0.05930787,0.022717254,0.0040846616,-0.020832906,-0.057279065,-0.032927006,0.025494916,0.03134259,-0.020606028,0.005802497,-0.02968032,-0.0039350353,-0.0015036723,-0.012216925,0.021904217,0.010521077,0.029535376,-0.07329403,-0.007771792,0.028554492,0.023038955,-0.013025264,0.06796918,-0.052512273,0.07005818,-0.02448832,0.0032247293,-0.044958852,-0.05715987,0.034494676,-0.027512556,-0.005208024,-0.06437074,-0.02317411,0.050333004,-0.026336247,-0.011288745,0.068225846,-0.011101109,0.042536553,0.028681124,-0.047878493,-0.008331501,0.015070467,-0.048146438,0.025603343,-0.0051931157,0.013928611,-0.03933374,0.04689974,-0.020949299,0.0367549,-0.00864966,0.04335203,0.022006879,0.024695195,-0.044045787,-0.019667989,0.031975415,0.07315607,0.010785195,-0.042977534,0.038304202,0.04911595,-0.04073049,0.025196627,0.070150964,0.03903391,-0.034659095,0.006626202,0.02682223,0.010110756,0.054591198,0.014204605,0.00910322,-0.024285117,-0.00285875,-0.020713704,0.02739061,0.037755877,-0.021835525,0.008406385,0.018770497,-0.0030821317,0.04775877,0.021889178,0.005652354,-0.014033026,-0.045994427,0.05428444,0.055233717,-0.026706723,0.009825184,-0.07357402,0.034730386,0.07064939,0.025583722,-0.0018097067,-0.05430395,0.07118083,-0.04447921,0.045093156,0.0723493,-0.020700807,0.034893848,0.004931599,0.01947378,-0.06504085,-0.00559231,0.026141647,0.011086125,-0.008998674,0.02450493,-0.0136272395,0.012930428,-0.0029620405,-0.013694862,-0.020406773,0.024872176,-0.032299995,-0.062527604,-0.025177049,-0.007689143,0.026609784,-0.041326657,-0.030861199,0.038856078,-0.030704314,0.017083384,-0.0032242844,-0.0103668105,0.032195054,-0.032815628,0.018953882,-0.018144267,0.053673647,0.030613631,-0.033051908,-0.011548211,-0.009068973,0.026611032,-0.02450572,-0.03206687,-0.030623306,0.024035156,0.032709546,-0.011172259,-0.025804376,0.015324018,-0.019593827,-0.015707819,0.04872318,0.056035217,0.03163032,0.006652402,0.051198285,0.016184937,-0.072876535,-0.010954603,-0.0033434902,-0.010441227,0.023668244,0.029545734,-0.006874796,0.05091532,0.0021992258,-0.057390206,-0.00022143382,0.0028339769,-0.008369133,-0.023935154,0.036606513,0.08108242,-0.0035534068,-0.013964961,0.0015474728,-0.018196717,0.009151744,0.03162802,-0.091467276,0.016191792,0.019853575,-0.025988175,0.031120103,0.006043062,0.014736543,-0.046746496,-0.015865412,0.014893859,0.011192315,0.04501501,-0.050639816,0.0724362,-0.052549433,-0.05009182,0.03765545,-0.038559,-0.050821148,-0.006744367,-0.044760313,-0.011589785,0.007532453,0.002408139,-0.01699039,-0.019286675,-0.0016515728,0.04890293,-0.071471214,0.028876713,0.0030998057,-0.004437243,0.013599934,0.047648232,0.005098266,-0.018377822,0.002997294,0.023492146,0.011802447,0.041372318,0.032923747,0.03360073,0.06003337,-0.005472623,0.0659707,-0.012882252,-0.013517789,0.0041188947,-0.021936905,-0.034118183,-0.05219825,-0.015609825,0.04585913,-0.017755479,0.007965653,-0.018651636,0.0023595067,-0.03261918,0.05210774,0.057122506,0.013865331,0.003141153,-0.05424252,0.012699462,0.0571639,0.016609652,0.0481689,0.041303556,0.01037024,-0.005162,-0.008590666,-0.00879843,-0.02362547,0.015231575,-0.017617209,0.044411324,0.054154534,0.01088332,0.037848476,0.015312337,0.05244816,0.039861523,-0.018131807,-0.07463654,-0.010488207,0.01626427,0.047202233,-0.013230784,0.030138964,-0.017681496,-0.0035507577,-0.023098731,0.09549765,-0.056806423,-0.047289684,-0.012045855,0.025214281,-0.016102288,-0.010970415,-0.068451524,-0.0015974452,-0.067143835,0.0065556257,0.012365443,0.019130029,-0.040321507,-0.018227272,-0.015409458,0.0039945403,0.011731604,0.01903345,-0.073620185,0.015931588,-0.0032266942,-0.015105949,0.039040394,-0.04093876,-0.022083605,0.019722622,-0.05086039,-0.053066142,0.030384462,0.052795187,0.030837798,0.06044368,-0.024574662,0.02254872,0.014809538,-0.07414445,0.02666385,-0.06372263,-0.0027807737,0.006433949,0.022983354,0.03336336,0.004105365,-0.07327604,-0.002652522,-0.008873741,-0.004636256,-0.008144882,-0.023285577,0.025983555,0.031305645,0.015272593,-0.055074655,-0.07941295,-0.011544331,-0.022005083,0.032033842,-0.087841935,0.088821165,-0.0035162768,-0.004242378,-0.04149758,-0.011472889,-0.008878206,-0.03951877,-0.0028179006,0.0040296195,0.014929951,0.00985801,0.03513071,0.041462317,-0.009260684,0.025284495,0.036620706,0.003178627,-0.028082209,-0.062357385,0.010194904,-0.06538023,0.06304081,0.0508888,-0.005561769,-0.054314874,0.038788676,-0.046895348,0.027113339,0.010982469,-0.0071559367,-0.014883568,-0.011709786,0.03334214,0.007148532,0.012530475,-0.02403689,-0.063392855,0.09999879,-0.0045282487,0.008150016,0.04421273,0.02058473,-0.012350736,-0.0016056701,-0.0054381043,-0.0018921399,0.057432983,-0.024743678,0.020339444,-0.022328187,-0.03472086,0.013146267,-0.019854639,0.022433177,-0.041386176,0.0068054814,0.009822309,-0.006725645,0.03557379,0.0031571593,-0.036887646,-0.0060918233,-0.055292208,-0.0020638863,-0.011087649,0.03858261,-0.0039329883,0.062034976,-0.020287694,0.00236946,0.047340102,0.025099957,-0.0125904335,-0.029932262,-0.00059778616,0.012446151,0.02397755,-0.02136093,-0.061564893,-0.0053686784,-0.028135806,-0.0028480305,0.057848614,-0.040391978,0.03664813,-0.045546543,-0.030600697,0.021301648,0.013692859,0.018818125,0.03147625,0.011426462,-0.039950427,0.017889405,-0.05809679,0.021665493,-0.014865232,-0.034818817,0.018960355,-0.019453054]	Image title: Star-Transformer Architecture\nTags: transformer, neural-network, architecture, connectivity, star-network\nKey objects: Layer 1, Layer 2, Layer 3, Layer 4, Layer 5, Layer 6, Layer 7, Layer 8, Connections\n---\nSummary:\nThis diagram illustrates the architecture of a Star-Transformer model. It depicts a unique connectivity pattern where each layer connects directly to every other layer, forming a star-like structure. The diagram highlights the layers as rectangular blocks, showcasing this full connectivity and the associated data flow.\nFull description:\nThe diagram showcases a Star-Transformer model's architecture. Each rectangular block represents a 'Layer,' labeled from 1 to 8. The key characteristic is the connections between these layers. Each layer is directly connected to every other layer. For example, 'Layer 1' is connected to 'Layer 2', 'Layer 3', 'Layer 4', 'Layer 5', 'Layer 6', 'Layer 7', and 'Layer 8'. This pattern repeats for all other layers, forming a star-like connectivity where each layer acts as a central node with links to every other layer. The diagram emphasizes this unique interconnection method, showing how information flows directly between any two layers in the network.\nText found in image:\n- Star-Transformer\n- Layer 1\n- Layer 2\n- Layer 3\n- Layer 4\n- Layer 5\n- Layer 6\n- Layer 7\n- Layer 8	{"tags": ["transformer", "neural-network", "architecture", "connectivity", "star-network"], "title": "Star-Transformer Architecture", "doc_id": "a6b71423-e314-4dbd-898e-e7aca11da3fb", "source": "./images/a-survey-to-transformers/image_5.png", "summary": "This diagram illustrates the architecture of a Star-Transformer model. It depicts a unique connectivity pattern where each layer connects directly to every other layer, forming a star-like structure. The diagram highlights the layers as rectangular blocks, showcasing this full connectivity and the associated data flow.", "doc_type": "image", "key_objects": ["Layer 1", "Layer 2", "Layer 3", "Layer 4", "Layer 5", "Layer 6", "Layer 7", "Layer 8", "Connections"], "parent_doc_id": "4bdb059e-f562-46e3-a99d-8fa0d4f6ecc9", "text_in_image": ["Star-Transformer", "Layer 1", "Layer 2", "Layer 3", "Layer 4", "Layer 5", "Layer 6", "Layer 7", "Layer 8"], "contextual_description": "The diagram showcases a Star-Transformer model's architecture. Each rectangular block represents a 'Layer,' labeled from 1 to 8. The key characteristic is the connections between these layers. Each layer is directly connected to every other layer. For example, 'Layer 1' is connected to 'Layer 2', 'Layer 3', 'Layer 4', 'Layer 5', 'Layer 6', 'Layer 7', and 'Layer 8'. This pattern repeats for all other layers, forming a star-like connectivity where each layer acts as a central node with links to every other layer. The diagram emphasizes this unique interconnection method, showing how information flows directly between any two layers in the network."}
b483f80a-ecfc-4102-9425-3e712981232e	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0068948064,0.018722065,0.03830222,0.05590207,-0.016472012,0.028873608,-0.022655727,0.03193301,0.007893728,-0.0008930355,0.036129482,0.014092977,-0.0039755693,0.014525225,-0.036115654,0.058316298,0.028556189,0.036940027,-0.009362605,-0.036918435,0.057351884,0.0029152744,-0.014844704,-0.00088009133,0.010546269,0.067348406,0.0010399744,0.0042258385,0.0004390947,0.0019924853,0.040464867,-0.036194906,0.04631962,-0.030362036,0.036219593,-0.014967781,0.029162144,-0.06462909,0.005498783,-0.0095374705,0.01884258,0.045055218,-0.031528626,-0.030998847,-0.020539222,-0.018259812,-0.087037474,-0.052718308,-0.019190183,0.019300237,-0.004599862,-0.0030854368,-0.0013310594,-0.01932265,0.005877369,-0.024228795,0.047641665,-0.026091363,0.012137854,-0.09422418,-0.018613407,0.0422772,-0.0039845994,0.005229111,0.00295877,-0.012589595,0.012927356,0.0021964223,0.012041157,0.13825455,-0.019315641,0.03492169,-0.00057406456,-0.027491342,0.09589234,0.044140354,-0.039501697,0.0055860206,-0.07829454,-0.014629418,0.024039738,0.042978685,-0.05081813,-0.035100583,0.087107725,-0.028181635,-0.039714634,0.009467336,0.048276,-0.060298786,-0.029560756,-0.011412734,0.008857342,0.08178801,-0.04082304,-0.08501887,-0.0116102435,-0.043913033,0.03575799,-0.055893607,0.012896306,0.011850517,0.09061086,0.12726206,0.026052851,-0.012525697,-0.020544974,-0.020993888,-0.019839369,0.010454178,-0.0042257067,0.016464802,-0.06788528,0.0014568472,-0.023226643,-0.0049445326,-0.038807217,-0.0027665305,-0.006605623,-0.018177072,0.034596484,-0.013306819,-0.022077635,0.008085972,0.05046119,0.048168235,0.036968973,-0.016257003,-0.029016238,0.006985393,0.065846264,-0.0018759647,-0.03878988,0.045716062,-0.027536679,0.024163507,0.059722714,-0.0050610676,-0.0076052216,0.017188588,-0.00238553,-0.01170322,0.006445182,0.030076005,-0.0094875,0.032715358,-0.052276153,0.04637254,-0.021214016,0.041698206,0.05234777,0.00040174345,0.03826028,-0.027447812,-0.009118576,0.0011087728,-0.02629183,-0.026798224,0.003132849,-0.044425398,-0.026959741,-0.032947473,-0.012265788,0.06226341,0.03584468,0.08186296,0.005755771,-0.00925211,-0.0025095192,0.039003108,-0.04310355,0.001491098,0.013405298,-0.05145129,0.029370442,-0.019152265,0.032597296,-0.043297116,-0.008305131,-0.00050471607,-0.02742446,0.045548353,-0.008306009,0.042288393,-0.038909864,0.029161282,-0.033598706,0.062043656,-0.067179516,-0.03365004,0.018947667,-0.037307408,0.0047270437,0.007295476,0.025312249,0.072162494,0.031259943,-0.07024185,-0.006819681,-0.046071567,0.003746762,-0.009803371,-0.034832895,-0.01514526,0.00267548,-0.007009613,0.014195821,-0.059188634,-0.0012783034,0.024015773,-0.00985766,0.022212802,-0.021170799,0.05252367,0.07938302,0.0079367785,-0.03955285,0.008428098,0.03859828,-0.016415507,-0.057925932,0.011452884,-0.009729477,0.05097005,-0.009914561,0.021763172,-0.02187583,0.027999828,-0.00035110355,-0.032246776,0.024948915,0.00628011,-0.046546195,0.031892523,-0.027901765,0.026342731,0.021494973,0.006731137,-0.036231708,0.04340812,0.08518276,0.041181535,-0.01342416,0.06505194,-0.002079189,0.014604286,0.02937041,0.05466805,0.03159876,0.036907844,-0.0285448,0.056767903,0.039878435,-0.027791347,-0.010698032,-0.04958109,0.01723911,0.0055172504,0.03654505,-0.0012125431,0.025494078,-0.016826248,-0.014769707,0.017841185,0.0044211326,-0.022597153,0.050607514,0.03846001,-0.051070403,-0.007557587,0.0631763,0.020587943,0.0093904445,-0.04641633,-0.06586653,0.031417802,0.0046976474,0.0009975777,0.011235907,-0.015410505,-0.008903566,-0.014479198,0.023210993,-0.013530726,0.042155955,-0.0030082702,0.0038078285,0.005072103,-0.027296728,-0.0028901517,-0.021578332,-0.036054198,0.016124249,-0.006155524,-0.004409163,0.016253978,-0.0021626055,-0.050980024,0.007843736,0.0069359825,0.069961496,-0.032483906,-0.0022794534,0.060116358,0.039564162,0.040541224,-0.01990712,0.013926573,0.038587335,0.033348616,0.0012269171,-0.020839108,-0.012794347,0.03887399,-0.001029725,0.05823257,0.07335896,-0.0038586888,-0.01107516,-0.006240749,0.013721416,0.0034476167,0.0037019711,0.032879673,0.049112026,-0.02851439,0.0039491905,-0.019624148,-0.08699513,0.066317536,-0.07204711,0.040792573,0.04208462,-0.009026295,0.004438787,0.000972381,0.02942405,0.04210634,0.0344237,0.02051201,-0.013732293,-0.050279234,0.032052867,-0.013004337,-0.044633385,0.020345611,0.0038792952,0.03283564,0.017234676,-0.022843495,0.031028163,0.012283822,-0.019513896,-0.0055052377,0.045281105,-0.06662956,0.016448457,0.043764748,-0.016575739,0.0032565347,0.070306525,-0.083670646,0.07815073,-0.037609797,0.03258056,-0.057399124,-0.058761097,0.025628908,0.057845138,-0.010616528,-0.009417295,-0.014947636,0.045273364,-0.040577576,-0.022170544,0.051239092,0.07217105,0.025649482,-0.027203096,0.008574441,-0.035947297,-0.032973118,-0.037883572,-0.036226593,0.026841175,0.00604669,0.0032593922,0.09400704,-0.025894491,0.026955888,-0.065815546,0.04246762,0.012991683,0.01931213,-0.056610826,0.011400516,0.04151084,0.06289523,0.01450909,0.014420547,0.02078438,0.036422618,-0.06124939,0.015303866,0.06699432,0.022265363,-0.08046891,0.0356918,0.058413345,0.03922457,0.014131741,-0.03614283,-0.012060965,-0.0052365977,0.013166231,-0.028231015,-0.0023812735,0.038435988,-0.024200004,-0.049573496,-0.0214217,0.04914138,0.0015737058,0.05775458,0.04913989,-0.032008488,-0.056856617,0.035876893,-0.011753319,-0.071493275,0.039677043,-0.04921088,-0.024108611,0.031607457,0.0046203188,0.0061505404,-0.029461699,0.07131771,-0.04383144,0.021685986,0.07319504,-0.034474023,-0.004290155,-0.008800508,-0.020335898,-0.038750432,0.017992066,0.022024529,-0.039762318,-0.0014274267,0.039569564,0.008986767,0.01556252,-0.022824561,0.0479952,-0.008510097,-0.030249504,-0.040878605,-0.06687296,0.0074770106,0.020707551,0.021451924,-0.014000158,-0.0055018696,0.011989986,-0.043892853,-0.031158632,-0.04590814,0.0004926031,-0.039318442,0.020433454,0.028241998,-0.011325764,0.065072164,-0.024384202,-0.0335188,-0.0137418285,0.037127197,0.015416578,-0.018592976,-0.054554854,-0.025776345,0.009460221,0.016697917,0.021431174,0.016986098,-0.024916984,-0.061020575,-0.001531417,0.044454485,0.005865833,0.05624549,0.01962422,0.009624917,0.03094256,-0.047811516,0.0068675904,0.031078817,-0.01807097,-0.0143724885,0.0011048019,-0.0041448483,0.06460687,0.0077294493,-0.026866168,0.051365823,-0.024570804,0.005979586,-0.03229288,0.032129515,0.029745826,-0.001961751,0.026850272,0.02187622,-0.020346534,0.023076799,0.0075132228,-0.02096182,0.027760409,0.042693477,0.0044966126,-0.009632784,0.034703214,0.011970268,-0.010633565,-0.0033202881,0.020869048,0.038118776,0.020467423,-0.10915258,-0.002634779,-0.05596286,0.0026187866,0.010633384,-0.07141851,-0.05697561,-0.020927092,-0.042409513,0.0008078753,-0.018672703,-0.009779424,-0.054185424,0.011738098,-0.025190582,0.062110215,-0.05562421,0.0274468,0.008845391,0.02299341,0.08448838,0.021767898,-0.017257016,-0.012080105,0.0134537425,-0.020562517,0.027920105,0.07030236,0.03711672,-0.018549733,0.076048225,0.009711209,0.0017589699,-0.05855351,-0.053816155,-0.05516023,-0.0083763,0.005506356,-0.005297376,-0.022123167,0.019394437,0.00065859326,-0.0049965754,0.00412634,0.011396171,0.0077259466,0.051931284,0.05286473,0.0020440868,0.020849643,-0.012456901,0.044273593,0.010944158,0.03365093,0.034857046,0.027906213,0.0062361695,-0.005278675,-0.008435535,0.00071762153,-0.027780378,-0.029290302,-0.039569438,-0.0031462018,-0.0026266226,0.02546658,-0.048915427,0.001378393,-0.0024422433,0.009864837,-0.045879565,-0.071245745,-0.009059216,-0.036333743,-0.0382749,0.016305551,-0.0076267472,0.017417602,0.03824906,-0.021992557,0.046733145,-0.11628116,-0.05411893,-0.02581515,0.036333613,-0.035940863,-0.006078033,-0.024157217,-0.022296559,-0.009386336,-0.028434152,0.074921496,0.013177313,-0.013906785,0.020416193,0.011931345,0.005589232,0.006594417,-0.009889081,-0.021103192,0.004647208,-0.037525937,-0.031070268,0.07395398,-0.04260666,-0.026089802,0.016501935,0.020270268,-0.10287621,-0.028200882,0.006974456,0.041307457,-0.013728656,0.053727016,-0.018580578,0.024759246,-0.035428897,-0.015127515,-0.027254758,0.0031465457,0.033641286,0.027466163,0.039880726,-0.038514502,-0.052865323,-0.018754547,0.022822134,-0.021072358,0.05629038,0.03156339,0.05958648,0.07742898,0.059602857,-0.031686332,-0.10069702,-0.03886733,-0.008288394,0.016148353,-0.0436817,0.027078507,-0.004006406,-0.038710576,-0.011736373,-0.0008795261,0.0010667664,0.014844126,0.0323658,-0.016711015,-0.023925984,-0.031343386,-0.0076369066,0.034122713,0.0033428879,-0.010039527,0.07748328,0.02695614,-0.052201677,-0.051149312,-0.045587704,-0.023781333,-0.02317707,0.0120665515,0.0063162837,0.03196718,-0.018391037,0.024943735,0.054070268,0.012580795,-0.048098262,-0.015755966,0.03935175,0.0424523,0.04452282,0.0118824635,-0.012458017,-0.11350683,0.016750459,-0.049485583,-0.07441979,0.031114386,-0.02226286,-0.006635069,-0.046986483,-0.012173202,0.002988517,-0.016974712,-0.009646798,0.013275592,-0.01898225,-0.022318155,0.010371125,-0.082730204,-0.017555702,-0.022308387,0.0021003934,-0.0069434703,0.047982592,0.02096068,-0.014608304,-0.012544479,0.0052817254,-0.01650293,0.02783877,-0.046684068,0.052196015,-0.01727112,0.0056828368,-0.0046151006,0.00600236,0.06507752,-0.022398135,-0.022857873,-0.010269838,0.020141296,-0.004470574,-0.0006035079,-0.0348291,-0.037227545,-0.055920746,0.007252192,0.020750862,0.0007053146,0.029461183,-0.015765034,-0.045350783,-0.054264233,-0.047551475,-0.014839829,0.049432103,0.024136972,0.019560086,-0.069068424,0.021744614,-0.030162152,0.042286236,0.026397752,0.0024107036,-0.024478327,-2.3967428e-05]	Keywords: band attention, global attention, dilated window attention, sparse attention, Longformer, Extended Transformer Construction, BigBird\nKey Objects: band attention, global attention, sparse attention models, attention heads\nRefers to Images: None\nHypothetical Questions:\n- Why would incorporating a masking mechanism benefit structured inputs?\n- What is the significance of being able to simulate a Turing Machine with sparse attention?\n- How does dilated window attention contribute to expanding the receptive field?\n---\nSummary:\nLongformer and Extended Transformer Construction (ETC) combine band attention with global nodes, while also incorporating dilated window attention and external global-node attention for increased receptive field and pre-training capabilities.\nOriginal Text:\nfor Question Answering tasks. They also replace some of the band attention heads in upper layers with dilated window attention to increase the receptive field without increasing computation. As a concurrent work to Longformer [10], Extended Transformer Construction (ETC) [1] utilizes combination of band attention and external global-node attention. ETC also includes a masking mechanism to handle structured inputs and adapt Contrastive Predictive Coding (CPC) [135] for pre-training. In addition to the band and global attention, BigBird [163] uses additional random attention to approximate full attention. Their theoretical analysis also reveals that the usage of a sparse encoder and sparse decoder can simulate any Turing Machine, which explains the success of those sparse attention models.\nContextualized Text:\nTo enhance performance, Longformer and Extended Transformer Construction (ETC) combine band attention with global nodes. ETC utilizes external global nodes, incorporates a masking mechanism for structured inputs, and adapts Contrastive Predictive Coding (CPC) for pre-training. Furthermore, dilated window attention is used to expand the receptive field. BigBird utilizes random attention to approximate full attention, and its theoretical analysis suggests that sparse attention models can simulate Turing Machines.	{"tags": ["transformers", "attention mechanisms", "sparse attention", "architecture"], "doc_id": "b483f80a-ecfc-4102-9425-3e712981232e", "summary": "Longformer and Extended Transformer Construction (ETC) combine band attention with global nodes, while also incorporating dilated window attention and external global-node attention for increased receptive field and pre-training capabilities.", "doc_type": "text", "entities": ["Longformer", "Extended Transformer Construction", "BigBird", "CPC", "CLS"], "keywords": ["band attention", "global attention", "dilated window attention", "sparse attention", "Longformer", "Extended Transformer Construction", "BigBird"], "key_objects": ["band attention", "global attention", "sparse attention models", "attention heads"], "contextual_text": "To enhance performance, Longformer and Extended Transformer Construction (ETC) combine band attention with global nodes. ETC utilizes external global nodes, incorporates a masking mechanism for structured inputs, and adapts Contrastive Predictive Coding (CPC) for pre-training. Furthermore, dilated window attention is used to expand the receptive field. BigBird utilizes random attention to approximate full attention, and its theoretical analysis suggests that sparse attention models can simulate Turing Machines.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["Why would incorporating a masking mechanism benefit structured inputs?", "What is the significance of being able to simulate a Turing Machine with sparse attention?", "How does dilated window attention contribute to expanding the receptive field?"]}
98644099-6881-4ec9-9d73-290838b34752	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01835379,0.0189048,0.007108045,0.05085333,-0.01505593,0.016267393,-0.006351606,0.057335503,0.03386211,-0.008628664,-0.009108932,-0.007990023,-0.016947005,-0.014986264,-0.000761933,0.021875948,0.0473881,0.06571626,-0.0087078335,-0.029400768,0.037628617,0.0066520115,0.00863684,0.02036575,-0.02454686,0.07008223,-0.007401883,0.010259421,0.030284595,0.010244212,0.0219656,-0.06284076,0.034731075,-0.001611967,0.050106894,0.012124417,0.036862206,-0.113588154,0.0032490264,-0.009011474,-0.0116704395,0.034743484,-0.047952905,-0.016314387,0.017919678,-0.04311462,-0.08869635,-0.0273457,0.0027923614,0.0033595043,-0.011580768,0.035110556,-0.02253483,-0.0026087821,-0.038721252,-0.010048775,0.0028735914,-0.034089368,0.025821736,-0.106897555,-0.012104372,0.016855683,-0.029391954,0.006546029,0.0048367023,0.003214716,0.0009801307,0.030795926,0.057677034,0.12530704,-0.00240489,0.005358735,-0.048712958,-0.071476296,0.10261005,0.0563139,-0.020490196,-0.0018622148,-0.054881226,-0.03347049,0.008911388,0.031070428,-0.037231598,-0.03977209,0.08300239,-0.02877046,-0.019256786,0.01345362,0.062206656,-0.027184453,-0.014874782,-0.022422558,-0.0016404628,0.06379747,-0.05533459,-0.031781152,0.0021419153,-0.04799185,0.026201373,-0.0799526,0.012471599,0.014382116,0.0898532,0.101509996,0.03210747,-0.041442275,-0.0009847039,-0.042097937,0.002307698,-0.0056257593,-0.021150328,0.04022276,-0.04484008,0.016928663,-0.05007611,0.013783256,-0.04147105,0.01407423,-0.014682298,0.001919705,0.017959222,-0.036251705,-0.004431127,0.034975104,0.05038451,0.037010364,0.05476631,-0.02965991,-0.038135603,0.026502302,0.051007226,0.000121324025,-0.0065323883,0.029822327,-0.05794622,0.0161829,0.09914736,0.0034303365,-0.0074295653,0.014465713,0.0034476123,-0.043790862,-0.048366528,0.022115113,0.009202401,0.021299284,-0.06040143,0.028535634,-0.011791577,0.064173676,0.025415523,0.009599055,0.064247645,-0.020842446,-0.015759276,-0.019541647,-0.0027635866,-0.008236148,-0.005389213,-0.04939508,-0.065478966,-0.024085503,0.0013871201,0.058240008,0.031990007,0.096971415,0.027201748,0.008162103,-0.012733099,0.022010876,-0.055019423,0.0050884886,0.0022711526,-0.022642763,0.010693357,0.028015299,0.027036928,-0.039543677,-0.016897377,-0.011932894,0.02628754,0.0819499,-0.026899664,-0.0028157192,-0.01782182,0.008631993,-0.008527257,0.062474158,-0.00024407827,-0.018753765,-0.0032495288,-0.06402116,0.0022708476,-0.014069931,0.051127683,0.06574182,0.025224049,-0.05539889,-0.022563249,-0.034555014,0.0023731464,-0.019629166,-0.024715258,-0.023592789,0.015930133,-0.029988473,0.030836476,-0.047594037,-0.009167934,0.03662758,0.003483773,0.011828958,-0.02951756,0.05045603,0.03152503,0.017517772,-0.022014432,0.02741037,0.021428786,-0.021382978,-0.031075519,-0.0017812024,-0.02272651,0.05877184,-0.011174986,0.030032115,-0.006318004,0.02186547,0.0019187,-0.0026923928,0.029894834,0.034509663,-0.04916755,0.019321712,-0.041578814,0.01869192,0.014860679,0.0180929,-0.0036937904,0.04238453,0.053908892,0.08598257,-0.003347532,0.07674757,-0.010622829,0.036702532,-0.0056099147,0.04774754,0.03752788,0.02799858,-0.016101735,0.040687505,0.067244574,-0.019088231,-0.0013103428,-0.021759277,0.0002884331,0.031421255,0.013037284,-0.018141523,0.019178849,-0.006752043,-0.0012705987,0.033709686,-0.008998416,-0.0019294338,0.018080998,0.035889044,-0.050835457,-0.007551082,0.027956296,0.038353592,0.027383804,-0.044576906,-0.043156568,0.04657473,0.03912362,-0.0072238063,0.029775357,-0.024863917,-0.009409922,0.008505852,0.024167242,0.008777426,0.040821623,0.0038489422,0.01032721,-0.0030261502,-0.026872287,0.0046882443,-0.023609143,-0.031448163,0.039386272,-0.007901754,-0.007504603,-0.010465635,0.0045132106,-0.04397222,-0.0030922838,0.018747162,0.10343481,-0.014069755,0.009957486,0.015046285,0.013684395,0.0033717705,-0.0308191,0.0372446,0.034902923,0.0015234973,-0.0052640582,-0.023433222,0.0009995052,0.010447338,0.0066968645,0.098431066,0.10344383,0.01054894,-0.014895539,0.020394364,0.024167506,-0.0062550684,-0.019288177,0.020409971,0.06323601,0.007528876,0.030136863,-0.03530784,-0.08448531,0.07825776,-0.07535387,0.034956396,0.060451537,-0.008139936,0.01793535,0.012074125,-0.01813539,0.0325182,0.026584085,-0.0163592,-0.0222891,-0.02194896,0.012326016,0.0019137987,-0.035386566,0.04851351,0.01928794,-0.01565924,-0.021774366,-0.00021441856,0.008343461,0.052798357,0.012577129,0.050511587,0.031339426,-0.026428314,-0.020676894,0.010906803,0.0018847183,-0.034789898,0.08383396,-0.098138556,0.06955007,-0.031126957,0.009666189,-0.007741052,-0.0750513,0.0409656,0.06693576,-0.01258171,0.0022761035,-0.022987906,0.04088851,-0.027672637,-0.017214088,0.018053098,0.04084132,0.043581814,-0.018666245,-0.03954219,-0.0020747434,-0.001759369,-0.026772752,-0.03720659,0.01216073,0.009861423,-0.0041237813,0.079070374,-0.045532893,0.05746341,-0.024459809,0.02228016,0.01036096,0.021449845,-0.01503592,-0.0005530221,0.010706288,0.04555546,0.021676254,0.0004897387,-0.004808754,0.026806014,-0.05294119,0.025653278,0.015517446,-0.0015950409,-0.07386476,0.009948326,0.014870618,0.016748574,0.023601895,-0.047700617,-0.009511652,0.0018594353,0.053922784,-0.020047415,-0.0050542965,0.016160365,-0.06418315,-0.015873028,-0.033386435,0.033458244,0.035127774,0.03561237,0.028174596,-0.043248106,-0.027355254,0.022998631,-0.013500501,-0.054293342,0.008250276,-0.013372264,-0.015347875,0.024025166,0.015173865,0.021079563,-0.05908637,0.1161655,-0.0109431585,0.021436585,0.0947349,-0.046939116,0.016128335,0.0067293798,0.014964878,-0.04927321,0.0072851684,0.02985954,-0.018177891,-0.016786959,0.043055087,0.03390298,0.011408723,-0.037939586,0.033795316,-0.002661939,-0.012869765,-0.058203515,-0.025084727,0.01846581,0.0070436406,0.029538678,0.006725148,-0.028813353,-0.0031182906,-0.01606522,-0.020555,-0.019753095,-0.021219902,-0.03377397,-0.0030562147,0.020910844,-0.0034839208,0.062253222,-0.0123569,-0.013039182,-0.008612755,0.056897923,0.06717862,0.014261001,-0.072408535,-0.027020166,0.0110387225,0.016334182,0.011745891,0.01592113,0.005688177,-0.058799703,-0.016066737,0.007905207,0.013771203,0.03567985,-0.014222307,0.019201234,-0.002991432,-0.048348103,0.009380015,-0.0044959397,0.013025224,-0.035827268,-0.030449772,-0.0058911857,0.040698823,0.053416964,-0.010475886,0.022101738,0.02448581,0.0004327611,-0.02230849,0.062435944,0.028370447,-0.00050434924,0.01657066,-0.007054676,-0.044577464,-0.0055222088,-0.0027722842,-0.033829298,0.037268396,0.05405094,0.01603088,0.009693087,0.03957129,0.017976066,-0.014761001,-0.012353324,0.018349908,0.02603384,0.046931367,-0.08607956,0.0035840215,-0.079852015,0.0018918968,0.0024543232,-0.064634696,-0.07743588,0.014985758,-0.06063568,0.021218698,-0.02517338,-0.011629541,-0.04657288,0.01773736,-0.039643247,0.07046695,-0.07032202,0.020511083,0.027006654,-0.006414183,0.07844264,0.024294998,0.016075425,-0.045377154,-0.019813385,-0.016264021,0.023146285,0.0843098,0.009810338,0.0059527955,0.0641124,-0.0036001063,0.02335797,-0.054830097,-0.075888775,-0.03627303,0.006008861,-0.021796435,-0.008671757,-0.029357092,0.0149641,-0.010154173,0.014713191,0.025522862,0.01372765,-0.012704588,0.07460453,0.030733649,-0.028236309,0.032258753,0.014971934,0.046039153,0.027573043,0.039793085,0.03889765,-0.0015499806,0.022646552,-0.007974415,0.0028754037,0.009431002,-0.02405037,-0.036080953,-0.04808455,0.0034538459,-0.006330155,0.0076815668,-0.03836648,-0.013492818,0.033779543,0.039759252,-0.021357633,-0.03752448,-0.03464836,-0.02064151,-0.05092557,0.004575824,-0.00074064184,-0.0011856403,0.06056671,-0.040331136,0.046469707,-0.09165068,-0.05409066,-0.03186995,0.015667805,-0.024361625,-0.0074862954,-0.03554236,-0.018585384,-0.0120687075,-0.043164033,0.058344312,-7.848043e-05,-0.036154673,0.0213198,0.008628888,-0.0047597694,-0.00971252,4.870373e-05,-0.011322222,-0.004745616,-0.0022103826,-0.025276614,0.08754719,-0.051804233,-0.018398058,0.017190829,-0.0076606823,-0.092101984,-0.024923192,0.022150353,0.04146008,0.0029689549,0.033996794,-0.021849643,0.012916002,-0.034232948,0.0025177994,-0.07587434,-0.008889935,-0.011916918,0.04630553,0.05986579,-0.0063548107,-0.047818847,-0.03156863,0.004437537,-0.027471973,0.063756235,0.059846535,0.06206704,0.08704288,0.02857668,-0.044222172,-0.048221074,-0.0031193723,-0.0030276543,0.023058068,-0.037572492,0.028921872,0.002106953,-0.004482666,-0.035507865,-0.027065726,-0.0005120954,-0.03004946,0.0009090586,0.00079932617,-0.02985505,-0.018276807,0.014485137,0.012816465,0.032029346,0.0054459404,0.08489494,0.027464973,-0.052004423,-0.05292213,-0.034323204,-0.04667997,0.016100448,-0.018626118,-0.019649811,-0.0131688155,-0.027108273,0.013333973,0.02553051,0.015529044,-0.040359415,-0.058309693,0.03232825,0.020602675,0.030067539,-0.02991643,-0.0087252855,-0.07012153,0.022255741,-0.023348274,-0.04888168,0.028488364,-0.020293826,-0.028186288,-0.036326014,-0.021689551,0.01702012,0.008195864,-0.028995126,0.0054338686,-0.002014822,-0.03799149,0.022543391,-0.08570521,0.0076823425,-0.0048216856,-0.008283186,-0.0035639752,0.046625,0.022372957,-0.00668865,-0.009011551,-0.007247239,0.029829802,0.020667275,-0.0027689214,0.07368483,-0.053909197,0.0278147,0.0009630593,0.011285524,0.08889137,-0.03403163,0.0016577024,0.007583763,0.032587938,0.005142955,0.04458329,-0.036292505,-0.03733224,-0.068645746,0.01367745,0.014898271,-0.0025745016,0.03617703,-0.030975496,-0.04909693,-0.06470177,-0.027316155,-0.013802218,0.05378041,0.024232632,0.0061025624,-0.055824075,0.011741383,-0.01633121,0.08086138,0.0022102701,-0.002276496,-0.018602943,0.01571153]	Keywords: sparse attention, factorized attention, sparse transformer, band attention, strided attention, block local attention, global attention\nKey Objects: Sparse Transformer, sparse patterns, data, band attention, strided attention, block local attention, global attention\nRefers to Images: None\nHypothetical Questions:\n- How does the choice of sparse patterns influence the performance of Sparse Transformer on different data types?\n- What are the advantages of using factorized attention compared to a standard attention mechanism?\n- Why is it beneficial to use a combination of different sparse attention patterns?\n---\nSummary:\nSparse Transformer employs a factorized attention mechanism, combining different sparse patterns based on the data's characteristics, such as using band and strided attention for periodic data like images and block local attention with global nodes for non-periodic data like text.\nOriginal Text:\nSparse Transformer [17] uses a factorized attention where different sparse patterns are designed for different types of data. For data with a periodic structure (e.g., images), it uses a composition of band attention and strided attention. Whereas for data without a periodic structure (e.g., text), it uses a composition of block local attention combined with global attention, where global nodes are from fixed positions in the input sequence.  \n4.1.1.3 Extended Sparse Attention. Apart from the above patterns, some existing studies have explored extended sparse patterns for specific data types.\nContextualized Text:\nTo adapt to various data types, Sparse Transformer utilizes a factorized attention approach. This involves combining different sparse patterns: for data with periodic structures, like images, it combines band and strided attention; while for non-periodic data, such as text, it utilizes a combination of block local attention and global attention, where global nodes are strategically positioned within the input sequence.	{"tags": ["NLP", "deep-learning", "architecture", "attention", "transformer"], "doc_id": "98644099-6881-4ec9-9d73-290838b34752", "summary": "Sparse Transformer employs a factorized attention mechanism, combining different sparse patterns based on the data's characteristics, such as using band and strided attention for periodic data like images and block local attention with global nodes for non-periodic data like text.", "doc_type": "text", "entities": ["Sparse Transformer"], "keywords": ["sparse attention", "factorized attention", "sparse transformer", "band attention", "strided attention", "block local attention", "global attention"], "key_objects": ["Sparse Transformer", "sparse patterns", "data", "band attention", "strided attention", "block local attention", "global attention"], "contextual_text": "To adapt to various data types, Sparse Transformer utilizes a factorized attention approach. This involves combining different sparse patterns: for data with periodic structures, like images, it combines band and strided attention; while for non-periodic data, such as text, it utilizes a combination of block local attention and global attention, where global nodes are strategically positioned within the input sequence.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does the choice of sparse patterns influence the performance of Sparse Transformer on different data types?", "What are the advantages of using factorized attention compared to a standard attention mechanism?", "Why is it beneficial to use a combination of different sparse attention patterns?"]}
489dc08c-d5e5-4ef0-9126-9fe022faae02	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.021909986,0.0018929648,0.015011728,0.014993102,-0.009723718,0.016403668,-0.031648774,0.06801686,0.013149828,-0.0065394705,-0.008089229,-0.034160763,0.010754311,-0.0052554435,0.0048655723,0.03087305,0.053143356,0.06982391,0.016400913,-0.017669838,0.052535523,0.016268201,-0.005786554,0.009296236,0.020406168,0.047150478,-0.000738587,0.0001818104,0.0044788267,0.0032219044,0.021103654,-0.06844637,0.052968137,0.032565877,0.052381765,-0.045161773,0.019356947,-0.08183607,0.045504063,-0.01936125,-0.025968622,0.0051334635,-0.017330522,-0.017701248,0.050539404,-0.06756027,-0.11829176,-0.03422266,-0.002135157,-0.0109636225,0.000103631006,0.022040121,-0.038918804,-0.01632266,-0.0077665034,-0.0806271,0.0039702132,-0.06589297,0.011848569,-0.09585889,-0.04395859,0.033791844,-0.019720374,0.0014725566,0.045914102,-0.003550706,-0.006747827,0.015341107,0.04742595,0.12719072,-0.0070050885,0.0019671638,0.015745286,-0.058053404,0.08707498,0.040469524,0.000542455,-0.0022168183,-0.06430512,0.015546191,0.05094921,0.06280382,-0.018405883,-0.035564113,0.08920418,-0.029726304,-0.047359437,-0.0013579697,0.032328706,-0.044502787,-0.02857283,-0.04292255,-0.012643068,0.09477774,-0.055673726,-0.025828032,-0.012966913,-0.024247725,0.021045038,-0.07825241,-0.02803201,0.002426445,0.10002317,0.093517296,0.016445557,-0.013008625,0.027922764,-0.0019696099,-0.0029642726,-0.016390774,-0.0066783847,0.02333543,-0.016810114,0.039820902,-0.04332313,0.027350638,-0.04423715,-0.00027529392,-0.011160836,-0.0072993482,0.023294536,-0.02979916,-0.03298892,0.0261449,0.039743457,0.055178083,0.0412073,-0.036719613,-0.032950085,0.0049520903,0.020575535,-0.030323865,-0.04114028,0.025049372,-0.014689005,0.037722968,0.09152092,-0.0102361655,-0.03643731,0.006734357,0.023333889,-0.054556675,-0.049294785,0.033709776,0.029657103,0.026732383,-0.08967171,0.028364476,0.0005740895,0.03539159,0.05240461,-0.0072275493,0.029684164,0.009951406,-0.037806552,0.019292556,-0.0021286728,-0.04729547,0.007175827,-0.049170643,-0.080133185,-0.022189138,-0.01858121,0.07438105,0.06102042,0.09801223,0.07005558,-0.0050348113,-0.011249454,0.02615384,-0.045234423,0.0051285843,0.004268484,-0.036550745,-0.013512807,0.019860297,0.009086974,-0.0032555074,-0.004008458,0.010104984,-0.02368753,0.04572696,-0.00843233,0.032399833,-0.016750721,0.020098666,-0.05202177,0.060983483,0.016694436,0.0077765775,-0.0050494773,-0.037026014,0.001117734,0.030215798,0.05040785,0.057997674,0.034018856,-0.062837005,0.003920207,-0.063665055,-0.011238692,-0.008594867,-0.008071044,-0.01580436,0.022231609,-0.026664222,-0.01013532,-0.03005544,-0.03532835,0.0450756,-0.00872942,0.01234063,-0.030065464,0.056750238,0.044555224,0.037708756,-0.023084044,0.0070027667,0.04318866,-0.049049277,-0.045987852,9.801762e-05,-0.04527407,0.01055968,-0.011939094,0.018368067,-0.0004077523,0.0021665099,0.0129328845,-0.0035164629,0.05755545,0.03720819,-0.017685486,0.026217898,-0.022961238,0.0030006953,0.024059506,0.0061919983,0.02033571,0.012856697,0.038564928,0.06153664,-0.0078088883,0.09385604,0.0018712954,0.03938427,-0.0047757193,0.043030277,0.02329402,0.0022694697,-0.045092855,0.05029627,0.05844596,0.009453071,-0.004755845,-0.010924141,-0.017698674,0.029227512,0.035043936,-0.025338443,-0.0016154872,-0.019510318,0.02470574,0.04002727,0.001375561,-0.023644352,0.0045005297,0.014606858,-0.04686525,-0.004115543,0.032374453,0.03793278,0.0051917606,-0.057948247,-0.04885209,0.00840045,0.051126167,0.0030838537,0.023511557,-0.03336798,-0.0196632,-0.030709583,0.022570822,-0.0012769782,0.02955046,-0.037390985,0.030959016,-0.01694449,-0.03920743,0.00300619,-0.016761066,-0.012595938,0.05902662,-0.021055747,-0.012388123,0.0106203845,-0.0048985523,-0.011327142,-0.020390263,0.045282125,0.12726197,-0.025033047,0.024681399,0.035567146,0.031109301,0.011671077,-0.04639101,0.033799145,0.05007398,-0.0007316971,0.004222078,0.0049691987,-0.006843488,0.008209552,0.012397866,0.05824622,0.064051166,-0.029752564,-0.004783986,0.02264076,0.014341847,0.03348535,-0.018285142,0.0005909978,0.04454041,0.027235795,0.043602288,-0.02399774,-0.10826451,0.0977513,-0.06338581,-0.014100552,0.08005939,-0.04691811,0.025264854,0.040040877,0.013967912,0.027214823,0.032823656,-0.00299489,0.024400609,0.009917283,0.007608846,0.020940695,-0.040476855,0.05290451,-0.009489439,0.0014119854,-0.01985622,-0.03297003,-0.01803544,0.03258034,0.026894718,0.021826828,-0.02791741,-0.028228335,-0.009575661,0.029475847,0.0054047867,-0.024250437,0.09320812,-0.13025872,0.08414208,-0.04112491,0.047316305,0.016900424,-0.07075213,0.025833553,0.044280894,-0.019081986,-0.004272732,-0.0036314165,0.05594158,-0.039853845,-0.04249133,0.0058302907,0.051257905,0.035921086,-0.05266611,-0.03637889,0.01602685,-0.011011044,-0.017008847,-0.053365476,0.010952143,0.0027939498,0.006427368,0.06311788,-0.014671942,0.04496876,-0.03418989,0.08220952,0.0021929936,0.021808425,-0.020718856,0.005377206,0.0121312775,0.03170734,0.022295175,-0.009440473,-0.012312502,0.048141334,-0.061055988,0.04776131,0.030298004,0.0490325,-0.08540209,0.03756931,0.00015957053,0.00087137934,0.013453967,-0.033429716,0.0014106344,0.01688069,0.027317848,-0.022803787,0.004795868,6.263723e-05,-0.060653195,0.009470284,-0.012259452,0.020655166,0.0023663433,0.03741421,0.015533478,-0.001743451,-0.031494398,0.009859567,0.0054612607,-0.062018283,0.015766345,-0.04857354,-0.037615944,0.02553821,0.0046780817,0.009711884,-0.020650206,0.090364166,-0.016147807,0.0072033755,0.08952869,-0.015552472,0.017396346,-0.003688951,-0.0040380466,-0.038362868,0.0028080281,0.00051280746,-0.00365323,-0.015634144,0.048852276,0.021128595,-0.0059989644,-0.019978218,0.06731863,-0.024140505,-0.00395439,-0.042451777,-0.0056851953,0.004267646,0.0155358,0.024775323,0.0135960495,-0.032379664,-0.02369671,-0.034247085,-0.025479103,-0.0350713,-0.024532832,-0.031685572,0.012071822,0.008046893,-0.0074900133,0.018542245,0.012938521,-0.02204503,-0.019886479,0.036900483,0.03471426,0.03246082,-0.08620551,-0.013469,0.003647365,0.012427519,0.016440224,-0.021568947,-0.005911962,-0.022758372,-0.014465597,0.021107538,-0.009375622,0.06914133,-0.0021304314,-0.0029404259,-0.012218608,-0.03791919,0.0013685573,-0.0061474233,0.023382474,-0.019115528,-0.0057900534,0.010155633,0.019481117,0.021931961,0.010847078,0.025388947,0.016324557,-0.018809294,-0.0174671,0.038178045,0.014957496,-0.00985387,-0.017914375,0.0017038847,-0.041089185,0.015853463,-0.03254895,-0.017179592,-0.0019386946,0.051940314,0.0056474744,-0.012648613,0.017487902,-0.0015559834,-0.029523227,0.033647615,0.017328283,0.055263195,0.046218805,-0.079863936,0.006254823,-0.048066273,-0.011226537,-0.002289874,-0.084485024,-0.03141328,0.017266728,-0.097778045,-0.0153864985,0.0064606303,-0.0435174,-0.0030361824,0.027424017,-0.018683912,0.063328646,-0.047048274,0.027382853,-0.008788301,0.010074164,0.066494845,0.051822692,-0.01105485,-0.027627844,-0.041224726,-0.013000201,0.031520583,0.060620043,0.036614478,0.017842602,0.09375325,0.023352908,-0.009146192,-0.055866268,-0.022615945,-0.0439355,-0.0063178446,-0.030306866,-0.007559918,-0.063824914,0.011668934,0.0027720463,-0.033303127,0.044800714,0.0027598543,-0.017639458,0.04946345,0.013049813,-0.01525138,0.03805752,-0.03305658,0.069641806,0.040283386,0.030247796,-0.023748646,-0.041539513,-0.018002005,0.030492652,-0.0065647853,0.00885984,-0.020894807,-0.009008125,-0.033039726,0.020207979,0.015973682,-0.013267721,0.0030100865,0.0011104163,-0.014030942,0.026431642,-0.0011890248,-0.019244561,-0.030931821,0.011048438,-0.016571455,-0.019928139,0.0018092107,0.0084571075,0.03976919,-0.03607041,0.031019881,-0.065508015,-0.028940592,0.0074493964,0.009547151,-0.035374362,-0.028981492,-0.024852535,-0.01488021,-0.017185269,-0.048068743,0.057716932,0.010290142,-0.06879324,0.025341352,0.0015653877,0.013712266,-0.042979002,-0.016288903,-0.021549245,0.0034136511,-0.0025441125,-0.015633054,0.07649117,-0.0052900794,0.011541307,-0.017421847,-0.011548675,-0.11486875,-0.017099418,0.046036404,0.038759112,0.012541666,-0.024904346,0.027104447,0.0052202274,-0.035532992,-0.0041971207,-0.010003874,0.0062622465,-0.03171875,0.0056510875,0.0333612,0.011434888,-0.065683566,-0.0035558685,-0.016548445,-0.034988042,0.016479889,0.08961052,0.031951133,0.10183284,0.050351862,-0.008082894,-0.07147648,0.009229797,0.003459105,0.0029455842,-0.027109042,0.024723072,-0.011904553,0.011788557,0.024894278,0.021798465,0.019571094,-0.06431059,0.014158469,-0.030362412,-0.029048132,-0.034555253,0.0132227605,0.022083012,-0.0021816764,0.005983326,0.06217013,0.0045974622,-0.051339664,-0.065255806,-0.054154683,-0.020217292,-0.0044511985,-0.032049812,-0.0059590745,0.059289806,-0.02117734,0.011065624,0.027074408,0.024464175,-0.036386415,-0.04211807,0.012834096,0.0039817076,0.022749094,-0.033692412,0.01299954,-0.0650881,0.022666747,-0.014313422,-0.027644092,0.046090703,-0.023178574,-0.044333737,0.015293833,0.010466773,-0.001908078,0.010921191,-0.008631702,-0.015926326,0.00055447675,-0.025726782,-0.0003706819,-0.041434698,-0.039529167,-0.0107633965,-0.0068932306,-0.028579496,0.012834672,0.018800298,-0.006943379,-0.0040951874,0.012255164,-0.029918626,0.0024480615,0.0028432698,0.042688932,-0.021452673,0.018833218,-0.019429514,0.009117996,0.051529616,-0.010486944,-0.010001566,0.031186434,0.026150387,-0.0070904763,-0.0151723875,0.003317544,-0.05578274,-0.04025022,-0.0029147153,0.020663928,-0.016045319,0.020461557,-0.02226362,-0.03488666,-0.072958566,-0.04051129,-0.0050332486,0.08556733,0.020377612,0.0172833,-0.074962735,0.061695945,-0.014177127,0.062966265,0.025124706,-0.008767167,-0.0315035,0.0045484416]	Keywords: extended sparse attention, binary tree, BP-Transformer, global attention\nKey Objects: binary tree, tokens, global attention\nRefers to Images: None\nHypothetical Questions:\n- How does BP-Transformers binary tree approach improve upon traditional global attention?\n- What are the advantages of using a hierarchical structure for connecting tokens in a sparse attention mechanism?\n- Can the binary tree approach used by BP-Transformer be applied to other data types besides text?\n---\nSummary:\nExtended sparse attention methods explore novel patterns for specific data types, such as BP-Transformer which constructs a binary tree to connect tokens through hierarchical nodes and paths.\nOriginal Text:\n4.1.1.3 Extended Sparse Attention. Apart from the above patterns, some existing studies have explored extended sparse patterns for specific data types.  \nFor text data, BP-Transformer [158] constructs a binary tree where all tokens are leaf nodes and the internal nodes are span nodes containing many tokens. The edges in this graph are constructed so that each leaf node is connected to its neighbor leaf nodes and higher-level span nodes containing tokens from a longer distance. This approach can be seen as an extension of global attention, where global nodes are hierarchically organized and any pair of tokens are connected with paths in the binary tree. An abstract view of this method is illustrated in Fig. 6(a).\nContextualized Text:\nBeyond the established sparse attention patterns, researchers have explored extended methods tailored for specific data types. For example, BP-Transformer utilizes a binary tree structure where tokens are organized as leaf nodes and connected through hierarchical span nodes. This approach extends global attention by organizing global nodes hierarchically and enabling connections between tokens via paths within the tree.	{"tags": ["sparse attention", "NLP", "transformer", "architecture"], "doc_id": "489dc08c-d5e5-4ef0-9126-9fe022faae02", "summary": "Extended sparse attention methods explore novel patterns for specific data types, such as BP-Transformer which constructs a binary tree to connect tokens through hierarchical nodes and paths.", "doc_type": "text", "entities": ["BP-Transformer"], "keywords": ["extended sparse attention", "binary tree", "BP-Transformer", "global attention"], "key_objects": ["binary tree", "tokens", "global attention"], "contextual_text": "Beyond the established sparse attention patterns, researchers have explored extended methods tailored for specific data types. For example, BP-Transformer utilizes a binary tree structure where tokens are organized as leaf nodes and connected through hierarchical span nodes. This approach extends global attention by organizing global nodes hierarchically and enabling connections between tokens via paths within the tree.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does BP-Transformers binary tree approach improve upon traditional global attention?", "What are the advantages of using a hierarchical structure for connecting tokens in a sparse attention mechanism?", "Can the binary tree approach used by BP-Transformer be applied to other data types besides text?"]}
fe63c8fb-2ea0-4b72-8d65-9a62f1653ce9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0428208,0.04326825,0.015397288,0.022970293,-0.023173548,0.012409314,-0.00034934402,0.05167848,0.044032604,-0.036550835,0.023690704,0.010704072,0.0023816805,-0.030244017,0.0059459885,0.026188575,0.0034981514,0.09137787,-0.0044519273,-0.04969289,0.04002564,0.038746864,-0.0061101387,-0.0010433764,0.0047698286,0.046019305,0.038394064,0.008045756,-0.02711019,0.03413474,0.024551319,-0.053216454,0.06537292,-0.0037004123,0.06254921,0.008778277,0.018614056,-0.077115975,0.0067545045,-0.013238638,-0.024265286,0.040879395,-0.045243874,-0.00356584,-0.0050615375,-0.04756743,-0.09967621,-0.03708652,0.005392294,-0.0034911607,-0.0008096551,0.02352805,-0.010251881,0.00037738073,-0.038565893,0.002764977,-0.0043898895,-0.032074284,0.0046541807,-0.08340298,-0.009803382,0.038813427,-0.013366918,0.007273941,0.027352417,-0.011099621,-0.002872997,0.01769859,0.049290642,0.14409605,-0.0077431733,-0.026072033,-0.0024543144,-0.03416995,0.085670196,0.027879871,-0.019617658,-0.027055547,-0.042892158,-0.042578038,0.0027909053,0.08139352,-0.03249249,-0.037288517,0.09184343,0.018869262,-0.025891284,0.0248748,0.059016485,-0.039156735,-0.0011899484,0.017834678,-0.005842688,0.03859753,-0.037809048,-0.04760882,0.015213368,-0.037560016,0.0065932525,-0.08599042,0.02419177,-0.008846699,0.09157865,0.10026502,0.06548841,-0.020990934,-0.003636026,-0.026282664,0.010313545,-0.0055940496,-0.0066092806,0.028057687,-0.046787877,0.028251357,-0.028941415,0.034962345,-0.058870856,0.033684634,0.026094656,-0.01307133,0.011993987,-0.024118641,-0.0026942983,0.043311127,0.08903928,0.029620636,0.038885362,-0.02977469,-0.048353203,0.024515256,0.05794668,-0.015882624,-0.026006207,0.008670263,-0.0053957365,0.02340933,0.112953335,0.0002964445,-0.004205136,0.05019284,0.0139937,-0.04664051,-0.063853264,0.07723156,0.011265033,0.040120237,-0.07460027,0.045739897,-0.027513376,0.02442008,0.043872043,0.0062700585,0.053683188,0.003219131,-0.02509921,0.00721876,-0.038905013,-0.0052490425,-0.0088952705,-0.035217542,-0.069869936,-0.04269178,-0.01327818,0.0568848,0.038998153,0.08706668,0.014106885,0.03226214,-0.0028774177,-0.020432709,-0.04291314,-0.0139902085,0.010491916,-0.040986486,-0.0052940454,0.015502794,-0.013312877,-0.047112755,-0.0027219134,0.010039608,0.016417984,0.06403349,-0.0019306367,0.03472288,0.016790576,0.028980942,-0.009545901,0.062458906,0.0107871555,-0.0054351776,-0.005128129,-0.025453495,0.010827566,-0.00870074,0.05711743,0.023014959,0.041524995,-0.059611615,0.013495714,-0.023365349,-0.0042208703,-0.011384648,-0.012287967,-0.021981025,0.024975885,-0.028991712,-0.0026691272,-0.037664637,-0.006815352,0.04377507,-0.010960884,-0.03023536,-0.023406964,0.076364145,0.06329154,0.008193964,-0.0017280991,0.009174593,0.017712737,-0.050464537,-0.055411637,-0.003424171,-0.028757924,0.057625875,-0.02824528,0.02784887,-0.012175969,0.012182453,0.016331695,-0.03135044,0.041562706,0.02059126,-0.04141281,0.019256957,-0.014836073,0.0036252225,0.03248728,0.00896794,-0.013623742,0.03547893,0.032871135,0.059464313,0.016523868,0.0658538,0.024770226,0.02955279,-0.012234601,0.06060603,0.019180559,-0.014515846,0.011178521,0.053427823,0.05710277,0.0019739738,-0.008511118,-0.027318122,-0.04669541,0.016413223,0.029909093,-0.01136027,0.035513446,0.012879196,0.0128165055,0.004046878,0.00805043,-0.02241607,0.0037769235,-0.008504711,-0.054858085,0.0059353267,0.027984176,0.05730394,0.01585719,-0.025807071,-0.015103064,0.0037225848,0.022060359,-0.0114542255,0.030184127,-0.038859993,-0.010667111,0.0028700267,0.048990045,0.0014363851,0.04371867,0.031520497,0.059319,-0.04084521,-0.002741717,-0.012857247,-0.04343857,-0.02664726,-0.0068634804,-0.014839605,0.0017784885,-0.012329228,0.006261251,-0.030511249,0.00023829378,0.025017653,0.12065903,-0.036090717,-0.009869012,0.028898342,0.02199382,-0.013278683,-0.043980382,0.04110736,0.027770111,0.021090802,0.016525978,-0.013471501,-0.018276673,0.0046471786,0.0465032,0.074587055,0.087659016,-0.013863889,0.0013770165,-0.02729016,0.043116644,0.027066858,-0.008636126,0.025303842,0.052375376,0.0018538896,0.042782504,-0.012093701,-0.09024693,0.05621077,-0.06610166,0.036443062,0.07604359,0.021041537,0.0076620136,0.001488442,-0.009015828,0.010491516,0.021064045,-0.008402006,0.0015322531,-0.017859593,0.0022560959,-0.0084199775,-0.05563858,0.032897133,-0.0007092862,-0.02412839,-0.0066101504,-0.006269023,0.029022459,0.056307938,0.01269149,0.035879616,-0.017336339,0.015692925,0.0005106717,0.028060885,-0.0069754957,-0.014864144,0.07501397,-0.07319191,0.020883491,-0.045005824,0.023015194,0.013892194,-0.06337344,0.023957722,0.042284828,-0.010957464,-0.023797704,-0.035235543,0.04054975,-0.003151119,-0.04245031,0.0064912797,0.08300136,0.05385706,-0.016926024,0.004556704,-0.006297311,-0.0054006903,-0.023840804,-0.055550847,0.031887498,0.012509739,-0.000718964,0.072151,-0.024034508,0.069232576,-0.019489214,0.013005009,0.004279419,0.033592805,-0.03546593,0.014412327,0.015469032,0.071860254,0.024911301,0.0061782324,-0.00081059686,0.00052873685,-0.052550912,0.033325918,0.012839187,0.021066865,-0.08988807,0.0098439315,0.030676406,0.015940106,0.021243999,-0.070992045,-0.004413581,0.022166498,0.019675395,-0.051058404,-0.041056514,0.014480316,-0.053482223,-0.03453981,-0.027442627,0.03561807,0.045112755,0.045589797,0.054869812,-0.00033368482,-0.016506748,0.0188052,0.0014605372,-0.046336032,0.011415671,-0.035710104,-0.031933133,0.025796752,0.014832958,0.0128583545,-0.04721789,0.111422494,-0.05414273,0.011296529,0.10160503,-0.024298014,0.031449903,-0.029584263,0.012460416,-0.04880708,0.026330102,0.029300695,-0.014064256,-0.022873666,0.024622781,-0.009954602,0.019856803,-0.023334578,0.041011598,-0.011281735,-0.023715144,-0.044036902,-0.013502502,0.007506123,-0.015008247,0.013379377,-0.002751484,0.0020636055,0.007465265,-0.024654439,-0.012096677,-0.036688365,0.0141346445,-0.023418143,0.0110167,0.019397028,-0.019087188,0.005662976,-0.01562943,-0.0026192248,0.016362766,0.055562485,0.049333464,0.023939824,-0.06633112,-0.020880839,0.0047141477,0.0117877545,0.019597987,-0.032204587,-0.007116157,-0.028449839,-0.033582132,0.00439519,-0.03842677,0.07766143,0.01305152,0.028173553,0.021858577,-0.050609063,0.029121814,0.0046721715,0.027248716,0.00150705,-0.051807996,0.0031556003,0.035662692,0.020237092,-0.0052382755,0.025289265,0.035775017,0.024368852,-0.027791673,0.05039249,0.023995478,0.032805126,0.031692676,-0.0029130883,-0.02101586,0.024140213,0.014698231,0.00036507947,-0.030869786,0.08071324,0.016379392,0.020763319,0.059250336,0.028097844,0.0053605163,0.021942085,0.026171938,0.038882162,0.03398078,-0.105626084,0.027226401,-0.0026516868,-0.013615044,-0.038931064,-0.034403972,-0.056876022,0.0058282777,-0.0574969,0.010261901,-0.014798807,-0.01984731,-0.028174972,0.013662787,-0.021549221,0.07486679,-0.05736155,0.016824126,0.0060673202,-0.041397974,0.08016914,0.04107893,0.012155176,0.015081664,-0.016724514,-0.028517436,0.05172942,0.05952512,0.0011855661,0.018846612,0.077709794,0.01764608,-0.001130056,-0.016457843,-0.07226937,-0.057547,0.034990482,-0.005402283,-0.025483025,-0.042607002,0.03785216,0.002446942,0.026920225,-0.011942129,-0.0074634166,0.023953013,0.050782554,0.0015510247,-0.029301465,0.042479835,0.0015364933,0.043497995,0.021653637,0.075222574,0.01727577,0.015986174,0.034756023,0.019076591,-0.0045153396,0.014576193,-0.03803275,0.0030447044,-0.038116965,0.047647048,0.01495627,0.0069920393,-0.016705973,0.0096026035,-0.0052686674,0.06901448,-0.053167645,-0.04731426,-0.02782912,-0.010426402,-0.023203038,0.027800335,0.01692492,-0.008900843,0.06285513,-0.03570872,0.05648473,-0.082304314,-0.019849794,-0.003048931,0.0012624945,0.0002370206,0.009446991,-0.009521445,-0.0027396195,0.00057537045,-0.040342927,0.06669182,0.012749287,-0.023664882,0.0023940778,-0.005061368,-0.004286937,0.013190777,-0.0005103268,0.019815337,0.0018342457,-0.00871559,-0.019649386,0.06371485,-0.008822625,-0.005321802,0.0029507084,0.020173348,-0.08871924,-0.025081206,0.011371538,0.042273894,0.007184225,0.016850604,0.007944479,0.046110544,-0.044891622,0.05244884,-0.03524714,0.023680555,-0.018039564,0.025386734,0.08292237,0.010377821,-0.058672484,-0.0007829283,-0.022890326,-0.01380642,0.06053309,0.016661955,0.06636393,0.086040035,0.06403287,0.015470966,-0.07327506,-0.028076809,-0.02322194,0.00796311,-0.02386561,0.031091815,-0.020271195,-0.018050913,-0.019679116,0.00015022566,0.025000395,0.021608157,0.032330774,0.010257301,-0.059494246,-0.0064718337,0.020426676,0.00050869666,0.032154012,-0.011337932,0.047123343,0.024062105,-0.05955744,-0.022705806,-0.07232565,-0.022866445,0.023650492,-0.029860485,-0.002719616,-0.014605173,-0.026319508,0.009677596,0.033231437,0.0024550534,-0.072053775,-0.031096717,0.00814837,0.0052693686,0.039570916,-0.054638803,-0.013216144,-0.075589046,0.01257243,-0.0258264,-0.027498147,0.039250262,0.009007733,-0.032441895,-0.028955713,-0.0109489495,0.020670163,0.0481009,-0.008134284,0.011667856,0.013713179,-0.05316048,-0.016737843,-0.0398456,0.0030515615,-0.02239092,0.024271097,0.0008171263,0.0496734,0.028982086,-0.021513289,-0.0045101186,-0.010830832,-0.017981257,0.02947245,-0.02155883,0.056065973,0.018619271,0.031208785,0.0054593333,0.025774669,0.09600301,-0.032170128,0.029195752,0.020662108,0.0351049,0.022595255,-0.0033502243,-0.021373969,-0.040564887,-0.052093424,0.03377052,0.03440701,0.029308392,0.016094659,-0.008936401,-0.026368696,-0.03180497,-0.04713168,-0.009661482,0.039926976,0.046324972,-0.0018734328,-0.07769753,0.02980247,-0.01774982,0.046699274,-0.036399275,-0.0033419633,-0.032253414,0.011045182]	Keywords: vision data, image transformer, axial transformer, sparse attention\nKey Objects: image pixels, attention modules\nRefers to Images: ./images/a-survey-to-transformers/image_6.png\nHypothetical Questions:\n- How does Axial Transformer's approach of applying attention over each axis improve performance?\n- What are the trade-offs between 2D block local attention and applying attention modules over each axis?\n- Can the techniques used in Image Transformer and Axial Transformer be adapted for other data types?\n---\nSummary:\nFor vision data, approaches like Image Transformer and Axial Transformer utilize block local sparse attention and independent attention modules over image axes to reduce computational complexity.\nOriginal Text:\nThere are also some extensions for vision data. Image Transformer [94] explores two types of attention: (1) flattening image pixels in raster-scan order and then applying block local sparse attention. (2) 2D block local attention, where query blocks and memory blocks are arranged directly in 2D plate, as depicted in Fig. 6(b). As another example of sparse pattern on vision data, Axial Transformer [54] applies independent attention modules over each axis of the image. Each attention module mixes information along one axis while keeping information along the other axis independent, as illustrated in Fig. 6(c). This can be understood as horizontally and vertically flattening image pixels in raster-scan order and then applying strided attention with gaps of image width and height, respectively.  \n\nContextualized Text:\nExtensions for vision data have been developed, such as Image Transformer applying block local sparse attention and Axial Transformer applying independent attention modules over image axes. Axial Transformer achieves this by applying attention modules over each axis, effectively flattening image pixels and applying strided attention to reduce computational complexity.	{"tags": ["vision", "transformers", "sparse attention"], "doc_id": "fe63c8fb-2ea0-4b72-8d65-9a62f1653ce9", "summary": "For vision data, approaches like Image Transformer and Axial Transformer utilize block local sparse attention and independent attention modules over image axes to reduce computational complexity.", "doc_type": "text", "entities": ["Image Transformer", "Axial Transformer"], "keywords": ["vision data", "image transformer", "axial transformer", "sparse attention"], "key_objects": ["image pixels", "attention modules"], "contextual_text": "Extensions for vision data have been developed, such as Image Transformer applying block local sparse attention and Axial Transformer applying independent attention modules over image axes. Axial Transformer achieves this by applying attention modules over each axis, effectively flattening image pixels and applying strided attention to reduce computational complexity.", "mentioned_images": ["./images/a-survey-to-transformers/image_6.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does Axial Transformer's approach of applying attention over each axis improve performance?", "What are the trade-offs between 2D block local attention and applying attention modules over each axis?", "Can the techniques used in Image Transformer and Axial Transformer be adapted for other data types?"]}
98280a37-3a17-4d91-9678-6dedc480afbb	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.046447854,-0.014577405,0.0119392695,0.052550305,-0.07782487,0.0056557674,-0.03847834,0.040352635,0.073088154,-0.0020231064,-0.056294806,-0.038236834,0.029717997,-0.019086218,0.025255537,0.034459732,0.04305543,0.057113346,0.011303521,-0.002130177,0.053665288,0.0040491014,0.012107679,0.011313655,0.034138028,0.029138135,0.034884185,-0.0042193285,0.0023544882,-0.0015618388,0.037195556,0.013827958,-0.03278388,0.02152113,0.021405842,-0.0286974,0.0327785,-0.0052992217,-0.0191072,-0.039870102,-0.06574689,0.04730687,0.056258142,0.07278632,-0.022746013,-0.036766674,-0.1036003,0.018024307,0.015825527,-0.038536035,-0.04191792,0.024244472,-0.03701776,0.0063388674,-0.022931661,-0.07858827,-0.048236348,-0.041563697,-0.0008344448,-0.06737764,-0.048975002,-0.011830113,0.018976063,0.0109607,0.011097487,-0.023119515,0.029669294,0.024691591,0.071130425,0.15666552,0.03988935,-0.01986229,-0.035182986,-0.05719496,0.097013965,0.004885431,-0.035884745,0.007852711,-0.06469601,0.052711982,0.07329413,0.079646535,-0.050688386,-0.020518532,0.08074119,0.024582114,-0.049954202,-0.026402298,0.02800048,-0.05488829,-0.021218413,-0.05537055,-0.02239445,0.04009722,0.017917357,0.007312027,-0.020400602,-0.039560247,0.033968877,-0.022073396,-0.0015685057,-0.0048124003,0.047765926,0.06293035,0.0541265,0.019690342,0.029813217,-0.016837245,0.0065606637,0.027501924,-0.0066451677,-0.009982507,0.0011962904,0.044367313,-0.06296276,0.038434774,-0.12038229,-0.025283525,-0.06029193,0.009116542,0.017482396,-0.008465967,0.048580077,0.02187847,0.039439835,0.04631186,-0.01777361,-0.03183056,-0.04526392,-0.026608562,0.063715175,0.018405052,0.015950669,0.034409944,-0.0132657895,0.040398423,0.06612381,-0.0023316662,-0.0058126026,-0.052415963,-0.018811055,-0.07580536,-0.059912495,0.05187648,-0.0002689472,0.011324798,-0.04967568,0.02489929,-0.020624187,0.06678211,0.02484264,0.004971956,0.049049903,0.0052258656,0.005312785,0.026750544,-0.0066986666,-0.013756393,0.019401513,-0.03240201,-0.08098198,-0.037882257,0.03000998,0.04874316,0.057652265,0.049708076,0.040776137,0.027548151,-0.035544373,0.025787024,0.015231286,-0.04237129,0.007715052,-0.0565497,-0.011426143,-0.011069684,-0.0071188193,-0.01652805,-0.045350913,-0.017716242,-0.01019839,0.10978343,0.010359706,0.040074594,-0.050909277,0.014780844,-0.03353081,0.044521086,0.019230312,-0.02267217,0.0068682004,-0.022141004,-0.059550382,-0.010546441,-0.007712965,0.027561855,0.031652443,-0.048822515,-0.020683868,0.015538526,-0.037186995,-0.013047964,-0.0013673013,0.015982723,-0.014759375,-0.012684093,-0.055261984,0.0073044603,-0.03148068,0.054425992,0.0011081009,0.008949155,-0.012884396,-0.0050237365,0.05804756,0.005165227,-0.020771503,0.061603386,-0.02766966,-0.0069804476,-0.012949197,-0.03945425,-0.039150592,-0.017490353,-0.049264953,0.01159452,-0.06662042,-0.0105518615,0.036618486,0.0162361,0.01752877,0.016076338,-0.04049885,0.00813005,-0.017365063,0.008789099,0.030304022,-0.009793236,-0.001160702,0.006338463,0.027739178,0.019030908,-0.02149574,0.048220865,0.010188793,-0.031255174,0.016848665,0.0416448,0.037402187,0.0307918,-0.028050581,0.032368936,0.011872907,-0.0025306558,-0.009129645,-0.040038247,-0.047662504,0.03443863,0.022695063,-0.0016316158,-0.014157391,0.005032217,-0.0005283665,-0.011079635,0.02372922,0.016252926,0.01185388,-0.0078087784,-0.02814478,0.02001727,0.017907651,0.02103765,-0.010802972,-0.026945421,0.018725187,0.04292423,-0.03318754,0.022727557,0.027049048,-0.026371388,-0.019304072,0.016638441,0.031502496,0.007120547,0.020208767,0.022024507,0.02290161,-0.03551387,-0.03006284,-0.0174704,0.043369226,-0.00088744145,0.046222623,0.0051490846,0.0419509,0.035226267,-0.025251223,-0.068808585,0.009164914,0.067699865,0.11321307,-0.042040434,0.020788448,-0.0050333627,0.004493733,0.011055616,-0.035255343,0.030097848,0.04630195,0.037123732,0.04367726,0.017323738,0.003126677,0.027953336,0.04860006,0.09149659,0.053304497,-0.014929104,-0.008526804,0.03635714,0.071222015,0.011305196,0.021162907,0.027684083,0.02782242,-0.012368518,0.061404064,-0.006144052,-0.06698239,0.023977486,-0.023740176,-0.0013256492,0.074353255,0.0006311969,0.007828031,0.0016178348,0.025833245,0.05346145,0.0062265876,-0.016248722,0.029008785,-0.00306084,0.011613749,0.005063961,0.021519579,0.046230543,-0.025318276,0.006882375,0.015899887,0.00794693,-0.022910578,0.00026606733,0.07017039,0.0034016275,0.019924087,-0.02620767,0.006603877,0.05826036,-0.017647604,-0.06542533,0.04799688,-0.080526166,0.051737472,-0.033218585,0.04434819,0.0018616982,-0.027857326,0.058611665,-0.022735832,0.018639034,-0.0047365413,0.0074335844,0.032099664,0.013251895,-0.004881694,0.025034321,0.00860668,-0.016909447,-0.0539649,-0.037131894,-0.05673536,0.017304173,-0.040380273,-0.054586943,0.02213484,0.011550863,-0.0045530396,0.03764687,-0.031958777,0.00042522952,0.022901146,0.022070996,0.03907532,0.006125037,-0.0010950229,-0.04896898,0.0067986334,0.03619244,0.05392269,-0.042616908,-0.01561803,0.024849372,-0.044886447,0.065056615,0.02683883,-0.0035546157,-0.04988167,0.0013558185,-0.007462941,0.0146142095,0.07553445,-0.038080323,-0.01407763,-0.012745183,0.04473581,-0.0348874,0.0019129294,-0.02192515,-0.055646587,-0.02473817,-0.032663602,-0.0018175106,0.05382311,0.029566895,0.022317784,-0.0065152375,-0.0029280244,0.025301425,0.037764575,-0.082988985,0.054560684,-0.015253715,0.0036830148,0.044017974,0.06927825,-0.0032804532,-0.049107112,0.107942745,-0.005121129,0.041668836,0.0992686,0.024238419,0.032345813,-0.055717442,0.014368712,-0.06019857,-0.017114498,-0.004358664,-0.030604212,-0.019275518,0.034985654,-0.0085118115,0.0039360123,-0.07568356,0.021823645,-0.06343456,-0.014048753,-0.011829347,-0.006773833,-0.018732201,0.02686404,0.028688023,0.01885059,-0.053849593,0.005807044,0.0030284994,-0.008766581,-0.019791208,0.014567422,-0.030373733,0.0087051615,0.005997811,-0.011102035,0.01772388,-0.027648887,-0.0061523966,0.03925973,0.013444009,0.0077685765,0.02833497,-0.1031526,-0.02193747,0.030947424,0.017532457,-0.023230828,-0.07987657,-0.04924227,0.002002757,-0.03009522,0.004030479,0.045352902,0.0023548433,0.04919302,0.0117574595,-0.0148637155,-0.050281744,0.0002156608,0.019885503,0.010276768,0.032294184,-0.010873164,0.014764701,0.0408326,0.03996803,-0.015754383,0.023033576,0.03929473,-0.023189964,-0.023966772,0.0438446,0.010171197,0.03820124,0.021212002,0.017178876,-0.057440534,-0.019490765,0.0027357077,-0.0030087,0.006211481,0.024461167,0.031354077,-0.01810171,0.028707534,0.048219875,0.020966562,-0.0013154265,0.008656326,0.043551132,0.044445198,-0.05289169,-0.038829863,-0.081932284,0.0027293202,0.03805254,-0.035491485,-0.011064651,0.014484936,-0.042391356,-0.013149888,-0.035029106,0.011309168,-0.06073126,0.00714624,-0.001266223,0.028991697,-0.0521136,-0.0068689967,-0.010333892,0.003908206,-0.002079949,0.03010806,-0.012919329,-0.02244062,-0.0054432447,0.022420278,0.026976528,0.04035236,0.0059759323,0.024785334,0.055710487,0.031524524,0.02234401,-0.05357712,0.013931333,-0.082355596,0.036987513,-0.037783723,-0.049250014,-0.022058591,0.028641462,0.011415237,0.02680568,1.7963635e-05,-0.035582416,-0.03628291,0.1106687,0.090309605,-0.008414734,-0.0655148,-0.02819226,0.025295619,0.022303423,0.015624723,0.027323358,-0.009132205,0.018671226,-0.0009805686,0.015096593,0.018185228,-0.008242601,0.033878576,-0.010742286,0.0040757735,-0.040675525,-0.050026592,0.021186331,0.036475502,-0.022900581,8.3305174e-05,-0.02931066,-0.041109595,-0.040544555,0.060307033,-0.02359221,-0.022334484,0.03956242,-0.010271155,0.035907134,-0.059175458,0.065705165,0.0064715543,-0.0633866,0.00085606484,0.002978854,-0.048168562,-0.02059044,0.025449816,-0.017330794,-0.009629709,-0.040540054,0.055111296,0.004750753,-0.011488483,0.020210253,-0.04781158,0.016650517,0.0063983058,-0.06073915,-0.059546936,0.049537707,0.060535602,-0.009522842,0.051126618,-0.01912226,-0.047101073,-0.04601106,0.017015126,-0.09685099,-0.016771372,0.020441871,-0.009543645,0.055775795,-0.019961258,0.0054061753,0.023800243,-0.073625885,-0.028750671,-0.04151553,0.024500202,-0.025204936,0.028718654,0.038910937,-0.0028716587,-0.04843781,0.03251594,-0.027856022,-0.021035193,-0.005601879,0.015668549,0.003328967,0.06498293,0.041395802,-0.05617075,-0.10113055,0.039131626,-0.006381397,-0.009594539,0.012051406,0.007595049,0.032215193,-0.028292751,0.013482898,0.015284025,-0.0072316076,-0.0048888847,0.0053231446,-0.004523772,0.017447136,-0.008447045,0.023101334,0.042083558,-0.04601363,-0.0036084205,0.07846254,0.02765836,0.02088225,0.039360743,-0.046186317,0.02173371,0.01858997,0.013442061,-0.0032371038,0.0029294295,0.007007466,-0.015321008,0.01260254,0.0014024036,0.009663471,-0.077414565,0.023604516,0.0028473316,0.03720934,-0.029495262,-0.025515014,0.012409945,0.040372714,0.014571616,0.019751905,0.01323411,-0.022444742,-0.056777734,0.020750966,0.0048465836,-0.040106624,0.021643475,-0.014110786,-0.05236439,-0.011387516,0.02138774,-0.028608212,-0.05174228,-0.04032316,0.054352358,0.0036436806,0.034018815,0.015602387,-0.008114906,0.00508542,-0.0005747392,-0.023026006,0.034465358,-0.010518817,-0.006367104,-0.0057209176,-0.060091384,0.029124046,0.0023042038,-0.012412791,0.012527181,-0.007160349,-0.019737775,-0.007106144,-0.01114419,0.008763578,0.022421014,-0.0015036019,-0.032636646,-0.034822773,0.025988318,-0.035631884,0.043644007,0.019243175,0.0033247168,-0.001882663,-0.061700825,-0.06738891,-0.014085005,0.025792228,0.020614438,0.009967646,-0.0382765,0.029967949,-0.0063712527,0.018152002,-0.0013615404,-0.012836726,0.0028459954,0.017383004]	Image title: Binary Decision Tree for Iris Dataset\nTags: decision-tree, machine-learning, classification, iris-dataset, algorithm, data-mining\nKey objects: Root Node, Internal Nodes, Leaf Nodes, Decision Boundaries, Iris Setosa, Iris Versicolor, Iris Virginica\n---\nSummary:\nThis diagram illustrates a binary decision tree used to classify Iris flowers based on their features. The tree's nodes represent decisions based on feature values, leading to classifications of different Iris species (Setosa, Versicolor, Virginica).\nFull description:\nThe diagram represents a decision tree for classifying Iris flowers. The top-most node, the 'Root Node', splits the data based on a decision boundary, represented by the value '0.3'. This split leads to two branches.  Each branch further splits based on additional decision boundaries, represented by values '0.8' and others, to progressively refine the classification. The leaf nodes at the end of the branches represent the final classifications: 'Iris Setosa', 'Iris Versicolor', and 'Iris Virginica'.\nText found in image:\n- 0.3\n- 0.8\n- Iris Setosa\n- Iris Versicolor\n- Iris Virginica	{"tags": ["decision-tree", "machine-learning", "classification", "iris-dataset", "algorithm", "data-mining"], "title": "Binary Decision Tree for Iris Dataset", "doc_id": "98280a37-3a17-4d91-9678-6dedc480afbb", "source": "./images/a-survey-to-transformers/image_6.png", "summary": "This diagram illustrates a binary decision tree used to classify Iris flowers based on their features. The tree's nodes represent decisions based on feature values, leading to classifications of different Iris species (Setosa, Versicolor, Virginica).", "doc_type": "image", "key_objects": ["Root Node", "Internal Nodes", "Leaf Nodes", "Decision Boundaries", "Iris Setosa", "Iris Versicolor", "Iris Virginica"], "parent_doc_id": "fe63c8fb-2ea0-4b72-8d65-9a62f1653ce9", "text_in_image": ["0.3", "0.8", "Iris Setosa", "Iris Versicolor", "Iris Virginica"], "contextual_description": "The diagram represents a decision tree for classifying Iris flowers. The top-most node, the 'Root Node', splits the data based on a decision boundary, represented by the value '0.3'. This split leads to two branches.  Each branch further splits based on additional decision boundaries, represented by values '0.8' and others, to progressively refine the classification. The leaf nodes at the end of the branches represent the final classifications: 'Iris Setosa', 'Iris Versicolor', and 'Iris Virginica'."}
8b4dc656-bd1c-4803-8827-7282b1f97186	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.028840872,-0.001842603,0.035599623,0.033870738,-0.03559692,0.048356134,-0.05697951,0.07002631,0.03879852,-0.038077462,0.016126383,-0.0055927504,0.017055457,-0.040158145,0.011493557,0.0004466669,0.015442305,0.061918765,0.010207447,-0.024861466,0.047522068,0.0041325088,-0.0077379956,0.019988127,0.018740233,0.062705725,0.007965954,-0.020207819,0.019043054,0.010869024,0.02067345,-0.07160737,0.036170676,-0.01002971,0.058732387,-0.02662514,0.017215375,-0.08536893,0.030500121,0.014969291,-0.010105761,0.057332,-0.015469025,0.016567994,0.03156184,-0.10762958,-0.10033957,-0.056552224,0.0075229025,-0.014876104,0.046065908,-0.006521866,-0.013995252,0.00303755,-0.035241786,-0.028909562,0.012596125,-0.06350819,-0.0061268634,-0.09646409,-0.03404857,0.008193988,-0.018671565,0.018524753,0.027775327,-0.04538854,0.03711836,0.030834762,0.063572936,0.1375097,0.019448603,0.007283161,-0.0041220123,-0.08328123,0.10557847,0.041104507,-0.029836891,0.0039814613,-0.032682963,-0.0007354357,0.016103664,0.07475278,-0.048410706,0.00643068,0.08097875,-0.03222236,-0.03268261,0.03200344,0.024227178,-0.02473389,0.0061754016,-0.019100634,0.002172203,0.061398413,-0.02325075,-0.056899372,-0.003445692,0.00064064626,0.005466767,-0.07189111,0.0029861748,-0.010392037,0.0902637,0.11572122,0.034367163,-0.02856501,0.009182243,-0.026609365,-0.0012128977,-0.013359739,-0.018646786,0.009402237,-0.07200971,0.023138879,-0.030281762,0.020556144,-0.033503093,0.020160895,-0.014225709,-0.019897347,0.031802434,-0.021715555,-0.030182833,0.024264272,0.043942604,0.071051784,0.024349399,-0.03485141,0.031141065,0.0016663139,0.05246009,-0.012356717,-0.036910776,0.05544621,-0.038012594,0.035612058,0.055407424,-0.023577258,-0.029721217,0.017636584,0.002217633,-0.02825557,-0.04920446,0.04719374,0.018533943,0.004055145,-0.082619734,0.002641119,-0.035235707,0.044229046,0.02798851,0.0053255083,0.058597945,0.00847999,-0.02793353,-0.02598405,-0.010562051,-0.010563635,-0.013458877,-0.067110464,-0.08281907,-0.011760613,0.0022835494,0.08111283,0.050619774,0.1301699,0.008539416,-0.0070855464,0.0132148685,0.018463397,-0.03420628,0.0013389061,-0.0027660667,-0.02076602,-0.0073710415,0.025660923,0.025417853,-0.027158123,0.016853135,-0.01664864,-0.0009837992,0.08387006,0.009248615,-0.011099395,-0.010935076,0.053697657,-0.023121959,0.072622426,-0.006629529,-0.0044719963,-0.021380877,-0.028529126,0.023908272,-0.0072709396,0.05057145,0.06248106,0.037715744,-0.029768607,-0.016765514,-0.055578493,0.002580374,-0.009704436,-0.03528178,-0.00057231984,0.014885543,0.002939269,-0.009132046,-0.047497068,-0.008938157,0.015607712,-0.0075218203,-0.009742645,-0.042395644,0.010662214,0.027160363,0.0121020945,-0.009005852,0.03464059,0.039687965,-0.021285318,-0.0017116471,0.020427164,-0.022188578,0.0560718,-0.022664769,0.037960023,-0.009273137,-0.0318022,-0.05795482,0.006426662,0.00931355,0.03468142,-0.009305584,0.020619253,-0.0445249,0.018850248,0.025585148,0.030809233,0.007869731,0.03743358,0.029630844,0.046906333,-0.034332063,0.08170378,-0.015006955,0.007190081,0.028234195,0.049840312,0.02177958,0.017944245,-0.012667689,0.059756584,0.049097795,0.022747671,-0.0151662715,-0.011110734,0.0068052267,-0.020208715,0.032445278,-0.02468887,0.009416346,0.016654784,-0.0066211517,0.053717043,0.009696401,-0.013256997,0.017340006,0.0068959766,-0.072011776,-0.0280552,0.03773421,0.036296703,0.0025102373,-0.031147921,-0.0271613,0.032755148,0.040427327,0.01962857,0.043511726,-0.012637659,-0.02815947,-0.00015765868,0.008226871,0.018710323,0.031175101,0.03135294,0.04898528,-0.013524186,-0.016085684,0.004364766,-0.012959949,2.0980846e-05,0.012573595,-0.044967372,-0.011297346,0.0328075,0.0072291913,-0.00979563,-0.0072780647,-0.010531938,0.07582301,-0.025574204,0.027722102,0.03516118,0.046156444,0.015195639,-0.045479216,0.02676091,0.009974458,0.017287005,-0.003520451,0.0046724565,-0.016927205,-0.0023444325,0.036835592,0.04513442,0.11422138,0.00032952422,-0.019794751,-0.0050545163,0.036641248,0.032567102,-0.0035273673,0.010513021,0.063840166,0.017725948,0.038806602,-0.026890831,-0.0876552,0.059884313,-0.08095424,0.046297934,0.08106673,-0.019733842,0.0034467948,-0.0037319418,-0.015235502,0.024572054,0.054435298,-0.010597806,-0.006557058,-0.015452383,0.008088659,-0.010891281,-0.010642604,0.072980754,-0.032963865,0.006650968,-0.005402477,0.00651164,0.026745401,0.006980149,0.01457675,0.012264134,0.018284345,-0.00859169,-0.0070497924,0.013895263,0.022913145,0.0009030155,0.08360335,-0.114031695,0.07735887,-0.051314652,0.018695673,-0.0126293525,-0.04556455,0.05400909,0.0624193,-0.0077947797,-0.010347047,-0.03975516,0.050104022,-8.729831e-05,-0.02603989,0.03303782,0.05129339,0.020921282,0.001026748,-0.036901448,0.021529099,-0.016766157,-0.033868082,-0.016958278,0.01960255,-0.016738366,-0.03607206,0.089788795,-0.0050793337,0.10034783,-0.07617668,0.03246551,0.0052563273,-0.020765025,-0.0005850474,-0.00041851331,0.013206662,0.025296446,0.012174774,0.013924048,0.012116814,0.005230672,-0.050947484,0.016805865,0.038376562,0.025688984,-0.074054115,0.035039216,-0.012222475,-0.01759563,0.03390826,-0.05713799,-0.0062927227,-0.0476595,0.043635182,-0.051773425,0.013956846,-0.012521937,-0.05592347,0.004710052,-0.01383044,0.017204456,0.046039645,0.03400858,0.022130262,-0.01977051,-0.07297885,0.049900893,0.008110768,-0.035876654,0.010720385,-0.0130601,-0.020750815,-0.002958001,-0.005651481,0.024078954,-0.014757339,0.11711866,-0.027353482,0.0041438527,0.07025887,-0.040330976,-0.005945015,-0.02413404,0.030096708,-0.036973312,0.0148675665,0.015225612,-0.01263435,0.024677416,0.0062557934,0.00926761,-0.0073905913,-0.022857571,0.029597517,0.022246215,-0.009556514,-0.0387645,-0.0183221,0.018825172,0.0034896184,0.0070639797,-0.016290167,-0.04683352,-0.0022874335,-0.043636486,-0.038956076,-0.043455046,-0.024871562,-0.020506041,-0.0086687,-0.006156385,-0.00821997,0.038070463,-0.021287713,0.016647173,-0.007902191,0.046807524,-0.011481689,0.031388327,-0.054266293,-0.065050825,0.010891481,0.0047843233,0.021980045,-0.0274927,0.028647674,0.0044816053,0.00041273588,0.025698913,0.0055190627,0.052668642,-0.024066348,0.0027035763,-0.004950237,-0.041433264,0.04140079,-0.023536554,0.030166633,-0.0362516,0.010491462,-0.008978838,0.043934107,0.06274537,-0.022784986,0.03147015,0.040111117,-0.03594308,-0.041227076,0.00825634,0.019640507,-0.034577724,-0.012247733,-0.004109408,-0.06355483,0.014965988,-0.009548843,-0.009659288,-0.005571983,0.066813186,0.040360026,0.033109903,0.074888624,0.003641281,-0.02588342,0.017232748,-0.00041130287,0.035422523,0.03814715,-0.08144724,0.012746815,-0.0693099,-0.004163619,0.0005387488,-0.07322004,-0.07414038,-0.008600196,-0.074005894,0.0020768237,-0.008922345,-0.02128844,-0.0177464,0.02983995,-0.06461764,0.043460805,-0.045982778,0.026970712,0.017647099,-0.008845028,0.07559056,0.04609468,0.017789558,-0.026691401,-0.032464344,-0.0020148174,0.039965075,0.06094925,0.017163785,0.010503325,0.047105845,0.009736434,0.012889618,-0.021499868,-0.016822508,-0.0112547185,-0.0011624021,-0.031855825,0.012461133,-0.023493184,0.032628603,-0.05569452,-0.012417772,0.0197835,0.008505318,0.026495354,0.031428188,0.05088717,-0.0032258944,0.02456372,-0.029423898,0.029431773,0.015600887,0.032136407,0.0013353429,0.014822745,0.031222424,-0.013936396,0.039228693,0.020192258,0.018545587,-0.01766294,-0.050588034,0.03243301,-0.036591806,0.0016576517,0.009802892,0.008723588,-0.012852473,0.02957995,-0.029120797,-0.037080195,-0.021799358,-0.016785212,-0.017473008,0.013887783,-0.003992787,0.02086624,0.06945828,-0.012940359,0.016087634,-0.06614262,-0.018991584,-0.007860493,0.038011998,-0.035250492,-0.0011196433,-0.031132875,0.01129646,-0.008292861,-0.027168842,0.06817761,0.026833763,-0.04768694,0.026235415,-0.020431835,0.019867813,-0.0010837485,-0.033505887,0.0020740503,0.0019286636,-0.009490012,-0.013689909,0.08507531,-0.037268084,-0.024682073,0.013602543,-0.03226686,-0.09139108,0.0030715892,0.032256793,0.011823108,0.02021959,0.028906563,0.025685282,0.00038720862,-0.06506183,0.0026331153,-0.053262014,-0.0033520495,-0.020882608,0.07576374,0.07474117,0.01066405,-0.045751475,0.007555787,-0.01974034,-0.031117873,-0.00028143567,0.034526464,0.038781606,0.10895289,0.061989274,-0.02381239,-0.023929298,-0.019047339,-0.010003615,0.0049883174,-0.053776983,0.034991626,-0.020855466,-0.020087212,0.031698324,0.01484544,0.014703067,0.0014180853,0.053919196,-0.01617654,-0.010129607,-0.030417759,0.014203964,0.013263353,-0.01592766,0.0012521342,0.07275814,0.016187804,0.0019428213,-0.010571973,-0.038087856,-0.009405278,0.03269698,0.020702405,0.021883499,-0.0039132102,-0.009373039,0.008986292,0.01785669,0.013532278,-0.02589536,-0.06221054,-0.0040442524,0.04486672,0.012096121,-0.017377,-0.014461208,-0.04730245,0.036383674,-0.012415486,-0.03596152,0.05634077,-0.040715802,-0.038007364,-0.010334193,-0.005363838,-0.010122139,0.029830957,0.011647273,-0.024655316,-0.008827641,-0.03934768,-0.009341449,-0.0973611,0.006150516,-0.0284544,0.011805933,-0.023911024,0.0247927,-0.022592984,-0.0017927653,-0.017396327,-0.013210789,-0.000349623,0.020477954,-0.020830091,0.050958917,-0.028360685,0.052777186,0.014210459,0.021781813,0.066298924,0.0016088045,-0.026994403,0.016851746,0.042490207,-0.0085297935,0.02142406,-0.018271638,-0.06717599,-0.015868083,0.0125808325,0.0526096,-0.0045397617,-0.016898375,-0.03307036,-0.031766854,-0.049002,-0.035175778,-0.016008252,0.06558621,-0.009794888,0.025490686,-0.03912346,0.011433926,0.026007423,0.0081984345,0.013880466,-0.01982159,-0.055625256,0.04473674]	Keywords: content-based sparse attention, sparse graph, input content, connections\nKey Objects: sparse graph, connections\nRefers to Images: ./images/a-survey-to-transformers/image_6.png, ./images/a-survey-to-transformers/image_7.png\nHypothetical Questions:\n- How does content-based sparse attention differ from position-based sparse attention?\n- What are the advantages of basing sparse connections on input content?\n- Can you describe an example of how input content might influence the connections in a sparse attention graph?\n---\nSummary:\nContent-based sparse attention methods create sparse graphs where connections are determined by the input content, rather than pre-defined patterns.\nOriginal Text:\n  \nFig. 6. Other types of sparse attentions. The red box indicates the query position, and the orange nodes/squares means corresponding tokens are attended to by the query.  \n  \n4.1.2 Content-based Sparse Attention . Another line of work creates a sparse graph based on input content, i.e., the sparse connections are conditioned on inputs.\nContextualized Text:\nA different approach, content-based sparse attention, creates sparse graphs by conditioning the connections on the input content. This means the connections aren't determined by a fixed pattern, but instead depend on the actual data being processed.	{"tags": ["sparse attention", "content", "architecture"], "doc_id": "8b4dc656-bd1c-4803-8827-7282b1f97186", "summary": "Content-based sparse attention methods create sparse graphs where connections are determined by the input content, rather than pre-defined patterns.", "doc_type": "text", "entities": [], "keywords": ["content-based sparse attention", "sparse graph", "input content", "connections"], "key_objects": ["sparse graph", "connections"], "contextual_text": "A different approach, content-based sparse attention, creates sparse graphs by conditioning the connections on the input content. This means the connections aren't determined by a fixed pattern, but instead depend on the actual data being processed.", "mentioned_images": ["./images/a-survey-to-transformers/image_6.png", "./images/a-survey-to-transformers/image_7.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does content-based sparse attention differ from position-based sparse attention?", "What are the advantages of basing sparse connections on input content?", "Can you describe an example of how input content might influence the connections in a sparse attention graph?"]}
aa4ab28c-e6ef-4879-999f-11759a0eb7d7	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.036424633,-0.039700095,0.01403546,0.05639764,-0.047511484,0.00037884156,-0.03275738,0.046293296,0.035800986,0.019477962,-0.058377314,-0.050280776,0.02202052,0.014922098,0.016191274,0.039579075,0.038237885,0.08158767,0.029571544,-0.0020673994,0.02712407,-0.0010454282,-0.0047702966,-0.0110902805,0.067758024,0.042262673,0.0453997,0.023031816,-0.036399364,-0.010164091,-0.0101917535,-0.016070338,0.0018368723,0.037893593,0.032942444,-0.061743774,0.0043323007,-0.013926712,0.010770287,-0.0017674587,-0.06510262,0.017274542,0.070082285,0.053633966,0.04811685,-0.04547053,-0.09143199,0.038391385,0.029856771,-0.006060955,-0.03204537,0.0071945745,-0.041917767,-0.0040490506,-0.076413445,-0.12610793,-0.06268031,-0.06936944,-0.024679067,-0.0545881,-0.087276705,-0.045283273,0.0028248262,-0.046610344,0.0024488578,-0.0131791355,-0.0026775422,0.01409037,0.043330267,0.16039072,-0.033973087,0.005861783,-0.018207056,-0.029898548,0.10392159,-0.0058012884,0.005905602,0.0015718439,-0.03883034,0.025564304,0.04258334,0.060869798,-0.04254308,-0.017435895,0.09098605,-0.008334229,-0.06779798,-0.042979695,0.028020062,-0.09506056,-0.027425086,-0.067930214,0.0030633365,0.06426282,0.0032859507,0.018675907,-0.045494504,-0.002518377,-0.0019954853,-0.013612293,0.01451202,-0.01840469,0.026399324,0.053219013,0.002094023,0.025441205,-0.0029259676,-0.0050593545,0.005707865,-0.0007781887,-0.067156255,-0.007592965,0.018889133,0.052181613,-0.02699977,0.0615988,-0.057839595,0.01883594,-0.0067917663,0.018980395,0.007919807,0.0054935277,0.004560015,-0.003101077,0.0056636375,0.10382875,-0.06034892,-0.055713322,-0.0014235533,-0.025304973,0.039406687,0.015646035,0.01598014,0.029870769,-0.01522695,0.047624476,0.028636172,-0.010098911,0.0031847032,-0.027311202,0.013547099,-0.01531689,-0.04604944,0.04669437,-0.0036879284,0.026657058,-0.034809902,-0.036025845,-0.013910221,0.055200476,0.021881824,0.00719272,0.029154703,-0.022183862,-0.047265686,0.038312405,-0.0044926275,0.0031441562,-0.024931096,-0.04584537,-0.055683233,-0.006377948,0.044513896,0.059533484,0.059413925,0.038392384,0.053756252,0.020199379,-0.051207326,0.005686769,-0.0065755364,-0.045801964,-0.01033291,-0.027908435,-0.02137061,-0.011085993,-0.03589547,0.019971251,-0.029836813,0.042281467,0.007894062,0.05826661,-0.007982062,0.031834748,-0.021107228,0.039788585,-0.018720608,0.0055147554,0.07262683,-0.02177637,0.009944563,-0.026831636,-0.03692259,0.041921705,0.029642355,0.038030524,0.05599005,0.008678855,0.012181221,0.011254514,0.0011754099,-0.0046509337,-0.0034394583,0.002731385,0.057013847,0.01487651,-0.04104539,0.050250396,-0.04235985,0.071615815,0.022550795,0.019364096,0.019862209,-0.00016324775,0.031564925,-0.0009973096,-0.0051903687,0.030414183,0.009167034,-0.05289184,-0.03028014,-0.04709541,-0.013781266,-0.047570888,-0.050854046,-0.023898648,-0.071639076,-0.014122067,0.03972046,-0.0032515484,0.045038573,0.030207368,0.016938075,-0.0027074923,-0.018165752,0.03181543,0.039059248,0.0022811512,0.023642145,0.022929974,-0.00056253746,0.017151311,0.027692504,0.051262148,0.025686264,0.0026292023,0.018845584,0.008874805,0.04762767,0.008335463,-0.055697106,0.0050487295,0.041291527,0.0045178314,-0.004294818,-0.009836867,-0.029361779,0.034616835,0.02734395,-0.039637394,-0.01546556,-0.010128766,0.007961702,0.039538197,-0.02330367,-0.011740796,-0.028378379,0.021388182,-0.055402305,0.044475373,0.031167518,0.015907213,-0.0067657144,-0.010578046,0.0081764925,0.01831866,-0.015485571,0.0029329334,-0.016594173,0.006363055,-0.0295545,-0.014871817,-0.02043928,0.01962572,0.037441134,-0.043601193,-0.016377522,-0.001270183,-0.0485958,-0.031052602,0.0021329182,-0.017200058,0.041230228,-0.027248723,0.033147912,-0.0016480006,-0.04009691,-0.034578074,-0.02300117,0.037867416,0.085151106,-0.052351024,0.050315857,-0.0051651103,0.016382707,0.00015321276,-0.008247454,0.0027240086,0.031673197,0.038923997,0.0011754418,0.020142637,-0.013677411,0.0235302,0.08477238,0.1121762,0.019021332,-0.044950362,-0.005940818,0.007306845,0.050043948,0.03862511,-0.019706482,0.032473158,0.06071489,0.016375804,0.057190675,-0.008459416,-0.0590892,0.062051967,-0.015685018,-0.047427434,0.063537404,-0.03131129,-0.008370953,0.0547931,-0.008582396,0.026576463,0.058700815,0.0013206535,0.0016407559,-0.03499269,0.013990954,0.012398231,0.009981395,0.02443036,-0.028418474,-0.0040675043,-0.017601103,-0.036472954,-0.008506034,0.0056084157,0.052648008,-0.025043333,-0.022526845,-0.015436316,-0.008322836,0.055452343,-0.00574911,-0.06475744,0.043746885,-0.06785553,0.06090925,-0.061188973,0.05147651,0.054509833,-0.032399464,0.074638516,-0.0298101,0.02496523,0.0045710006,-0.0011221082,0.05216058,0.010795938,-0.021038504,0.0033381006,-0.0011066152,-0.020977627,-0.0062738615,-0.03580474,-0.054766793,0.017327758,0.013629172,-0.044670388,-0.0062943953,0.0061012898,-0.033289663,0.022953968,-0.03479276,0.01806895,-0.025640529,-0.010047977,0.017050505,0.03357734,-0.0313902,-0.044111717,0.02286312,0.03649501,-6.418242e-05,-0.00034093583,0.023021208,0.08803934,-0.052184824,0.03144945,-0.0062935106,-0.0073750187,-0.01737439,0.013820831,-0.00707337,-0.012709962,0.07472383,-0.038048096,0.009814648,-0.009351883,-0.012365185,-0.0012964681,0.025891805,-0.011275572,-0.039153717,0.0010107905,-0.026090773,-0.029240826,0.05238955,0.02436717,-0.0082870005,0.041503448,-0.0103705255,0.044779927,0.039742213,-0.03663143,0.01877591,0.01315219,0.050706387,0.019678216,0.062880844,-0.01434778,-0.019973755,0.051803567,0.0026688394,0.042022806,0.046622287,-0.017395629,0.03144634,-0.0056866226,0.012547951,-0.0021711881,0.019619072,-0.004437927,0.03288415,0.020854253,0.046401627,0.02167398,-0.00012071253,-0.08904293,0.027878439,-0.042837285,-0.04010671,-0.0008517926,-0.04161954,-0.024892956,0.033511832,0.026272466,-0.0054722503,-0.06556122,0.011458744,-0.020947598,-0.026598921,-0.08463253,-0.04823296,0.036709007,-0.045923382,0.011227261,-0.015567285,-0.039613485,-0.03443307,-0.022605749,0.010873503,0.023070468,0.02200648,-0.009059278,-0.062139396,-0.02605196,0.05569362,0.012825432,-0.012179052,-0.032641247,-0.02562022,0.017657934,-0.04875499,0.062303483,-0.011117963,0.04775127,0.013126431,0.012980099,-0.03380823,-0.023745066,0.019535646,0.029120203,-0.016473021,0.030090889,0.032496065,-0.004832587,0.046663437,0.025338637,-0.042213015,0.018745754,0.021119881,-0.055551663,-0.058571935,0.016463215,0.006638314,0.0046472927,0.006795159,0.021153655,-0.07884861,-0.010652673,-0.026770348,-0.022038378,0.0034527928,0.036263574,-0.01776221,0.04149281,-0.006983008,0.020876953,-0.01200739,0.06695632,-0.04663127,0.03740839,0.02067454,-0.0076734074,-0.035486545,-0.034675684,-0.020156268,0.01264304,-0.04023691,0.0018068289,0.007930048,-0.039521035,-0.01730368,0.0105909845,-0.04965605,-0.016128406,-0.02561817,0.061285473,0.070194885,-0.06191802,0.010513366,0.028761154,-0.017590513,-0.006042784,0.07469179,-0.005357063,-0.0033275853,-0.018089116,0.042290322,0.047444567,0.006405663,0.012609217,0.0640255,0.07447309,0.036607936,-0.012890109,-0.008584041,0.004551369,-0.045545697,-0.0132121425,-0.08322474,-0.023328355,-0.06732629,-0.010962817,-0.006985238,-0.030087547,0.032464832,-0.047718275,-0.066989765,0.060035065,0.06683772,0.015233959,-0.034505635,-0.075625636,0.034421146,0.071901105,0.016755186,0.007118218,0.008518233,0.06130243,0.022359483,0.0034244878,0.05454957,-0.02753733,0.016900795,0.0154306,0.008556095,0.015367361,-0.0048517245,0.031408377,0.05340507,-0.040841,0.004754351,-0.009431555,-0.04721494,-0.02590454,-0.004866773,-0.011170918,-0.007245825,0.0075105336,-0.011121962,0.022044322,-0.021582266,0.07393364,0.019564444,-0.019190865,0.04054913,0.013408407,-0.018506637,0.0016796577,0.016968926,-0.012966704,-0.02387694,-0.0514597,0.045976076,0.055166557,-0.021722987,0.004263674,-0.052805595,0.01753091,-0.028596787,-0.019503633,-0.086313814,0.045127343,0.048340943,0.025937295,0.0168565,0.043256413,0.00075359014,-0.08114149,0.0032343317,-0.037296016,-0.030880742,0.06595436,-0.026040923,0.055265337,-0.07416006,0.035109006,0.035415053,-0.045322593,0.0124906525,0.013290173,0.01998241,-0.037247166,0.0039688926,-0.006186328,0.01034396,-0.06535039,-0.009778741,-0.05267391,0.0031029552,-0.007952414,0.049091857,0.015893819,0.05672712,0.03037427,0.007381051,-0.052513525,0.0024775057,0.035813425,-0.0015757898,-0.050909813,-0.00057522405,-0.027779754,-0.027933637,-0.031308297,0.04752184,-0.028529279,-0.017032225,0.014536634,0.0050758757,0.005295966,-0.0031896506,0.044285536,0.011447247,-0.062236693,0.022694977,0.038798634,0.014064684,-0.01921396,-0.03639764,-0.016881324,-0.006221663,-0.010015885,-0.02348914,-0.018867834,0.062172644,0.014115498,-0.015463251,0.028305002,-0.0031857844,-0.0289969,-0.041814353,0.019422855,-0.04862161,0.04332218,0.002538444,0.0016633893,-0.01818825,0.032454867,0.04149612,0.02490634,-0.008207543,-0.0011330907,-0.054556165,0.046416365,0.018626843,-0.012119949,0.06543175,0.002397184,-0.03412707,-0.026934769,-0.011716082,-0.06325578,0.0036216723,-0.073562525,0.03775847,-0.011896638,-0.008992706,0.042649195,0.009222525,0.01325386,0.004023155,-0.020234423,-0.008774913,0.028164355,0.031994443,-0.033574264,-0.038993124,0.03242812,-0.035365965,-0.022002764,0.03911918,0.0072579826,-0.026816176,-0.041504074,-0.038921177,-0.045523774,-0.028388238,0.027751805,0.006827264,-0.016094107,-0.02983091,-0.060903855,0.0538418,0.0046437983,-0.019370178,0.011278706,-0.006852892,-0.05754175,-0.028934248,0.0021735092,-0.004223691,0.04715984,-0.062677845,0.030593475,0.011417087,-0.010559081,-0.003259139,0.013430108,-0.015901498,-0.0036153612]	Image title: Binary Search Tree (BST) Visualization\nTags: tree, binary-search-tree, data-structure, algorithm, graph\nKey objects: Root Node, Child Nodes, Parent Node, Directed Edge, Binary Search Tree\n---\nSummary:\nThis diagram illustrates a Binary Search Tree (BST) structure. It shows a root node connected to several child nodes, which are further linked to their own children, creating a hierarchical tree-like structure. Nodes are linked using directed edges, demonstrating the parent-child relationships within the tree.\nFull description:\nThe diagram depicts a Binary Search Tree (BST) with a single root node connecting to several child nodes. Each child node also has connections to other nodes below, creating branches. The edges between nodes represent the hierarchical relationships in the tree structure. A single label '4.1 BST' is presented at the bottom of the image.\nText found in image:\n- 4.1 BST	{"tags": ["tree", "binary-search-tree", "data-structure", "algorithm", "graph"], "title": "Binary Search Tree (BST) Visualization", "doc_id": "aa4ab28c-e6ef-4879-999f-11759a0eb7d7", "source": "./images/a-survey-to-transformers/image_6.png", "summary": "This diagram illustrates a Binary Search Tree (BST) structure. It shows a root node connected to several child nodes, which are further linked to their own children, creating a hierarchical tree-like structure. Nodes are linked using directed edges, demonstrating the parent-child relationships within the tree.", "doc_type": "image", "key_objects": ["Root Node", "Child Nodes", "Parent Node", "Directed Edge", "Binary Search Tree"], "parent_doc_id": "8b4dc656-bd1c-4803-8827-7282b1f97186", "text_in_image": ["4.1 BST"], "contextual_description": "The diagram depicts a Binary Search Tree (BST) with a single root node connecting to several child nodes. Each child node also has connections to other nodes below, creating branches. The edges between nodes represent the hierarchical relationships in the tree structure. A single label '4.1 BST' is presented at the bottom of the image."}
00d0a22f-0bbf-45a9-8a87-b3bda1a19824	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01634956,-0.024085125,-0.020841435,0.031537294,-0.0456069,0.09281629,0.0058527673,0.039833914,0.006084482,-0.00709711,0.009502995,0.020724962,0.020002754,0.0063312794,-0.05082672,-0.010296659,-0.011169767,0.09013572,-0.012824583,-0.030116694,0.03840383,0.008593516,0.013213413,-0.004653058,0.028443992,0.057845827,0.06717432,0.0011013643,0.025508894,0.0144209955,0.008447013,-0.068463475,0.04511931,0.0057125185,-0.00045648034,0.019016447,-0.023407841,-0.05192518,-0.007885692,0.006150449,-0.03427832,0.053222887,-0.014510919,0.04547015,0.0039960295,-0.04480279,-0.05980481,-0.012798112,0.00061290327,-0.012833061,-0.05258416,0.04900321,-0.05811171,-0.020212095,-0.0058355085,0.0078868875,0.005015015,-0.022832897,-0.00087244617,-0.08256038,-0.031630665,0.012298617,-0.006285006,-0.0061131148,0.007792064,-0.028092844,0.07110516,0.0051433444,0.03515429,0.13168086,-0.0074568437,-0.0005768191,-0.040375866,-0.055674676,0.10583862,0.02814549,-0.017901007,0.016131269,-0.024176538,0.009928282,0.013994139,0.04318303,-0.023357669,-0.016012948,0.07694232,-0.010811658,-0.055296857,-0.012398539,0.053082883,-0.04584551,-0.015735352,-0.011937442,0.023078829,0.065757,-0.012992891,-0.04723373,-0.018182227,-0.041171987,-0.034502313,-0.0942859,0.027745211,-0.0033518586,0.08126138,0.11702953,-0.004715252,-0.017214578,-0.004173304,-0.059228707,0.038130093,-0.017810091,-0.01884334,0.026277293,-0.0718715,-0.0026782686,-0.023153573,0.01941564,-0.05754901,-0.022172267,-0.010633253,0.0058624484,0.026477179,-0.044009183,-0.0065990067,0.019068608,0.049161617,0.05641696,0.0073121204,-0.0194609,-0.038495153,0.017113917,0.04224005,0.0077712485,-0.026971819,-0.015944928,-0.03694975,0.02574552,0.07963431,-0.013134625,-0.028500523,0.01026071,5.3576096e-05,-0.028445158,-0.080689766,0.012495357,0.014258569,0.020343848,-0.06824084,0.03642698,-0.024956854,0.059504412,0.0055870633,-0.030678667,0.02823542,-0.030033125,-0.017669955,-0.017641928,-0.009874712,-0.024314484,-0.022476075,-0.027602866,-0.030175114,-0.002222293,-0.011765329,0.050598904,0.055050444,0.08456403,0.006828388,-0.011767319,-0.0035811872,0.044806324,-0.048465658,-0.024880735,0.03130644,-0.030073337,-0.038699694,0.0032382496,0.026726527,-0.06285677,0.028808445,-0.00025592837,0.011629626,0.010523782,-0.018426748,0.0068840217,-0.0131715955,0.011317112,0.001982497,0.068272434,-0.025348535,-0.008191126,0.01923219,-0.047205504,-0.041701727,-0.007177758,0.014424778,0.048859358,0.03601825,-0.06719649,-0.037186395,0.026862465,-0.03742841,0.05322569,0.0010745729,-0.005985569,0.05505552,-0.008762531,-0.0119028175,-0.0038718972,-0.040621497,0.017576573,-0.01555153,-0.015572838,-0.008171075,0.05336305,0.0014890094,0.016894944,-0.0260688,0.03477243,0.03648896,-0.010045047,-0.057634138,0.022556892,-0.041429542,0.031345192,-0.009691428,0.023725068,0.016376251,0.004103127,0.007208178,0.017429354,0.061770625,0.048683103,-0.04202398,0.04021626,-0.07799209,-0.003269343,0.0468691,-0.013955197,-0.017146349,0.02892098,0.044266477,0.05143456,0.012419258,0.082749195,-0.008330931,0.008673758,0.038704906,0.05009421,-0.020572891,0.02148495,-0.044952042,0.05173154,0.06983445,0.009768797,-0.0052809427,-0.042614415,0.01005221,0.007958199,0.059652437,-0.02926798,0.054602962,0.042125873,0.022174781,0.011601542,-0.016665133,-0.0007431549,0.01962646,-0.027351683,-0.017849682,-0.020948794,0.05762182,0.02555596,0.017365856,0.001204348,-0.043945245,0.05354696,0.029595545,0.014968867,0.063949086,-0.027656842,-0.034550853,-0.023170916,-0.005428199,0.011244559,-0.0032466247,0.038389217,-0.0004815174,-0.046172455,0.0051459093,-0.03838523,-0.016708026,-0.041202836,0.006375119,-0.013856578,-0.031128176,0.023250649,-0.00023824564,-0.017766502,-0.03197474,0.01624038,0.14622283,-0.020825252,0.015405604,0.026653029,-0.005018087,-0.017191345,-0.06340943,0.036384128,0.035090763,0.02038034,-0.016042413,-0.035050996,-0.034793653,-0.013157449,0.044579126,0.06777433,0.057358675,-0.007931611,-0.0039338185,0.011631892,0.041085016,-0.016754758,-0.022537742,0.008662236,0.0089526735,-0.022230128,0.05584282,-0.060962804,-0.07023098,0.079984784,-0.05998553,0.035711106,0.08635224,-0.019711573,0.014658813,0.0016069254,-0.006973874,0.024180489,0.03134039,-0.030863302,-0.04438303,-0.037975807,0.031447526,0.0074164537,-0.010957419,0.013268521,0.04675734,-0.015718691,-0.0015933508,0.03540269,0.04720742,0.031549647,0.023065807,0.011027097,0.014478754,-0.00013704356,0.021456514,0.036260117,-0.020140946,-0.03489304,0.0690107,-0.07639213,0.04477193,-0.03455751,0.017278709,-0.021521138,-0.04668141,0.040930826,0.039065648,-0.0024702682,0.03170923,-0.04581467,0.047388643,-0.021561159,-0.018517649,0.056608833,0.024251971,0.0053741764,-0.0346274,0.0041352916,-0.06041629,0.01669724,-0.03683406,-0.041320264,0.052777905,-0.026269924,-0.024454387,0.08477862,-0.00032885047,0.0015175783,-0.04262867,-0.004487649,0.004701635,-0.009079798,-0.03837608,0.005239846,0.039643936,0.06894255,0.028379662,-0.016997866,0.009779281,0.018340673,-0.06741747,0.003584055,0.01510825,0.0043935305,-0.06542291,0.00476124,0.030241717,0.014979758,0.016352033,-0.036070168,-0.046621736,-0.010998075,0.032501075,-0.012084055,0.012271711,0.0090842005,-0.059062663,0.017402563,-0.040056195,0.01889807,0.05936708,0.012072347,0.01964495,-0.021643301,-0.032286726,0.028429626,-0.0012873617,-0.06672381,-0.00019418371,-0.024228247,-0.042824212,0.030121729,0.0107440585,-0.0065237037,-0.063678965,0.09401985,-0.014093279,0.04830981,0.08405639,-0.0075311903,0.026806613,-0.04254856,0.030299261,0.015321223,0.058665,-0.009283786,0.012798876,-0.032885805,0.045434073,0.01405937,0.050199367,-0.0047581005,0.066075005,0.013367208,0.0065042726,-0.013511943,-0.0051957257,0.015945455,0.008547851,0.0491126,0.033434138,0.02936967,-0.040014982,-0.022631707,0.029004116,-0.0067150523,-0.05011429,0.012530541,-0.014361352,-0.0034970914,-0.015452194,0.052170888,0.0089673195,0.008058615,0.010748191,0.032585803,0.04345942,0.039330564,-0.053881336,-0.0021507859,-0.015584343,0.0048520733,0.0023739105,0.0121824015,0.012693819,-0.017880721,-0.022044621,0.029205825,-0.005863994,0.041944377,0.01921586,-0.010171522,-0.019287499,-0.0034676604,0.021300511,0.005458827,0.01375884,-0.00108026,-0.042824592,-0.0058883936,0.023949724,0.03203751,0.013684204,0.024972212,-0.002353381,0.026331192,-0.050611977,0.070134245,0.02592282,-0.035586894,0.024733307,-0.008814903,-0.023574619,0.033562228,0.011152894,0.008007261,0.016854268,0.017127763,2.2722517e-05,0.014434689,0.053259797,-0.02008185,0.021449216,0.019444307,-0.040230043,0.023417713,0.046301335,-0.06532895,-0.018620085,-0.004773418,-0.008699952,0.021515964,-0.041287113,-0.034176067,-0.0016139891,-0.03912061,-0.01623303,-0.01308071,-0.038777445,-0.045708623,0.02922401,-0.024589444,0.095886685,-0.045623675,-0.009954133,-0.026942125,-0.008601723,0.10362076,0.028075952,0.014679169,-0.030081253,0.0075214277,-0.034845673,0.049944244,0.109587505,0.011184907,0.032751616,0.033512544,-0.0019745138,-0.011356349,-0.038984727,-0.015478332,-0.04283646,-0.033487562,-0.00906379,-0.00029833586,-0.029663429,0.0012481835,0.017490413,-0.011662726,-0.013137343,0.026556378,0.0019940988,0.0785312,0.034940388,-0.009119233,-0.0056074895,0.047802027,0.04059787,0.013903662,0.049654987,-0.036358006,0.030230494,0.007582497,-0.023823667,0.02884156,0.01974542,0.032313656,-0.02747482,-0.08150069,-0.012068024,0.026888711,-0.0039246758,-0.054755867,0.07410175,-0.0021434587,0.015797935,-0.021101454,-0.05125628,0.0041716285,-0.064899854,-0.025327582,0.023964223,0.04098582,-0.011912815,0.040489547,-0.053653955,0.058150556,-0.08762379,0.003623534,-0.03766803,0.004754607,-0.019916706,0.00407224,-0.025735866,-0.03562922,-0.006921029,-0.054136064,-0.014398728,0.043203082,-0.013209734,0.018827522,-0.037554417,0.008549042,0.032931447,-0.01971061,-0.00618402,-0.02224432,0.009520993,-0.02236225,0.06456295,0.0035162114,-0.003886323,0.04117496,0.04106076,-0.07476746,-0.015684998,0.05663877,0.049414113,-0.041214522,0.026145857,-0.009523325,0.019544678,-0.0009945297,0.0013656534,-0.05276241,-0.05549064,-0.028445233,-0.023130279,0.01846273,-0.020564593,-0.043323603,-0.06704974,-0.022958647,-0.045062546,0.06231799,0.03803448,0.063854255,0.019534377,0.07597897,-0.00089250156,-0.006480269,-0.016401142,-0.035602383,0.030115591,-0.032217056,0.035526805,-0.009172174,0.009178027,0.026293138,0.0001632227,-0.016142275,-0.010627853,-0.005790585,0.0035267735,-0.052902058,-0.020462496,-0.0017127477,0.02496168,0.01597641,-0.024376655,0.03235018,0.045901455,-0.019394055,0.0045120507,-0.056450862,-0.044155248,-0.029877095,-0.0014847331,-0.0059724143,-0.0012304471,-0.015356852,0.034256294,0.05605149,-0.001996175,-0.05615727,-0.041052397,0.019145869,0.012115297,0.08307296,0.01403729,-0.012688211,-0.09696325,0.05178867,-0.004969356,0.044143423,0.06245188,0.0069043837,-0.046002816,-0.008597426,0.01387427,-0.021954412,0.008507159,-0.01046061,0.052462596,0.008583117,-0.0004669192,-0.011498503,-0.053636953,0.020107128,-0.045441378,-0.024077497,-0.02451443,0.05076957,0.015346632,0.026917461,-0.018253628,0.011637422,0.0075481175,0.041786153,-0.031925768,0.06794535,-0.008792088,0.041886564,-0.013570335,0.003912636,0.10934762,-0.038274772,0.0010112391,0.017331235,0.013055191,0.012336984,-0.029397734,-0.015319687,-0.009963163,-0.04609105,0.02499797,0.02226094,0.0026724963,-0.024816453,-0.015193087,-0.036955517,-0.05595885,-0.040396668,-0.012827488,0.036363013,0.037884768,-0.0031887335,-0.03329641,0.013314536,-0.05382642,0.033427794,-0.022549342,-0.0015637567,0.01448939,0.0051147207]	Keywords: random feature maps, approximation, attention scores, stability, Performer\nKey Objects: random feature maps, attention scores\nRefers to Images: None\nHypothetical Questions:\n- Why are non-negative attention scores desirable in linearized attention?\n- How does Performer's positive random feature map address the limitations of the trigonometric approach?\n- What are some potential consequences of using random feature maps that do not guarantee non-negative attention scores?\n---\nSummary:\nTo address instability issues arising from the use of trigonometric random feature maps, Performer's second version introduces positive random feature maps that guarantee non-negative attention scores, resulting in improved stability and approximation results.\nOriginal Text:\nAlthough the trigonometric random feature map leads to an unbiased approximation, it does not guarantee non-negative attention scores and thus could lead to unstable behaviors and abnormal behaviors. To mitigate this issue, the second version of Performer [19] proposes positive random feature maps, which uses h ( x ) = exp ( -   2 $\\_{L}$, l $\\_{2}$) , 1 = 1, f$\\_{i}$ = exp and thus guarantees unbiased and nonnegative approximation of dot-product attention. This approach is more stable than Choromanski et al. [18] and reports better approximation results.  \nIn addition to using random feature maps to approximate standard dot product attention, Peng et al. [95] and Choromanski et al. [19] also explore approximating order-1 arc-cosine kernel with h ( x ) = 1, 1 = 1, f$\\_{i}$ = ReLU. This feature map has been shown to be effective in various tasks including machine translation and protein sequence modeling.\nContextualized Text:\nLinearized attention methods sometimes utilize trigonometric random feature maps to approximate the unnormalized attention matrix. However, these maps do not always guarantee non-negative attention scores, potentially leading to unstable behaviors. To overcome this, Performers second version uses positive random feature maps, which ensure non-negative attention scores, leading to greater stability and improved results.	{"tags": ["linearized attention", "approximation", "transformers", "deep-learning"], "doc_id": "00d0a22f-0bbf-45a9-8a87-b3bda1a19824", "summary": "To address instability issues arising from the use of trigonometric random feature maps, Performer's second version introduces positive random feature maps that guarantee non-negative attention scores, resulting in improved stability and approximation results.", "doc_type": "text", "entities": ["Performer"], "keywords": ["random feature maps", "approximation", "attention scores", "stability", "Performer"], "key_objects": ["random feature maps", "attention scores"], "contextual_text": "Linearized attention methods sometimes utilize trigonometric random feature maps to approximate the unnormalized attention matrix. However, these maps do not always guarantee non-negative attention scores, potentially leading to unstable behaviors. To overcome this, Performers second version uses positive random feature maps, which ensure non-negative attention scores, leading to greater stability and improved results.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["Why are non-negative attention scores desirable in linearized attention?", "How does Performer's positive random feature map address the limitations of the trigonometric approach?", "What are some potential consequences of using random feature maps that do not guarantee non-negative attention scores?"]}
b6176c75-b420-46c4-b5fe-6871a65d6b9f	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.078421466,0.028850911,-0.03488233,0.07759872,-0.082147196,0.011784809,-0.037339576,0.020604966,0.053844355,-0.014348336,0.013990541,-0.023272702,0.008592413,-0.03863659,0.020229239,0.097944185,0.011118387,0.09377859,0.0028398833,-0.0050572106,0.03297416,0.017712807,-0.039731856,-0.012898469,0.024059955,0.084940836,-0.030015914,0.013596453,-0.030138075,0.019689836,-0.010209255,-0.018278992,0.03891579,0.021257011,-0.012845408,0.008021928,0.003777475,-0.038342044,-0.020560827,-0.027091084,-0.022057984,0.016338175,-0.0005706518,0.0033584777,0.033571403,-0.028637065,-0.083967306,-0.02190288,-0.0125908265,-0.020432387,-0.06329756,0.079496905,0.010258915,0.018839443,-0.024256632,-0.019603768,-0.052455164,-0.015559138,-0.05072523,-0.010053851,-0.01655318,-0.054593705,-0.039394423,-0.008976958,0.04036882,-0.051050674,-0.012277591,0.04424368,0.045277588,0.14407751,-0.023580635,-0.039849058,-0.048924793,-0.08415766,0.09200188,-0.015467182,0.013012309,-0.014489798,0.00010898219,0.0063241515,0.03477536,0.061618954,-0.047274396,0.011474234,0.052084375,0.01538449,-0.00690736,-0.06628056,0.06731302,-0.09394845,0.026867513,0.0040680673,0.008687601,0.020856215,-0.010806378,-0.035826284,0.0034694814,-0.005685641,-0.031660166,-0.067907,0.042257577,-0.03403358,-0.0037994168,0.056757517,0.006194475,0.012379657,-0.00013397772,-0.01877314,0.03044636,0.05264415,-0.007521132,0.016006324,-0.0144932,0.03662661,0.05402553,0.058088556,-0.06043064,0.005809591,0.07756912,-0.029799413,0.0405618,-0.0068075377,0.009895681,0.0074206,0.044850826,0.03996491,-0.03555351,-0.058609806,0.0016838511,-0.019125998,0.047854457,0.031838417,0.013439023,0.031456735,-0.07887399,0.054783896,0.008129053,0.010929588,-0.015505152,-0.009040581,0.0011188383,-0.05931816,-0.023100605,0.06164369,0.017395828,0.017058602,-0.042516325,0.015531693,0.014636614,0.0993997,0.013932259,-0.028336769,0.030891512,-0.022491122,0.0034145478,-0.010708851,-0.054595113,0.04236359,0.029512506,-0.06441773,-0.037140016,-0.046777453,-0.010638528,0.07356935,0.06466828,0.042722218,0.057683688,0.020721821,-0.036458913,-0.077599525,0.006959151,-0.059075717,0.012453945,-0.00051333575,-0.007974916,-0.029669471,-0.0037983737,-0.026794653,0.033007465,0.0049022892,-0.0025636798,0.027482405,0.040022075,-0.019262413,-0.00948973,0.071027145,0.02051508,0.03448335,0.021502614,-0.036943566,0.0040280693,-0.035356354,-0.02334207,-0.0007492542,-0.0004113788,0.0321539,0.03852377,0.026788106,0.04773244,-0.021891778,0.008259427,0.006368902,-0.012754086,-0.017061362,0.0045353393,-0.002792632,-0.021557907,0.051101066,-0.0021013035,0.047998574,-0.036393654,0.02528701,-0.00599562,0.005022598,0.014328687,0.049165968,-0.01544157,0.047901537,0.028263783,-0.028032355,0.008517743,-0.034939874,0.011470438,-0.010866206,-0.017722754,-0.05941464,-0.06428366,-0.029276403,-0.012009933,0.07966121,-0.008124726,0.013261811,0.013128148,0.044923786,0.0046662083,0.040762424,8.1643564e-05,0.03861174,-0.0064797592,0.0778381,-0.045198925,-0.0005816579,-0.018106364,0.045916677,0.049025748,-0.0248643,0.042625144,-0.031187661,0.06437649,0.039884582,0.010230164,0.039793123,0.07243934,-0.037372846,0.029647745,-0.027120333,-0.0024483805,0.013461276,0.04243989,-0.032879047,-0.026483241,-0.022184012,-0.011255156,0.005385018,-0.015282638,0.013727463,0.01998121,-0.019763486,-0.06427689,-0.038648866,0.044861767,0.042861033,0.013283095,0.03564934,0.016043015,0.0068743476,0.0031996144,0.062976964,0.0030414981,-0.020390255,-0.03877243,0.016537884,0.044783458,0.01718215,-0.0036298214,0.03162158,0.009884377,-0.046342466,-0.042969372,-0.035758242,-0.007493616,-0.03982934,0.028170858,0.018502444,0.029728737,0.028742779,-0.00082116254,-0.0089455135,0.013073324,0.015417898,0.096554324,-0.026916938,-0.010990488,-0.00043157215,0.01871763,-0.0017374846,-0.033064872,0.0054665618,0.002215587,0.044361096,0.021525359,-0.025367089,0.0047766315,-0.01177845,0.0350434,0.12300626,0.063777745,0.0022444976,0.02227673,0.0629839,0.06521246,0.0026529983,-0.03659282,0.05499732,0.05384149,0.059021834,0.06486555,-0.054210708,-0.03443788,0.040573988,-0.035346057,0.08173745,0.07502214,0.012882011,0.05188012,0.01147102,0.057326704,-0.027731925,0.030849962,0.030056674,-0.04047215,-0.004548005,0.03243824,-0.006169745,0.021295438,0.04277639,-0.01641957,0.048831236,-0.006457065,-0.028725473,-0.023875525,0.026238788,-0.00012306248,-0.044600826,-0.04019424,-0.004868965,0.0064833076,0.025905218,0.03818326,-0.053869933,0.027575579,-0.047211852,0.0066629536,0.0077128564,0.013082892,-0.04440934,-0.017562225,0.014417997,0.01187629,0.04373869,-0.063684784,-0.010668971,-0.00071123836,-0.0065744496,0.01808452,-0.005013449,-0.0033096408,0.011593879,-0.0063906056,0.0140237985,-0.0015333463,-0.01512026,-0.04936827,0.00033350385,-0.0023020953,0.041558765,-0.05377246,0.017082179,0.0015242492,0.0135689005,-0.013590942,0.03594782,0.0048229783,0.024757579,-0.0016612075,0.00028814338,-0.006890631,0.06701858,0.041349467,-0.04740159,-0.02533495,-0.0061874455,-0.055031314,0.00057131436,0.045039855,0.02944029,-0.07652033,-0.041906606,0.019234981,-0.0027296757,0.022523426,-0.06832111,0.03297649,-0.029987467,0.034649894,-0.037829414,-0.03279099,0.03347838,-0.04163053,-0.03645781,-0.018972369,0.028410604,0.04342898,0.046196774,-0.048983116,-0.0062452545,0.025632543,0.062318724,-0.027712557,-0.050444666,0.036989056,0.010109535,-0.017686376,0.051795803,0.026713924,-0.022842549,-0.008716046,0.057099905,0.026661336,-0.04183464,0.0316806,-0.05229904,-0.008256054,-0.03462974,0.030010896,0.07056155,0.04042621,-0.0077532297,0.041429017,0.009943356,0.032034557,0.017252795,0.024128119,-0.012772497,-0.024549056,0.009430579,-0.024322158,-0.06270469,-0.0051831775,-0.014544005,-0.006880499,-0.0302131,-0.064695016,-0.02248125,0.021575306,-0.028551327,-0.020927435,-0.022249823,0.018115018,0.0173098,-0.093646154,0.009315503,0.035906445,0.032454718,-0.06662552,0.04410316,0.012299577,-0.012456518,0.052772824,0.031154322,-0.013076704,0.0036816623,0.025133867,-0.028156353,-0.036542326,-0.063617155,-0.011505326,-0.018445775,-0.033197954,0.02039868,-0.02052127,0.020981751,-0.015420886,0.040582813,-0.024972191,-0.018319758,-0.026269019,0.0038924043,0.006109882,-0.0015118222,-0.004234431,-0.030213816,0.025692854,0.05706907,-0.01646324,0.02683931,-0.009028636,0.033910364,-0.02719518,0.023919122,-0.046805423,-0.010009972,-0.019109517,0.03891098,-0.006482015,0.0110956915,-0.02849815,-0.018390732,0.0142167155,0.008556767,0.054395292,0.04572239,0.06341812,0.017398017,0.059331093,0.0010133185,-0.069446385,0.00304066,-0.00025536306,-0.0512219,0.020085476,-0.011265717,-0.04230161,-0.042331766,-0.077218875,-0.08104452,-0.019110776,-0.0058617783,-0.028717337,-0.012139347,0.020986835,-0.05365387,-0.0036277976,-0.008558172,0.034624938,-0.009031676,0.01965697,-0.030538369,-0.042071547,-0.016451735,0.02976482,0.056434143,0.04384351,-0.017312206,-0.0053758514,0.022819035,-0.03247963,0.029401291,0.03422885,0.03901645,0.088004135,0.030106004,0.010905939,-0.06548264,0.035847977,-0.034516625,-0.012090656,-0.06063571,-0.075605504,0.042130727,-0.067498595,0.0069460655,-0.024674896,-0.026656056,-0.040101115,0.052282143,0.035498153,-0.04571052,-0.038802758,-0.017800523,0.035242494,-0.017753031,0.020633267,0.012646588,0.030678676,0.022323562,-0.032261875,0.05984432,0.0025585692,0.037438344,0.010162211,-0.025574638,-0.005423037,-0.06797855,-0.017270107,0.008272119,0.0072179753,0.034010865,-0.0053509763,-0.065207794,-0.079063095,-0.008638702,-0.015670441,-0.0045017684,0.033291746,0.009991704,-0.01569669,-0.009017438,-0.02046825,0.001385883,-0.03572618,0.017902441,0.030676015,0.03422923,-0.041176073,-0.047301814,0.030640801,-0.021866648,-0.055336017,-0.018033173,0.015769066,0.022586597,0.02294953,-0.014112393,0.0012913798,0.029150724,-0.00514294,-0.025418982,-0.03175849,0.03888551,0.0105060795,-0.009256207,0.031415727,-0.0029103972,0.0035189746,0.016860543,0.044442564,0.003985365,-0.016090883,0.018585842,0.061467998,0.043009456,0.0058232644,0.027188668,0.012361506,-0.05140264,0.022023551,-0.008801112,0.026062995,0.024359273,0.030704875,0.06407407,-0.022949815,-0.05454566,-0.008890159,-0.044192765,-0.037784852,0.056529097,0.06302861,-0.03239542,0.05145914,0.059537362,-0.042365957,-0.03244801,-0.001250989,-0.004392257,0.013813954,-0.029323343,0.014656173,-0.05006662,-0.023702256,0.002046416,0.0048059127,0.037056915,0.014809631,-0.012789727,0.010932927,-0.0352413,-0.002259605,0.009909005,0.032453977,-0.05466693,0.013704032,0.0030759787,0.047203258,0.017919114,0.015472527,-0.03707881,-0.00090991135,-0.001863487,0.015295448,-0.037096728,-0.000106095875,-0.0041610193,0.030298155,-0.040936824,0.017224526,-0.019798283,-0.095587105,-0.019548846,-0.04648369,-0.016917495,-0.05179743,-0.014511825,-0.050840102,0.04658971,0.007176685,0.016618136,0.04788701,0.018670674,-0.049161643,-0.019373242,-0.0004179259,-0.036008596,0.101525806,0.027019266,0.011379561,-0.0073905867,-0.029349608,-0.0068112514,-0.0464729,0.0072233723,0.027021764,0.0041222395,0.06784518,-0.008304131,0.017430585,-0.026640752,0.045873955,-0.05598277,0.0057103983,0.022871444,0.033955168,-0.042083565,-0.0053100185,0.009174779,-0.035362415,0.04276437,0.06599532,0.025174698,0.022210166,-0.017633233,0.014330797,-0.006524158,-0.027716523,0.05667044,-0.050909087,-0.017334377,0.024089774,-0.047626242,0.014748134,-0.034188405,0.0102735385,-0.0045453464,-0.03610017,0.0063363886,0.035845835,0.013905827,-0.009599206,-0.0091032935,0.027369551,0.017137382,0.01201054,0.012564763,-0.05433162,-0.006998039,-0.0067663076,-0.00051274034]	Image title: Diagrams of Grid Paths\nTags: grid, path, line, turn, direction\nKey objects: Grid, Path, Sharp Turn, Straight Line, Vertical Line\n---\nSummary:\nThe image displays two diagrams illustrating paths within a grid. The first diagram shows a path with a sharp 90-degree turn, while the second diagram showcases a straight-line path extending vertically through the grid.\nFull description:\nThe image consists of two separate diagrams set against a grid background. The first diagram, labeled '(1) Sharp Turn', depicts a path that abruptly changes direction at a 90-degree angle. The second diagram, labeled '(2) Straight Line', illustrates a path that proceeds directly upwards in a vertical orientation through the grid.\nText found in image:\n- (1) Sharp Turn\n- (2) Straight Line	{"tags": ["grid", "path", "line", "turn", "direction"], "title": "Diagrams of Grid Paths", "doc_id": "b6176c75-b420-46c4-b5fe-6871a65d6b9f", "source": "./images/a-survey-to-transformers/image_7.png", "summary": "The image displays two diagrams illustrating paths within a grid. The first diagram shows a path with a sharp 90-degree turn, while the second diagram showcases a straight-line path extending vertically through the grid.", "doc_type": "image", "key_objects": ["Grid", "Path", "Sharp Turn", "Straight Line", "Vertical Line"], "parent_doc_id": "8b4dc656-bd1c-4803-8827-7282b1f97186", "text_in_image": ["(1) Sharp Turn", "(2) Straight Line"], "contextual_description": "The image consists of two separate diagrams set against a grid background. The first diagram, labeled '(1) Sharp Turn', depicts a path that abruptly changes direction at a 90-degree angle. The second diagram, labeled '(2) Straight Line', illustrates a path that proceeds directly upwards in a vertical orientation through the grid."}
891d332a-cfeb-465d-9bd5-1231083af41f	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.021916052,0.0072748507,0.0045533506,0.018031154,-0.00051890966,0.04826961,-0.038103357,0.052853577,0.026555097,-0.029857963,0.0024545486,-0.010093578,-0.004263859,0.00020330382,-0.0043457085,0.01298715,0.025909046,0.11094042,-0.024507347,-0.009683509,0.07506725,0.011276523,0.017814811,-0.0057573365,0.015932353,0.045016486,0.028305773,0.00056536286,0.04332986,-0.024439154,-0.022403512,-0.077340126,0.038151592,-0.02343143,0.031952087,-0.019695977,0.019886643,-0.079410695,-0.016415667,0.039703805,-0.021243867,0.062655255,0.007564921,0.0076893736,0.04282026,-0.062683016,-0.095482014,-0.033850048,0.04213974,-0.034784254,0.02439146,0.026451418,-0.039135143,0.015027291,-0.04166251,-0.03546883,-0.009250748,-0.08356355,0.0051849396,-0.08178249,-0.030940158,0.031711586,-0.04094033,-0.018334245,0.03561097,-0.016436026,-0.006422419,0.0052771103,0.062009413,0.12842852,-0.008958274,0.020869527,0.003375803,-0.10704665,0.1131986,0.049410306,0.00040658156,-0.0016309425,-0.06292325,-0.016178377,0.022110399,0.07362565,-0.05378098,-0.006932778,0.093401246,-0.040754177,-0.0017279277,0.0028477293,0.022343876,-0.03753329,-0.013688249,-0.011191241,-0.03175473,0.09762925,-0.030803232,-0.045486383,0.04536984,-0.016448235,0.011980637,-0.08381704,-0.0031826256,-0.008858676,0.057065126,0.09720769,-0.015495994,-0.029832061,0.006989756,-0.013900804,0.03141029,0.002170728,0.00034322348,0.044847976,-0.08124303,0.012465862,-0.021599019,-0.005268139,-0.053045817,0.013880218,-0.02209882,-0.032907397,0.00017078902,-0.016953595,0.032281373,0.019391919,0.03626339,0.071402885,0.051559027,-0.04480242,0.003652296,0.012896377,0.057429112,-0.026402995,-0.0137325125,-0.017590914,-0.024602713,0.028233243,0.048272833,-0.03342117,-0.051429875,-0.0013232664,-0.0044867,-0.032597333,-0.049315646,0.03998892,0.02765793,0.023215698,-0.083437726,0.021163955,-0.033634532,0.08769523,0.046739794,0.04034138,0.054521102,0.033447538,-0.05215741,-0.02168813,-0.0054065664,-0.03136028,-0.013797918,-0.045705955,-0.045564476,0.013467279,0.0047633545,0.075778715,0.064885296,0.11282661,-0.028855756,-0.00038167907,0.034149546,0.03424704,-0.012434604,0.0022477296,-0.0017235219,-0.023668734,-0.016274804,0.041713413,0.00681435,-0.018238507,-0.007044122,-0.017436072,0.017730648,0.091847,-0.019309143,-0.010010857,-0.02163089,0.057600588,-0.016476028,0.0717138,0.0040798676,-0.01767469,0.023752864,-0.03318987,0.011383636,0.020260083,0.022950066,0.051434483,0.049861748,-0.03937125,-0.0065845973,-0.0350246,-0.0055483407,-0.008891204,-0.006135251,-0.005832095,-0.0044594775,-0.0033293567,0.00063023245,-0.04451493,0.0018408184,0.008717763,0.007736965,0.01109428,-0.05037895,0.008269656,0.026967756,0.039196465,-0.009273087,0.035826787,0.027046438,-0.015335917,-0.056592472,0.02376239,-0.042352002,0.044636767,-0.007308465,0.0552559,-0.0020452242,-0.0010716161,-0.013337739,-0.01699242,0.018989448,0.0093722725,-0.040408425,0.020131709,-0.041760318,0.029459639,-0.0070440737,0.01330925,-0.024973286,0.025747098,0.024793409,0.054732352,-0.027770706,0.104383156,-0.02809991,0.027128372,0.0505097,0.037384443,0.025556643,-0.005927883,-0.011035744,0.044337254,0.027315235,0.019494573,-0.023368428,-0.008260136,0.015110276,-0.0004951917,-0.0053797346,-0.0065685655,0.03286254,0.014761119,0.02363247,0.028404888,-0.017530052,0.011452482,0.029068833,0.018544603,-0.030664757,0.025129542,0.024102071,0.036420673,-0.02444107,-0.023059215,-0.029814452,-0.0007146203,0.075066485,-0.0123685235,0.036418393,-0.039249334,-0.04861678,0.010316282,0.017782524,0.021594135,0.022561342,0.023059698,0.06481199,0.02728391,-0.05263992,0.026904494,0.023143878,0.007667501,0.029796466,-0.048876762,0.0095532555,0.016725628,-0.00028122036,-0.03771574,-0.0028163937,0.016581165,0.09186856,-0.03328496,0.0593958,0.05470956,0.060789242,0.015114097,-0.055673774,0.04865685,0.017697519,0.0032740382,-0.009061815,0.04458413,-0.017592499,-0.018825023,0.004674012,0.054657236,0.108530626,0.033364892,-0.0099694505,-0.014239905,0.006342337,0.018364985,0.005940885,0.010894768,0.042517286,-0.005562932,0.052154865,-0.009382038,-0.066743925,0.04838003,-0.04232096,0.0015539165,0.072760135,-0.025249202,0.008320992,0.02813414,-0.027716205,-0.0031346234,0.004749848,0.00431731,0.019698992,-0.041705154,0.021726383,-0.06311673,0.021592584,0.053157784,-0.029824728,0.011902615,0.0005572922,0.0137231015,0.033957854,0.023964636,0.02308394,0.004425578,-0.029066706,0.02438024,-0.007934722,-0.01574419,0.019311437,-0.0030819026,0.06354246,-0.10258595,0.06529297,-0.05505753,0.049085416,0.0078060967,-0.04044107,0.041313294,0.079428665,0.004708335,0.015282214,-0.033847474,0.0601984,-0.039715495,-0.026802294,0.059381273,0.023943398,0.054490365,0.02745319,-0.031040331,-0.0022390434,0.012495716,-0.01004138,-0.042282853,0.018232355,-0.025366908,0.012817007,0.06179325,-0.040989127,0.08875227,-0.023067188,0.033294316,0.02055796,-0.029468182,-0.008509725,-0.015672054,0.0112574855,-0.00038338016,0.031863526,-0.024965882,0.027748812,0.015268247,-0.054423608,0.023353867,0.016967421,0.047710363,-0.10990106,0.027740452,0.010793825,0.031236583,0.006443429,-0.062529504,0.0021907114,0.004707187,-0.0025915392,-0.008411337,0.0009701894,0.012666626,-0.0649922,-0.016500233,-0.011218473,0.021183617,0.0715126,0.053724565,0.008479364,0.008001564,-0.044116184,0.060223237,-0.023095002,-0.0496131,0.009218466,-0.026976846,0.014350936,0.024663292,0.0064418004,0.020131389,-0.038895372,0.068235956,-0.0018563989,0.010607301,0.07826793,-0.030216774,-0.006522735,-0.035568297,0.0053475183,-0.0037309597,-0.005354005,0.004746046,-0.031950418,0.0004747194,0.025665736,0.022421708,0.0120450845,-0.014386168,0.059310384,0.004212535,-0.02434927,-0.011520528,0.006743093,0.04590633,0.00029874448,0.022272764,-0.017817063,-0.039032575,-0.025234334,-0.015444168,-0.014096365,-0.025005821,-0.031090742,-0.022843877,0.00579742,0.028300256,-0.00022948472,0.038290963,-0.042155217,-0.0065579885,0.002640818,0.036808763,0.028004501,0.01965084,-0.05339309,-0.0542074,0.003235255,-0.010812352,0.024866397,-0.0055558574,-0.0024366623,-0.013412307,-0.021954224,0.0402803,-0.020335166,0.010154374,-0.0071275206,0.027910808,0.028709648,-0.034861702,0.01409595,-0.02825674,0.041015845,-0.009344341,-0.011955554,0.011452837,0.021587284,0.07472937,0.0019999398,0.02062314,0.022921199,-0.0159559,-0.013431831,0.018060064,0.025292166,-0.022210876,-0.005879515,-0.021587797,-0.032173432,0.014212256,-0.016578108,-0.0030722178,-0.039194666,0.07035665,0.04951684,-0.019046383,0.036462177,-0.00013304048,0.014449843,0.03998273,0.02009505,0.023386661,0.07131362,-0.063112475,-0.008849276,-0.09124933,-0.0008364549,0.013262038,-0.06532752,-0.04582965,0.008713599,-0.0945494,0.0014962705,-0.022773955,-0.051475476,-0.019673973,0.0149653535,-0.03557995,0.03312538,-0.02047358,0.0036557605,0.05217259,0.0066226455,0.07877781,0.029298529,0.011876047,-0.015540447,-0.048181042,-0.015939817,0.06705147,0.029086588,0.011840559,0.009112232,0.01891402,0.042099964,-0.010879273,-0.038747363,-0.049587756,-0.017692043,0.0070143854,-0.016951261,0.01091194,-0.03479006,0.016162751,-0.012305586,0.004294371,0.0045705736,0.0017389017,-0.004202547,0.019532101,0.04477835,-0.017015003,0.033543147,-0.030938366,0.055070233,0.005939309,0.07254826,0.009176831,-0.0043579,0.03512144,0.008076677,0.035264477,0.026873142,-0.011213506,-0.0056828987,-0.045935735,0.026455777,-0.05230155,0.0129732555,-0.041382924,0.045870632,-0.0026248307,-0.0043732794,-0.011897904,-0.012951116,-0.054045685,0.020282205,-0.013757545,-0.0059846845,-0.0013113865,-0.005581773,0.061646733,-0.017224628,0.028056268,-0.055426978,0.0044325115,-0.004054712,0.038716625,-0.001003166,-0.006135555,-0.054567356,0.0077140443,0.00052956154,-0.002803827,0.05909679,0.00029103822,-0.01212927,0.038579516,-0.027269341,0.010172828,0.006674038,-0.05713207,-0.036946632,-0.023181679,-0.010605403,-0.0028393639,0.062863655,-0.043697473,0.003914456,0.0072888015,-0.022833487,-0.086433224,0.021156972,-0.0035304092,-0.0012588543,0.031081628,0.04132552,0.013710314,-0.020729939,-0.027129242,-0.014824476,-0.08808473,-0.0077742194,0.00014418806,0.046548247,0.028151676,0.013071171,-0.02576951,-0.011951309,-0.03623123,-0.035160024,0.01307669,0.04814789,0.025857659,0.074821964,0.019946914,-0.04544403,-0.07354213,-0.027925061,0.016966086,0.011335282,-0.08269261,0.03245656,-0.018154481,0.00080654136,0.036588077,0.0042480193,-0.006949726,-0.038160782,0.03526759,-0.039367568,-0.020322653,-0.056561347,0.0031283053,0.0031361314,-0.012385891,-0.009372874,0.042768672,0.008601562,-0.024891432,0.0039087604,-0.090538375,-0.028933644,0.012388168,-0.020210285,0.013896416,0.012949017,-0.0586732,-0.016807394,0.013893923,0.025744231,-0.023580886,-0.031046966,-0.02020139,0.045145076,0.0042892196,0.0022514954,-0.033274814,-0.06813071,0.039039258,-0.0014960826,-0.04370278,0.039739665,-0.019786386,-0.051938027,-0.0048013343,0.0031745115,0.004780436,0.03875849,0.01447218,-0.016173897,-0.019753674,-0.008551207,0.0023465995,-0.0835939,-0.028518666,-0.009112756,0.008877792,-0.028639253,0.043970726,0.008190754,0.0033261997,0.026522432,0.007291443,0.011375496,0.0048478805,-0.021833101,0.031083582,-0.032030243,0.071823195,-0.027624335,0.019022454,0.050842736,-0.021426216,-0.031283963,0.039865967,0.03089656,0.0062319622,0.0126929,0.02119652,-0.052940056,0.006987359,0.016262574,0.041385684,-0.007425992,0.010815047,-0.07787645,-0.021286504,-0.043679986,-0.02943724,-0.008180128,0.050638594,-0.007824303,-0.006490026,-0.04675935,0.05381714,-0.028674375,0.04939648,0.0028117332,0.0018473255,-0.045928553,-0.004402452]	Keywords: content-based sparse attention, maximum inner product search, k-means clustering, routing transformer, sparse graph\nKey Objects: queries, keys, cluster centroid vectors, sparse graph\nRefers to Images: None\nHypothetical Questions:\n- How does Maximum Inner Product Search improve the efficiency of content-based sparse attention?\n- What are the advantages of using k-means clustering for content-based attention?\n- How does the grouping of queries and keys by k-means clustering affect the model's ability to capture complex relationships in the data?\n---\nSummary:\nTo create content-based sparse attention, a common approach is to select keys that are likely to have high similarity scores with a given query, often utilizing techniques like Maximum Inner Product Search (MIPS). The Routing Transformer employs k-means clustering to group queries and keys, ensuring each query attends only to keys within the same cluster.\nOriginal Text:\nA straightforward way of constructing a content-based sparse graph is to select those keys that are likely to have large similarity scores with the given query. To efficiently construct the sparse graph, we can recur to Maximum Inner Product Search (MIPS) problem, where one tries to find the keys with maximum dot product with a query without computing all dot product terms. Routing Transformer [11] uses k-means clustering to cluster both queries { q$\\_{i}$ } T i - 1 and keys { k$\\_{i}$ } T i - 1 are same set of centroid vectors { $\\_{i}$ } T i - 1 k$\\_{i}$ . Each query only attends to the keys that belong to the same cluster. During training, the cluster centroid vectors are updated using the exponentially moving average of vectors assigned to it, divided by the exponentially moving average of cluster counts: $\\_{i}$   $\\_{i}$ + (1 -  ) (  q$\\_{i}$ q$\\_{i}$ +  j = $\\_{j}$ ( k$\\_{j}$ ) , (8) c$\\_{}$   c$\\_{}$ + (1 -  ) $\\_{i}$ , (9)    $\\_{i}$ , (10)\nContextualized Text:\nA method for content-based sparse attention involves selecting keys that likely have high similarity scores with a query, often leveraging Maximum Inner Product Search (MIPS). As an example, the Routing Transformer uses k-means clustering to group queries and keys, which limits attention to keys within the same cluster.	{"tags": ["attention", "sparse attention", "NLP", "transformers", "content-based"], "doc_id": "891d332a-cfeb-465d-9bd5-1231083af41f", "summary": "To create content-based sparse attention, a common approach is to select keys that are likely to have high similarity scores with a given query, often utilizing techniques like Maximum Inner Product Search (MIPS). The Routing Transformer employs k-means clustering to group queries and keys, ensuring each query attends only to keys within the same cluster.", "doc_type": "text", "entities": ["Routing Transformer"], "keywords": ["content-based sparse attention", "maximum inner product search", "k-means clustering", "routing transformer", "sparse graph"], "key_objects": ["queries", "keys", "cluster centroid vectors", "sparse graph"], "contextual_text": "A method for content-based sparse attention involves selecting keys that likely have high similarity scores with a query, often leveraging Maximum Inner Product Search (MIPS). As an example, the Routing Transformer uses k-means clustering to group queries and keys, which limits attention to keys within the same cluster.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does Maximum Inner Product Search improve the efficiency of content-based sparse attention?", "What are the advantages of using k-means clustering for content-based attention?", "How does the grouping of queries and keys by k-means clustering affect the model's ability to capture complex relationships in the data?"]}
4f6d48cc-69da-48ca-847d-e6654b6e3f4d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.013943935,-0.0012869615,0.020366238,0.047369298,0.008418424,0.04753237,-0.015080292,0.0123658385,0.015428406,-0.004283512,-0.01775956,-0.00717444,0.0021849219,0.01635544,0.0028706149,0.024722924,0.0423121,0.07541889,-0.019680442,-0.038085967,0.055080764,0.02918858,0.015083191,0.027362186,0.015629793,0.03897495,0.029269965,0.024234122,-0.0065102563,-0.018329514,0.0013669546,-0.05619864,0.026803276,-0.029744834,0.023454541,-0.038070586,0.025658803,-0.0514852,-0.0033833862,0.0056137834,-0.009358709,0.06262992,-0.027706025,0.0069467505,-0.005514937,-0.052130464,-0.078735374,-0.038990945,0.04842523,0.008871621,-0.0029198902,0.01345751,-0.04031813,-0.030804383,-0.059878077,-0.036826972,0.012437071,-0.079407535,0.0030486095,-0.05466819,-0.014867373,0.041798823,-0.014889596,-0.027062362,0.040688895,-0.04353682,-0.017070893,-0.0101954825,0.03637868,0.118595436,-0.0164686,-0.020011785,-0.022650437,-0.10466309,0.07700823,0.07404196,-0.013124403,-0.03172463,-0.09605727,-0.030169822,0.016395628,0.06903883,-0.025007674,-0.0119577665,0.081888236,-0.03257868,-0.013192418,0.027787147,0.043620676,-0.06889624,-0.0005881798,-0.013807738,-0.008336626,0.090689376,-0.022992065,-0.0543353,0.0065432205,-0.015277756,0.013316409,-0.07842471,-0.010553116,0.018284582,0.06486854,0.11725214,-0.008889561,-0.026937068,-0.036231406,-0.015407413,-0.00029473918,0.005348138,-0.016010782,0.026819756,-0.07483717,0.0023536682,-0.017375391,0.008405864,-0.049048863,-0.009958967,-0.0012371233,-0.014320689,0.031866953,-0.034284294,0.00159773,0.02696907,0.07236953,0.06531693,0.04444757,-0.04795099,-0.007779754,-0.0110566495,0.06258578,0.021369742,-0.04098062,-0.0065985275,-0.019732077,-0.014821153,0.058429535,-0.02273871,-0.026596587,0.00804674,-0.0057439273,-0.011716526,-0.0316163,0.03205534,0.02034449,0.016854087,-0.07077678,0.030001946,-0.044646945,0.05902658,0.042235162,0.007889338,0.06038715,0.004974378,-0.018048296,-0.031348493,-0.016775299,-0.014306319,-0.0028373748,-0.029167464,-0.046256654,-0.024802022,0.002172211,0.056391407,0.06339449,0.11479373,-0.025922485,-0.0026033337,0.028468043,0.01666821,-0.006925062,0.024234721,0.008960219,-0.02284132,-0.007393507,0.020825189,0.026343727,-0.0477589,-0.031555474,0.0045253937,0.02444661,0.083869115,-0.0004686682,0.016400171,-0.034490652,0.026900742,-0.012671074,0.04056545,0.003130302,-0.026801115,0.02126766,-0.016213862,0.012924281,0.04382041,0.037042856,0.06516596,0.03962399,-0.071516745,-0.024897702,-0.029071128,0.004469776,-0.011867817,-0.010347051,-0.014444284,0.011983468,-0.035121758,0.038429793,-0.035212554,0.028586352,0.010128448,0.016713528,0.0026255415,-0.035860308,0.041352797,0.029735636,0.011504569,-0.013632314,-0.0027842487,0.06506257,-0.034953598,-0.05201674,0.026266947,-0.017755713,0.05025371,-0.002442267,0.040488694,-0.01273801,0.009308733,0.006293045,-0.032088306,0.008056608,0.01717463,-0.03781422,0.008200127,-0.02510333,0.030143475,0.005494162,-0.0122397365,-0.03638649,0.033599727,0.047433615,0.040642563,0.00027624238,0.07034383,-0.036334682,0.007448066,0.04000222,0.03183621,0.02298665,0.043739777,-0.010495106,0.032778542,0.03406558,-0.019569274,-0.0003579069,-0.05100448,0.0031645705,0.033877883,-0.0022115698,-0.030385273,0.030195948,-0.011727555,0.035094075,0.014085196,-0.008971729,0.006402375,0.007800428,0.042254414,-0.050400343,0.0014650376,0.029620364,0.03701221,0.0007963119,-0.030016731,-0.02925541,0.0032919256,0.03597434,-0.031808566,0.018107489,-0.021187685,-0.018013818,0.0056478097,0.052794803,-0.003406889,0.051941816,0.012204727,0.036014076,0.038714126,-0.049828414,0.0028210557,-0.018424422,-0.025182916,0.010683668,-0.007805421,0.011663415,-0.002745283,0.012602512,-0.0859024,-0.0034707845,0.013171925,0.09132079,-0.028845891,0.046625335,0.038086478,0.06039894,0.0066754953,-0.024806013,0.046330906,0.03537088,0.002287855,0.0100643365,0.013542559,-0.0351809,0.00026362095,0.01210461,0.09484486,0.10381403,0.008409587,-0.0020120032,-0.0148622235,-0.010196008,-0.004276526,-0.013722985,0.04529644,0.035553787,-0.0074915634,0.04019715,-0.0040222122,-0.06898128,0.06266246,-0.04719603,0.03542286,0.03385217,0.0026005213,-0.0073528667,0.016918337,-0.035203483,0.01637311,0.032852706,0.04595506,0.013144013,-0.04851398,0.040026266,-0.04057938,-0.025822798,0.05407831,-0.02060335,0.0065200413,0.03632073,0.0048474483,0.009151772,0.0315703,0.023746649,0.026227228,0.017800162,-0.0070356983,-0.02109836,0.03599169,-0.018927863,0.019721748,0.065297246,-0.08342781,0.031202767,-0.042537693,0.037931107,0.01666789,-0.044461533,0.0035612595,0.043706834,-0.0069693,0.028825888,-0.027214231,0.068186834,-0.03632352,-0.054208875,0.035426073,0.060986493,0.05731015,-0.00087007624,-0.01284617,-0.012630592,0.0056587807,-0.061997592,-0.023718504,0.0132375425,-0.01981315,0.0141705135,0.06707899,-0.022673085,0.054513432,-0.024511324,0.04673721,0.017535942,-0.028249681,-0.023604967,-0.018201634,0.027313272,0.03512236,0.016173385,-0.030670969,0.039912697,0.033157278,-0.054310672,0.0100846635,0.027269099,0.025816528,-0.10671469,0.04797912,0.051230513,0.039042994,-0.0046871286,-0.025808142,-0.008761646,0.020804493,0.022142317,-0.040806632,-0.03204746,0.017299555,-0.030469043,0.01687455,0.002812488,0.039444603,0.06627074,0.03953004,0.026136972,-0.03234662,-0.034749765,0.024216833,-0.027586583,-0.05372761,0.01842224,-0.008450477,0.00049902435,0.037772257,0.036261447,0.011792912,-0.051383555,0.058058575,-0.060346287,0.019955145,0.05072819,-0.032466486,0.017213082,-0.022898749,-0.0045612943,-0.038640708,0.0040162187,0.0026459827,-0.041718215,-0.0047551147,0.014223885,0.01678215,0.0061278148,-0.024143191,0.039466795,-0.003403672,0.017623564,-0.03497999,-0.0312441,0.024638994,0.0005108843,0.033105932,-0.026477292,-0.014132805,-0.031006172,-0.01195823,-0.026068248,-0.04411298,-0.011100529,-0.0019086794,0.0033575795,0.01237083,-0.010855198,0.07748063,-0.012505439,-0.034388073,-0.0013687131,0.052380163,0.05674498,0.023238154,-0.06941936,-0.054222412,0.0071405345,0.014528659,0.010262236,0.010687457,-0.034341726,-0.035463165,-0.038979385,0.03365765,0.0055929143,0.01972967,0.012883464,0.026977573,0.023325613,-0.057990883,0.012622709,-0.017713627,0.028274842,-0.0046249176,-0.06255281,0.011762524,0.0561951,0.053510986,0.0081555545,0.050054315,-0.009325927,0.020064147,-0.0060035735,0.043712094,-0.0026039162,0.0043787276,-0.015821576,-0.021788174,-0.040892597,0.031241214,-0.02942627,0.0218503,-0.0069107963,0.039016444,0.02274782,-0.03817517,0.040318836,0.029472731,0.014262869,0.027703013,0.05069077,0.044737905,0.05111403,-0.07030238,0.009949793,-0.07314612,-0.047377087,0.013144963,-0.06789624,-0.039688412,-0.010769677,-0.089731745,-0.009977918,0.006406183,-0.029635029,-0.074027464,0.013138499,-0.03333021,0.06351311,-0.04437725,0.018668346,0.018942289,-0.0016458322,0.09367862,0.025004208,0.017398104,-0.027157938,-0.0137529215,0.00076276547,0.032868784,0.045296554,-0.0070514646,0.015004538,0.043618634,0.004341167,0.0066220965,-0.061748773,-0.03982084,-0.060190946,0.0114852255,0.0033921585,-0.028628802,-0.017613733,0.015830107,0.017123094,0.0165296,0.023271222,-0.011106322,-0.022935146,0.05003645,0.013569351,0.010885625,0.028955158,-0.004574221,0.063395135,0.0022848188,0.086210005,0.04729778,0.014955334,0.03282452,0.031638876,0.026307406,0.04096394,-0.021371322,-0.0444741,-0.02028833,0.008074959,-0.028850295,0.009125047,-0.048273325,0.04768615,-0.009339834,0.019214967,-0.05062594,-0.040394016,-0.034376148,-0.017480696,-0.028065374,-0.022309497,-0.005485855,-0.020846406,0.05342733,-0.020690054,0.06621343,-0.115488045,-0.016306104,-0.020048326,0.011737624,-0.048326593,-0.014672464,-0.017556403,0.013503156,0.0044542607,0.022479873,0.04502279,0.001478665,-0.01707179,-0.0032190408,-0.03662572,0.010271719,0.027683336,-0.038837865,-0.017759275,-0.010974769,-0.007786481,-0.017479863,0.06713206,-0.038765747,0.00069133984,0.032958437,-0.021444434,-0.09110932,-0.00927001,0.035357174,0.03138311,-0.00031590482,0.025031077,-0.0033742546,0.011765429,-0.055944934,-0.0015060047,-0.0658259,-0.0024140568,-0.010155364,0.0126989,0.010496301,0.002976751,-0.04103001,-0.052288756,-0.018815085,-0.03054937,0.042967524,0.05715991,0.041234694,0.060864583,0.06418724,-0.045737196,-0.0858997,-0.017275034,-0.00039807454,-0.018491201,-0.048989803,0.038785856,0.017191403,-0.0040528583,-0.010172918,0.010156902,-0.0086270105,-0.028879175,0.03610354,-0.030168353,-0.027034326,-0.032020744,0.00024054856,0.028646104,0.0046242103,-0.0133023625,0.060984697,0.04716378,-0.053982724,-0.006069331,-0.07638217,-0.023846405,0.008279201,0.001228321,-0.010013895,0.015272253,-0.047291063,0.013730614,0.05006825,-0.0024747972,-0.017332189,-0.026586238,0.018834153,0.018011477,0.056586586,0.005537713,-0.031665135,-0.09609611,0.026807263,-0.021901825,-0.057485644,-0.0068922844,0.009889866,-0.01628557,-0.033026826,-0.0061756484,0.010032195,0.01366976,-0.0054573985,0.0082125645,-0.014815278,-0.002156476,0.01807426,-0.076471396,-0.034315825,-0.011901758,-0.015373096,-0.040101912,0.05016397,0.048374984,0.0037478865,0.0006798772,0.016454175,0.02291869,-0.009409897,-0.05095731,0.038428076,-0.031444762,0.03660764,-0.047210626,0.02286211,0.06351188,-0.034367934,-0.023128275,0.0226538,0.021288939,0.0026965295,-0.0025867529,0.0095992815,-0.0770891,-0.018205319,0.012207625,0.04512094,-0.014815158,0.03153793,-0.03841772,-0.058893207,-0.055863738,-0.053957928,-0.03427044,0.038150717,0.009794934,0.009377748,-0.06115576,0.036316425,-0.057159297,0.04546165,-0.0008004525,-0.011130526,-0.02635903,-0.004194048]	Keywords: k-means clustering, locality-sensitive hashing, LSH attention, routing transformer, reformer\nKey Objects: Queries, Keys, Clusters, Hashing Buckets\nRefers to Images: None\nHypothetical Questions:\n- How does k-means clustering improve the efficiency of attention mechanisms?\n- What is the purpose of locality-sensitive hashing (LSH) in the context of attention?\n- What are the potential limitations of relying on clustering or hashing for content-based attention?\n---\nSummary:\nRouting Transformer utilizes k-means clustering to assign queries to keys, allowing each query to attend only to keys within the same cluster. Reformer further employs locality-sensitive hashing (LSH) to group similar queries and keys, enabling efficient attention by focusing on tokens within the same hashing bucket.\nOriginal Text:\nwhere |  | denotes the number of vectors currently in cluster  and   (0 , 1) is a hyperparameter. Let P$\\_{i}$ denote the set of indices of keys that the i-th query attend to. P$\\_{i}$ in Routing Transformer is defined as  \n$$P _ { i } = \\{ j \\, \\colon \\mu ( q _ { i } ) = \\mu ( k ) \\}.$$  \nReformer [66] uses locality-sensitive hashing (LSH) to select key-value pairs for each query. The proposed LSH attention allows each token to attend only to the tokens within the same hashing bucket. The basic idea is to use an LSH function to hash queries and keys into several buckets, with similar items fall in the same bucket with high probability. Specifically, they use the random matrix method for the LSH function. Let b be the number of buckets, given a random matrix R of size [ D$\\_{k}$, b/2] , the LSH function is computed by :  \n$$h ( x ) = \\arg \\max \\left [ x R ; - x R \\right ].$$  \nThe LSH attention allows the i-th query to attend only to key-value pairs with indices\nContextualized Text:\nTo efficiently construct a content-based sparse graph, Routing Transformer and Reformer leverage different techniques. Routing Transformer uses k-means clustering to group queries and keys, ensuring that each query attends only to keys within the same cluster. Reformer enhances this process with locality-sensitive hashing (LSH), which assigns similar queries and keys to the same hashing bucket, enabling focused attention.	{"tags": ["attention", "transformers", "NLP", "algorithms"], "doc_id": "4f6d48cc-69da-48ca-847d-e6654b6e3f4d", "summary": "Routing Transformer utilizes k-means clustering to assign queries to keys, allowing each query to attend only to keys within the same cluster. Reformer further employs locality-sensitive hashing (LSH) to group similar queries and keys, enabling efficient attention by focusing on tokens within the same hashing bucket.", "doc_type": "text", "entities": ["Routing Transformer", "Reformer"], "keywords": ["k-means clustering", "locality-sensitive hashing", "LSH attention", "routing transformer", "reformer"], "key_objects": ["Queries", "Keys", "Clusters", "Hashing Buckets"], "contextual_text": "To efficiently construct a content-based sparse graph, Routing Transformer and Reformer leverage different techniques. Routing Transformer uses k-means clustering to group queries and keys, ensuring that each query attends only to keys within the same cluster. Reformer enhances this process with locality-sensitive hashing (LSH), which assigns similar queries and keys to the same hashing bucket, enabling focused attention.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does k-means clustering improve the efficiency of attention mechanisms?", "What is the purpose of locality-sensitive hashing (LSH) in the context of attention?", "What are the potential limitations of relying on clustering or hashing for content-based attention?"]}
8113da72-f05a-418b-a917-e76e72ec7cb8	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.038778745,0.0017750465,0.01696422,-0.003539495,-0.014747057,0.030573674,-0.011613307,0.04158689,0.029952109,0.002035541,0.0030324918,-0.0011550479,-0.007651943,0.0037624426,-0.00024413277,-0.0071665216,0.07996067,0.04755101,-0.02197187,-0.027117066,0.047451627,-0.010554936,0.015047303,0.003394071,0.01235847,0.049041126,0.017298698,0.004953316,-0.008697191,-0.036276724,-0.014140272,-0.04727905,0.014094795,0.0030897055,0.067224555,-0.04060924,0.025850644,-0.073691435,0.021292312,0.0019273105,-0.031828295,0.043696664,-0.0047807787,0.009691991,0.007166048,-0.0602886,-0.08627347,-0.03928445,0.036519226,0.004800784,-0.00043012184,0.017762633,-0.031518858,-0.01760338,-0.03611477,-0.026607484,0.0097785685,-0.05611526,-0.00027477965,-0.094289124,-0.02950471,0.008216018,-0.0122651,-0.013423322,0.008584567,-0.014008949,0.012361058,0.0030800113,0.047501784,0.1219518,-0.027813427,0.0150351785,-0.008398715,-0.0719948,0.086644925,0.0859614,0.0056964294,-0.025257776,-0.09726836,0.002084982,0.0071547506,0.09108499,-0.0330027,9.6122276e-05,0.086104535,0.0362104,-0.009212881,0.03299541,0.059290037,-0.044295706,0.0069257165,-0.06638951,-0.023585496,0.078344494,-0.014795813,-0.018654212,0.0021295692,-0.016120806,-0.0027699182,-0.07527871,-0.0025004013,0.021584393,0.06902654,0.1013123,0.00053024985,-0.043890197,-0.03173544,0.00046193766,0.01469839,-0.00647037,-0.018075302,0.034719694,-0.049150083,0.01937148,-0.05715219,0.031598773,-0.032553792,0.03744413,0.0026600861,-0.03920451,0.02426462,-0.026913049,-0.005755629,0.018838638,0.04519496,0.07138401,0.027378045,-0.026701987,-0.020119859,-0.0054481956,0.061657406,0.012012673,-0.04782677,-0.001571799,-0.008877517,-0.023343755,0.05248726,-0.03405546,-0.03063854,-0.0041818162,-0.023107834,-0.007970858,-0.038339365,0.021954117,0.016619042,0.013107639,-0.08718885,0.029602284,-0.035956148,0.038393784,0.04291832,-0.011116268,0.038315248,-0.030796725,0.0048344266,0.00057291955,-0.034364227,0.0072232164,-0.006689751,-0.083155654,-0.072538555,-0.026511751,0.021302585,0.098042995,0.026610399,0.109191,-0.028010905,-0.007931015,0.009753228,0.019840151,-0.008134915,0.0055791377,0.0017090159,-0.046092812,-0.0148293935,0.005600768,0.045065463,-0.034118038,0.011107904,0.008860246,0.02318373,0.07352751,-0.023391107,0.022287374,-0.044980824,0.024966741,-0.032026514,0.06805628,0.019455835,-0.028071804,-0.007576565,-0.014422779,0.031020097,0.021911902,0.047687504,0.05858998,0.0385384,-0.068096,-0.009850334,-0.06631522,0.0034782775,0.0006297635,-0.018949019,-0.021408128,0.0009423209,-0.0016746089,-0.014445214,-0.03188984,0.011909262,0.02197075,-0.00052443467,0.005275508,-0.026787575,0.03513633,0.015759863,0.017376937,-0.031418502,0.0112475855,0.049622785,-0.034418557,-0.044955302,0.017773464,-0.006208432,0.040590726,-0.039071023,0.03438128,0.008067474,0.025312092,0.016186288,-0.009221021,0.02408419,0.0059524183,-0.017987603,0.031307593,-0.024577105,0.01878331,0.05389714,0.029577142,-0.026517395,0.038359623,0.052125514,0.08852947,-0.015483881,0.0739656,-0.011575019,0.015893877,0.02205212,0.00060483563,0.019724486,0.0184026,-0.01407625,0.046633027,0.07398523,-0.013829007,0.00043868006,-0.036048084,0.006224136,0.03627042,-0.0060020876,-0.03129759,0.006039915,-0.019981842,0.023668904,0.019215187,0.009332724,-0.0012035825,0.021462975,0.021377504,-0.039273016,-0.005344196,0.039388526,0.02343842,0.007084846,0.0079736505,-0.028572196,0.008379146,0.040748097,-0.017851477,0.015341607,-0.016318105,-0.012962017,0.018848306,0.01472494,0.0076140147,0.024974441,0.02424856,0.07481228,0.0013683349,-0.010767774,-0.012485926,-0.012819786,-0.034509484,0.020204792,-0.00042104092,0.00093029375,0.0027598306,0.0007860033,-0.082387894,-0.035510026,0.0011886526,0.1283526,-0.0087125385,0.031022877,0.063081644,0.030013466,0.008090899,-0.056791134,0.027154576,0.0463988,0.030189963,0.01856988,0.002441817,-0.0061287098,0.009341651,0.03419636,0.06980473,0.062899396,-0.021556245,-0.014889866,-0.013781344,0.013915278,0.03343605,-0.0055277133,0.033795167,0.037980236,0.00978277,0.047763918,-0.009877777,-0.07764923,0.06454156,-0.041801922,0.039347574,0.042965055,-0.041684072,-0.012014833,0.022562625,-0.024223387,0.05675545,0.037502375,0.04076611,-0.006967678,-0.042064764,0.0007779729,-0.00017211566,-0.03011363,0.032082047,-0.015380126,-0.013473305,0.04708318,-0.008476207,0.00021674919,0.04131164,0.0054013897,0.04711045,-0.021257943,-0.010611687,-0.014194648,0.047847737,-0.013045481,-0.0044623595,0.063266546,-0.09818738,0.07347416,-0.046501596,0.04106277,0.0058005005,-0.05755173,0.025227066,0.037694026,0.005934076,0.0013956953,-0.031278655,0.06546132,-0.02403764,-0.032651145,0.0218908,0.06270655,0.051292766,-0.032773856,-0.020361787,0.004805061,0.0046130847,-0.057770856,-0.026318448,0.018392341,-0.013285255,0.019128822,0.05111498,-0.030527908,0.070316024,-0.034176435,0.033045493,0.023543593,0.0013521546,-0.011078481,0.011352982,0.0245438,0.023751514,0.038375303,-0.006373084,0.04219765,0.025159314,-0.057314944,0.016803283,0.014137698,0.035881735,-0.07841774,0.056646395,0.05378214,0.020369958,0.0054661753,-0.02014105,-0.013869075,-0.006103875,0.05395612,-0.03223879,-0.012797185,0.015804354,-0.021328673,0.010602902,0.00023967866,0.02638468,0.01313384,0.034284156,0.043375053,-0.025352016,-0.022916557,0.026206411,-0.012387495,-0.07247084,0.0033503196,-0.032541756,0.0036680112,0.0099289,0.021071324,0.022190386,-0.038963344,0.08296224,-0.049387775,0.012287032,0.058138933,-0.056774065,-0.0034481413,-0.032574687,0.01858946,-0.022688996,-0.003263671,0.01816724,-0.02121021,-0.00645164,-0.01221519,0.0068636984,0.026759192,-0.03349814,0.0705337,-0.020929197,-0.022681758,-0.043746106,-0.049439404,-0.0061212173,0.014325253,0.0183817,-0.01462647,-0.023943372,-0.03959115,-0.0151108,-0.03228174,-0.033795256,0.009364168,-0.018602123,-0.022093413,0.029042631,0.020044593,0.06320776,0.01383292,-0.029067341,0.0098626325,0.060937464,0.073592365,0.035493135,-0.036350306,-0.030146226,-0.0012581543,0.018804481,-0.0043130075,-9.0057336e-05,-0.03787472,-0.02506999,-0.018609352,0.0144034615,-0.015906679,0.07496731,0.011985464,0.017542293,0.02625489,-0.0457522,0.019458573,-0.021898866,0.03475378,-0.00010309858,-0.026892938,0.020891443,0.04733635,0.05828729,0.01893666,0.045387607,0.015321914,0.00860558,0.0022503221,0.038612384,0.03495873,-0.017736342,-0.022175036,-0.01634181,-0.06952231,0.022216799,-0.045455284,0.042090982,-0.0036751397,0.0406125,0.038946528,-0.030128315,0.031889793,0.015428212,-0.017227424,0.0006177785,0.010485304,0.04805753,0.04482483,-0.086716495,0.017399041,-0.062458757,-0.03410445,0.001701418,-0.08138379,-0.049369726,0.030958457,-0.081488706,0.02491983,-0.009156242,-0.033546876,-0.074874625,0.013867654,0.0030739638,0.07865978,-0.062040858,0.017080858,0.0019204607,-0.01378764,0.07946651,0.028774599,0.015542656,-0.022282314,0.009409511,0.03041032,0.018904714,0.041840002,-5.885479e-05,0.00033643123,0.061806742,0.007557325,0.005751003,-0.044859923,-0.045149658,-0.02891846,0.01685004,-0.02731753,-0.025788045,-0.039570227,0.016107263,-0.00662644,-0.02565261,0.061197065,-0.035231434,0.017469324,0.03789922,0.015904287,0.03511372,0.015774786,0.009138974,0.05116605,0.026006127,0.06632499,0.01267819,0.02439406,0.02536868,0.035446893,0.037800018,-0.009709466,-0.0350058,-0.050835796,-0.017513903,0.010641294,-0.0072039785,0.027573297,-0.04441629,0.035752174,-0.027263997,0.0057258834,-0.08436448,-0.030204546,-0.03917167,-0.022320546,-0.03277406,0.0041159648,-0.03387328,0.0020902713,0.03228954,-0.038210496,0.014454364,-0.10861234,-0.021682777,-0.03394112,0.015299909,-0.059242703,-0.020201778,-0.03859771,0.009169513,0.009379687,-0.0025941166,0.024698958,-0.0038569777,-0.054121286,-0.010923196,-0.016923152,0.018825173,0.034275774,-0.032532573,-0.019176096,-0.021890247,0.025776923,-0.0032904258,0.09500822,-0.016405301,-0.008344941,-0.009404848,0.029298062,-0.085905634,-0.008313157,0.047972586,0.036341336,-0.017529415,0.024312431,-0.00065838656,0.027190916,-0.021769755,-0.01809235,-0.07798182,0.008648476,0.008231379,0.043990694,0.030522406,0.005069354,-0.03966234,-0.016039468,-0.01944063,-0.04221077,0.06642543,0.044295497,0.009138685,0.07516346,0.049452994,-0.039314687,-0.06694679,-0.023095645,-0.010215035,-0.015051941,-0.034103245,0.026883015,0.03108909,-0.014836831,0.0009682748,-0.013596871,0.011065323,-0.009237581,0.037226733,-0.030114312,-0.03860765,-0.004795839,-0.005917745,0.00043451894,-0.034936436,0.008258716,0.076629065,0.051770996,-0.02031432,-0.018231021,-0.047874145,-0.02944986,-0.022549635,0.007951292,-0.034276124,0.018755939,-0.018216372,0.043221448,0.044580754,0.02755775,-0.037191257,-0.03632448,0.01141437,0.003395998,0.036388513,0.019786512,-0.0411521,-0.099128224,0.039787345,-0.014065834,-0.030531114,0.024163531,0.02169337,-0.03318415,-0.040254485,0.008194725,0.02815673,0.019869182,0.014493288,0.001900001,0.020225557,-0.016253553,0.013795738,-0.06636937,-0.01036364,-0.0022104026,0.017151162,-0.050434988,0.041832574,0.036941808,0.024769835,0.012924422,0.022320157,0.018446445,0.028133191,-0.05335065,0.051562287,-0.012079475,0.049194448,-0.03276615,-0.0029696142,0.06512386,-0.01933036,-0.046903905,0.0053307163,0.024224473,-0.015978664,0.010341811,0.0020460407,-0.053524204,0.0072493553,0.008430022,0.035151012,-0.018270256,0.022752378,-0.019803148,-0.042845067,-0.07810981,-0.020907223,-0.008093719,0.036291245,-0.0010865908,-0.017532568,-0.039098036,0.023638297,-0.04284688,0.062450927,0.009175167,-0.022838354,-0.025221374,0.027577732]	Keywords: locality-sensitive hashing, LSH, attention, sparse connection\nKey Objects: Queries, Keys, Hashing Buckets, LSH Attention\nRefers to Images: None\nHypothetical Questions:\n- How does LSH ensure that similar queries and keys are grouped together?\n- What are the advantages of using LSH over other sparse attention methods?\n- How does SAC's reinforcement learning approach contribute to adaptive attention?\n---\nSummary:\nReformer uses locality-sensitive hashing (LSH) to efficiently select key-value pairs for attention by hashing queries and keys into buckets, allowing each query to attend only to pairs within the same bucket.\nOriginal Text:\n$$h ( x ) = \\arg \\max \\left [ x R ; - x R \\right ].$$  \nThe LSH attention allows the i-th query to attend only to key-value pairs with indices  \n$$P _ { i } = \\{ j \\, \\colon h ( q _ { i } ) = h ( k ) \\}.$$  \nSparse Adaptive Connection (SAC) [78] views the input sequence as a graph and learns to construct attention edges to improve task-specific performances using an adaptive sparse connection. SAC uses an LSTM edge predictor to construct edges between tokens. With no ground truth for edges, the edge predictor is trained with reinforcement learning.\nContextualized Text:\nTo improve attention efficiency, Reformer utilizes locality-sensitive hashing (LSH). This method involves using a random matrix R to hash both queries and keys into several buckets.  Consequently, each query only attends to key-value pairs that fall within the same hashing bucket, as determined by the function $$h ( x ) = \text{argmax} [ x R ; - x R \right ].  Sparse Adaptive Connection (SAC) builds on this by learning to construct attention edges using an LSTM edge predictor, trained through reinforcement learning.	{"tags": ["NLP", "deep-learning", "attention", "transformers", "sparse attention"], "doc_id": "8113da72-f05a-418b-a917-e76e72ec7cb8", "summary": "Reformer uses locality-sensitive hashing (LSH) to efficiently select key-value pairs for attention by hashing queries and keys into buckets, allowing each query to attend only to pairs within the same bucket.", "doc_type": "text", "entities": ["Reformer", "LSTM", "SAC"], "keywords": ["locality-sensitive hashing", "LSH", "attention", "sparse connection"], "key_objects": ["Queries", "Keys", "Hashing Buckets", "LSH Attention"], "contextual_text": "To improve attention efficiency, Reformer utilizes locality-sensitive hashing (LSH). This method involves using a random matrix R to hash both queries and keys into several buckets.  Consequently, each query only attends to key-value pairs that fall within the same hashing bucket, as determined by the function $$h ( x ) = \\text{argmax} [ x R ; - x R \\right ].  Sparse Adaptive Connection (SAC) builds on this by learning to construct attention edges using an LSTM edge predictor, trained through reinforcement learning.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does LSH ensure that similar queries and keys are grouped together?", "What are the advantages of using LSH over other sparse attention methods?", "How does SAC's reinforcement learning approach contribute to adaptive attention?"]}
dcaecbed-f382-4a74-9f9e-fed3f2ae75e9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.038453996,0.046343748,-0.004576978,0.043006524,-0.019231092,0.03612178,-0.009857729,0.075045966,0.019959899,0.01043954,-0.017263934,-0.011944164,-0.0061810203,-0.018369539,-0.0012357864,-0.012237387,0.031988688,0.08050453,-0.0045935903,-0.027388962,0.04719145,0.021262156,0.0029965786,0.011188226,0.005775257,0.044864286,0.028666234,0.002512896,0.010668925,-0.033117235,0.005280028,-0.064527534,0.042008854,0.0044225524,0.03347429,-0.031245798,0.02653637,-0.07798118,0.014311718,-0.0038064448,0.034822457,0.032417074,-0.025091242,-0.019481195,0.024362603,-0.08020321,-0.0632706,-0.042810697,0.025464961,-0.0054818126,-0.019757297,0.015814574,-0.022623282,-0.008212049,-0.06400543,-0.018807948,0.01233216,-0.048979577,0.0048469007,-0.08730578,-0.028927658,0.01864595,0.004464024,0.014115134,0.045050927,0.01461601,-0.0002983506,0.038786817,0.05632069,0.15855819,0.011022658,0.03441183,-0.010199987,-0.052484777,0.110130996,0.029796207,-0.015795419,0.014883779,-0.075734176,-0.019624727,0.035379633,0.038210545,-0.037713118,-0.0063508935,0.05763223,0.0018508817,-0.019089827,0.029622741,0.055201873,-0.03429719,0.005344377,-0.0195523,-0.01539768,0.0793513,-0.029297391,-0.030230068,0.0067236377,-0.034315817,0.009296014,-0.05744363,0.05157548,0.003114233,0.07846029,0.12270119,0.027202288,-0.011219978,-0.006333836,-0.024117539,-0.003578264,-0.004694993,-0.023742478,0.026022824,-0.05532641,0.022782126,-0.033207722,0.022656461,-0.019118654,0.007690473,0.010503051,-0.021495866,0.028419852,-0.028349321,-0.0011931583,0.03514879,0.049559325,0.07136265,0.016734518,-0.016733594,-0.027954074,-0.0074566263,0.073610485,-0.016333094,-0.028152615,0.03259095,-0.027014688,0.00014214097,0.05794105,-0.039172836,-0.0048957723,0.0027535209,0.031137327,-0.045090348,-0.045703113,0.009714207,0.010908881,0.013261684,-0.07374339,0.048467986,-0.029368013,0.040643726,0.041548267,-0.0026281644,0.047514822,-0.0071732765,-0.033368524,-0.026919255,-0.049726434,-0.009594075,-0.015731039,-0.04904161,-0.072519206,-0.025707848,0.005716537,0.09454169,0.004030519,0.1241204,0.015435666,0.013779564,0.01069926,0.0038295148,-0.005384405,-0.013785629,0.030061875,-0.0044509047,-0.018552316,-0.010901606,0.0140637215,-0.031719204,0.044140223,0.00615627,-0.0130926045,0.089247964,-0.0384912,-0.013408313,-0.026273528,0.027090728,0.0077010244,0.035729837,0.007685393,-0.027099632,0.018754097,-0.03674784,-0.024826327,-0.00737061,0.02279959,0.049993597,0.039686155,-0.063888855,0.00504318,-0.024525145,0.022675425,-0.0098945,-0.031650785,-0.007990886,0.030099487,-0.034925718,0.02725338,-0.021675188,0.0017471033,0.035303585,-0.026616361,0.012078282,-0.020901939,0.06578847,0.03969189,0.035808243,-0.045568876,0.036569573,0.0021868646,-0.023043603,-0.013742524,0.0031463935,-0.014798005,0.05325585,-0.029797323,0.030024312,-0.011109878,-0.009932697,-0.013913464,-0.032101206,-0.013884864,0.028933262,-0.05119305,0.03487831,-0.051362026,0.019932503,0.022284597,0.02139991,-0.042693973,0.02972457,0.0340748,0.08006182,-0.007582351,0.08523475,0.010834798,-0.0007314107,0.010109079,0.039201725,0.016946232,-0.007122191,-0.028735459,0.033767316,0.057751067,-0.0054321224,-0.009329751,-0.042912878,0.010051074,0.014485124,0.01930083,-0.028353555,0.030971676,-0.016892824,-0.0038422316,0.04600663,0.0057675615,0.013619719,0.042620312,0.015689611,-0.06523043,0.005109681,0.019964857,0.03216923,0.02488108,-0.017259613,-0.019401949,0.04512142,0.017552253,-0.035160895,0.046219647,-0.02539981,-0.03416836,0.008825728,0.020495597,0.0034990828,0.030412575,0.020636305,0.035129018,-0.040196285,-0.024717728,0.015271048,0.0018756433,-0.018880473,0.035552375,0.007958026,-0.015401914,-0.008971016,-0.014354045,-0.057902835,-0.010055016,0.02070434,0.12533738,-0.015042907,0.024977254,0.037926797,0.028126962,-0.0013117505,-0.02687445,0.042778354,0.03608022,-0.004064639,-0.002605551,-0.0362755,-0.032475594,0.016638625,0.055248845,0.06927721,0.10780742,-0.008793501,-0.010697592,0.012171001,0.047269285,0.033451002,-0.0060463846,0.024078742,0.07523032,-0.0066511463,0.06301649,0.0056970487,-0.07608494,0.07089067,-0.043955326,0.047749944,0.06993549,-0.008833957,-0.015381141,0.039405998,-0.013323851,0.023610653,0.048767388,-0.0048857047,0.014109212,-0.03675957,0.016059939,0.009653369,-0.042772774,0.03958581,-0.018206425,-0.014939959,0.0028017717,0.020259334,0.009348887,0.008921927,0.03082229,-0.0004080955,-0.012819829,-0.010742516,-0.012212132,0.01657987,0.014267413,-0.032423135,0.07912353,-0.10332182,0.07116522,-0.05458704,0.036634613,-0.02152118,-0.06360101,0.07147686,0.061365016,-0.0046823984,0.01411271,-0.030855313,0.04977165,-0.013101067,-0.03144638,0.047580883,0.07485952,0.037549406,0.03037625,-0.007436496,-0.001273618,0.013495001,-0.006211741,-0.04189402,0.06080898,0.0048892484,-0.0076767136,0.08474995,-0.02598827,0.029231947,-0.053411655,0.01793134,0.021847373,-0.0054567866,-0.031955767,-0.022861682,0.010247546,0.072977,0.0382659,-0.0056084637,0.011172554,0.0321433,-0.07702198,0.019468216,0.027854688,-0.007552695,-0.08659092,0.008452291,0.03474549,0.0042881137,0.0318197,-0.0660486,-0.018958326,-0.0037732234,0.01835472,-0.0343275,-0.036931798,0.03520881,-0.04592373,-0.02495479,-0.0064540654,0.031355776,0.044848077,0.030485615,0.039880037,-0.0126074385,-0.03735809,0.03448772,-0.00050073833,-0.058823407,0.008598671,-0.027898815,-0.04354039,0.03940145,0.009070351,0.036476564,-0.05204096,0.10119061,-0.028460262,0.0056509194,0.074061245,-0.057710014,-0.012713108,-0.00045722333,0.045276545,-0.043479715,-0.00077446376,0.008727303,-0.02931257,-0.0154484,0.030601798,0.009998598,0.044440646,-0.029157788,0.06940541,0.0021349986,-0.008427926,-0.048725728,-0.024354499,0.02134952,0.018708175,0.008190406,0.005888785,-0.012904666,-0.011215606,0.02396699,-0.006678181,-0.042107984,-0.003391455,-0.011089956,-0.028089983,0.014258132,0.015704215,0.042237494,0.001681986,-0.016883051,0.0063942424,0.049963005,0.051986985,0.0039899154,-0.080070466,-0.046050154,0.021213071,0.008600837,0.027708566,0.019020215,-0.031719767,-0.027327485,-0.024189696,-0.0117461905,-0.0038462162,0.04723947,-0.020686515,0.022046147,0.0123379445,-0.046469294,0.0009404281,-0.012488405,0.049855083,-0.017230978,-0.033782277,0.020631501,0.011083927,0.04289362,-0.0129898,0.04176319,0.027336467,0.024049943,-0.025413351,0.0564551,0.0110928705,-0.032372613,0.022856222,-0.0054782424,-0.05936271,0.03189955,0.006007418,0.017889427,-0.016168006,0.02710245,-0.00032410427,-0.024056632,0.034374546,0.029514858,-0.012636035,0.008626597,0.01586221,0.03628236,0.041204344,-0.114515394,-0.0049051065,-0.088358186,-0.011137172,-0.005652137,-0.097604685,-0.04623396,-0.010824357,-0.060542516,0.024306266,-0.03841344,-0.06039084,-0.044789743,0.028437585,-0.040497866,0.06244249,-0.02730333,0.010300824,0.007746609,-0.022426236,0.093419306,0.039389778,0.025690923,-0.02048718,-0.005301976,-0.006044859,0.01794729,0.033680737,0.008183637,-0.018467147,0.055679373,-0.0005701658,-0.0133903185,-0.026181059,-0.05164981,-0.022197133,-0.0028465146,0.013862609,-0.0056095324,-0.036233258,-0.0075473906,0.013194523,-0.0008961998,0.050764147,0.014510001,-0.0065109814,0.060168106,0.040302366,-0.024334013,0.050073847,-0.014812448,0.042513665,0.0012099238,0.060604528,0.044671204,-0.009430251,0.032360476,0.008797696,0.030334152,0.020368863,-0.022661483,-0.011259778,-0.034039594,0.0077737635,0.013256626,0.01336237,-0.04905543,0.009198764,-0.023239799,0.058393467,-0.037171118,-0.039765738,-0.010929461,6.1531427e-06,-0.027316052,-0.0102318935,0.011226487,0.0082303975,0.034050554,-0.03723441,0.030547129,-0.0774232,-0.028707327,0.0003230021,-0.0073319655,-0.05040765,-0.010894396,-0.02925625,0.0088777,0.006682149,-0.03171382,0.033880662,0.017731141,-0.03246885,0.014562763,-0.025446419,-0.013148848,-0.006634578,-0.02494732,-0.017305935,0.0075582755,-0.007194272,-0.0022043488,0.08101224,-0.06933559,-0.0112438435,-0.0057592303,0.014519693,-0.062012278,-0.009679415,0.024547579,0.00918966,0.008148479,0.01021298,0.021719273,0.026751114,-0.03221964,-0.010010557,-0.0772952,-0.016872762,0.010330359,0.038193163,0.01777603,-0.0032924048,-0.02488823,-0.016847834,-0.01566495,0.00386535,0.035764135,0.016741367,0.053403966,0.08553129,0.030001549,-0.03188841,-0.024884392,-0.031104347,-0.0077345436,0.01017757,-0.031513717,0.028608538,0.020698112,0.010002131,0.00010797566,-0.020681424,-0.007870309,-0.01133802,0.025059823,-0.06031696,0.001788858,-0.0031745862,0.012224885,0.002905428,0.0151644545,-0.011790626,0.068388335,0.052213818,-0.040583268,-0.049529605,-0.045533527,-0.036151256,-0.0025671273,-0.028691191,-0.03032387,-0.030288214,0.0013706526,0.00062347343,0.07430691,-0.0025515005,-0.041424826,-0.05690467,0.032152813,0.019783793,0.0157453,-0.0027015454,-0.045688357,-0.044258147,0.04221373,-0.014467337,-0.044842288,0.00667555,0.0038517716,-0.044225648,-0.037178464,-0.0010677534,0.025403148,0.033312038,-0.004971091,0.01099022,0.0062713977,-0.05477181,0.026677579,-0.08036304,0.007519229,0.0064355433,0.028148428,-0.020819958,0.056381434,0.038484953,0.0007968615,-0.035660595,0.004657216,0.0318819,0.03351162,-0.0075964085,0.034315232,0.0005608577,0.0186318,-0.015454361,-0.00039107338,0.07930869,-0.035792567,-0.007058276,0.0066203144,0.031523492,0.031659473,0.013184903,-0.014701371,-0.033218853,-0.039321855,0.028184185,0.024933936,0.028143927,0.020395497,-0.037707772,-0.029866716,-0.029730078,-0.038311392,-0.055156004,0.046959557,0.009304389,-0.0029827992,-0.07543525,0.02775399,-0.03201897,0.042440247,-0.017646765,0.0039608846,-0.039562892,0.008989867]	Keywords: sparse attention, Sinkhorn normalization, block sparse attention, locality\nKey Objects: Queries, Keys, Key Blocks, Query Blocks, Sorting Network\nRefers to Images: None\nHypothetical Questions:\n- How does Sinkhorn normalization contribute to the sparse attention mechanism?\n- Why is it beneficial to limit attention to key blocks instead of individual keys?\n- In what ways does this approach complement or differ from other sparse attention techniques like block local attention?\n---\nSummary:\nSparse Sinkhorn Attention assigns key blocks to query blocks using a sorting network and Sinkhorn normalization, limiting each query's attention to keys within its assigned block to enhance the model's ability to capture locality.\nOriginal Text:\nSparse Sinkhorn Attention [132] first splits queries and keys into several blocks and assigns a key block to each query block. Each query is only allowed to attend to the keys in the key block that is assigned to its corresponding query block. The assignment of key blocks is controlled by a sorting network, which uses Sinkhorn normalization to produce a doubly stochastic matrix as the permutation matrix representing the assignment. They use this content-based block sparse attention along with block local attention introduced in Sec. 4.1.1 to enhance the ability of the model to model locality.\nContextualized Text:\nTo further reduce computational complexity, Sparse Sinkhorn Attention divides queries and keys into blocks. Each query's attention is then limited to the keys within a specifically assigned key block. This assignment is managed by a sorting network leveraging Sinkhorn normalization to create a permutation matrix, which ultimately enhances the model's capability to model locality.	{"tags": ["attention", "sparse", "transformers", "locality"], "doc_id": "dcaecbed-f382-4a74-9f9e-fed3f2ae75e9", "summary": "Sparse Sinkhorn Attention assigns key blocks to query blocks using a sorting network and Sinkhorn normalization, limiting each query's attention to keys within its assigned block to enhance the model's ability to capture locality.", "doc_type": "text", "entities": ["Sparse Sinkhorn Attention"], "keywords": ["sparse attention", "Sinkhorn normalization", "block sparse attention", "locality"], "key_objects": ["Queries", "Keys", "Key Blocks", "Query Blocks", "Sorting Network"], "contextual_text": "To further reduce computational complexity, Sparse Sinkhorn Attention divides queries and keys into blocks. Each query's attention is then limited to the keys within a specifically assigned key block. This assignment is managed by a sorting network leveraging Sinkhorn normalization to create a permutation matrix, which ultimately enhances the model's capability to model locality.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.1 Sparse Attention"}, "hypothetical_questions": ["How does Sinkhorn normalization contribute to the sparse attention mechanism?", "Why is it beneficial to limit attention to key blocks instead of individual keys?", "In what ways does this approach complement or differ from other sparse attention techniques like block local attention?"]}
3190f39a-d311-417f-8b06-a04a1f7687e0	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0025404617,0.013784728,-0.026979093,0.02591405,-0.057490513,0.075017534,0.020005833,0.062642686,0.022358326,-0.030199518,0.00016123844,0.0028982733,0.0010451876,0.0018853673,0.002251669,-0.0009772325,0.02060014,0.05553154,0.011949231,-0.04901412,0.032429747,-0.0062625636,0.016459577,0.0087379515,0.030009327,0.047528133,-0.0023357598,0.0067174355,-0.031507947,0.024320079,-0.0028881037,-0.052648734,0.050883945,0.019401068,0.04744554,-0.021595968,-0.017162187,-0.1063946,-0.0056023127,-0.0022778583,-0.031281825,0.040793654,-0.04378318,-0.02027783,0.0009068211,-0.0688204,-0.07094284,-0.030309446,0.019679904,-0.01669591,-0.04079695,0.004379021,-0.02042948,-0.0246879,-0.037068967,0.018572789,0.0033172395,-0.030924827,-0.0038609388,-0.090755135,-0.0037000752,-0.01835598,-0.0019752395,-0.00879932,0.024944587,-0.00046433395,-0.00067242753,0.0040484136,0.022579022,0.1266665,-0.004346819,0.011168262,-0.0018717431,-0.058170263,0.09649508,0.030404417,-0.0050379112,-0.013794339,-0.072572134,0.0061514135,0.022624895,0.031530686,-0.083875254,-0.017813785,0.08039416,-0.012075729,-0.02328263,-0.0009188973,0.04245068,-0.06784445,0.028087607,-0.0054064603,0.018146526,0.06331185,-0.02072112,-0.025493367,-0.027296491,-0.034263052,0.0143148815,-0.06487164,0.016332202,0.01054338,0.07735117,0.10848948,0.02631932,-0.032503113,0.012517966,-0.046532866,-0.010902912,0.0016304473,-0.015535524,0.020974435,-0.058815274,0.008859676,-0.036830187,0.021261992,-0.07199198,0.032726735,-8.148648e-05,-0.025581185,0.042454433,-0.027573379,-0.012353178,0.015970852,0.04446612,0.021439608,0.025314076,-0.013335555,-0.04192657,-0.009965805,0.065298535,0.0036171053,-0.0328141,0.0032246006,-0.016781978,0.019785032,0.09539263,-0.018425528,-0.01030667,0.012317824,-0.012798492,-0.015411822,-0.042595517,0.015699146,-0.004623542,0.05237065,-0.039954416,0.023004487,-0.011327415,0.06883588,0.033290762,-0.009636085,0.046342906,-0.012930024,-0.032270256,-0.02273849,-0.023281043,-0.042292446,0.00036686397,-0.020247953,-0.049567148,-0.014240796,-0.003672361,0.0596257,0.045960378,0.1245805,-0.0071893483,-0.01871097,0.0012186914,0.0126151815,-0.04470139,-0.025626663,0.034247994,-0.003695357,-0.014571653,-0.0060556913,0.015434838,-0.0139358975,-0.008535649,0.010414264,-0.037237387,0.09055524,-0.016840242,0.009497356,-0.017184673,0.050556444,0.022110172,0.038144894,-0.049473375,-0.022643965,0.0036365108,-0.03129304,0.004894777,0.003340796,0.02428404,0.041712362,0.037673194,-0.075841874,0.0008183893,-0.049451508,-0.002763852,0.012692521,-0.06036826,-0.0065559284,0.05138276,-0.014120426,0.013671301,-0.004142532,0.01976274,0.03718279,0.011299884,-0.002757222,-0.025553713,0.04884768,0.011354937,0.028930375,-0.023853267,0.0032121413,-0.0009904471,-0.005794564,-0.031875666,0.011305979,-0.027800549,0.051661972,-0.051377025,0.033924546,-0.009775393,0.04824686,-0.029177878,-0.010766492,0.011064439,0.0281093,-0.056269314,0.01824912,-0.043798044,0.01932169,0.015933307,0.010606555,-0.039279103,0.0407767,0.066823095,0.07888184,-0.012434936,0.08748605,0.022619162,0.015915228,0.036139634,0.069840625,-0.002984955,0.006337151,-0.021984344,0.033644423,0.029108698,-0.016149938,0.010391322,-0.035672087,0.012140301,-0.0020073776,0.05858197,-0.031120548,0.035182204,-0.0076136123,-0.031711757,0.052350547,0.0015762245,-0.003184304,0.06774126,0.030637676,-0.017818935,0.006507192,0.02955358,0.06840138,0.031004885,0.010627639,-0.041484185,0.0042039114,0.011015992,0.004364103,0.023919972,-0.021735761,-0.006398113,-0.03352249,0.008581602,0.0008608896,0.0059506935,-0.007853028,0.03985131,-0.048809394,0.017929351,0.008896246,-0.001046954,0.008579881,0.03382142,-0.00093486795,-0.0017154762,0.003428007,0.0035319093,-0.0407268,0.012876902,0.03334068,0.07910583,0.0009940146,0.03000603,0.028988324,0.053522766,0.020311091,-0.045112494,0.0665142,0.0402155,0.0094099175,-0.033208493,-0.018403474,-0.025307598,0.032155503,0.026753526,0.08737251,0.0878152,0.0046902467,-0.021747304,-0.0089171585,0.05018763,-0.009980169,-0.01037134,0.007770728,0.061690092,-0.008179446,0.034121264,-0.032615498,-0.08533312,0.06052905,-0.046993107,0.069759615,0.058681857,-0.0221854,-0.0044634673,0.00769048,0.010423306,0.008528782,0.05446277,-0.0010973566,-0.0080367355,-0.0233923,0.010489915,0.008336443,-0.010607729,0.051758993,0.037292868,-0.009270874,0.0020595943,0.019443326,0.03041176,0.039577033,-0.0010962745,0.008713256,0.047457628,-0.008218046,-0.016927125,0.027863879,-0.001606378,-0.02120602,0.044678956,-0.05277528,0.05124222,-0.03812979,0.040210858,-0.01167313,-0.05063622,0.042078905,0.059013076,0.013151889,-0.009111294,-0.02550256,0.072668724,-0.04975073,-0.03643252,0.059202753,0.05514098,0.016095784,-0.013284284,0.010651422,-0.014570493,0.011174419,-0.0017380656,-0.060554877,0.05145243,0.012043653,0.003909379,0.090923436,-0.032025643,0.04791941,-0.0720887,0.021877795,0.045984402,0.015575224,-0.04285743,0.010409776,0.026017906,0.09168677,0.014817379,0.017500171,0.052854232,0.028025059,-0.06952857,-0.01860067,0.0095008295,-0.008301228,-0.08994728,0.028123384,0.022424633,-0.002038972,0.0054897154,-0.04817778,0.0014494134,0.03205082,0.043216653,-0.033892155,-0.0046123825,0.017281774,-0.04811321,-0.053826034,-0.01026309,0.047783703,0.0381708,-0.0081210015,0.045130525,-0.008433012,-0.03772302,0.022063984,-0.021162432,-0.033869416,0.018811338,0.0010647657,-0.055368584,0.038055282,-0.020836966,0.0052037383,-0.054543372,0.07806157,0.0036433074,0.0148910275,0.061007634,-0.022259288,0.026593503,-0.006032766,0.024764057,-0.021472106,0.041576527,0.017213162,-0.047422208,-0.037874725,0.054256056,0.03159105,0.024673799,0.009068492,0.070005305,-0.022897154,0.03345805,-0.031953022,-0.010701773,0.019128485,0.0076194783,0.019249078,0.03410906,0.014974678,-0.037573963,-0.010909432,0.010175279,-0.009934581,-0.0043317047,-0.019747699,0.0017965664,0.045575418,-0.011039147,0.060317367,-0.018541947,-0.02658961,-0.009388732,0.045840453,0.030089317,0.013564283,-0.06324122,-0.03229852,0.042082325,0.008202994,0.017339172,0.030861378,0.008178002,-0.040758535,-0.025844034,0.01819183,-0.030623721,0.036814123,-0.0016948159,0.019859053,0.027464824,-0.08904521,-0.018718993,0.0019243637,0.03284228,-0.020127498,-0.04343059,0.016145367,0.03971408,0.008225426,-0.024275687,0.0069862586,-0.0330681,0.045955427,-0.0071298643,0.06420556,0.026925372,-0.05877169,0.04513112,0.008915314,-0.0739171,-0.0030695745,-0.028754223,0.014711171,0.0115649365,0.03319961,0.0059998278,0.007396629,0.046971533,0.040497683,0.03125639,0.001930493,-0.007290987,0.060700577,0.031345244,-0.09134476,-0.004455107,-0.075627014,-0.01670934,0.024678383,-0.0757512,-0.06589741,0.012540878,-0.046975385,0.008492023,-0.021136988,-0.037517384,-0.041817535,0.018304547,-0.017245451,0.07133411,-0.032924555,0.021051234,-7.162052e-05,-0.020434123,0.05331491,0.03590633,0.036906086,-0.011945494,-0.03492776,-0.01933989,-0.014574925,0.07187182,0.009029175,0.015929272,0.07385205,0.0074066743,-0.006797512,-0.020405587,-0.08215007,-0.05792896,-0.010824127,-0.02208526,0.009678485,-0.02928015,0.007783516,-0.031218784,0.018148582,-0.012988622,0.0291621,-0.02144109,0.05311471,0.052864663,0.0002462268,0.00939004,0.011958336,0.02461335,0.022715978,0.062378477,0.019061975,0.024202315,0.030618448,0.009679406,0.008610416,0.029792573,0.0031821833,-0.041284844,-0.072464675,-0.022264877,-0.023036955,-0.017243624,-0.050451346,0.03453474,0.0057234373,0.013433362,-0.03140338,-0.041222326,-0.019056717,-0.031863853,-0.019030964,0.0049591777,0.027856918,-0.0010437857,0.032317176,-0.030122995,0.06351102,-0.11866673,-0.03872309,-0.020706195,0.008944997,-0.034172922,-0.0007065448,-0.05709433,-0.030565359,-0.01832274,-0.04342353,0.02415427,-0.0047391243,-0.036888506,0.015461509,-0.030208658,-0.0062909527,0.009549691,-0.005270189,-0.041713372,0.008233981,-0.044543993,-0.011377477,0.061149962,-0.04292066,0.01103273,0.033968247,0.013726241,-0.0741141,-0.01408267,0.04116927,0.033110976,-0.006479681,-0.012340417,-0.012728388,-0.006174512,-0.027608361,0.0054847556,-0.04381578,0.0283873,0.007171321,-0.009413011,0.05660607,0.025227943,-0.03650537,-0.03334294,-0.017998619,0.008881938,0.05980907,0.034756713,0.01846749,0.055387925,0.051209494,-0.016047588,-0.038376667,-0.017042428,-0.026606591,0.031810302,-0.018022073,0.037112296,0.0025264572,0.02893343,-0.02408151,-0.045903847,0.0012277756,0.030044679,0.012975095,-0.0327531,-0.025698002,-0.029341914,0.045810502,0.010566477,0.012431068,-0.03288089,0.059301257,0.0005320087,-0.021872152,-0.023185788,-0.05642177,-0.028055271,-0.031505734,-0.004311454,-0.0127238,-0.021003272,-0.03339968,0.019748347,0.018215345,0.01671263,-0.06662164,-0.0434493,-0.0010499269,0.016861083,0.05565909,-0.0007261224,0.03552869,-0.099008135,0.0325019,-0.002775703,-0.025737243,0.023845762,-0.008103977,-0.03244313,0.0024346712,0.019807199,-0.015280942,0.0025202008,-0.024928475,0.044678845,0.031414203,-0.039165873,0.038524605,-0.07725665,-0.012228135,0.0014378477,-0.009026836,-0.017520048,0.03814166,0.04949807,0.0026439438,-0.0029020468,0.014909874,0.015273471,0.034094464,0.008758339,0.04711078,-0.025148075,0.029855873,-0.0012157436,0.008607322,0.07416534,-0.027896,-0.014587328,0.020990908,-0.011273061,0.015300178,0.023144007,0.015090901,-0.032681294,-0.025103841,0.038028922,0.03970484,-0.0015202528,0.075987935,0.012398574,-0.02302563,-0.043929625,-0.038775146,-0.015829032,0.047973044,0.061972916,0.0028689029,-0.102790244,0.0065688514,-0.042450547,0.040549446,0.028663347,0.04256521,0.013559704,0.018598963]	Keywords: linearized attention, attention mechanism, complexity, softmax, unnormalized attention matrix\nKey Objects: Attention Mechanism, Complexity, Unnormalized Attention Matrix\nRefers to Images: None\nHypothetical Questions:\n- Why is the computational complexity of standard attention quadratic?\n- How does disentangling softmax(QK) into QK enable a reduction in complexity?\n- What is the purpose of the unnormalized attention matrix?\n---\nSummary:\nLinearized attention methods aim to reduce the computational complexity of standard attention mechanisms by approximating or replacing the unnormalized attention matrix, allowing for a linear complexity with respect to sequence length.\nOriginal Text:\n### 4.2 Linearized Attention  \nAssuming Q , K , V $\\_{R}$ T  $^{D}$, the complexity of computing softmax( QK $^{T}$)V is quadratic w.r.t. sequence length T , as illustrated in Fig. 7(a). If softmax( QK $^{T}$) can be disentangled into QK $^{T}$, we can compute QK $^{T}$V in reversed order (i.e., Q $^{'}$( K $^{T}$V )), leading to a complexity of O( T ) .  \nLet  A = exp( QK $^{T}$) denote un-normalized attention matrix, and exp(  ) is applied element-wise, the regular attention can be rewritten as Z = D $^{-1}$ A V , where D = diag(  A 1 $\\_{T}$) 1 T is the all-ones column vector of length T ; diag(  ) is a diagonal matrix with the input vector as the diagonal.\nContextualized Text:\nTo address the quadratic complexity of standard attention mechanisms, linearized attention methods approximate or replace the unnormalized attention matrix (exp( QK )) with a more manageable form. This allows computations to be rearranged, enabling a reduction in computational complexity from quadratic with respect to sequence length to linear, as illustrated in Fig. 7(a).	{"tags": ["NLP", "deep-learning", "architecture", "efficiency"], "doc_id": "3190f39a-d311-417f-8b06-a04a1f7687e0", "summary": "Linearized attention methods aim to reduce the computational complexity of standard attention mechanisms by approximating or replacing the unnormalized attention matrix, allowing for a linear complexity with respect to sequence length.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["linearized attention", "attention mechanism", "complexity", "softmax", "unnormalized attention matrix"], "key_objects": ["Attention Mechanism", "Complexity", "Unnormalized Attention Matrix"], "contextual_text": "To address the quadratic complexity of standard attention mechanisms, linearized attention methods approximate or replace the unnormalized attention matrix (exp( QK )) with a more manageable form. This allows computations to be rearranged, enabling a reduction in computational complexity from quadratic with respect to sequence length to linear, as illustrated in Fig. 7(a).", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["Why is the computational complexity of standard attention quadratic?", "How does disentangling softmax(QK) into QK enable a reduction in complexity?", "What is the purpose of the unnormalized attention matrix?"]}
d119249d-0041-4c46-ac8f-6dc2f7ced711	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.012770247,0.017937988,-0.008712668,-0.0001818369,-0.036298845,0.083135344,0.026629928,0.050792478,0.020048056,-0.03300788,0.012445376,-0.011127048,0.01809055,-0.020145683,-0.013036749,-0.02929711,0.019407686,0.067243636,-0.01083729,-0.02895761,0.01974163,-0.011231401,0.008574644,-0.016098356,0.028911607,0.02614166,0.0194532,0.0037761428,-0.006850592,0.033958808,-0.007209088,-0.062470514,0.04651075,-0.0011768257,0.027767155,-0.042293817,-0.018508943,-0.10722237,-0.0071679787,-0.0025049082,-0.021616122,0.0678423,-0.061084032,0.010070501,0.023279812,-0.06349238,-0.09824798,-0.029722942,0.0070030782,-0.04461073,-0.040701345,0.0065884856,-0.04028686,-0.008837867,-0.022398265,0.022890775,0.012304792,-0.037373774,0.002640684,-0.10941311,-0.014165173,-0.007468876,-0.0010421446,0.0014306732,0.02132495,0.002620758,0.022949576,-0.0007630753,0.068616614,0.11380644,-0.026617616,0.014666585,-0.000546451,-0.052657362,0.09552438,0.032007962,-0.0034028948,0.0045289774,-0.062129892,0.00415635,0.007900318,0.03864549,-0.0661657,-0.021083677,0.08671266,-0.013175531,-0.023316298,0.022822907,0.049904056,-0.042888865,0.0007807193,0.011732551,0.0063286102,0.03538796,-0.023403795,-0.051052596,-0.012665501,-0.032397218,0.0048148837,-0.06056189,0.043342795,-0.01228372,0.078506894,0.10976256,0.040955037,-0.03089384,0.0028794324,-0.035341308,0.010970332,0.011159427,-0.019664368,0.026589418,-0.06501177,0.010825199,-0.0234309,0.050659947,-0.07384724,0.04762043,0.03053843,-0.0075671277,0.04256622,-0.0061843554,0.0009574705,0.013554801,0.069348566,0.012440089,0.051942904,-0.025171218,-0.027259143,0.005763103,0.055712663,0.009685711,-0.041000135,-0.0006691198,0.0056926976,0.028277218,0.086592525,-0.037286445,-0.012503865,0.023485513,-0.023536177,-0.03341222,-0.044854853,0.033015795,-0.029216643,0.038802538,-0.06954241,0.03997285,-0.009144775,0.06418379,0.013774997,0.01315719,0.030899258,-0.012143848,-0.020108823,-0.0015247809,-0.01968052,-0.0518182,0.027768884,-0.006817852,-0.047368027,-0.009264828,0.012169419,0.06185486,0.04741582,0.0908069,0.009666805,0.00011482328,0.04954831,0.028329974,-0.023584144,-0.021965778,0.022292865,-0.040512126,-0.039919782,0.0039220126,0.01355768,-0.036624204,0.0013031455,0.010718147,0.007937238,0.06997634,-0.020722948,0.017525077,-0.021777842,0.03653522,0.0054939194,0.033333644,-0.03924876,-0.008530385,0.0016255144,-0.059172772,0.006433242,-0.019659773,0.026704103,0.031275183,0.05319337,-0.08810526,-0.03773798,-0.0532088,-0.0052882247,0.0071991747,-0.040953826,-0.0067218286,0.039832756,-0.012525171,0.013355022,0.006375202,0.015513455,0.022144306,0.023804454,-0.014470248,-0.024154143,0.03705782,-0.0035223255,0.027105777,0.0018545119,-0.0029819047,-0.014770453,-0.034126047,-0.0376204,0.003292491,-0.0095611205,0.051310945,-0.051504564,0.06452585,-0.0030278852,0.029777022,-0.019888623,0.0029936275,0.021485278,0.042503413,-0.048818335,0.012286934,-0.048968837,0.007090466,0.039294466,0.015449626,-0.052187208,0.034330696,0.03547768,0.074986584,-0.017665785,0.09687346,0.019761473,0.015184234,0.02257837,0.04962164,-0.0012564277,-0.0074723545,-0.019189596,0.063644215,0.048217755,0.0061818184,0.010290624,-0.07142995,-0.020166948,-0.015029447,0.049116604,-0.030215854,0.020772722,0.038428012,0.0022377549,0.065300435,-0.015977638,-0.00036801177,0.022278018,0.017328504,-0.008193451,-0.008708457,0.03338597,0.045418043,0.034082435,0.0016083601,-0.04071881,0.0015567945,0.015179823,-0.0109075345,0.027365841,-0.025364332,0.0029756029,-0.017940065,0.0013628511,0.0005821434,-0.008661706,-0.0019563222,0.030914713,-0.019234408,0.01773368,0.00057287776,-0.01900813,-0.024890477,0.017270464,-0.0037957581,-0.02108073,0.02987321,-0.00060552533,-0.039938245,-0.0057906867,0.021142643,0.09588862,-0.0014404197,0.028305214,0.026488574,0.015335238,0.00027190885,-0.06963401,0.071392566,0.061754562,0.0032505773,-0.009529353,-0.012671377,-0.02766353,0.024989791,0.040644694,0.07373068,0.096465684,-0.01221872,-0.018823074,0.001329563,0.037671145,-0.02555897,-0.0015108517,0.030051934,0.033125103,-0.00837482,0.036063444,-0.05073508,-0.07158844,0.05096768,-0.029664494,0.07232241,0.07074004,-0.0318319,-0.0021295801,-0.007834755,0.009896565,0.0140093705,0.031607643,0.004727865,-0.004218175,-0.032443427,-0.0011965038,0.0005708418,-0.0122055635,0.054623216,0.021773849,-0.008822061,0.011736217,0.02743193,0.01010159,0.04497317,0.0048407293,0.023141518,0.016137106,-0.009772247,-0.020225942,0.0067888075,-0.003640294,-0.009863003,0.04311054,-0.05227268,0.044452973,-0.02786813,0.016439794,-0.02222796,-0.042307876,0.009944637,0.08484496,-0.011876898,-0.004742571,-0.023238484,0.06890051,-0.0421456,-0.042645033,0.068921626,0.064636216,0.017900813,0.002278155,0.00025744416,-0.009834303,0.029499955,-0.031852864,-0.053802893,0.044173792,-0.007933896,0.014665242,0.10781994,-0.056576785,0.04610069,-0.048069257,-0.02065388,0.023261776,0.009761698,-0.047871456,0.00038204048,0.029662251,0.087213196,0.021781296,0.0037005963,0.048533812,0.013571993,-0.050910175,-0.008583553,0.014706823,0.0069920956,-0.06578388,0.051817402,0.015798837,0.023694577,0.00093929336,-0.02416665,-0.018078113,0.010440849,0.040327758,-0.026734889,0.00021698025,0.0024505786,-0.05962739,-0.03632843,-0.055796634,0.049242184,0.031975243,-0.009444209,0.02776491,0.0017318747,-0.028848674,0.005392183,-0.017420273,-0.018543147,0.010321843,-0.015271766,-0.03580427,0.039684985,-0.01524965,-0.012072692,-0.054154437,0.07380315,0.023037976,0.009759921,0.05435929,-0.019913718,0.015315431,-0.008967175,0.028103784,-0.046265207,0.056658186,0.015834734,-0.018918399,-0.027337624,0.04956431,0.039297722,0.025399318,0.006691021,0.07546665,0.0043800767,0.054760806,-0.03393713,-0.0034425256,-0.0035819886,-0.0129349,0.03782656,0.029256696,0.014858771,-0.049085386,-0.0041889446,0.0067923833,-0.005030215,-0.0027239353,0.007925578,-0.005566882,0.059685353,-0.026525563,0.017859217,0.00047972958,-0.033935968,-0.0065010106,0.043535724,0.01900089,0.053610127,-0.07947809,-0.019535497,0.011972435,-0.012164443,0.025734758,0.031546965,0.01558502,-0.021855574,-0.012408388,0.035376314,-0.034335952,0.04382137,0.013838061,0.024089716,0.025322387,-0.07085447,-0.005702633,0.01647801,0.040569384,-0.012250144,-0.04623403,0.01611534,0.04569618,0.027001105,-0.0132795675,0.024919003,-0.010427548,0.02991853,-0.006400651,0.066684775,0.035024337,-0.059271157,0.05431399,0.016535478,-0.100952625,0.0032696475,-0.03214683,0.019278085,0.014775598,0.060858607,0.005646778,0.0068423976,0.06307852,0.009496271,0.037034556,0.038646244,0.01364328,0.025224686,0.050919097,-0.06991372,-0.0045998227,-0.07497689,-0.045946013,0.011356344,-0.04927727,-0.0560806,0.02934821,-0.050615836,0.01724977,-0.0071462225,-0.02840486,-0.0349132,0.027067795,0.0021188743,0.071876526,-0.01279068,0.05171051,0.0010917964,0.0026902636,0.040146507,0.04257577,0.028041039,-0.031656425,-0.032957513,-0.016342247,-0.0026174542,0.04389912,0.016002862,0.014121782,0.06324477,-0.006785003,8.800983e-05,-0.04959373,-0.08008429,-0.07571523,-0.014873772,-0.02108508,0.004667433,-0.050884925,0.034402095,-0.020498706,-3.49988e-05,-0.012326079,0.014228538,-0.003425373,0.033537887,0.005268879,0.013026894,0.02781513,0.003112675,0.043986883,0.023344401,0.05029625,-0.0026875578,0.0130622545,0.026773276,-0.012437795,0.027979849,0.01918839,-0.015059469,-0.05570591,-0.04683986,-0.024951043,-0.018300444,-0.009689033,-0.017933145,0.025147453,0.01993786,0.011953502,-0.05180399,-0.010757424,-0.04490611,-0.03116775,-0.0046899375,0.01278747,0.05787063,-0.00437558,0.03412937,-0.016760753,0.073258124,-0.100204505,-0.032310534,-0.031840947,0.031158458,-0.041579627,-0.0009503881,-0.05027446,-0.027970005,0.00081706204,-0.046717703,0.008483992,0.00498447,-0.011393178,0.0136224665,-0.037683118,-0.0061163306,0.011991112,0.0021146964,-0.006698369,-0.01552954,-0.045752484,-0.005224755,0.06259166,-0.016878042,0.020573933,0.02464639,-0.013778686,-0.08306885,-0.011332867,0.016717223,0.049324542,-0.007732937,-0.017025316,-0.017828321,0.009625227,-0.039381515,0.023652209,-0.03809248,0.0031255842,-0.006469608,-0.026274808,0.051484637,0.038068242,-0.040176168,-0.039002553,-0.020002224,0.004750852,0.03383533,0.014180803,0.028190542,0.05708934,0.06794096,-0.03151571,-0.0571974,0.015766852,-0.034006864,0.012416696,-0.024798833,0.065415256,0.012680745,0.03506572,-0.03613005,-0.058417805,0.019016787,-0.0021282744,0.00023278738,-0.010595684,-0.059741497,-0.028567728,0.033748087,0.013615696,0.020167585,-0.020484481,0.06395357,0.006080811,-0.037190422,-0.0077605015,-0.062248416,-0.02390525,-0.018994898,-0.009960486,-0.015966063,-0.011971416,-0.04399164,-0.008035542,0.023630103,-0.017341305,-0.059003558,-0.040556666,0.016761731,0.03192735,0.06440698,-0.009163765,0.03911587,-0.10084265,0.0528314,-0.0071198274,-0.040703677,0.031788934,-0.009101241,-0.047235943,0.004666404,0.05464959,-0.029462328,0.024960028,-0.0284228,0.042732358,0.022208009,0.0025543002,-0.0009895018,-0.07484045,-0.000113080256,-0.011193231,-0.025931388,-0.015134742,0.03909688,0.029598346,0.014537179,0.007512377,0.026099434,-0.0064117755,0.006388923,0.017993428,0.07815803,-0.06870452,0.059768587,0.006316662,0.017761651,0.07789827,0.013004205,-0.0078038867,0.034824807,0.029026017,0.00432488,0.01461808,0.026529526,-0.0040652305,-0.018969147,0.016966445,0.025170952,-0.0142071415,0.04985178,0.007171689,-0.023422817,-0.020911073,-0.04229432,-0.0244099,0.04477602,0.04545147,0.01850975,-0.073744215,0.020628458,-0.044413712,0.027082631,0.019246036,0.027337866,-0.0084182415,0.028487353]	Keywords: linearized attention, unnormalized attention matrix, feature map, approximation\nKey Objects: Linearized Attention, Feature Map, Unnormalized Attention Matrix\nRefers to Images: ./images/a-survey-to-transformers/image_8.png\nHypothetical Questions:\n- How does linearized attention reduce computational complexity compared to standard self-attention?\n- What is the role of the feature map ( ) in linearized attention?\n- What are the benefits of replacing the unnormalized attention matrix with an approximation?\n---\nSummary:\nLinearized attention methods approximate the unnormalized attention matrix using feature maps, allowing for a linearized computation of the matrix by computing  Q( Q )(  ( K $^{T}$)V.\nOriginal Text:\nLinearized attention is a class of methods that approximate or replace the unnormalized attention matrix exp( QK $^{T}$) with  Q( Q )  ( K $^{T}$, where   is a feature map that is applied in row-wise manner. Hence the computation of unnormalized attention matrix can be linearized by computing  Q( Q )(  ( K $^{T}$)V $^{6}$, as illustrated in Fig. 7(b).  \nFig. 7. Illustration of complexity difference between standard self-attention and linearized self-attention.  \n  \nTo gain further insights into linearized attention, we derive the formulation in vector form. We consider a general form of attention  \n$$z _ { i } = \\sum _ { j } \\frac { \\sin ( q _ { i }, k _ { j } ) } { \\sum _ { j } \\sin ( q _ { i }, k _ { j } ) } v _ { j },$$\nContextualized Text:\nAs a way to reduce computational complexity in Transformer models, linearized attention is used.  It involves approximating the unnormalized attention matrix with a feature map applied in a row-wise manner, allowing for a linearized calculation by computing  Q( Q )(  ( K $^{T}$)V.	{"tags": ["architecture", "attention", "transformer"], "doc_id": "d119249d-0041-4c46-ac8f-6dc2f7ced711", "summary": "Linearized attention methods approximate the unnormalized attention matrix using feature maps, allowing for a linearized computation of the matrix by computing  Q( Q )(  ( K $^{T}$)V.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["linearized attention", "unnormalized attention matrix", "feature map", "approximation"], "key_objects": ["Linearized Attention", "Feature Map", "Unnormalized Attention Matrix"], "contextual_text": "As a way to reduce computational complexity in Transformer models, linearized attention is used.  It involves approximating the unnormalized attention matrix with a feature map applied in a row-wise manner, allowing for a linearized calculation by computing  Q( Q )(  ( K $^{T}$)V.", "mentioned_images": ["./images/a-survey-to-transformers/image_8.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["How does linearized attention reduce computational complexity compared to standard self-attention?", "What is the role of the feature map ( ) in linearized attention?", "What are the benefits of replacing the unnormalized attention matrix with an approximation?"]}
fc9da745-8dcb-4756-8d1c-99114d13d9d0	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.040181983,-0.010711975,-0.007993987,0.039387763,-0.051386353,0.05871018,-0.024660712,0.056536585,0.025661943,-0.033674646,0.059981667,-0.010331232,0.018317433,0.025196787,0.0020677473,0.013188759,0.0143337175,0.058945466,0.0025388487,-0.037988294,0.015679993,0.030039996,0.045141794,-0.010832871,-0.005722212,0.0102764815,0.0074878447,-0.04607135,-0.03063271,0.0518721,-0.032230366,-0.02940361,0.03679546,-0.018778067,0.044651564,-0.042371254,0.015133767,-0.0813533,-0.008496989,-0.025926715,-0.02092426,0.051751297,-0.052752648,-0.014226047,-0.025304142,-0.07439789,-0.100222215,-0.026747558,0.026015127,-0.049171828,-0.006664116,0.017900113,-0.010690184,-0.03157088,-0.04204239,-0.02784496,-0.031315688,-0.0308409,-0.0037462928,-0.11455811,-0.03164827,-0.021135772,0.0140695255,-0.03138359,0.022977805,-0.012179844,0.058718868,-0.0049544815,0.07228751,0.117510356,-0.04517366,0.0053052045,0.016479896,-0.0500549,0.117034316,0.024469838,0.0029041183,-0.008932694,-0.06572399,0.03273708,0.012554327,0.11864904,-0.05749492,-0.039298937,0.09960712,0.013765461,-0.011954665,0.014910748,0.05174499,-0.03454273,0.0015644232,-0.042834718,-0.003908387,0.03764895,-0.013295545,-0.046394784,-0.025740396,0.0010932379,-0.032870214,-0.08685218,0.014369362,0.027461238,0.091226585,0.0804343,0.022280008,-0.022126371,-0.01842288,-0.0071698376,0.0141882235,0.0008160563,-0.030784637,0.029603846,-0.013946088,0.0015923062,0.019969542,0.061477546,-0.064452164,0.036129963,0.033191442,-0.055925187,0.03936396,0.0033344105,-0.013895884,0.038515784,0.0637476,0.04508513,-0.029255014,-0.06047061,-0.023932736,-0.040876713,0.04974532,0.015270913,-0.048710708,0.01370465,-0.018540243,0.054103404,0.058574017,-0.016493693,0.0017988546,0.024913417,0.0069991266,-0.057317775,-0.059583742,0.015832452,-0.0034442036,0.05722832,-0.06719631,0.012467279,-0.05050347,0.04885513,-0.013932401,-0.0041926694,0.013608395,0.004386724,-0.005132969,0.04317315,-0.019269573,-0.03129176,0.056293186,-0.09858844,-0.041360974,-0.049246375,0.055379212,0.06405581,0.03827616,0.08388268,-0.035706494,0.020196412,0.012907549,0.023678739,-0.008988238,0.0017618553,-0.00441091,-0.0037368245,0.02695354,0.015676448,0.013131779,-0.010269372,0.012656161,-0.024036082,-0.0009044166,0.039195497,0.010701665,0.029992,-0.024399174,0.036348294,0.008071013,0.018168679,0.042935036,-0.03494149,0.0105115,0.010771802,-0.057237867,-0.012340995,0.0029609273,0.061487842,0.058609825,-0.055449165,-0.039018802,-0.03396933,0.008483931,0.028254418,-0.03230275,-0.009380041,0.035401545,-0.041225843,-0.008769165,0.021895401,-0.0019037976,0.009062247,0.022990879,-0.014850733,-0.04759803,-0.007172043,0.005463609,-0.01278441,-0.012320424,0.0416185,0.028137105,-0.03868585,-0.045033224,0.0005147332,-0.01860714,0.010631704,-0.03963688,-0.0113555,-0.06806213,0.020908996,-0.019023713,0.022797586,0.02174599,0.070005774,-0.03438526,0.0019435264,-0.016807076,0.016352031,0.0010993555,0.03740979,-0.07319212,0.0009542183,0.01131055,0.046876572,-0.002301693,0.06267393,0.033382148,-0.020122651,0.016369984,0.04308384,0.012447303,0.008006198,-0.04115783,0.063817464,0.016925987,0.04027702,0.002311196,-0.077047996,-0.011893355,0.009010462,0.032573957,-0.033014555,0.016598083,-0.00060821156,-0.00843167,0.069485046,0.0050321734,0.0050950986,0.02979453,0.014439272,-0.038796566,-0.025089348,0.008620943,0.046557423,0.01628425,-0.0036817393,-0.012472194,-0.019535633,0.043812733,-0.009563213,0.007127282,-0.025578566,-0.05278517,-0.010925969,-0.0045417673,-0.0032258828,-0.017232599,0.03500209,0.03209343,-0.06595119,-0.032479014,-0.014148382,-0.015762413,-0.06397772,-0.042694315,-0.039623503,0.007904672,0.014188156,-0.0036433581,-0.023214782,-0.0012365456,4.7492278e-05,0.091072105,-0.03982811,-0.018013451,0.0071103056,0.00435037,0.054340094,-0.0759728,0.011346766,0.04596023,0.022261158,0.024727317,0.005301551,0.0001368332,0.032143634,0.07034737,0.09646946,0.05266584,0.0030927346,0.0006248795,0.016834533,0.06304041,0.02416645,0.017862251,0.032680724,0.062395684,-0.0027623826,0.034308136,-0.06673287,-0.048369102,0.022640148,-0.05651935,0.036586393,0.059709743,-0.04846606,0.008156887,0.0019460825,0.006904842,-0.008783284,-0.0026801773,-0.017816538,-0.02448339,-0.063222036,0.0039951536,0.010061472,-0.0015635733,0.04428123,0.018530369,0.008811535,0.011735618,8.559632e-05,0.01526496,0.004688355,0.001080484,0.003415362,-0.03154667,-0.027922926,-0.019519495,0.04221667,-0.0014894266,-0.023903383,0.03870168,-0.04637834,0.048146963,-0.013313146,0.016680496,-0.04946047,-0.07099777,0.040546786,0.017274817,-0.016995683,-0.019839577,-0.033647288,0.04195843,-0.018023493,-0.047652736,0.0527518,0.01735484,0.0008086501,0.007100154,-0.032246765,-0.024322778,-0.022430945,-0.071108006,-0.057679668,0.010954834,0.007718682,-0.011087233,0.10105034,-0.02616407,0.056009654,-0.035261665,0.0028307992,-0.0017887413,-0.023889512,-0.045300078,-0.03835289,0.016362682,0.0482918,0.050723758,-0.005063754,0.01793772,0.047743723,-0.043859303,-0.008178351,0.0544737,0.024184054,-0.054385833,0.04226492,0.033898138,0.038878042,0.02557462,-0.063799754,0.011526162,-0.017538052,0.035577316,-0.032901965,-0.037982557,-0.0022063362,-0.039327867,-0.012707067,-0.01066988,0.022988722,0.016540729,0.03770623,0.008964723,0.008958934,-0.008130305,0.019268222,0.007857332,-0.024773482,0.016798154,0.0018657732,0.0030238286,0.009517007,0.031184634,0.0011993736,-0.047152508,0.04847408,0.0013284961,-0.0008208963,0.042956922,-0.00881056,0.031271685,-0.03965221,-0.0033630293,-0.04245587,0.038992707,0.009939068,-0.055366173,-0.002864361,0.03950244,0.028694695,0.027878879,0.005616108,0.05628802,-0.002531854,-0.0035914658,-0.05107019,-0.0015962521,-0.014577627,0.037284467,0.034144137,0.01653116,-0.0047958386,-0.027563483,0.013041856,0.021844694,-0.029766293,-0.020563684,0.0142997,-0.031643756,-0.003630991,-0.061171077,0.005526093,-0.025539095,-0.015429239,-0.026375195,0.020570293,0.032960672,0.021010807,-0.04416395,-0.00021421976,0.04745813,0.034114257,-0.010757374,-0.014375957,-0.006947683,-0.013232678,-0.025303828,0.023657743,-0.030615343,0.029074872,0.0077309124,0.0053243055,-0.007638702,-0.08946633,-0.038864598,0.037764266,0.043053225,-0.021792294,0.011484849,-0.010707834,0.073046215,0.04619457,-0.018877769,0.03904208,-0.0017374954,-0.022236357,-0.022011122,0.046006706,0.021860376,-0.070349805,0.018052071,0.040403236,-0.08774447,0.0076649264,-0.02632323,0.03209138,-0.026258072,0.021730518,0.030695124,0.049370196,0.03500872,0.053222522,0.026534582,-0.0049461774,-0.026446955,0.03794762,0.033570614,-0.09306409,-0.03515948,-0.0820662,-0.013595215,-0.00035499473,-0.037694786,-0.08959185,0.029578205,-0.038477734,0.009531038,0.0107312715,-0.019173061,-0.035162915,-0.00047807215,0.030886965,0.027379321,0.011294922,0.049727675,-0.020179762,-0.009576677,0.021331869,-0.015968224,0.023778893,-0.005878244,-0.022404015,0.025173094,0.05047145,0.05525077,0.0003206959,0.018317457,0.04302651,0.016725918,-0.013875229,-0.042234678,-0.05142687,-0.011020478,0.0065634754,-0.025602454,-0.02316522,-0.07907053,0.013143603,-0.019628096,0.009358421,0.031828325,-0.014797921,0.017208451,0.04037306,0.037454836,0.019846851,-0.009377908,-0.011161557,0.038875315,0.0534193,0.0130336825,0.016411848,0.027556106,-0.008703292,-0.0093005905,0.004516836,-0.043101303,-0.0049771685,-0.0070498725,-0.011529233,-0.03155636,-0.02394194,0.032659117,0.018175796,-0.017856626,0.060744613,0.010599699,-0.04394784,-0.04698063,0.007988343,-0.023892393,-0.0035469541,0.034578275,0.03063491,-0.024934232,-0.009433308,-0.027250681,0.09518691,-0.06928435,-0.011959491,-0.035612572,0.054063905,-0.049525086,-0.044623297,0.021352807,-0.00070640235,0.0071957777,0.003593558,0.058475718,0.019554634,-0.013139845,0.028566552,-0.012037048,-0.004002727,-0.016308991,-0.03382957,-0.024301376,0.014881963,0.010358269,-0.0052860403,0.053499438,-0.007123434,-0.017399509,0.031242466,-0.03663441,-0.041843265,-0.025068011,0.02477175,0.036547583,0.024396215,-0.056677677,0.023896951,0.062140234,-0.05574433,0.016978567,-0.030057905,0.0025211992,0.0056696734,0.056172602,0.050688297,-0.0018073651,-0.043041434,-0.036427535,0.002709169,-0.038992535,0.03890761,0.037874334,0.010104652,0.047371324,0.03574483,-0.014173043,-0.05419745,-0.036388915,-0.0608696,0.041928805,-0.03557483,0.043596663,0.0007543589,-0.021541074,-0.03093739,-0.0070825364,0.019023253,0.024553657,0.034852073,0.044732627,-0.05445037,-0.00063727045,0.022331592,0.022350322,-0.03238008,-0.019584525,0.03846092,0.03667284,-0.06016681,-0.023541009,-0.09538079,0.00015564858,0.01292935,-0.0068005584,-0.009085888,-0.003296058,0.012893519,-0.03496143,0.0323124,0.015786178,-0.058972694,-0.07204643,0.05080616,0.032400776,0.07665183,-0.01458321,0.001270711,-0.12536116,0.052268784,0.010804988,-0.044804,0.035700984,0.011042853,-0.032512587,0.03762713,0.04923147,-0.012780992,0.038393483,0.0052787736,0.009909182,0.028347395,-0.01127181,0.02763367,-0.010915405,-0.0015063399,0.005973007,-0.016335642,0.022207865,0.027729226,0.024229947,0.011581689,0.017234012,-0.003582361,0.039228883,0.010993008,0.023501154,0.03403305,-0.050966658,0.034053613,-0.036653496,0.03171689,-0.0052880323,-0.00811361,-0.028501883,-0.005157988,0.0450547,-0.015846562,0.059516087,0.040555928,-0.009295837,-0.038692273,0.030649679,-0.0237157,-0.011260976,0.024190743,-0.018192556,-0.021459358,-0.015462888,-0.04589619,0.031692553,0.009080668,0.020415895,0.033142,-0.03864775,0.01756844,-0.02508041,0.03257095,0.0038062495,0.031340692,-0.02170897,0.031706568]	Image title: Standard and Linearized Self-Attention\nTags: self-attention, neural-network, NLP, linear-transformation, softmax, architecture\nKey objects: Query (Q), Key (K), Value (V), Linear Transformation, Softmax, Output (Z)\n---\nSummary:\nThis diagram illustrates two variations of self-attention mechanisms: standard self-attention and linearized self-attention. Both architectures process query (Q), key (K), and value (V) inputs, but they differ in how they combine these inputs to produce the final output Z.\nFull description:\nThe diagram shows two self-attention mechanisms. In (a), the 'Query' (Q), 'Key' (K), and 'Value' (V) are each passed through a linear transformation. The results of the key and query linear transformations are then combined with the softmax function. The result of this operation is multiplied by the value's linear transformation, producing the output Z.  In (b), the 'Query' (Q) is first linearly transformed, then multiplied by the key's linear transformation, which is further multiplied by the values linear transformation, resulting in the output Z.\nText found in image:\n- Q\n- K\n- V\n- Linear\n- Softmax\n- Z\n- (a) standard self-attention\n- (b) linearized self-attention	{"tags": ["self-attention", "neural-network", "NLP", "linear-transformation", "softmax", "architecture"], "title": "Standard and Linearized Self-Attention", "doc_id": "fc9da745-8dcb-4756-8d1c-99114d13d9d0", "source": "./images/a-survey-to-transformers/image_8.png", "summary": "This diagram illustrates two variations of self-attention mechanisms: standard self-attention and linearized self-attention. Both architectures process query (Q), key (K), and value (V) inputs, but they differ in how they combine these inputs to produce the final output Z.", "doc_type": "image", "key_objects": ["Query (Q)", "Key (K)", "Value (V)", "Linear Transformation", "Softmax", "Output (Z)"], "parent_doc_id": "d119249d-0041-4c46-ac8f-6dc2f7ced711", "text_in_image": ["Q", "K", "V", "Linear", "Softmax", "Z", "(a) standard self-attention", "(b) linearized self-attention"], "contextual_description": "The diagram shows two self-attention mechanisms. In (a), the 'Query' (Q), 'Key' (K), and 'Value' (V) are each passed through a linear transformation. The results of the key and query linear transformations are then combined with the softmax function. The result of this operation is multiplied by the value's linear transformation, producing the output Z.  In (b), the 'Query' (Q) is first linearly transformed, then multiplied by the key's linear transformation, which is further multiplied by the values linear transformation, resulting in the output Z."}
09545795-62c3-4119-9cc5-315362a4b942	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.019425878,-0.013521714,0.013055909,0.04414327,-0.02572618,0.062090732,0.015664399,0.072005585,0.04046115,-0.02148579,-0.0070683956,-0.012183531,0.018141346,0.012422406,-0.019886237,0.012410099,0.019966831,0.0842049,-0.016392339,-0.026983362,0.023973756,0.0044723707,0.02974445,-0.033156477,-0.0040972694,0.03393921,0.013786337,0.0017613431,0.0015522512,0.02256275,-0.011013917,-0.023275757,0.007494975,-0.003906455,-0.00063940696,-0.0094840815,0.029545125,-0.09361104,-0.0075207236,0.016963683,0.010522279,0.07012188,-0.030173903,-0.014348734,-0.0075633735,-0.030885145,-0.05530835,-0.0313919,0.008029776,-0.030761212,-0.023670793,0.026766226,-0.05439064,-0.016729496,-0.021215701,-0.021386778,-0.010943693,-0.03420247,0.01479168,-0.11716557,-0.017590832,-0.0036596057,0.0053761713,-0.026401568,0.024303904,0.011548883,0.028637048,-0.0024076006,0.04877226,0.13320899,2.6222639e-05,0.010733343,0.0097891,-0.076727584,0.11355501,0.015677655,-0.009361533,-0.02601917,-0.040121257,-0.029577557,0.012165556,0.03545597,-0.049270704,-0.01984042,0.11673937,-0.011398397,-0.0068744384,-0.017568784,0.03368285,-0.08557475,0.011522746,-0.006283835,0.006023623,0.055952847,-0.042811904,-0.058830306,-0.056427866,0.012040703,0.009636557,-0.04808304,0.028460845,0.02454694,0.11682903,0.0898031,-0.02185781,-0.032013707,0.0014760917,-0.02474313,0.03218483,0.0007412754,-0.01788918,0.05529535,-0.10498383,0.026560767,-0.056852177,0.018123874,-0.025635418,0.0027127669,-0.02444592,0.015363564,0.0016725172,-0.031086894,-0.025615837,-0.018311301,0.06682193,0.015015729,0.034780946,-0.02844838,-0.017610678,-0.022311829,0.051504217,-0.002268961,-0.024473343,0.010774132,-0.032215267,0.041784257,0.06122502,-0.002215472,-0.022755131,0.030736629,-0.0019052814,-0.06612651,-0.026900282,0.02943664,0.0105461255,0.07342982,-0.04870034,0.027696311,-0.053701844,0.060074344,0.031286698,0.014145234,0.035655726,0.012212533,-0.03590823,0.031920534,-0.018404178,-0.058015518,-0.00023587704,-0.0028766238,-0.03432425,-0.037966907,-0.021302514,0.05777338,0.020438407,0.08285083,-0.03700775,-0.010845274,0.0401352,0.046239816,-0.059954073,-0.025545372,0.010414817,-0.05981435,-0.0054721856,0.000748264,9.7102515e-05,-0.00969197,-0.0037928123,0.007244759,0.0060729235,0.022705173,0.020026319,0.044952177,-0.048317164,0.03844656,0.016267642,0.09028376,-0.0010310507,-0.02219934,0.014934805,-0.029742202,-0.04402813,0.0027749527,0.014911267,0.040754095,0.0560172,-0.07316323,-0.047986306,-0.027909402,-0.05111746,-0.0030266584,-0.018566143,0.01927144,-0.005530511,-0.0023919216,0.0147013,-0.038970858,0.0023620473,0.013701661,0.020169534,-0.018459413,-0.035907716,0.044835784,0.036742993,0.017778981,-0.0033237922,0.05321768,0.0056934045,-0.07078963,-0.05520969,0.0016917676,-0.04685481,0.04292606,0.0009806709,0.057131678,-0.036170274,0.02058134,-0.009876548,-0.026782798,0.03878973,-0.0050271307,-0.036330573,-0.005828209,-0.019462012,0.025852587,0.0008003064,0.004992195,-0.039737973,0.0051801577,0.037358865,0.022774467,-0.008984891,0.10120192,-0.025903203,0.023955727,0.0485616,0.039551567,0.014767591,-0.02676243,-0.006295572,0.042886008,0.016785271,-0.017995793,0.016764212,-0.05323998,-0.035763986,-0.00023908743,0.036334045,0.0016486809,0.028457914,0.006576167,-0.017781368,0.014456555,-0.008303841,-0.003651435,0.04829746,0.0529864,-0.025773797,-0.056891523,0.02927989,0.026067954,0.008568635,-0.01073362,-0.018995814,0.04143653,0.025551127,-0.008009687,0.0332671,-0.0539193,-0.03821417,0.0036542225,0.019171039,0.0131439585,-0.003576849,-0.0050176326,0.02407821,-0.0718906,-0.022464527,-0.040830303,-0.014030028,-0.015102396,-0.014927215,0.020747054,-0.038590595,0.030288238,-0.018448994,-0.07412243,-0.008627069,0.014408633,0.116638616,-0.015833903,0.008554082,0.064478554,0.048686184,0.017700488,-0.07384359,0.04731143,0.054765053,0.0096187955,0.018019803,0.023665568,-0.01089311,0.004165796,0.046326593,0.06573774,0.053577777,0.0030660615,-0.01630476,0.027389126,0.016716188,-0.019319674,-0.009032033,0.010028034,0.062608674,-0.0044822036,0.015121945,-0.007101985,-0.0474156,0.056910206,-0.070085615,0.05845131,0.074231744,0.018443841,-0.010656553,0.01025954,0.0006314471,0.028965194,0.021880925,-0.007927761,-0.003668476,-0.028226137,0.01260972,-0.024368696,0.03379336,0.046046603,0.021640126,0.0007207944,0.030007407,-0.04062706,0.004669101,0.044448122,-0.025772927,-0.0037111577,0.015175029,0.008624534,-0.0019044789,-0.00896773,-0.01503641,-0.016678568,0.051762518,-0.08065701,-0.010290927,-0.050961714,0.026379352,-0.024073632,-0.08898378,0.020696234,0.0397462,-0.012003723,0.019799924,-0.040431168,0.045270566,-0.031686455,-0.040406723,0.041659284,0.07303705,0.022880437,-0.014580292,0.00229484,-0.033161264,0.032211177,-0.037324637,-0.032730203,0.03929668,-0.008953179,-0.020699222,0.10334008,-0.034850165,0.056585703,-0.045603823,0.04653582,0.049751695,-0.020814167,-0.017002,-0.0034019516,-0.0016442747,0.06705259,-0.019445162,-0.02285642,0.023846705,0.0434822,-0.068702854,0.004241129,0.025605837,-0.01415828,-0.06872853,0.0055198087,0.054217476,0.059280477,-0.009263654,-0.018314157,-0.022162775,-0.0055987304,0.00874799,-0.030423138,-0.026382316,0.04158814,-0.0749088,-0.009314707,-0.022335961,0.021178605,0.02075609,0.042996187,0.007332487,-0.006724474,-0.008460759,0.066714354,-0.015583353,-0.06604824,0.03164468,-0.034628443,-0.030251293,0.04583805,0.031428352,0.009257696,-0.06767367,0.08050448,-0.012670654,0.04811671,0.08072988,0.018445816,0.008586319,-0.0284044,0.006425002,-0.033449482,0.013361108,0.0024471935,-0.05630602,-0.058837485,0.01359088,0.0139794545,0.02101593,-0.027057823,0.055026453,0.022358248,0.009340432,-0.008132234,-0.005705688,0.002201616,0.014028611,0.031239899,0.009218189,-0.01932684,-0.015898049,-0.005164327,0.004076314,-0.019459426,-0.03433608,0.0095301345,-0.025970168,-0.008373411,-0.028844757,0.03705289,0.003650434,-0.025362702,-0.014218985,0.07464676,0.018537369,-0.008291992,-0.014056844,-0.009804295,0.008396647,0.015334962,0.005546839,0.05296195,-0.00553292,-0.02396324,-0.039587513,0.028623398,-0.028197369,-0.012480898,0.0491719,0.027401647,0.046075378,-0.056993805,-0.027282136,-0.00649433,0.0049039214,-0.04825721,-0.009874858,0.038804483,0.06744394,0.04322251,0.018258177,0.035190936,-0.012194852,-0.026038373,0.024406644,0.034076843,0.022831058,-0.04751178,0.01860232,-0.00035776838,-0.069978036,0.00085787167,-0.05499516,-0.018137883,-0.028682886,0.058190003,-0.027716126,0.0064919335,0.052589994,-0.000650703,0.024903208,0.024239497,0.01943396,0.054794103,0.03359427,-0.049862865,0.024186693,-0.038470406,-0.051546905,0.016336437,-0.079298384,-0.06732237,-0.02157988,-0.0675611,0.011393762,-0.026848072,-0.027978068,-0.016700476,0.013533385,-0.035013042,0.075238705,-0.010372213,0.0033408888,0.002828479,0.012765047,0.06849999,0.032374285,0.011243424,-0.034134876,-0.024321062,0.033693463,0.010146459,0.04149399,0.043871343,0.0046873195,0.028044187,-0.0068557058,0.011834316,-0.03579528,-0.07062207,-0.033007376,-0.01545749,-0.025845367,-0.0075858445,-0.019703364,0.04553909,0.044917688,-0.02517537,-0.023941606,-0.0006264183,-0.002428798,0.06317747,0.004301439,0.023636362,0.03354911,0.025649391,0.06293584,0.040631708,0.05485529,0.024775581,0.0182206,0.020613965,0.0025038554,-0.0034196442,0.03629997,-0.018016636,0.019236175,-0.033872016,0.025284352,0.007923048,0.01774391,-0.018052725,0.044342734,0.017997403,0.0055400445,-0.04192493,-0.05574902,-0.0051484606,-0.032207813,-0.04396183,-0.0033230486,0.02392445,-0.00078146846,0.054903656,-0.012967285,0.077869944,-0.09212507,0.0045717196,-0.024000723,0.02687379,-0.026899848,-0.015072984,-0.0330501,-0.04228052,0.013674732,-0.046794992,0.039895557,0.029440407,-0.021279503,-0.0032862376,-0.016501639,-0.01831873,0.023698054,-0.040463265,-0.0025262104,-0.013598571,-0.037407145,-0.004240758,0.04628233,0.035443548,-0.008086963,0.035830773,-0.030768657,-0.085529886,-0.01920084,0.049249075,0.044346116,-0.003657832,0.05867229,0.014637685,0.03177671,-0.027210202,-0.01842464,-0.007827815,-0.006260692,-0.008377665,-0.017624952,0.016126908,0.00022503268,-0.03671514,-0.04809693,-0.020951137,-0.023223553,0.077122174,0.021235554,0.03340689,0.042035524,0.047560647,-0.03458931,-0.09028968,-0.030511996,-0.05274557,-0.020925032,-0.03837613,0.054416176,-0.020665692,0.0062981583,-0.019767163,-0.023173546,-0.020961842,0.0054485644,0.015852556,-0.016714536,-0.025281923,-0.023561286,0.007625933,0.016994314,0.015982002,-0.023639731,0.046068616,0.0129579315,-0.030567907,-0.0012065773,-0.055091422,0.017895943,-0.022318874,-0.029627034,-0.0006707988,-0.039959904,0.0017533253,-0.006472743,0.039244004,0.012212265,-0.056517523,-0.049323115,0.030641954,0.05095309,0.029178709,0.0017847219,0.011533219,-0.06190577,0.029291837,-0.0045978674,-0.045246013,0.07620099,0.019031692,-0.05493996,-0.0006549826,-0.039022963,0.0058914972,0.0015369194,-0.013774777,0.030578453,0.039282832,0.027580198,0.024594253,-0.104783855,-0.0060819383,-0.0004151304,-0.0064245574,-0.019525113,0.05334782,0.019452738,-0.014326501,0.029227288,0.0047969227,0.010249063,0.0020924143,0.019306017,0.04705527,-0.03192016,0.015962986,-0.01611122,0.016140461,0.04540012,-0.009316443,-0.01329329,0.033141002,0.028328056,0.008976612,-0.015493202,-0.0046359287,-0.027018553,-0.0004929242,0.03110128,0.059033416,0.013301768,0.03871343,-0.047074962,-0.008436709,-0.038386878,-0.053853665,-0.0051652053,0.013750698,0.018119806,0.015090963,-0.036428254,0.042760883,-0.03621494,0.043082733,0.022205371,0.009060685,-0.02619284,-0.003868131]	Keywords: attention mechanism, scoring function, kernel function, similarity, vectors\nKey Objects: attention, vectors, scoring function, kernel function, query, key, value\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of using a scoring function in the attention mechanism?\n- How does the kernel function transform the input vectors?\n- What are the benefits of using a generalized attention formulation?\n---\nSummary:\nThis text chunk presents a generalized formulation of attention, demonstrating how it can be expressed using a scoring function and a kernel function to measure similarity between input vectors.\nOriginal Text:\n$$z _ { i } = \\sum _ { j } \\frac { \\sin ( q _ { i }, k _ { j } ) } { \\sum _ { j } \\sin ( q _ { i }, k _ { j } ) } v _ { j },$$  \nwhere sim(  ,  ) is a scoring function measuring similarity between input vectors. In vanilla Transformer, the scoring function is the exponential of inner product exp(  ,  ) . A natural choice of sim(  ,  ) is a kernel function K( x, y ) =  ( x )  ( y ) $^{T}$, which leads to  \n$$z _ { i } = \\sum _ { j } \\frac { \\phi ( q _ { i } ) \\phi ( k _ { j } ) ^ { T } } { \\sum _ { j } \\rho _ { i } ( q _ { i } ) \\phi ( k _ { j } ) ^ { T } } v _ { j } \\\\ = \\frac { \\phi ( q _ { i } ) \\sum _ { j } \\phi ( k _ { j } ) ^ { T } } { \\phi ( q _ { i } ) \\sum _ { j } \\phi ( k _ { j } ) ^ { T } }.$$\nContextualized Text:\nTo provide a generalized attention mechanism, the text presents a formula where z_i represents a weighted sum of values v_j, with weights determined by the similarity between query q_i and key k_j, as measured by a scoring function. The scoring function can be a kernel function K(x, y) = (x)(y)^T, which transforms the query and key into feature vectors  before measuring similarity. This approach allows for a flexible way to compute attention and highlights its underlying mathematical structure.	{"tags": ["NLP", "deep-learning", "attention", "transformer", "kernel"], "doc_id": "09545795-62c3-4119-9cc5-315362a4b942", "summary": "This text chunk presents a generalized formulation of attention, demonstrating how it can be expressed using a scoring function and a kernel function to measure similarity between input vectors.", "doc_type": "text", "entities": ["Transformer", "ReLU"], "keywords": ["attention mechanism", "scoring function", "kernel function", "similarity", "vectors"], "key_objects": ["attention", "vectors", "scoring function", "kernel function", "query", "key", "value"], "contextual_text": "To provide a generalized attention mechanism, the text presents a formula where z_i represents a weighted sum of values v_j, with weights determined by the similarity between query q_i and key k_j, as measured by a scoring function. The scoring function can be a kernel function K(x, y) = (x)(y)^T, which transforms the query and key into feature vectors  before measuring similarity. This approach allows for a flexible way to compute attention and highlights its underlying mathematical structure.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["What is the purpose of using a scoring function in the attention mechanism?", "How does the kernel function transform the input vectors?", "What are the benefits of using a generalized attention formulation?"]}
cefd5cdc-24d0-4530-955a-529649e2b5d2	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0041563925,0.012905319,0.0073283873,0.050472666,-0.028261777,0.06022946,0.020228999,0.066839755,0.03601312,-0.034489464,-0.008838817,-0.011063322,0.016226862,0.0075142267,-0.035687238,0.01023255,0.028308047,0.06612934,0.002435362,-0.02308725,0.03295138,0.0059576943,0.022055188,-0.02886289,0.029657865,0.030870402,8.070286e-05,0.0043410477,0.0018135933,0.016812654,0.0044208015,-0.06858206,0.02206691,0.018818913,0.014582656,-0.03597912,-0.009300148,-0.061648905,-0.007219192,-0.0061305882,-0.015946675,0.044190153,-0.05880669,0.0027656178,0.015227627,-0.08033642,-0.09141816,-0.030165121,0.03360393,-0.031185126,-0.01637078,-0.004061374,-0.054653704,-0.044820685,-0.0105441315,0.015745956,0.009724161,-0.04386003,0.0343082,-0.09627172,-0.014450948,-0.019270996,-0.015882352,5.4769724e-05,0.02504457,0.019799883,0.041302696,0.018879512,0.044389553,0.12285206,-0.054319117,0.016026944,0.008544293,-0.06440544,0.09573593,0.03850256,-0.044776402,-0.036034517,-0.07172457,-0.009556829,0.002077741,0.047600973,-0.023406442,-0.020584105,0.07809103,-0.023368897,-0.03320592,0.018308591,0.035525844,-0.050918933,0.018177278,0.012219313,0.005431694,0.064319454,-0.04068413,-0.070936665,-0.017913101,-0.022528522,0.02223913,-0.06663419,0.03041496,-0.017275948,0.0842773,0.11924156,-0.0046461164,-0.05253067,-0.008615552,-0.03375491,0.03258876,0.033376265,-0.020901186,0.02970711,-0.08824648,0.0003894472,-0.03687064,0.031215742,-0.040012587,0.009068343,0.015532486,-0.029009938,0.024308007,-0.045711916,0.011840396,0.020904113,0.055385936,0.037070833,0.055694863,-0.026564226,-0.04603713,0.016905718,0.075055264,0.017432233,-0.04960685,0.010108315,-0.0124036325,0.022846105,0.063365646,0.011259852,-0.009233884,0.02595156,-0.021478394,-0.018571194,-0.054639272,0.02218726,-0.0009941633,0.02653964,-0.062856495,0.038377237,-0.021608723,0.053422645,0.02809109,0.029344436,0.006230629,0.0004406815,-0.01537854,-0.017398957,-0.010066517,-0.072792485,0.011063281,-0.0036392445,-0.040277768,-0.020690452,-0.008840268,0.09021853,0.022425039,0.13137385,0.004704637,-0.02363979,0.034103867,0.050707705,-0.02903024,-0.018770322,-0.005026725,-0.03973142,-0.010241233,0.0029170662,0.031614188,-0.04611777,0.0046349615,-0.03361289,-0.02171728,0.03983671,-0.0067057647,0.036738995,-0.03469592,0.031367343,0.031681377,0.038490925,-0.0100793885,-0.044443626,0.0028091501,-0.025915494,-0.00884555,0.00572476,0.0069824588,0.037502047,0.03957503,-0.08727291,-0.029456453,-0.040704764,0.0044892128,-0.022339957,-0.051475573,-0.021184424,0.035156872,-0.0065789446,0.0069814436,0.010469181,0.032911833,0.050358165,0.010320373,-0.006774574,-0.03261175,0.048707377,-0.0037871164,0.011019503,0.0022345854,0.01978072,0.027056878,-0.0069954842,-0.035420425,0.011235645,-0.013960554,0.061020702,-0.02233777,0.051630974,0.008902536,0.020066317,-0.0054551265,-0.027027829,-0.013295169,0.030622581,-0.057348106,0.033323724,-0.042725246,0.030313816,0.027929015,0.0008495946,-0.02031229,0.031484768,0.045679003,0.080993764,-0.029244518,0.083176956,0.0016708898,0.0069925645,0.022542756,0.014019052,-0.004882614,-0.0072761565,-0.01211557,0.048069976,0.06704824,0.009799294,0.011561177,-0.05301571,-0.020930154,-0.0116274785,0.027641809,-0.01853112,0.022923995,0.003131866,-0.016535327,0.0272666,0.022145402,-0.005983844,0.016279332,0.025114099,-0.0056043635,-0.019656276,0.044248696,0.031692393,0.01777675,0.011610538,-0.016331755,0.011657371,0.03956353,-0.005905781,0.03838928,-0.029604355,-0.027750472,-0.01219937,0.019025903,-0.0013709256,0.02324246,0.0055070487,0.0180456,-0.048681095,-0.010570808,-0.00016374835,-0.02742397,-0.01954563,0.03973565,0.0045996094,-0.027033059,0.022623355,0.0006639071,-0.04446949,-0.011489712,-0.002466337,0.095035054,-0.01020411,0.017440395,0.00601599,0.048351247,0.033361994,-0.045010917,0.07216443,0.041956518,0.021821896,0.014635188,-0.029063791,-0.02158974,-0.0022336687,0.022666551,0.078135595,0.10285915,0.015811218,0.014117889,0.019457994,0.019759076,-0.02316719,-0.008792478,0.040837377,0.041808963,-0.0029509265,0.05537499,-0.025206514,-0.06574554,0.048490226,-0.05751354,0.04562503,0.06227972,0.0001522493,-0.006381717,0.022763943,0.001074901,0.023711577,0.027694182,-0.0014826064,-0.0129065635,-0.040851306,0.0011852711,0.009484009,-0.00924623,0.044520788,0.009787336,-0.008751342,0.041980844,0.009732188,0.011397549,0.013267229,0.012407729,0.01998984,0.039341122,-0.013599275,0.00056032964,0.0059505175,-0.012609423,-0.018440774,0.06777647,-0.06250588,0.025823826,-0.042075265,-8.315727e-06,-0.014208652,-0.060751133,0.0018536461,0.037053693,-0.022055179,0.0018485663,-0.011424749,0.052851327,-0.066109836,-0.010389257,0.029622173,0.06923271,0.009310067,-0.011984598,-0.018551316,-0.026311845,0.0036246895,-0.028152747,-0.050187897,0.052879613,0.0004387526,-0.011213954,0.102258295,-0.01768414,0.062453467,-0.04893282,0.016778776,0.014750625,-0.0032390421,-0.022864062,-0.025232235,0.019405587,0.07767219,0.03556844,0.010312167,0.059119653,0.019797618,-0.0592938,0.013125848,0.03447669,0.0049960883,-0.054057594,0.025294883,0.045574866,0.033578284,0.00033665897,-0.021510853,-0.0080379415,0.005935341,0.05043688,-0.027783524,-0.006544527,0.026833996,-0.049043402,-0.020525998,-0.027355194,0.024436222,0.02798923,0.020917002,0.050417617,-0.022801002,-0.040273566,0.046844892,-0.013058723,-0.02535387,0.02367001,-0.027089614,-0.03012093,0.03525192,0.01038369,-0.019156298,-0.07218631,0.067011766,-0.04673864,0.018305164,0.05643472,-0.044035833,0.0055511245,-0.0059377067,0.030216476,-0.043939833,0.0353449,0.035523288,-0.020986538,-0.043101348,0.030840695,0.03523227,0.020123625,0.0055639716,0.06768117,-0.006838286,0.013863577,-0.015853707,-0.022171387,0.021863686,0.012245648,0.039999485,0.015605184,0.010049856,-0.022521324,0.0047667557,-0.013185486,0.0045058476,-0.006670708,-0.009950597,-0.023763202,0.025636826,-0.05720187,0.038577635,-0.009144692,-0.017927185,0.014791758,0.044595353,0.033138372,0.013800211,-0.07675637,-0.024849992,-0.0023115375,-0.0039029026,0.032213036,0.013106107,0.0072840494,-0.013646678,-0.026157597,0.045793563,-0.0074320566,0.017359672,0.012199564,0.013235258,0.050053895,-0.07216548,-0.009288709,-0.0021225328,0.0037171864,-0.060717467,-0.021795308,-0.0014141187,0.019457327,0.04330463,-0.0054630223,0.021156121,-0.037633244,0.02616641,-0.002458818,0.06870112,-0.0057876837,-0.042359397,0.0076497244,-0.039343618,-0.065564625,0.032108527,-0.015247206,-0.012542764,0.010856522,0.025911445,-0.0028537533,0.010478437,0.042127375,0.034543116,0.01610408,0.0005944258,0.041997086,0.013319732,0.051942643,-0.08507467,0.012297392,-0.048583366,-0.040715784,0.014418836,-0.061238356,-0.050275937,0.016808817,-0.074450344,0.022926627,-0.0063978,-0.021169499,-0.075217,0.0032100556,-0.0106750075,0.065838605,-0.012330218,0.04996764,-0.0067489804,-0.016226782,0.08206779,0.056548506,0.020776767,-0.01541216,-0.039447907,0.008982825,0.001655979,0.07183987,0.03453634,0.01527738,0.05462968,0.02140652,0.009337555,-0.043280836,-0.062486272,-0.05086894,-0.020936955,-0.029038535,-0.005246977,-0.021475803,0.029260194,0.0051486557,0.011872084,-0.025608324,0.03556013,-0.0005071712,0.07119034,0.018163688,-0.000112058915,0.04527718,0.017191956,0.04267138,0.035995364,0.077041104,0.004519286,0.006618052,-0.0029983555,-0.020709481,0.024310961,0.036703955,-0.015437887,-0.019106837,-0.030349987,0.0038385764,-0.008466042,0.002788746,-0.032191016,0.056804016,0.012885741,0.014133303,-0.058434863,-0.04991403,-0.013433801,-0.054549754,-0.023912255,0.014098383,0.033864353,-0.037821624,0.01292254,-0.023207659,0.06813993,-0.12068266,-0.015228106,-0.023897525,0.036977313,-0.02302075,-0.014352475,-0.050661817,-0.042004395,0.014890614,-0.023457965,0.012474761,-0.005398671,-0.012025395,0.011769307,-0.018215826,-0.005546519,0.017205821,-0.009681855,-0.0133787645,-0.010645225,-0.027499784,-0.0005503378,0.07094954,-0.017932404,-0.0010256099,0.041038238,-0.015513569,-0.05907882,0.007281246,0.03916435,0.011186037,0.002064967,-0.012719045,0.00023590153,0.0372887,-0.048254225,-0.00023723138,-0.063095786,-0.005573497,-0.0034112178,0.01578803,0.025208933,0.0021248918,-0.022730375,-0.052045546,-0.027919833,-0.027093379,0.03875007,0.034303956,0.052035928,0.05385163,0.05537028,-0.05269712,-0.046083722,-0.030534936,-0.02273835,0.0037408643,-0.03444309,0.06859439,0.0013411349,-0.005481536,-0.03410619,-0.019010032,-0.0037410087,0.015594025,0.019226298,-0.009637655,-0.016149638,-0.022532804,0.025287274,0.02276869,0.021313779,-0.020261131,0.07454539,0.03009112,-0.037070967,-0.026481856,-0.059768286,-0.05693562,-0.0038746148,0.005112936,-0.018562939,-0.017901246,-0.025345678,-0.022647083,0.04171313,-0.014362388,-0.05746919,-0.02262673,0.022228112,0.044793703,0.070559755,0.008727966,0.010666001,-0.1202378,0.042204086,-0.0029603904,-0.038840134,0.050208535,-0.026960788,-0.0338973,-0.017278085,-0.004529099,-0.039988678,0.00873741,0.008420314,0.021263909,0.03564265,-0.008960872,0.004097865,-0.10802785,-0.011869176,-0.034478154,6.331488e-05,-0.027327906,0.05165361,0.011156484,0.0133240875,-0.0067627314,-0.020268315,-0.00322498,0.021530028,-0.007506819,0.06742004,-0.027421499,0.042741444,-0.010792909,0.013861123,0.061609387,-0.055669397,0.012849556,0.022315295,0.04397202,0.0047990954,0.019416923,0.025720358,-0.041692764,-0.029260118,0.033671807,0.030919991,-0.011522057,0.05273815,-0.011635688,-0.02940945,-0.038042817,-0.03860513,-0.070162565,0.01847726,0.058003955,0.020071618,-0.08207111,0.00876811,-0.0492319,0.06222834,0.032839723,0.017230371,0.017040273,0.023078473]	Keywords: linearized attention, feature map, cumulative sums, RNN, autoregressive attention\nKey Objects: feature map, cumulative sums, memory matrix\nRefers to Images: None\nHypothetical Questions:\n- How does pre-computing cumulative sums improve the efficiency of the Transformer decoder?\n- In what way does linearized attention allow Transformer decoders to function like RNNs?\n- What are the components of the memory matrix, and how does it enable the model to 'retrieve a value'?\n---\nSummary:\nLinearized attention can be achieved by computing cumulative sums of feature-mapped keys and values, enabling Transformer decoders to function similarly to recurrent neural networks (RNNs) by pre-computing certain terms.\nOriginal Text:\nwhere  Q denotes outer product of vectors. Based on this formulation, attention can be linearized by first computing the highlighted terms $\\_{j}$  ( k$\\_{j}$ )  v$\\_{j}$ and $\\_{j}$  ( k$\\_{j}$ ) $^{T}$. This could be especially beneficial for autoregressive attention, as the cumulative sums S$\\_{i}$ = $\\_{j}$$\\_{-}$$\\_{1}$  ( k$\\_{j}$ )  v$\\_{j}$ and u$\\_{i}$ = $\\_{j}$$\\_{-}$$\\_{1}$  ( k$\\_{j}$ ) can be computed from S$\\_{i}$$\\_{-}$$\\_{1}$ and u$\\_{i}$$\\_{-}$$\\_{1}$ in constant time. The effectively enables Transformer decoders to run like RNNs.  \nAn interpretation of Eq. (16) is that the model maintains a memory matrix by aggregating associations represented by outer products of (feature mapped) keys and values, and then retrieve a value by multiplying the memory matrix with feature mapped query with proper normalization. There are two key components in this approach: (1) feature map  (  ) , and (2) aggregation rule.\nContextualized Text:\nTo enable Transformer decoders to run like RNNs, linearized attention allows for the pre-computation of cumulative sums of feature-mapped keys and values. Specifically, these cumulative sums (S =   (k)  v and u =   (k)) can be calculated from previous values, allowing for efficient computation of the attention mechanism.	{"tags": ["attention", "transformers", "linearization", "RNN"], "doc_id": "cefd5cdc-24d0-4530-955a-529649e2b5d2", "summary": "Linearized attention can be achieved by computing cumulative sums of feature-mapped keys and values, enabling Transformer decoders to function similarly to recurrent neural networks (RNNs) by pre-computing certain terms.", "doc_type": "text", "entities": ["RNN", "Transformer"], "keywords": ["linearized attention", "feature map", "cumulative sums", "RNN", "autoregressive attention"], "key_objects": ["feature map", "cumulative sums", "memory matrix"], "contextual_text": "To enable Transformer decoders to run like RNNs, linearized attention allows for the pre-computation of cumulative sums of feature-mapped keys and values. Specifically, these cumulative sums (S =   (k)  v and u =   (k)) can be calculated from previous values, allowing for efficient computation of the attention mechanism.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["How does pre-computing cumulative sums improve the efficiency of the Transformer decoder?", "In what way does linearized attention allow Transformer decoders to function like RNNs?", "What are the components of the memory matrix, and how does it enable the model to 'retrieve a value'?"]}
b6a5bda8-1131-4ab4-b117-e21e5f6897e2	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.021140015,-0.0008182994,0.0014361343,0.011681913,-0.055978294,0.07547504,0.011771317,0.043347854,0.0043869247,-0.010870382,0.019234478,-0.0012705331,0.037388135,0.002361798,-0.06060704,0.007327148,0.038282275,0.082386196,0.0035243116,-0.003787646,0.03140934,0.007178728,0.013797573,-0.040543143,0.020932607,0.031658247,0.035469227,-0.016516315,0.024192732,0.017923122,0.007466776,-0.025645485,0.037078004,-0.04110393,0.017893946,0.0076391096,-0.027872164,-0.06443684,-0.02010391,0.007506601,-0.044540666,0.08934692,-0.09128169,0.006187205,-0.006414202,-0.056141917,-0.12485462,-0.051883508,0.019137219,-0.013582335,-0.04473241,0.016125783,-0.07818148,-0.016365202,0.0046539917,-0.010938408,-0.0030444567,-0.03348154,0.008262124,-0.100997955,-0.03862038,-0.014974772,-0.017931273,0.012310763,-0.014617598,-0.020385917,0.028633792,0.00035764076,0.053994115,0.11800607,-0.035351995,-0.012704631,-0.039944615,-0.05990423,0.09601399,0.052045792,-0.056335434,-0.03378518,-0.06948411,-0.02316896,-0.020939603,0.05706293,-0.03330458,-0.056453526,0.0835658,-0.022009375,-0.02451597,0.0154645275,0.07171879,-0.06944673,-0.03515539,0.006364138,0.042772092,0.0570785,0.007970142,-0.06515889,-0.03138035,-0.055430956,0.01749722,-0.049046665,0.026413377,0.0027737992,0.060985945,0.110795714,0.023103919,-0.03850883,-0.02423186,-0.038480755,0.04347741,0.026124645,-0.009522079,0.0078017833,-0.04567304,-0.005508986,-0.031170564,0.0069049853,-0.031364348,0.019316046,0.005280232,0.016029658,0.033354554,-0.035868913,-0.01822246,-0.0063157124,0.036857657,0.045744736,0.030919807,-0.023901107,-0.014384347,0.02750868,0.05842131,0.022663321,-0.0030171603,-0.018902201,-0.0133505585,0.026343478,0.10293259,-0.0045913137,-0.007138766,0.0021399024,-0.013424238,-0.037791327,-0.035627346,0.030218955,0.00036834835,0.036183126,-0.08552065,0.026554374,-0.0043806178,0.055222392,0.021110674,0.0038797108,0.036268443,0.0018055143,-0.023778442,-0.012185818,0.0033335635,-0.04554054,0.0065684547,-0.012754045,-0.06588932,-0.031260215,-0.0058082915,0.050767086,0.012660493,0.059827838,-0.015704358,0.022265812,0.06724881,0.050731823,-0.033178277,-0.014571237,0.033602003,-0.039701007,-0.023975851,-0.011305869,0.018618101,-0.054800745,-0.014507694,-0.006197882,0.022775283,0.035921387,-0.009543086,0.032172345,0.00303955,-0.006426735,0.021029608,0.042118434,-0.03639586,-0.014435415,-0.008286671,-0.03730191,-0.031495973,-0.0015574372,0.041974083,0.057779763,0.043410137,-0.094689034,-0.06373912,-0.036929473,-0.023992777,-0.004090583,-0.012951735,-0.02634997,0.02760891,-0.022936406,0.025145829,-0.0024031003,0.018200275,0.002706093,0.017029686,-0.038442776,-0.0288221,0.0431602,0.019498846,0.005782631,0.024578135,0.014719226,0.038413864,-0.025288323,-0.061509665,0.03454101,0.0023110257,0.037015393,-0.027094008,0.006740559,-0.05211076,0.036249667,0.0026442201,0.008460694,0.043661147,-0.00061119726,-0.020999856,0.03460988,-0.037556805,0.00515309,0.004858943,-0.0087874755,-0.04641382,0.02751255,0.04273566,0.03335832,-0.014332916,0.07957954,0.00089875446,-0.004810682,0.048440997,0.05839952,-0.0005624265,0.028485736,0.01310977,0.053098373,0.063766725,0.021945145,0.008038336,-0.038553268,0.035558682,-0.0043339045,0.009386521,0.006280741,0.0047997013,0.05168682,0.026860986,0.01662125,-0.008501712,-0.0096751405,0.03338621,-0.0005737303,0.0020019312,-0.02760493,0.059845887,0.038729765,0.03655103,-0.049897987,-0.043495327,0.0048453715,0.04382032,-0.01256942,0.027779233,-0.032052465,-0.03144883,0.0006352931,0.011314897,0.0002303403,0.026209747,0.026562672,-0.007863093,-0.008912652,-0.015585706,-0.036986012,-0.027551068,-0.036337975,0.0049144593,-0.003413125,-0.010559748,0.01670842,-0.002411008,-0.028453464,-0.053007595,0.020532971,0.10788221,-0.03207573,0.015223941,0.0007781193,0.007304242,-0.019668434,-0.06593702,0.04316083,0.06338701,0.020956507,-0.0016435251,-0.017697066,-0.012160223,0.022720994,0.008507362,0.07969383,0.06049162,-0.03113895,-0.018555403,0.024511028,0.02297846,-0.036899563,-0.010596371,0.025485842,0.02177757,-0.016309535,0.029638223,-0.015897343,-0.06524099,0.07635799,-0.05847691,0.04636394,0.066315904,0.011821628,0.007326984,-0.02301914,0.037131466,0.032953303,0.01498253,0.0054232664,-0.028180605,-0.05778488,-0.012309594,-0.006152706,0.0035824499,0.012631231,0.02675115,0.010224313,0.003686358,0.012292321,0.04587343,0.03539434,0.02183691,0.01253915,0.036334172,-0.01751817,0.010419079,-0.0015299376,-0.00084761105,-0.015852934,0.049075812,-0.048143744,0.02469533,-0.024651092,-0.00858499,-0.0074371286,-0.05866697,-0.018542267,0.06725789,-0.049948957,0.008061958,-0.0069743474,0.07050763,-0.03686274,-0.009722178,0.05221641,0.083351515,0.0133706955,-0.023636905,0.00057162135,-0.03289383,0.004402217,-0.039121877,-0.016646737,0.036885656,0.020911785,-0.02336858,0.07900099,-0.0017948269,0.08697861,-0.02224724,-0.015050666,0.020080999,-0.034783807,-0.015519254,0.018311279,0.04770029,0.07009808,-0.0075125606,-0.026416799,0.05516923,0.02575781,-0.04507956,0.012422082,0.028885793,0.01451573,-0.036117177,0.024000796,0.034801338,0.045604054,0.0021256544,-0.033152502,-0.037706792,0.0018747706,0.028306697,-0.0023109498,-0.0060853325,0.043359615,-0.050918497,-0.009194575,-0.03681339,0.058280967,0.00893885,0.011106876,0.03218806,-0.018350787,-0.046026528,0.019165069,0.019475745,-0.05231993,0.013863583,-0.06002241,-0.045823812,0.05337644,0.037864387,-0.017986367,-0.073246196,0.06417799,-0.015935142,0.056385547,0.058229778,0.025362186,0.010521283,-0.014370613,0.03267187,-0.03539927,0.044067416,0.019639073,-0.022287171,-0.034070544,0.04069388,0.03672458,0.0164098,0.022643007,0.0389605,-0.035215154,0.058246095,0.0013458179,-0.037697755,0.026445717,0.0023703184,0.045885395,0.006491962,0.017343469,-0.03246987,-0.022599634,0.019712863,0.027731344,-0.014723839,0.021722697,-0.008389318,0.03109813,-0.053582944,0.0155032575,0.017578207,-0.0044991416,-0.0063794437,0.046342768,0.047157317,0.046108007,-0.07964128,-0.0051258747,-0.005558045,0.016270194,-0.002302686,0.02402351,0.0037143955,-0.03538295,-0.004111942,0.025188133,0.0033715027,0.051338557,0.05393619,0.0189805,0.028473176,-0.044579428,0.0037611197,0.011789713,0.0394622,-0.015191022,-0.043678164,-0.016559942,0.04206598,0.04541037,-0.022657938,0.0261835,-0.015068974,0.028890388,-0.030512536,0.08373462,0.04561395,-0.019817768,0.0308895,0.0055416073,-0.06639913,0.01320798,0.021930387,-0.017709196,0.046966884,0.07321525,-0.01904322,0.0024363373,0.06191799,0.0020774452,0.041999552,0.015157709,0.018797003,0.04278315,0.04586676,-0.028022068,0.004338693,-0.04051391,-0.04198888,0.013472278,-0.02828306,0.0035792408,0.0050374405,-0.027293239,0.0038294354,-0.018162807,-0.0006473364,-0.05202817,0.015119274,0.016433023,0.09657414,-0.05150246,0.024616469,-0.016064946,0.0041236486,0.07404012,0.0047229263,0.016677467,-0.030341957,-0.038779046,0.0058880905,0.018366773,0.05542099,0.02585504,0.023078837,0.06040522,0.01950918,-0.009858367,-0.07041877,-0.051866204,-0.04417878,-0.04550822,-0.006349852,0.0033160667,-0.04151488,0.063851155,-0.011546766,-0.0036987045,-0.031272046,-0.020144736,-0.0062903175,0.05791574,0.017380636,-0.040816884,0.034028888,0.02211116,0.055933144,0.031039106,0.06916841,-0.0010735963,0.0031044306,0.0059476416,-0.029525641,-0.013184079,0.021756405,-0.03786039,-0.041073028,-0.04799515,-0.010309571,0.021134203,-0.025119038,-0.0066134036,0.017499592,0.023200218,-0.0052659265,-0.06354965,-0.050474342,-0.048992973,-0.0571118,-0.04111161,0.007868752,0.017505586,0.025992276,0.0322828,-0.036679037,0.057731662,-0.06526923,-0.024541017,-0.045400903,0.017405469,0.010594855,-0.04884592,-0.017396607,-0.029043246,-0.013214789,-0.03160872,-0.014080927,0.03290172,-0.008764345,0.0038074686,-0.0070340554,-0.0010540687,0.011319471,-0.028101247,-0.0035198038,-0.02275624,-0.013303338,-0.01201062,0.05589562,-0.018154267,0.014361389,0.025351007,-0.059879094,-0.07047738,0.002091963,0.03817424,0.07876304,-0.0026353253,0.024647769,-0.0049656886,0.0035640006,-0.039125774,0.0045145233,-0.046089496,-0.0047841417,-0.008666861,0.0052111372,0.025208812,0.010232724,-0.061720002,-0.0048404383,-0.009935988,-0.007912719,0.028177056,-0.013077009,0.03323115,0.03724218,0.08281692,-0.023856983,-0.0797779,-0.005437758,-0.015910385,-0.005965758,-0.050663378,0.053468157,0.0047094654,0.0042794663,-0.03771712,-0.030668389,0.005926455,0.0045977384,0.00080787344,-0.0056056655,-0.042331256,-0.016132532,0.0050400207,0.042421672,-0.016197711,0.005435537,0.03876165,0.06003162,-0.043832757,0.02080865,-0.049483534,-0.02198824,0.007366187,-0.0038545653,0.005857134,-0.018336507,-0.019484634,0.00039374255,0.038275048,-0.0005479587,-0.048612893,-0.05677545,0.014266217,0.014114646,0.087383054,-0.020577498,0.0028140838,-0.10048386,0.057662416,-0.00941004,-0.032419454,0.07339691,0.010792083,-0.036429744,-0.008962184,0.016060151,-0.0263704,0.020696761,0.0054237214,0.04000799,0.003881273,-0.006547819,0.0057501807,-0.04406225,0.022335887,-0.018803021,-0.018187443,-0.014467326,0.040403303,0.017480977,0.02394925,0.018664554,0.0027230338,-0.035139196,0.027749145,0.0025145884,0.10123873,-0.065835476,0.058499783,-0.03252832,0.026771024,0.097136125,-0.0007433313,0.0075489376,0.00966789,0.02166309,-0.0073400107,-0.0005464609,0.015167945,-0.0186664,-0.024781918,0.008742607,-0.014933014,-0.008572552,-0.00096958014,-0.031015145,-0.028890949,-0.025401931,-0.026368773,-0.009990196,0.040447272,0.03642619,0.005810644,-0.05268459,0.008585445,-0.03667506,0.06741147,0.028703565,-0.011968329,-0.00299883,-0.029787192]	Keywords: feature map, elu, linear transformer, transformer\nKey Objects: feature map, elu\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of a feature map in the context of linearized attention?\n- Why might a feature map like elu(x)+1 perform similarly to standard dot product attention?\n- How does the choice of feature map impact the overall performance of the Linear Transformer?\n---\nSummary:\nLinear Transformer employs a feature map (x) = elu(x)+1 which, despite not aiming to directly approximate dot product attention, has demonstrated performance comparable to standard Transformer models.\nOriginal Text:\n4.2.1 Feature Maps. Linear Transformer [62] propose to use a simple feature map  ( x ) = e lu ( x$\\_{i}$ )+1. This feature map does not aim to approximate dot product attention, but is empirically proved to perform on par with the standard Transformer.  \n$^{6}$Similarly, the partition term D can be computed with  ( Q ) (  ( K ) $^{T}$1 T ) in linear time.  \nPerformer [18, 19] uses random feature maps that approximate the scoring function of transformer. The random feature maps take functions f$\\_{1}$ ,    , f$\\_{i}$ : R  R and h : R D  R .  \n$$\\phi ( x ) = \\frac { h ( x ) \\left [ f _ { 1 } ( \\omega _ { 1 } ^ { T } x ), \\cdots, f _ { m } ( \\omega _ { m } ^ { T } x ), \\cdots, f _ { i } ( \\omega _ { 1 } ^ { T } x ), \\cdots, f _ { i } ( \\omega _ { m } ^ { T } x ) \\right ], \\quad ( 1 7 )$$  \nwhere $\\_{1}$,    , $\\_{m}$  D are drawn from some distribution D  P ( R $^{D}$) .\nContextualized Text:\nTo facilitate linearized attention, the Linear Transformer utilizes a simple feature map, (x) = elu(x)+1. This feature map does not directly attempt to replicate dot product attention but has been empirically shown to achieve similar performance to the standard Transformer architecture.	{"tags": ["transformer", "architecture", "linearization"], "doc_id": "b6a5bda8-1131-4ab4-b117-e21e5f6897e2", "summary": "Linear Transformer employs a feature map (x) = elu(x)+1 which, despite not aiming to directly approximate dot product attention, has demonstrated performance comparable to standard Transformer models.", "doc_type": "text", "entities": ["Linear Transformer", "ReLU"], "keywords": ["feature map", "elu", "linear transformer", "transformer"], "key_objects": ["feature map", "elu"], "contextual_text": "To facilitate linearized attention, the Linear Transformer utilizes a simple feature map, (x) = elu(x)+1. This feature map does not directly attempt to replicate dot product attention but has been empirically shown to achieve similar performance to the standard Transformer architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["What is the purpose of a feature map in the context of linearized attention?", "Why might a feature map like elu(x)+1 perform similarly to standard dot product attention?", "How does the choice of feature map impact the overall performance of the Linear Transformer?"]}
53b336be-ad75-41ce-b70e-78dbf98bc905	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03201639,0.0023319575,0.0055487826,0.049685523,-0.028495613,0.07860727,-0.0043758405,0.029803041,0.007460735,-0.0115122385,-0.0026436185,0.027257364,-0.0033036866,0.018857058,-0.034731995,0.0014183128,-0.0020646735,0.01416537,-0.024411805,-0.045059457,0.02553886,0.0076171337,0.054922767,0.016035434,0.027793253,0.065783784,0.05276241,-0.013781259,0.010800395,0.055440985,0.016801415,-0.031170467,0.025496999,-0.003690578,0.005781163,0.03528798,-0.0034002597,-0.066207066,0.0038657477,0.0039748386,-0.02107181,0.053001754,-0.025374733,0.0075411694,-0.0006856192,0.021478374,-0.06700441,-0.006053182,0.0046286043,-0.011941332,-0.06709077,0.021458264,-0.024850117,-0.00873694,0.0035877342,0.016051091,0.039698686,0.0057327305,0.0038485704,-0.09758461,-0.04976262,-0.005898989,-0.023400916,-0.013285003,-0.026826777,-0.04316613,0.06265895,-0.022355063,0.08831707,0.13257052,0.023498097,-0.0082583735,-0.026172465,-0.041029193,0.10198859,0.04314585,-0.026082497,0.03313423,-0.011616595,-0.04987795,0.012976522,0.047390312,-0.029020358,-0.0045206896,0.07774749,-0.008959071,-0.03409548,-0.04992601,0.0505209,-0.04657643,0.003971094,0.0010593609,0.02619647,0.042194284,0.009328736,-0.05134027,-0.03645131,-0.006052195,-0.0024460887,-0.07260103,0.030211285,-0.008687464,0.09236424,0.12173952,-0.019622272,-0.03525923,-0.0009736557,-0.024601948,0.012474659,-0.019073838,-0.01397393,0.018979765,-0.09525447,0.003138187,0.018129999,0.00029596468,-0.030008182,-0.014718996,-0.03230029,0.025552528,-0.004475327,-0.019784668,-0.0045065144,0.01724854,0.01987506,0.038218994,0.007960096,-0.03607643,-0.02250314,0.00369919,0.066154726,-0.0009201472,-0.011045475,0.009119726,-0.011210826,0.051293474,0.04858273,-0.028882911,-0.018161004,-0.001529598,-0.0026256698,-0.0085830735,-0.03482624,0.023597833,0.010059247,0.013140135,-0.06679534,0.04567062,-0.057758965,0.044252776,0.032327816,-0.016057953,0.033403646,-0.0387833,-0.031113224,-0.016508725,0.012745382,-0.020935856,0.024018826,-0.042824462,-0.032124117,-0.0101333475,-0.001875087,0.058517378,0.0058479533,0.070064284,0.029138617,-0.0076703667,0.010963057,0.04963863,-0.041054063,-0.021894122,0.024153527,-0.038791925,-0.021994531,-0.014633166,0.02931415,-0.027448263,-0.00031675023,-0.026912369,0.023840155,0.006856856,-0.0029368165,0.018291507,-0.02258052,0.07346612,-0.026442224,0.059682317,0.008890517,-0.040078033,0.00718941,-0.07344918,-0.057637226,-0.0073865536,0.02208907,0.06679838,0.029836072,-0.061714653,-0.040681683,0.010780184,-0.03375102,0.017004916,0.01377029,-0.009009196,0.02074218,0.010421689,0.029456388,0.008085346,-0.018158745,0.0141541995,0.016057538,-0.0038679417,-0.051293783,0.050729863,0.0065467446,0.029266207,0.030013492,0.016149353,0.039399426,-0.021644115,-0.020033287,0.02139806,-0.038867168,0.049736578,-0.0094157895,-0.009947078,-0.027225329,0.0076534976,0.040356882,0.019217113,0.058802627,0.052069034,-0.049486306,0.056351695,-0.08265513,0.0071395035,0.04865348,-0.035229735,-0.012270785,0.015972098,0.028455403,0.06938487,-0.02369324,0.06375947,0.00030882354,0.040402032,0.04230976,0.04304949,-0.025934625,0.036450326,-0.025201427,0.071127795,0.0584911,-0.019941522,-0.0032381658,-0.055233855,0.031031637,0.02024985,0.031212311,0.016052334,0.037550803,0.012032892,-0.009084024,-0.0077666747,-0.011690689,0.00041770763,0.048682448,-0.00815251,-0.018974388,-0.045069236,0.02978824,0.048142202,0.015796797,0.03394565,-0.061531056,0.039259445,0.018645294,-0.00062829134,0.030254498,-0.018692436,-0.015096296,-0.020417906,-0.009068503,0.003687988,0.018190028,0.049439557,0.0077876057,-0.0024895961,-0.017654208,-0.028025482,0.023026135,-0.07290984,-0.028025776,0.0028962237,-0.02229584,0.033394333,0.000858254,-0.0479129,-0.03666442,0.015509513,0.10736366,-0.02527474,0.039272524,0.00787763,0.02209688,-0.007882967,-0.039642166,0.04889854,0.013943058,0.013927873,-0.0004906733,-0.054183453,0.024083164,0.024948573,0.04230126,0.078462765,0.08452463,-0.06298615,-0.012452445,0.0088231005,0.018160053,0.004918585,-0.022062812,0.013084337,0.009566927,0.0065533505,0.032593664,-0.021698546,-0.037281785,0.03464515,-0.049601454,0.0430516,0.07275994,0.005403631,0.012423616,0.021231407,0.029431062,0.032348618,0.03133514,-0.020534148,-0.021655621,-0.044989176,0.051711563,0.0026251054,-0.056488242,0.05265685,0.05451255,0.011622419,0.00022738807,0.0018155905,0.035255093,0.016420156,-0.011325476,0.0138118975,0.06036793,-0.011024429,0.003971662,0.010417848,-0.02964387,-0.006606703,0.06571553,-0.06802922,0.030418899,-0.017690647,0.022762794,-0.045613687,-0.0411576,0.023721267,0.042883486,0.0074653644,-0.017528776,-0.038693834,0.03399025,-0.026075833,-0.016572323,0.066770256,0.018092029,0.010053957,0.00463428,-0.008216324,-0.06458132,0.0045718225,-0.062980846,-0.012332128,0.037555568,-0.032838758,0.004543905,0.09274602,-0.04000651,0.008336145,-0.041378245,0.00642149,0.005304051,-0.015372414,0.001307879,0.029158788,0.00828484,0.07633918,0.018295247,-0.021090817,0.031753734,0.01327873,-0.061737787,0.04963038,0.024951696,-0.034247007,-0.0826831,4.1664844e-05,0.028483812,0.025927512,0.0005180011,-0.02099613,-0.055391792,-0.020224607,0.023964232,-0.013458327,0.017585745,-0.0072296886,-0.05373251,0.0146499295,-0.05044841,0.01680084,0.04077725,0.041759007,0.051985625,-0.035398193,-0.045859456,0.038456056,-0.008861255,-0.07758456,0.019529102,-0.015493073,-0.01746885,0.007837906,0.04913464,-0.003689371,-0.050197538,0.06936382,-0.024285246,0.03724357,0.06863389,0.0031279465,-0.0050111082,-0.05419141,-0.013682622,-0.03184203,0.053777326,0.0077347043,-0.01402835,-0.06332659,0.054942258,-0.019720228,0.054510433,0.00087970786,0.068209685,3.66046e-05,0.010653529,-0.006784196,-0.068762146,-0.0035084558,0.014395613,0.034938782,-0.0063839406,0.01246858,-0.026454132,-0.03227716,0.016934283,-0.015876928,-0.013384278,-0.038481753,-0.009208596,0.02422859,0.011112108,0.06890709,-0.03717345,0.016521202,0.010451076,0.039081935,0.04554561,0.057164457,-0.05673246,-0.045774825,-0.0032976044,-0.0062949224,0.028883459,0.025045525,0.011649123,0.0018850538,-0.045126166,0.016524512,0.030451244,0.056861397,0.0079073105,0.019345388,-0.022780372,-0.053946435,0.034924537,-0.009669411,0.056320123,0.022965036,-0.02487037,-0.007957196,0.01716489,0.019284535,-0.0005946518,0.051934842,0.00020631983,0.026170261,-0.05494041,0.05866497,0.02760967,-0.019232797,0.055693362,0.025624508,-0.016386343,0.041637022,-0.012432071,0.01191865,0.008655009,0.009841345,0.015258297,0.0034108616,0.06737424,0.011955839,0.027783193,0.0329858,-0.035043634,0.018873438,0.02550218,-0.042794097,0.013684685,-0.062082145,-0.008639521,-0.006861602,-0.022494419,-0.07288864,-0.016402472,-0.043474186,-0.010596634,0.028829101,-0.026231429,-0.06495406,0.029535532,-0.069645666,0.07472465,-0.018016968,-0.00953644,-0.023990594,-0.0007995065,0.11967356,0.054700837,0.033016793,-0.005265802,0.012050958,0.00085989694,0.016716383,0.09630076,0.034940846,0.024519568,0.013436365,-0.0021473968,-0.0072096856,-0.022163108,-0.039981548,-0.005427389,-0.03225271,-0.026825804,-0.015690535,-0.033195097,-0.013203736,0.023378206,-0.015422728,0.0042956653,0.014007861,-0.0012844112,0.06679655,0.034881935,0.0008763727,0.008707892,0.052927736,0.063149884,0.02385627,0.0726004,0.0037187603,0.022366455,0.02022462,-0.032856468,0.01212066,0.0062798443,-0.0035187525,-0.012546195,-0.06962944,-0.025389602,0.02622075,-0.019478388,-0.06669863,0.03620157,0.022584205,0.0154347615,-0.0584598,-0.032736152,0.024125475,-0.078844145,-0.01976594,0.031823564,0.029926734,-0.024027897,0.05250588,-0.07848498,0.06242208,-0.08856594,-0.012895431,-0.056863572,-0.00095460296,-0.038150616,-0.03388161,-0.012443846,-0.016990786,-0.026802976,-0.04648381,-0.009384374,0.0009211828,0.012352827,-0.01684942,-0.05439864,0.016314086,0.024611022,-0.03283762,-0.0003461201,-0.018211352,-0.0015996847,-0.0277746,0.058727674,-0.01494837,0.008652807,0.038342904,0.05952622,-0.070870884,-0.033018935,0.06513622,0.024604056,-0.05013691,0.06292693,-0.0035136156,0.0015606302,-0.024819454,-0.021411225,-0.07046345,-0.0468185,0.0012092218,-0.015660333,0.068279445,-0.0234006,-0.047718838,-0.07447665,-0.01748166,-0.0467058,0.06218177,0.05321637,0.022629775,-0.019064905,0.014437682,-0.0311759,-0.06804624,-0.020485299,-0.022401042,0.03314098,-0.03888781,0.045028787,-0.029935053,-0.00033437385,0.029420214,0.018161714,-0.02424211,-0.012397952,0.018678747,0.01455094,-0.03066408,-0.0037380955,-0.002769906,0.0011322473,0.028163914,-0.0322544,0.041699976,0.03133787,-0.04074203,0.00033647753,-0.013590642,-0.012028197,-0.016412104,0.025022997,0.0044859773,-0.0077877063,-0.018094653,0.053784803,0.03347194,0.01000737,-0.055658754,-0.042915303,0.022049742,0.03991998,0.057406344,0.005471489,-0.026489018,-0.0654943,0.016345546,-0.010137145,0.020204294,0.071774766,-0.0057624453,-0.033123527,-0.007207234,-0.025873441,-0.032400988,0.03576676,0.007901694,0.028687254,-0.012210507,-0.027897123,-0.014867047,-0.060045622,0.0077072126,-0.034375045,0.008968479,-0.020011988,0.0627976,0.009636589,-0.011480153,-0.040862527,0.0035877149,0.018197434,0.036267072,-0.025778947,0.042002358,-0.02185656,0.033776086,-0.014092029,0.008628288,0.07684902,-0.025165929,-0.045469865,0.012127705,0.030779347,0.013816934,0.012797241,-0.02177215,-0.023036193,-0.01186558,0.029218921,0.024089405,-0.008848947,0.01104547,-0.044426117,-0.03062335,-0.013501033,-0.07029087,-0.02133726,0.017609078,0.025075393,0.0014907764,-0.013514798,-0.008387993,-0.012145127,0.01639489,-0.028581448,0.008755458,-0.041824386,0.00037046068]	Keywords: random fourier feature map, trigonometric functions, random feature attention, approximation, Performer, RFA\nKey Objects: random fourier feature map, trigonometric functions, feature map\nRefers to Images: None\nHypothetical Questions:\n- Why did Performer draw inspiration from random Fourier feature maps?\n- What is the difference between the feature map used in the first version of Performer and its use in Random Feature Attention (RFA)?\n- How does normalizing the queries and keys before applying the feature map impact the approximation process?\n---\nSummary:\nPerformer utilizes random Fourier feature maps, originally used to approximate Gaussian kernels, by employing trigonometric functions to approximate attention. Random Feature Attention (RFA) builds upon this by normalizing queries and keys before applying the feature map.\nOriginal Text:\nwhere $\\_{1}$,    , $\\_{m}$  D are drawn from some distribution D  P ( R $^{D}$) .  \nThe first version of Performer [18] is inspired from the random Fourier feature map [105] that was originally used to approximate Gaussian kernel. It uses trigonometric functions with h ( x ) = exp (   2 $\\_{L}$, l $\\_{2}$) , 1 = 2, f$\\_{i}$ = sin, f$\\_{z}$ = cos. This approach has also been used in Random Feature Attention (RFA) [95], with the difference that h ( x ) is set to 1 as the queries and keys are $\\_{2}$ -normalized before applying the feature map.\nContextualized Text:\nTo approximate the attention mechanism, Performer utilizes random Fourier feature maps, originally used to approximate Gaussian kernels. Specifically, the first version employs trigonometric functions like sine and cosine with h(x) = exp(^2_L, l_2), where 1 = 2, f_i = sin, and f_z = cos.  Random Feature Attention (RFA) builds upon this by normalizing queries and keys before applying the feature map.	{"tags": ["attention", "transformers", "deep-learning", "approximation"], "doc_id": "53b336be-ad75-41ce-b70e-78dbf98bc905", "summary": "Performer utilizes random Fourier feature maps, originally used to approximate Gaussian kernels, by employing trigonometric functions to approximate attention. Random Feature Attention (RFA) builds upon this by normalizing queries and keys before applying the feature map.", "doc_type": "text", "entities": ["Performer", "RFA", "Random Feature Attention"], "keywords": ["random fourier feature map", "trigonometric functions", "random feature attention", "approximation", "Performer", "RFA"], "key_objects": ["random fourier feature map", "trigonometric functions", "feature map"], "contextual_text": "To approximate the attention mechanism, Performer utilizes random Fourier feature maps, originally used to approximate Gaussian kernels. Specifically, the first version employs trigonometric functions like sine and cosine with h(x) = exp(^2_L, l_2), where 1 = 2, f_i = sin, and f_z = cos.  Random Feature Attention (RFA) builds upon this by normalizing queries and keys before applying the feature map.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["Why did Performer draw inspiration from random Fourier feature maps?", "What is the difference between the feature map used in the first version of Performer and its use in Random Feature Attention (RFA)?", "How does normalizing the queries and keys before applying the feature map impact the approximation process?"]}
eb1fd6db-aca0-4ef0-983e-939b3fd9107e	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.018292736,0.018091615,-0.052763093,0.014581103,-0.0440742,0.076247804,-0.011604967,0.013422018,0.016123181,-0.027552158,0.0372983,-0.06362843,0.003881469,0.020284357,-0.014819761,-0.0009415111,0.03992297,0.06799141,-0.018825458,-0.026466029,0.07171664,0.021188015,0.009116711,-0.021180244,0.009559026,0.042695336,0.030437386,-0.004101945,0.026645025,-0.0067551685,0.033829827,-0.018275503,0.021173483,0.012192771,0.032812934,-0.023124062,-0.01225109,-0.02991961,0.0013052664,-0.036357317,7.904585e-05,0.03382449,-0.061132718,0.020145165,-0.021200221,-0.059444573,-0.08965224,-0.02787494,0.027498245,-0.014694956,0.012647827,0.015407052,-0.050781213,-0.036345895,-0.028849022,0.002674941,0.020052744,-0.0043679625,0.057860006,-0.09167654,-0.041959822,0.04568751,0.011137981,0.0041139103,-0.0030818523,-0.009030557,0.024475243,0.0024147686,0.07373,0.114730865,-0.00023441746,0.008213488,-0.009946523,-0.04097242,0.08511552,0.018865976,-0.037266716,-0.04275502,-0.0028568264,-0.02076394,-0.0058756657,0.0678572,-0.0070310193,-0.031422704,0.094406135,-0.021151222,-0.008939867,0.037495203,0.02035901,-0.05266761,0.014836999,-0.00036698874,-0.020802105,0.10777592,-0.015430616,-0.034150485,-0.013018989,-0.036721703,-0.03907655,-0.07944322,0.027997827,0.01493761,0.08102745,0.103973605,0.010135718,-0.017993523,-0.013655099,-0.01653966,0.021884957,0.044977657,-0.023505324,0.004776094,-0.04408749,0.02429668,-0.05536623,0.040849097,-0.047593728,0.027994677,0.03587471,-0.046644825,0.026129011,0.034012776,-0.012340902,-0.015859786,0.01178894,0.02286797,0.035528705,-0.0026837457,-0.021668497,-0.01773434,0.07763288,-0.03144036,-0.04162333,-0.0163874,0.0020436726,0.010135372,0.064608626,-0.025740344,-0.03206729,0.020189561,-0.027544595,-0.0016452998,-0.06651804,0.026662545,0.015400378,0.02244705,-0.062097125,0.06704969,-0.024659494,0.050377093,0.02076459,-0.04475942,0.025868421,0.034624908,0.006636842,-0.012252143,-0.027129937,-0.031559676,-0.005671244,-0.033696055,-0.041477084,-0.046864334,0.013502515,0.07127093,0.04456298,0.07097159,0.0012477466,-0.01883149,0.04265169,0.027814915,-0.014769045,-0.016020171,-0.021319544,-0.026565844,0.027015204,0.016099961,0.03577209,-0.06950003,0.002684803,-0.003750001,-0.028251331,0.014702052,-0.0039119474,0.026203787,0.039280176,0.02669784,-0.033758465,0.07177131,0.015080315,-0.0353331,0.010763455,-0.011529652,-0.007921201,0.0060025128,0.0047089863,0.044287603,0.019399118,-0.062265966,0.0039214743,-0.028944671,-0.024971107,0.04217371,-0.027229067,-0.018746085,0.005607618,-0.02036344,-0.015768483,-0.017665286,0.0022568214,0.005823984,0.009640662,-0.026271729,-0.03729162,0.04953926,-0.013858407,0.059430797,-0.030235095,0.033002704,0.03779646,-0.02531378,-0.058455642,-0.017362405,-0.05588059,0.049077716,-0.04285847,0.02128045,0.029367734,0.039766878,0.026285145,0.0081562055,0.011363084,0.04895463,-0.048138358,0.048236668,-0.0549487,0.009656218,0.0066014417,0.012591496,0.0028581838,0.039513737,0.029469047,0.050946377,0.014770513,0.08122779,0.053744886,-0.0032925003,0.01471085,0.055699833,-0.048482895,0.02217624,-0.054113287,-0.0052842894,0.06426746,0.021306276,-0.025589162,0.03485703,0.03353453,0.009748916,0.04234203,0.0145882685,0.017876016,-0.008872721,0.0031526468,-0.01950865,0.05288634,-0.004093851,-0.005545579,0.024615416,-0.016874684,0.015275448,0.055595674,0.045052048,0.042001013,0.020993058,-0.041374695,0.04649835,0.012276458,-0.001478152,0.03232859,-0.032796886,-0.0069755106,0.03987645,0.027957495,-0.014679388,0.032409724,0.019450646,0.021072345,-0.03319186,0.021803962,0.011198563,0.0041882927,-0.014201445,0.023887163,-0.017501382,-0.043451615,0.003861042,0.024171818,-0.012527684,0.008549475,0.03436696,0.1420187,-0.027865386,-0.0023179052,0.0478375,0.018517796,-0.010232034,-0.051944718,0.06755481,0.05042956,0.013689369,0.024889773,-0.020600883,-0.009269715,0.012170786,0.021761024,0.10942501,0.054032207,-0.03526661,0.013418232,0.032073043,0.051037505,-0.008611688,-0.0004169781,0.021526089,0.045612052,0.031739976,0.01070955,0.00030293444,-0.035043553,0.057393998,-0.097778,0.03514285,0.056808073,0.013024853,0.01671346,0.032590404,0.020527637,0.026292305,0.022702942,-0.02410956,0.004386625,-0.051775087,0.021315487,-0.0075494177,-0.048653923,0.04304834,0.030630426,-0.032801013,0.029730141,0.03428149,0.015973419,-0.0005450927,-0.0001642986,-0.007121125,0.012924332,-0.042000372,0.0043224758,0.024580918,0.0144355325,-0.08832551,0.06194552,-0.089152806,0.030373802,-0.02064109,0.023800408,-0.042085707,-0.0429948,0.008387803,0.05119042,0.0016072713,0.015975973,-0.03874973,0.08535086,-0.025321366,-0.01967321,0.03398744,0.04763165,0.020103632,-0.02555746,-0.027095495,0.0016931309,0.024865204,-0.02538414,-0.0018550132,0.05927031,0.0313506,-0.03549611,0.07527267,-0.017923143,0.036658205,-0.054690752,0.031624105,0.028197438,-0.036266025,-0.009354433,0.026376497,-0.022346236,0.037211947,0.023604194,0.026333304,0.019837981,-0.0021821966,-0.05816073,0.042074557,0.04718605,0.008101865,-0.040645704,0.00952344,0.024432452,-0.013600842,0.017607564,-0.10396262,0.0027090935,-0.015665986,0.002591305,-0.049995076,-0.0011955036,0.055476982,-0.033400327,0.013616204,-0.016763171,0.02614739,0.0096340375,0.026360746,0.026462767,-0.0004878866,-0.029500827,0.054693833,0.008318319,-0.07153666,0.0087615475,-0.03406707,-0.053998016,0.0021904248,-0.016977714,-0.010626941,-0.06759722,0.06041097,-0.019695897,0.017581608,0.03122934,-0.0023868214,0.026758231,-0.032403838,0.027612867,0.006893035,0.027394129,0.00012600483,-0.035838965,-0.044469666,0.02453459,-0.004329712,0.007448724,0.009286487,0.047743186,0.0012600312,0.00551924,-0.037130512,-0.019623157,0.011596234,0.058373988,0.066749595,-0.004222537,0.044605035,-0.030666124,-0.018741816,0.040638182,-0.01777203,-0.012750804,-0.012588611,-0.020676859,-0.012942299,-0.01420348,0.020211335,0.011262745,-0.016640984,0.012247804,-0.0022745377,0.07934838,0.008404194,-0.07310668,-0.0038969829,-0.02652826,0.014824232,0.004049228,0.0065448973,-0.0069272825,-0.01634104,-0.035570566,0.017728548,-0.028318884,0.039328586,0.006098494,0.0112003,0.029435867,-0.028069878,0.0013555269,0.00301486,-0.015986947,-0.011721007,-0.013730094,0.025708755,0.046977933,0.06834381,-0.026421864,0.016550137,0.07297929,0.039332435,-0.02447796,0.05444659,0.056319796,-0.012709197,0.023400337,-0.019061051,-0.04495776,0.044530123,-0.003348728,-0.011316665,0.009321942,0.0020028576,0.036046285,0.010771966,0.02729591,0.029395489,0.0010263121,0.004504842,0.021744197,0.031101197,0.015471944,-0.053385288,-0.024745928,-0.024456197,-0.024177149,0.018162422,-0.05517245,-0.01869413,0.010488274,-0.04297543,0.007017985,-0.008733398,-0.024073249,-0.0510403,0.0076427865,0.011847164,0.049726866,-0.00054611824,0.012673883,-0.022671178,0.017193792,0.110943526,0.022341846,0.043241806,-0.020073008,0.013828209,0.00090786273,0.018705312,0.08628808,0.014155919,0.036120825,0.055348463,-0.00041769297,0.0221147,0.013505185,0.012701435,-0.03923197,-0.024584813,-0.040167987,0.011622766,-0.0063237953,0.0024690267,0.011192632,-0.021702582,-0.0010027714,0.024053663,0.016431512,0.06473452,0.020916715,-0.002483597,0.009030274,0.015874932,0.04549127,-0.007865905,0.048684847,0.034778964,-0.017647374,-0.005186116,-0.02926973,-0.019996747,0.006647935,-0.013097519,-0.0055826674,-0.07648194,0.014788031,-0.018332342,-0.024685219,-0.029425891,0.056465827,-0.0064230603,0.028079735,-0.056319274,-0.08344869,0.0048172134,-0.054226052,-0.031855,0.024893053,0.053406324,-0.010296565,0.04187809,-0.06672471,0.048423436,-0.0938628,-0.017779015,-0.001707919,0.03003749,-0.03830176,-0.0114306,-0.03834075,-0.024954086,0.02233357,-0.0053054816,-0.05227227,-0.0022146218,-0.016904818,-0.03695967,-0.044176932,0.0004519984,0.0058740005,-0.047302444,-0.004469205,-0.012187646,-0.028831923,-0.03274682,0.06374377,-0.040448256,-0.043547016,0.011058749,0.010017909,-0.08784094,-0.017659202,0.0073857736,0.055795826,-0.0151887005,-0.008770839,0.022077216,0.026099084,-0.030093042,-0.082284786,-0.048363548,0.007946506,-0.009465305,0.02311579,-8.859932e-05,0.032373272,-0.06734808,-0.043173835,0.007960573,-0.02306333,0.04575836,0.006844664,0.015406088,-0.0058709662,0.059748918,-0.06304094,-0.078319885,-0.002072893,-0.0005305572,-0.03295031,-0.028161261,0.066752985,-0.01493346,-0.026434088,0.0031780847,-0.023594558,0.018152276,-0.0050302637,0.015280738,-0.007766569,-0.003149116,-0.0039056276,-0.05237889,0.029230991,0.019938974,0.0054463656,0.055115413,0.043701123,0.034211062,-0.0049152337,-0.07926997,-0.043186028,-0.025797868,0.03718855,-0.024982536,-0.059068106,0.017264202,0.00025686927,0.066497624,-0.022325054,-0.045812793,-0.046103776,0.062150314,0.015783735,0.04322052,0.028382897,-0.03551089,-0.070696935,0.026974646,-0.02119681,-0.030969255,0.048145413,-0.008278625,-0.018177476,-0.02859812,-0.03853792,-0.012634947,0.032802258,-0.0041936464,-0.0007041126,0.019605123,0.0062845903,0.015919887,-0.075481415,-0.014183332,-0.045273773,-0.021620912,-0.0067125834,0.0445197,-0.009768116,-0.020427698,-0.04176542,0.008641194,-0.0066791517,0.050463054,-0.01034885,0.018754302,0.017727984,0.0426759,-0.071437515,0.01755334,0.065693334,-0.032559622,-0.030145805,-0.009107208,0.030020986,0.013461989,-0.029679235,-0.007981006,-0.01016213,-0.02293723,0.010544804,0.004313746,0.011266046,0.03584374,-0.040778548,-0.04303054,-0.04144988,-0.013818307,-0.0065629357,0.059999514,0.071862675,0.01312735,-0.060523853,0.004481075,-0.07276862,0.0743386,-0.016298631,0.0076294905,0.028315183,-0.03816616]	Keywords: feature map, orthogonality, ReLU, feature space\nKey Objects: Feature Map, Input\nRefers to Images: None\nHypothetical Questions:\n- Why is orthogonality a desirable property in a feature space?\n- How does using ReLU on the input and its negative contribute to orthogonality?\n- In what ways might this feature map improve the performance of attention mechanisms?\n---\nSummary:\nSchlag et al. [113] developed a feature map designed to promote orthogonality in the feature space by defining a partial function utilizing ReLU on a combination of the input and its negative.\nOriginal Text:\nSchlag et al. [113] design a feature map that aims at facilitating orthogonality in feature space. Specifically, given an input x  R $^{D}$, the feature map  : R D  R 2 V D is defined by the partial function  \n$$\\phi _ { i + 2 } ( j - 1 ) D ( x ) = R e L U ( [ x, - x ] ) ; R e L U ( [ x, - x ] ) _ { i + j } \\ \\ f o r \\ i = 1, \\dots, 2 D, j = 1, \\cdots, v. \\quad ( 1 8 )$$  \n4.2.2 Aggregation Rule. In Eq. (16) the associations {  ( k )  $\\_{j}$ } are aggregated into the memory matrix by simple summation. This is adopted by several studies [18, 19, 62]. However, it could be more beneficial for the network to selectively drop associations as new associations are added to the memory matrix.\nContextualized Text:\nTo enhance attention mechanisms, Schlag et al. [113] created a feature map intended to promote orthogonality within the feature space. This map, defined by a partial function using ReLU applied to a combination of the input and its negative, aims to improve the structure and efficiency of attention models.	{"tags": ["deep-learning", "architecture", "feature engineering"], "doc_id": "eb1fd6db-aca0-4ef0-983e-939b3fd9107e", "summary": "Schlag et al. [113] developed a feature map designed to promote orthogonality in the feature space by defining a partial function utilizing ReLU on a combination of the input and its negative.", "doc_type": "text", "entities": ["ReLU", "Schlag et al."], "keywords": ["feature map", "orthogonality", "ReLU", "feature space"], "key_objects": ["Feature Map", "Input"], "contextual_text": "To enhance attention mechanisms, Schlag et al. [113] created a feature map intended to promote orthogonality within the feature space. This map, defined by a partial function using ReLU applied to a combination of the input and its negative, aims to improve the structure and efficiency of attention models.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["Why is orthogonality a desirable property in a feature space?", "How does using ReLU on the input and its negative contribute to orthogonality?", "In what ways might this feature map improve the performance of attention mechanisms?"]}
6dafc86d-0c86-4db3-9427-af12212de79b	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.018760303,0.03583167,-0.016698001,0.04600595,-0.011718236,0.077560455,0.02898599,0.0035468324,0.048947264,0.0055163903,0.00019842254,0.016009433,-0.0001435151,0.042304453,-0.017745188,-0.01174796,0.067650855,0.095047414,0.0043725385,-0.035411872,0.048953928,-0.025782952,0.015418099,0.015607233,0.047205903,0.06551192,-0.02697778,-0.0046422845,0.026430175,-0.03856748,-0.017531306,-0.04364842,0.02700808,0.015297758,0.053700108,-0.03691548,0.052251652,-0.020867145,0.002391451,0.006388713,-0.0039516645,0.050699856,-0.055646647,0.005271432,-0.025771085,-0.06801471,-0.04594853,-0.019092577,-0.0010669294,-0.0069473917,0.03025114,0.017805068,-0.07878347,-0.07805231,0.0009814712,0.0044231694,0.044636,-0.022388423,0.04242424,-0.044940464,-0.013762557,-0.019931221,-0.0064061456,-0.0349803,0.04443229,0.006943924,0.023120308,-0.023045659,0.047908388,0.13630804,-0.0043302635,-0.010253763,0.0017801056,-0.06509928,0.10773487,0.013715639,-0.022667259,-0.028589295,-0.05395058,-1.5683227e-05,-0.0062320614,0.054917984,-0.030236293,0.009957831,0.098861665,0.015077899,-0.08628623,-0.004663369,0.09971152,-0.074034,0.025691949,-0.066455014,-0.001298455,0.06632378,-0.042620327,-0.03632427,-0.011106386,-0.017608576,-0.039306037,-0.022816801,-0.0022063104,0.024220573,0.058862578,0.118057206,-0.0028776377,-0.027345076,-0.0017144869,0.017399283,0.0497798,0.06477798,-0.01075034,0.00056019163,-0.036153212,-0.026514003,-0.04227636,0.027147627,-0.019379545,0.046274938,-0.060474925,-0.0025800879,0.021071026,-0.012920385,0.030196503,0.026074734,0.04511777,0.02334254,0.0337724,-0.04871783,-0.058939703,-0.00013684323,0.04474025,0.01865729,-0.040731784,0.027838312,-0.014603303,0.0078012273,0.06404754,0.015919086,-0.027620057,-0.0055259136,-0.050892882,-0.010354175,-0.07170236,-0.0034482677,0.044782043,0.030707683,-0.073503904,-0.014283779,-0.013596349,0.077500105,0.037950963,0.009758929,0.050342374,-0.021073036,-0.038987745,0.0055469917,-0.0116532305,0.004136337,-0.0033424585,-0.089526035,-0.058835026,-0.02811542,0.015198746,0.084993295,0.0096751535,0.10172708,0.021402795,-0.021714184,0.03064187,0.022853214,-0.025492193,-0.04916537,0.013887512,-0.0059085605,0.0018923307,-0.025552416,0.028163433,-0.039998613,-0.003399397,-0.012707071,-0.059859928,0.08132493,-0.018764904,0.0036258914,0.021225905,0.064792894,0.002751321,0.04710836,-0.008904307,-0.08929103,0.029689414,-0.0064792153,-0.009751668,0.010713577,-0.016490692,0.056976087,0.029567821,-0.097281836,0.035606273,-0.040087916,-0.01604529,-0.0149172805,0.02025034,-0.016661763,0.03911749,-0.03426953,0.03006926,0.028865447,-0.034053594,0.028388487,-0.02282399,0.011262551,-0.0049363086,0.011139189,0.030092435,0.054465838,-0.04846036,0.009960198,-0.019451315,-0.035936844,-0.0057111075,0.04465534,-0.027137525,0.04698942,-0.030634074,-0.010092586,-0.0014868038,0.011345664,-0.021958979,0.024789797,-0.01835129,0.015477104,-0.018705618,0.078790925,-0.035133943,0.013494635,-0.030820414,0.0042952634,0.043579783,-0.0099429805,0.020332428,0.04331096,-0.052170552,0.06587142,-0.0004879375,0.008971476,0.0076616807,0.013692701,-0.025148047,0.046498906,-0.043681655,0.041029572,0.0686209,-0.039587747,0.05304445,0.010203347,-0.0014894604,0.0031396444,0.018305583,-0.01907977,0.0529018,-0.0178223,-0.0062571503,0.035185013,0.036046475,-0.042052757,0.017661229,0.012490003,-0.024380619,-0.022783644,0.034734163,0.07311081,0.00049214455,-0.001994152,-0.023917599,0.048556022,0.0066643995,0.011650403,0.0046081417,-0.040101163,-0.019048389,-0.010427164,-0.030178353,0.015520755,0.043875933,0.001565517,0.0074920156,-0.058106218,-0.01702633,0.009732341,0.0059454786,-0.018914228,0.0118862465,0.01498374,-0.032219198,0.006265595,-0.016410686,-0.037880145,0.01985067,0.014639365,0.12487947,0.0045708157,0.0494418,-0.0047832667,0.049513023,0.026901482,-0.06223119,0.064698726,0.016455965,0.056854714,0.039606124,-0.05352437,-0.030991921,0.007159778,-0.003525084,0.061267532,0.028895061,-0.0133550055,0.010785381,0.039280046,0.00906568,0.024476422,-0.053603712,0.054609954,0.019193737,0.005524533,0.010118181,-0.0018102449,-0.067759566,0.04705393,0.00032774318,0.048049513,0.075473614,-0.011180055,-0.0139403995,0.029849522,-0.03877569,0.07219112,0.058890942,-0.017594472,0.019294247,0.021022134,0.029935122,0.009753507,-0.007045601,0.017790856,0.0030707223,-0.0075511863,-0.033753928,0.012539131,-0.026626099,0.04525348,0.010332051,0.012206688,0.056476247,-0.053961404,0.0018538433,0.020565212,0.010510808,-0.03454216,0.023556173,-0.08787953,-0.023567954,-0.015092472,0.024656724,-0.024484314,-0.029356064,0.01141535,0.0015033728,0.018031241,0.019939562,-0.02792189,0.049005758,-0.030519031,0.028362805,0.034449212,0.003553052,0.024795791,-0.01665683,-0.013118064,-0.0034972574,-0.009696781,-0.045648906,-0.021517321,0.07061727,0.026406258,-0.0469898,0.08095431,0.021667954,0.024644265,-0.046126056,0.055952493,0.04348867,0.004041216,0.022838378,-0.012110408,0.0014189053,0.061919097,0.014187214,0.0107168965,0.031872228,-0.03490434,-0.068353936,0.059152037,0.033473134,-0.015211305,-0.056938417,0.013001348,0.03530844,0.0022236307,0.04262311,-0.024136068,-0.007591188,0.008974813,0.0316847,-0.020402346,-0.026356872,-0.010927942,-0.026416143,0.010683836,0.01648871,0.04488801,-0.007279123,0.025243051,0.011683381,-0.024459193,-0.017640484,0.040249888,0.024997218,-0.076708354,0.012125676,-0.07327969,-0.024285596,0.06420175,0.035867754,-0.00982449,-0.044021662,0.06941075,-0.032703068,0.02875016,0.022444265,-0.049716003,0.018617975,-0.008483124,0.027710518,-0.016112454,0.06502124,-0.020922568,-0.027416252,-0.07087529,0.032701593,0.048519507,0.03318765,-0.056710694,0.06681165,0.021603482,0.0044784546,-0.028788587,-0.0030075442,0.0005435893,0.016218025,0.029977242,-0.009114027,0.03507228,-0.04490606,0.007185567,-0.008029666,0.000759331,-0.016719613,0.00056880707,0.000977436,-0.02570277,-0.038666345,0.02184839,-0.012833861,-0.039676763,-0.00057847827,0.04429068,0.062187962,0.02252778,-0.017090207,-0.015878467,-0.00642925,0.03323934,0.024350934,0.027991535,-0.029237056,0.0108281225,-0.026650278,0.007842538,-0.0069781733,0.017814126,-0.024663514,-0.0067927213,0.017119365,-0.04910893,0.014503594,-0.09796704,-0.0025585943,-0.04345427,0.011932791,0.012393023,-0.005329445,0.06841455,0.03464027,0.053499795,-0.06846004,0.007832598,-0.0067885765,0.037683725,0.010505683,-0.012090302,0.00407992,-0.03728271,-0.027170358,0.0031273824,0.006399824,0.0434129,-0.015578236,0.0044694976,0.0513587,0.004915627,-0.0009953409,0.024397312,-0.022385072,-0.0056007085,-0.006867459,0.046132654,0.034257818,-0.050736804,-0.008655444,-0.035650115,0.036362357,0.022718556,-0.027043864,-0.059938457,0.0059086885,-0.07139767,0.01886308,0.0004238834,0.011954581,-0.08679468,0.019723093,-0.032816567,0.048125073,-0.051576536,0.029673444,0.006044962,0.005152598,0.11662697,0.060037505,0.018233329,-0.044112038,0.014789047,-0.00011430583,0.007626019,0.027578251,0.018479604,0.022659363,0.046829317,-0.021715255,-0.00668244,-0.023369262,-0.006234071,-0.010721057,-0.014298128,0.011363425,-0.027168043,-0.024962144,0.012869742,0.002276867,0.026361518,0.009913385,-0.018041268,-0.0028690945,0.080751345,0.0072597316,0.021707239,0.035629936,0.032620527,0.025223754,0.04074284,0.04277341,0.0090492405,0.0138737615,0.005969758,0.004455586,0.039454404,0.00847763,-0.019859089,-0.0017945988,-0.0711299,0.019345252,-0.017775726,-0.036487818,-0.06929016,0.044633243,0.0100085195,0.051699467,-0.039385024,-0.022247676,0.0170689,0.008369048,-0.04332977,-0.0077363257,0.021290915,-0.011213809,0.04328398,-0.04055224,0.08680986,-0.03254427,0.0029096582,-0.0024564152,-0.015017607,-0.055802558,-0.019804236,0.023451693,-0.021540862,-0.014929857,-0.024492621,-0.032141283,0.026779009,-0.06835193,-0.013833551,-0.025789306,0.03308098,-0.019337898,-0.006644102,-0.006427893,0.011560612,-0.0034692595,-0.0038221728,0.066525884,0.0044426452,-0.0063540884,-0.005858267,0.018189272,-0.03493885,-0.05361317,-0.010585941,-0.032351363,-0.025783647,0.0011910385,0.026052153,-0.0018123655,-0.005184259,-0.029032694,-0.023124782,0.038557626,-0.03218587,0.06379909,-0.015759632,0.015132843,-0.030171242,-0.00674929,-0.027454747,0.0025973846,0.031500764,0.061778933,-0.0063125817,0.027698742,0.015522241,-0.06995755,-0.07463726,-0.028768394,-0.03070504,0.0053211204,0.0064878087,0.09674115,-0.01921602,-0.049563024,0.0020951321,-0.033918094,-0.021458607,-0.03520633,0.010427414,-0.03679596,-0.0048694084,-0.034096774,-0.009368783,0.027017094,0.013997843,0.00693579,0.09044984,0.05467803,-0.018508248,-0.032633502,-0.029704021,-0.01812545,-0.028146101,-0.025492296,-0.009833978,-0.017270202,0.023175064,0.0037253837,0.029577203,0.028178278,-0.006741727,-0.02549209,0.019234655,0.037223596,-0.011617498,0.03498827,-0.027906263,-0.05926624,0.0104166875,0.02554576,0.0051992093,-0.03269166,0.01325539,-0.040998425,-0.014787762,0.0017017227,-0.0025965103,-0.0055651036,-0.03317817,0.016998352,-0.009609136,-0.00963555,0.056331422,-0.045469057,-0.055182256,-0.023143895,0.038833305,-0.0013089143,0.030980432,0.0154587515,-0.017306913,-0.0453645,0.012850324,0.02098469,0.013056141,-0.07194485,0.03395141,0.01109236,0.0145065095,0.023804199,0.031356577,0.033412974,-0.035747483,-0.009208738,-0.0005015072,0.03417176,0.03563527,0.014094143,-0.031560175,-0.02992953,-0.02591732,0.0018584059,0.031049335,0.0049609398,0.036998987,-0.013788858,-0.028815296,-0.015335235,-0.03328901,-0.052247975,0.046736315,0.069790155,-0.024898862,-0.05202582,-0.020021915,-0.023053708,0.004478903,0.019164823,0.015859436,-0.0026176102,-0.03044004]	Keywords: gating mechanism, memory matrix, local dependency, RFA\nKey Objects: memory matrix, gating scalar, associations\nRefers to Images: None\nHypothetical Questions:\n- Why is it beneficial to favor recent context when aggregating associations?\n- How does the learnable scalar 'g' contribute to the gating mechanism?\n- What is the impact of the exponential decay of historical associations on the overall performance?\n---\nSummary:\nRFA introduces a gating mechanism to the summation process when adding new associations to the memory matrix, using a learnable scalar 'g' to weight existing associations and new ones, thereby favoring recent context.\nOriginal Text:\nRFA [95] introduces a gating mechanism to the summation to model local dependency in sequence data. Specifically, when adding a new association to the memory matrix S , at a particular time step, they weigh S by a learnable, input-dependent scalar g , and the new association by (1 - g ) (and a similar mechanism to u ). With this modification, history associations are exponentially decayed and recent context is favored in each timestep.\nContextualized Text:\nTo further refine the aggregation of associations within the memory matrix, RFA introduces a gating mechanism. This involves weighing the existing memory matrix S with a learnable scalar 'g' and the new association with (1 - g), allowing for the exponential decay of historical associations and prioritizing the influence of recent context within the sequence data.	{"tags": ["attention", "transformers", "architecture", "RFA"], "doc_id": "6dafc86d-0c86-4db3-9427-af12212de79b", "summary": "RFA introduces a gating mechanism to the summation process when adding new associations to the memory matrix, using a learnable scalar 'g' to weight existing associations and new ones, thereby favoring recent context.", "doc_type": "text", "entities": ["RFA"], "keywords": ["gating mechanism", "memory matrix", "local dependency", "RFA"], "key_objects": ["memory matrix", "gating scalar", "associations"], "contextual_text": "To further refine the aggregation of associations within the memory matrix, RFA introduces a gating mechanism. This involves weighing the existing memory matrix S with a learnable scalar 'g' and the new association with (1 - g), allowing for the exponential decay of historical associations and prioritizing the influence of recent context within the sequence data.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["Why is it beneficial to favor recent context when aggregating associations?", "How does the learnable scalar 'g' contribute to the gating mechanism?", "What is the impact of the exponential decay of historical associations on the overall performance?"]}
3b8805d8-58c7-467c-9d53-cadb10d20f4d	abe8c200-bfa1-4355-947e-23ea618c310d	[0.008022476,0.009211666,-0.024930958,0.03801036,-0.05604753,0.08515806,0.017842283,0.042619728,0.023144957,0.0009582747,0.013968648,-0.00624061,-0.0010286559,0.021613346,-0.005907206,-0.017884934,0.03306796,0.092621446,0.034743957,-0.035318617,0.039833058,5.46319e-05,0.016022291,-0.0076454477,0.038652886,0.017325778,-0.006423824,0.05580727,0.0067606885,-0.038274642,-0.010854623,-0.076069094,0.049777348,0.043777473,0.033525996,-0.04036127,-0.008330871,-0.034448363,-0.027211202,0.030338723,0.0053699547,0.05707463,-0.057247754,-0.011397412,-0.027313745,-0.042706396,-0.052236423,-0.043849695,0.047694124,-0.026197802,0.0017410684,0.035945043,-0.025036246,-0.073574305,0.0043025645,0.03362151,0.0057903305,-0.07469115,0.043242462,-0.074822806,-0.014465168,-0.020104349,-0.04182524,-0.002629439,0.055689447,0.0044400115,0.010363498,0.0011285397,0.027271584,0.1307763,-0.00557113,0.008460266,0.002978416,-0.062159702,0.11740025,0.028676,-0.011690435,-0.057206117,-0.064622425,-0.00020921085,0.02412539,0.046905175,-0.04241562,-0.020913962,0.08020538,-0.015112093,-0.060717925,-0.013337453,0.06338311,-0.06500061,0.03305607,-0.030279433,-0.011856211,0.056122005,-0.030662037,-0.020595256,0.019430652,-0.016057141,-0.0055229184,-0.021950297,0.020509318,-0.011036735,0.05719131,0.12627766,0.044792894,-0.054740317,0.0030045146,-0.023458796,0.028186075,0.04906669,-0.0235159,0.03605932,-0.0700524,-0.012148241,-0.045806795,0.015055607,-0.024451276,0.015346258,0.0006435793,-0.038839635,0.065869,0.0014328156,0.019445721,0.02533099,0.04121958,0.029572306,0.019662227,-0.012367056,-0.013799094,0.029949399,0.06046421,-0.02680133,-0.007957672,0.014992515,0.016256588,0.006702652,0.04934168,-0.014601092,-0.034327775,0.021365823,-0.05454022,0.0022172509,-0.038770996,0.051950578,0.04035261,0.009003897,-0.029756697,0.012702219,-0.0037892382,0.05306202,0.03642341,-0.0032343813,0.030616058,0.018392602,-0.010615032,0.0065712747,-0.052238002,-0.026308093,0.009095697,-0.011411474,-0.06383004,-0.033038363,0.012633683,0.114376664,0.00034251082,0.13113104,0.012462876,-0.015303198,0.032598555,-0.00078915484,-0.050818153,-0.0367054,0.028172886,-0.033939444,-0.041284647,-0.0019885572,0.025230125,-0.038471032,-0.00082977966,0.021363279,-0.032502107,0.049146865,-0.02293923,0.032922044,0.03156333,0.04772396,0.023331802,0.01949368,-0.0074803713,-0.12094614,0.058120113,-0.025710575,0.0062839924,-0.0143597135,0.0047655627,0.05206358,0.03615742,-0.050187945,-0.00019981893,-0.011312832,0.032584485,0.01505505,-0.015203344,-0.03867513,0.042687837,-0.008570481,-0.011833896,-0.011901996,-0.005721457,0.043784857,-0.023869976,-0.0033412036,-0.010003339,0.031227263,0.022084422,0.050008792,-0.037729055,0.01276762,0.009022793,-0.032778714,-0.03419903,3.9210096e-05,-0.036318872,0.054377675,-0.035258874,-0.015488365,0.004568311,0.028243722,0.039635964,-0.023600966,-0.01796639,0.02955308,-0.033478126,0.060587518,-0.056422252,0.016145485,-0.030625992,0.012227231,0.0052200877,0.021126477,0.03270679,0.041246742,-0.025616797,0.090301245,0.04817339,-0.026054054,0.009019986,0.025536235,0.0053157443,-0.00037461933,-0.01767192,0.022338158,0.05546639,-0.012135541,0.0074222907,0.02297588,0.011155307,0.004753967,0.023169216,-0.022287238,0.052629557,-0.02236321,-0.014279992,0.018748477,0.047273897,-0.019574046,0.04192551,-0.027675277,0.008617527,0.010264157,0.039242353,0.028435988,0.012136443,0.02040755,-0.03027588,0.036103114,0.009767182,0.02706349,0.045473002,-0.0024502992,-0.034152977,-0.018346447,0.0018982056,0.007389376,0.016794246,-0.009142354,0.017276028,-0.06585405,0.00013134662,0.014337398,0.017628135,-0.026091345,0.03499741,-0.0068936567,-0.0037370967,-0.011760515,0.00892892,-0.004950238,0.01723069,0.010387211,0.122307725,0.009943843,0.019681338,0.0032521095,0.021773556,0.0022153477,-0.089369744,0.061702985,0.008054354,-0.005370702,-0.0016219085,-0.043112345,-0.050911807,0.0076912246,0.016472736,0.105537266,0.06348749,-0.010980909,-0.006247455,0.046648934,0.03780016,0.007298112,-0.023472294,0.021173798,0.035513084,0.01630194,0.027174722,-0.00589987,-0.079645015,0.074444234,-0.03080906,0.032284338,0.06582662,-0.04870959,-0.012835778,0.003222565,-0.0045225406,0.0116133075,0.05027123,-0.025481487,0.020494042,-0.02350726,0.018025162,-0.009239917,0.0114913555,0.025534319,-0.01418511,-0.047903877,0.010301113,0.0057054134,-0.037252776,0.0130392695,0.04355168,-0.010551129,0.054890275,-0.017866302,0.017447406,-0.0049190368,0.021976769,-0.03126752,0.0098243365,-0.08070084,0.019452225,-0.028220015,-0.009904781,-0.011453289,-0.031732414,0.034509256,0.04313432,0.015395272,0.016520109,-0.032079652,0.08027039,-0.064808495,0.026290232,0.022373015,0.03318387,-0.01976039,0.008130969,-0.0060693203,0.011652215,0.0256197,-0.03944352,-0.0376498,0.06372939,0.011266755,0.00666694,0.07202705,-0.009328653,0.006718809,-0.02728408,0.033686798,0.046522647,-0.009853287,0.018369293,0.0193057,0.024591053,0.030523157,0.024662336,0.017823251,0.04027622,0.00988802,-0.07066489,0.024366604,0.055609994,0.010790169,-0.040845033,0.011268522,0.050089266,-0.006616719,0.026188819,-0.028085776,0.019340323,0.014977066,0.048134852,0.022103528,0.014003825,0.008071838,-0.022138363,-0.019311743,0.002374412,0.05294944,0.014572167,0.010851743,-0.00047485318,-0.02005149,-0.011081565,0.06609465,-0.0075784386,-0.061000515,0.019284545,-0.022405047,-0.021910883,0.03496598,0.005940938,-0.025670644,-0.05240855,0.069247,-0.0035985806,-0.041135598,0.009741945,-0.07179239,0.04779463,-0.008370286,0.052678037,-0.024663758,0.03594411,0.037768487,-0.035006743,-0.024286756,0.0070688343,0.02598778,0.017365204,-0.022204597,0.10383547,0.025212184,0.0395239,-0.033523172,-0.004599401,-0.0017285984,0.026275694,0.013366296,-0.0058649643,0.025680877,0.000729038,-8.163556e-05,0.0177774,-0.012559238,-0.007866487,0.009070251,0.011679386,0.052182805,-0.021779409,0.04440338,0.020995548,-0.0014720394,0.0063810432,0.06606768,0.047225256,-0.00841959,-0.068708934,-0.011028153,0.0071432474,0.017241787,0.017945718,-0.0008538529,-0.006711316,-0.0104964,0.004240732,0.005262251,-0.0015240237,0.04438449,0.014955032,0.03235803,0.024359094,-0.036532585,0.01015327,-0.023234025,0.000885136,-0.02995347,0.0025976007,0.048316613,0.017781029,0.069997124,-0.008981985,0.02397335,-0.07006622,-0.0081544975,-0.022370577,0.050128948,-0.005055477,-0.018015772,0.023371339,-0.05505452,-0.04880227,0.05822774,0.0065823225,0.051097736,0.010111696,-0.0049863034,0.04405353,0.002529592,0.014599707,0.020466793,0.0130453035,-0.008585345,0.03729303,0.06418939,0.036541913,-0.062441748,0.023518134,-0.022885755,-0.03786631,0.055633597,-0.09262749,-0.021173507,0.018259285,-0.050148077,0.010546636,-0.00049399084,-0.030471144,-0.06671466,0.018250084,-0.020674583,0.07121749,-0.036353342,0.027195767,-0.01273989,0.0101219565,0.089352496,0.052843112,0.039187416,-0.010631462,0.015091814,0.006137005,-0.002234213,0.048887488,-0.017102702,0.008016304,0.05141316,-0.042086314,0.009991005,0.041140884,-0.044245906,-0.024597973,-0.047868673,0.00019637117,-0.01242145,0.022112407,0.012219574,0.015307982,0.011648605,-0.03427466,0.008624207,-0.03284151,0.06791526,0.006601747,0.023460211,0.006685575,0.027774118,0.022897318,-0.0021994896,0.09103292,0.045523163,-0.0064056455,0.008892411,-0.018215206,0.017736368,0.034699544,-0.008797546,-0.0051258174,-0.07397452,0.009609789,-0.017681682,-0.014346631,-0.07653492,0.0384882,0.017570725,0.060652073,-0.061239664,-0.029497616,-0.005292445,-0.01910061,-0.034783464,-0.019873656,0.0573559,-0.025235306,0.031053916,-0.052739266,0.080263376,-0.10192602,0.00978436,-0.020051263,0.0053208335,-0.037938308,-0.0035111813,-0.018394882,-0.048959345,0.003653345,-0.02876901,-0.026767919,0.036742236,-0.036088083,-0.0017361415,-0.04030725,-0.0055842763,-0.0075983466,-0.036467314,-0.024490034,0.0147233885,-0.01820916,0.0016049779,0.06848266,-0.03286422,-0.024861565,0.017709078,0.022804044,-0.05475293,-0.032245204,-0.02381344,-0.018528938,-0.004014741,-0.02641072,0.044748902,0.005038225,-0.013028439,-0.046040468,-0.07497531,0.022623945,0.0063258098,0.022653718,-0.02425672,0.027102424,-0.03408317,-0.026583182,-0.041481733,0.018535797,0.005052678,0.07091027,0.009703843,0.04165219,0.05596014,-0.047451917,-0.05555426,-0.056967236,-0.055085152,-0.03424014,-0.02748233,0.07768074,0.0038561006,-0.0035223411,0.022033978,-0.01777677,-0.04958148,0.015856568,0.015801836,-0.034546692,-0.06494011,-0.06116335,0.004640702,-0.011564355,-0.0077429754,-0.04218029,0.05223746,0.04758794,-0.0035860771,-0.025063168,-0.04797532,-0.023818333,-0.009573526,-0.028866293,-0.0066317813,-0.0069305995,-0.02323091,0.0034132067,0.046005256,0.0382704,-0.04710046,-0.01609586,0.049095158,0.010846119,0.01775192,0.05215301,-0.014606746,-0.05440152,0.027720824,0.009869492,-0.024618275,-0.009417528,-0.014158148,-0.032703068,-0.032283694,-0.013265639,-0.010134046,0.059346568,-0.021346642,0.02582728,0.02717961,-0.007535598,0.04903707,-0.018107178,-0.035196498,-0.005550911,0.032514494,-0.04284116,0.04655659,0.010290575,0.003195942,-0.01853571,0.010882527,0.02108781,0.035709426,-0.02300391,0.05230285,0.027213275,0.026374385,0.039524216,0.022698773,0.041348703,-0.02729903,0.006096713,0.011851678,0.020283418,0.031772323,-0.018611893,-0.023058172,-0.010895162,-0.045517072,0.045587692,0.04164321,0.024772642,0.008901418,0.0041118604,-0.030494323,-0.012196093,-0.034850266,-0.013713868,0.03897055,0.053132176,-0.028237931,-0.07818606,0.00836688,-0.038195077,0.0060564103,0.024525164,0.03091349,-0.009115601,-0.038401995]	Keywords: memory matrix, write-and-remove, gating scalar, association, normalization\nKey Objects: memory matrix, association, gating scalar\nRefers to Images: None\nHypothetical Questions:\n- Why would simple summation of associations in the memory matrix limit its capacity?\n- How does the 'write-and-remove' approach improve the memory capacity compared to simple summation?\n- What is the purpose of the input-dependent gating scalar in the 'write-and-remove' process?\n---\nSummary:\nSchlag et al. [113] suggest increasing the memory capacity of the system by employing a write-and-remove approach, which involves retrieving, updating, and removing associations within the memory matrix using an input-dependent gating scalar.\nOriginal Text:\nSchlag et al. [113] argue that simple summation limits the capacity of the memory matrix and thus propose to enlarge the capacity in a write-and-remove fashion. Specifically, given a new input key-value pair ( k$\\_{i}$, v$\\_{i}$ ), the model first retrieve the value v$\\_{i}$ currently associated with k$\\_{i}$ using matrix multiplication. It then writes to the memory matrix a convex combination of v$\\_{i}$ and v$\\_{i}$ , using a input-dependent gating scalar g , and removes the association v$\\_{i}$ . They also propose sum normalization ( normalizing  ( q$\\_{i}$ ) ,  ( k$\\_{i}$ ) by the sum of their components before updating the memory matrix) instead of normalizing with the denominator in Eq. (16) for this aggregation rule.\nContextualized Text:\nTo improve the capacity of the memory matrix, Schlag et al. [113] propose a write-and-remove approach. This method addresses limitations of simple summation by retrieving, updating, and removing associations within the matrix using a learnable, input-dependent gating scalar, which influences the convex combination of values.	{"tags": ["architecture", "attention mechanism", "memory", "linearized attention"], "doc_id": "3b8805d8-58c7-467c-9d53-cadb10d20f4d", "summary": "Schlag et al. [113] suggest increasing the memory capacity of the system by employing a write-and-remove approach, which involves retrieving, updating, and removing associations within the memory matrix using an input-dependent gating scalar.", "doc_type": "text", "entities": ["Schlag et al."], "keywords": ["memory matrix", "write-and-remove", "gating scalar", "association", "normalization"], "key_objects": ["memory matrix", "association", "gating scalar"], "contextual_text": "To improve the capacity of the memory matrix, Schlag et al. [113] propose a write-and-remove approach. This method addresses limitations of simple summation by retrieving, updating, and removing associations within the matrix using a learnable, input-dependent gating scalar, which influences the convex combination of values.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.2 Linearized Attention"}, "hypothetical_questions": ["Why would simple summation of associations in the memory matrix limit its capacity?", "How does the 'write-and-remove' approach improve the memory capacity compared to simple summation?", "What is the purpose of the input-dependent gating scalar in the 'write-and-remove' process?"]}
c1c11395-9c5d-4877-b686-6be0282fa997	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.013227935,0.009400813,-0.028173747,0.013896296,-0.011893211,0.062977836,-0.011120416,0.015593571,0.04060441,-0.035627488,-0.018428527,-0.040585928,-0.0003558124,-0.019942818,0.01687124,0.021032361,0.012005044,0.09380047,-0.019698227,-0.05460983,0.046577178,0.044721097,0.029010182,0.01611617,-0.006202574,0.010056464,0.018813599,-0.0108153345,-0.0062076724,-0.015171197,0.0069400966,-0.047691338,0.0577667,-0.0054127905,0.027495015,-0.05767027,0.022441007,-0.07495944,0.021653058,-0.0013214278,-0.00971603,0.058284484,-0.015773872,0.01570323,0.0017752807,-0.06579289,-0.07609381,-0.01787886,0.02420115,-0.047544505,-0.0049251006,-5.124814e-05,-0.044113703,-0.01978905,0.020514376,0.03406453,0.025513949,-0.077581555,0.02200185,-0.08513717,0.00089315046,0.018773125,-0.032574806,-0.010575684,0.03865627,-0.050085798,-0.012503982,-0.013847978,0.074699305,0.1348481,-0.02464291,0.013103019,0.038502578,-0.03443805,0.12618811,0.044647966,-0.021916604,-0.015667787,-0.05498089,-0.034867905,0.006321093,0.048656814,-0.078527935,-0.02825493,0.06417444,-0.040287588,-0.012804355,0.0010665569,0.066314146,-0.04308774,0.006986256,0.01268152,-0.015574694,0.067104556,-0.04740526,-0.085507885,-0.013430872,-0.01228595,0.0006393693,-0.054815114,0.03065928,-0.022583365,0.060903367,0.10185567,-0.0035880902,-0.028110549,-0.007441949,-0.0887496,-0.008366762,0.008062952,-0.014287439,0.020357573,-0.05085242,0.021856187,-0.015642967,0.013701861,-0.05501293,0.0151767805,0.020850234,-0.045721885,0.033316106,0.017327722,-0.02306084,0.018866196,0.056005366,0.022890037,0.021157846,-0.03318933,-0.03888587,-0.008835688,0.025311254,0.005927482,-0.03334717,-0.0053505953,-0.024695473,0.018218499,0.07875279,-0.0052025625,-0.022267452,0.020182867,-8.293496e-05,-0.044931762,-0.08459485,0.03242503,0.012723652,0.049614683,-0.07762346,0.009319656,-0.043702662,0.08496173,0.026627269,-0.012872661,0.046506576,-0.025658075,-0.009242515,-0.0129684005,0.0031039168,-0.054747593,-0.010031765,-0.022702185,-0.03427242,-0.020486444,-0.022870382,0.071417406,0.031279806,0.10780993,0.009514966,-0.036257368,0.012764459,-0.01997299,-0.03697645,-0.007994717,-0.026243083,-0.041094102,0.009514359,0.016316665,0.020216404,-0.04803822,-0.01678162,0.008641053,-0.003849534,0.07472383,0.016196452,0.008958964,0.010976738,0.07933378,0.0042077177,0.061884426,-0.03341523,-0.02229339,-0.01307163,-0.02827817,0.00014873093,-0.011399837,0.037821356,0.009044857,0.04513117,-0.05524332,-0.014209983,-0.040429328,0.0005268881,-0.014989614,-0.03649294,-0.004470399,0.02977219,-0.005216055,0.027719714,-0.040420484,0.009250695,0.021464402,-0.02913064,-0.0046157357,-0.041357063,0.03198497,-0.025038859,0.041634023,-0.0074370373,0.031423498,0.01152555,-0.019783683,-0.041671026,-0.024056783,-0.042271603,0.03723004,-0.03818051,0.031452693,0.022226479,0.020575969,-0.026531236,-0.029071985,0.031974424,0.022715574,-0.0225457,0.052111037,-0.02386012,0.017426318,0.024285337,-0.004245246,-0.025463443,0.051401712,0.019824913,0.06093677,-0.016436657,0.091750965,-0.0052667265,-0.0065245214,0.002273745,0.040005952,-0.011994188,-0.0017072908,0.003151757,0.024440732,0.027566949,0.007332577,-0.0034372169,-0.0047422317,0.01894782,-0.008339925,0.038196117,-0.019630404,0.0245243,-0.0053112265,-0.019981407,0.07218539,-0.02439863,-0.018152881,0.027259376,0.017771214,-0.0145855285,-0.011357416,0.031121243,0.05585149,0.01788399,-0.017026927,-0.04001236,0.02432716,0.05253378,-0.008401011,0.024058951,0.010539266,-0.041765135,-0.024043296,0.011841171,0.0043178974,0.0012356356,-0.022347782,0.048640575,-0.021181425,-0.046616856,0.037434526,0.03023642,-0.031427708,0.014967326,-0.009792378,0.00089858,0.014823212,0.0083683375,-0.020158103,0.0055229035,0.01840694,0.08027522,-0.038474463,0.0126140835,0.03600156,0.023673812,0.033167183,-0.05328631,0.059161462,0.025161577,0.01805451,-0.008933076,-0.009957048,-0.028404646,-0.0071854405,0.031818744,0.070275865,0.09077958,-0.01597577,-0.00999832,0.032780852,0.023426915,0.008213499,-0.008682467,0.032214563,0.032561112,0.0054764533,0.036770083,-0.02226625,-0.05585022,0.075852975,-0.1023387,0.020419898,0.076067,0.0036181984,-0.024555504,-0.013737847,-0.00960702,-0.014871856,0.007680668,0.043975066,-0.016889272,-0.033655144,0.041938066,-0.0018685414,-0.011831041,0.05606818,-0.024766743,-0.0068384423,0.0062807603,0.045891456,0.02493085,0.010467238,0.03578679,0.017476527,0.009738508,-0.010998224,-0.01500105,-0.0024910474,0.011468625,0.0061829356,0.023771275,-0.06961617,0.06014971,-0.04584219,0.026099686,-0.0152930645,-0.0790959,0.03072505,0.07391799,-0.023267373,-0.0036661488,-0.059765425,0.06579879,-0.065444976,0.02780338,0.07332422,0.030162845,0.029337749,-0.012446577,-0.006503948,0.004430624,0.0059938636,0.006881334,-0.024211744,0.03263171,-0.016018055,-0.0013794873,0.05484326,-0.06944639,0.07784556,-0.07535644,-0.0042469297,0.0468064,0.001065656,-0.010167106,-0.023592519,0.011461766,0.05950157,-0.0039849784,0.0007093216,0.020321796,0.0071793194,-0.056886144,0.035984244,0.011432517,0.03298187,-0.091468,0.02531431,0.015147555,-0.002479412,0.021463709,-0.041072275,0.008401807,-0.0028746536,0.044405967,-0.03269556,-0.014399089,0.024131408,-0.0641182,-0.029760968,-0.033991706,0.035565555,0.035473004,0.00015251152,0.044245984,-0.025071694,-0.041033827,0.05004636,-0.011187883,-0.023822933,0.021618012,-0.038300674,-0.0010738714,0.0001664951,-0.008685125,0.014729933,-0.048822943,0.08222529,-0.040934633,-0.00066737627,0.049614284,-0.014499982,-0.0013927012,-0.033195928,0.0019822456,-0.02800613,0.020724114,0.059382964,-0.028888835,-0.020388117,0.051032677,-0.005895244,0.010955102,-0.003152593,0.062391352,-0.0046010567,-0.019916622,-0.035614457,-0.066620246,-0.0072888667,0.009674944,0.0053387927,-0.0037078445,-0.014907992,-0.008864285,-0.043566015,0.0060026664,-0.027067885,-0.04329215,0.01636245,0.013467408,0.037579004,0.0064365286,0.029628195,-0.016524855,-0.02537494,-0.0272149,0.04964941,0.02617857,8.448446e-05,-0.073868915,-0.07453212,0.006735006,-0.0007658488,2.2964841e-06,-0.02765085,0.029707057,-0.027938703,-0.03394053,0.012855827,-0.006014437,0.013071872,-0.026271423,0.0636146,0.0452235,-0.030383343,0.017090999,0.015223348,0.0262859,-0.038763154,-0.0017275794,0.044025008,0.06029743,0.058889847,-0.025809884,0.010200171,0.010718436,-0.017139668,0.0062637785,0.019742943,0.010669954,-0.038264558,0.028397737,-0.009169186,-0.0778902,0.049719058,-0.009676081,0.0146262515,-0.035479143,0.021558467,0.02969975,0.029387258,0.03977317,-0.011551386,-0.010226564,0.010026189,0.013058994,-0.012115832,0.0545588,-0.067984685,0.017849611,-0.043057747,-0.024352293,0.002258275,-0.047162324,-0.03000633,-0.01511405,-0.052713636,0.005199278,-0.022726396,-0.010311843,-0.06657403,-0.0147080915,-0.011444165,0.06296,-0.04719205,-0.012322002,0.0030063197,-0.004538644,0.05537371,0.022123318,0.03930507,0.034002926,0.021233607,-0.030115504,0.029988931,0.06471396,-0.01702733,0.011957666,0.072243445,0.0072991014,0.010920599,0.013543328,-0.044810157,-0.049293503,0.012134616,-0.008720296,-0.005721663,-0.022761732,0.029649537,-0.018464468,-0.013926646,-0.011791436,0.028572805,0.00058169296,0.053810835,0.014475044,-0.016695276,0.043871645,-0.008920329,0.049445212,0.015537325,0.1066627,0.0050220396,-0.00024526252,0.030180264,0.008221161,0.073309645,0.03875123,-0.0510725,-0.029990824,-0.04850526,-0.02141851,-0.030479295,-0.012816957,-0.02974589,0.06772853,0.010186761,0.0052602417,-0.042673998,-0.04284997,-0.05049476,-0.033740632,-0.025794426,-0.009720719,0.0061926562,-0.026225278,0.054124594,-0.024391018,0.06732848,-0.0572372,0.017207436,-0.01602863,0.006645287,-0.008613442,-0.02910484,-0.009419423,-0.020737523,-0.00322153,-0.021482782,0.078478046,-0.0011741321,-0.023320397,0.03337183,-0.034340646,0.00914391,0.014107903,-0.036558133,-0.041797172,-0.00998623,-0.02456743,-0.0064948797,0.08171811,-0.044614896,-0.02520228,0.023147326,-0.019750781,-0.060532898,0.007333261,-0.008616161,0.02894624,0.0013265896,0.019341907,0.024984129,-0.0024979638,-0.046778183,-0.007114085,-0.067210585,0.0039809262,-0.022173388,0.02185181,0.016595274,0.022002283,-0.026706005,0.022471158,0.03192251,0.01355952,0.05657519,0.07199473,0.06980751,0.059073973,0.023977064,-0.052530773,-0.042326953,-0.0102558555,-0.022852596,0.0547046,-0.05704953,0.03806487,-0.025486492,-0.011113136,0.0078381775,0.015689993,-0.03539513,-0.0077330815,0.03939131,-0.01929129,-0.051061112,-0.038458616,0.016034327,-0.019015452,0.02418017,-0.0074102273,0.0806363,0.013953286,-0.03581927,-0.034119032,-0.069385245,-0.025048845,-0.0010842073,-0.032933272,-8.682752e-05,0.016849276,-0.050962113,-0.0052386546,0.007859192,-0.005509765,-0.0016687037,-0.0383236,0.0047872323,0.04104124,0.021785518,0.0038291526,-0.026432144,-0.080127716,0.023172097,-0.004899355,-0.0684963,0.05162758,-0.043587603,-0.025567586,0.033314083,0.01754499,-0.027034912,0.020816095,-0.0021592807,0.007783822,0.0038863819,-0.028962707,-0.038423546,-0.059052534,-0.020011181,0.00034503598,-0.0068228617,-0.038307514,0.05407642,0.042729154,-0.004549675,0.015906153,0.0030499368,0.040855166,0.009227776,0.009176659,0.08320822,-0.04593464,0.056719754,0.012536355,0.012018622,0.063315585,-0.0162123,0.01835694,0.01378344,0.008911803,0.013218502,0.026542991,0.0042517018,-0.06891396,-0.00014259697,0.04895723,0.018797884,0.0015443837,0.0216078,-0.010282828,-0.025083544,-0.018794034,-0.033087883,0.017132001,0.04021565,0.020296684,0.023253154,-0.079025745,0.014983154,-0.010574252,0.03918224,-0.020203017,0.024048274,-0.07250182,0.027415058]	Keywords: query prototyping, memory compression, attention mechanisms, query prototypes\nKey Objects: Queries, Key-Value Pairs, Attention Distributions\nRefers to Images: ./images/a-survey-to-transformers/image_9.png\nHypothetical Questions:\n- How does query prototyping differ from sparse attention?\n- What are the potential drawbacks of using query prototypes instead of processing all queries individually?\n- In Clustered Attention, how are the centroids chosen and what determines the number of clusters?\n---\nSummary:\nQuery prototyping and memory compression techniques reduce attention complexity by decreasing the number of queries or key-value pairs, respectively.\nOriginal Text:\n### 4.3 Query Prototyping and Memory Compression  \nApart from using sparse attention or kernel-based linearized attention, one could also reduce the complexity of attention by reducing the number of queries or key-value pairs, which leads to query prototyping and memory compression 7 methods, respectively.  \n4.3.1 Attention with Prototype Queries . In query prototyping, several prototypes of queries serve as the main source to compute attention distributions. The model either copies the distributions to the positions of represented queries or filling those positions with discrete uniform distributions. Fig. 8(a) illustrates the computing flow of query prototyping.  \nFig. 8. Query prototyping and memory compression.  \n  \nClustered Attention [138] groups queries into several clusters and then computes attention distributions for cluster centroids. All queries in a cluster share the attention distribution calculated with the corresponding centroid.\nContextualized Text:\nTo reduce the computational complexity of attention mechanisms in Transformer models, query prototyping and memory compression methods can be employed. Query prototyping reduces complexity by using a limited number of query prototypes to compute attention distributions, which are then applied to other queries. For example, Clustered Attention groups queries into clusters and calculates attention distributions for cluster centroids, with all queries in a cluster sharing the same distribution.	{"tags": ["attention", "transformers", "optimization", "NLP"], "doc_id": "c1c11395-9c5d-4877-b686-6be0282fa997", "summary": "Query prototyping and memory compression techniques reduce attention complexity by decreasing the number of queries or key-value pairs, respectively.", "doc_type": "text", "entities": ["Clustered Attention"], "keywords": ["query prototyping", "memory compression", "attention mechanisms", "query prototypes"], "key_objects": ["Queries", "Key-Value Pairs", "Attention Distributions"], "contextual_text": "To reduce the computational complexity of attention mechanisms in Transformer models, query prototyping and memory compression methods can be employed. Query prototyping reduces complexity by using a limited number of query prototypes to compute attention distributions, which are then applied to other queries. For example, Clustered Attention groups queries into clusters and calculates attention distributions for cluster centroids, with all queries in a cluster sharing the same distribution.", "mentioned_images": ["./images/a-survey-to-transformers/image_9.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.3 Query Prototyping and Memory Compression"}, "hypothetical_questions": ["How does query prototyping differ from sparse attention?", "What are the potential drawbacks of using query prototypes instead of processing all queries individually?", "In Clustered Attention, how are the centroids chosen and what determines the number of clusters?"]}
50683ed8-48c8-4195-9be2-90b468db6950	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.045454167,0.009910125,0.010407111,0.008853438,-0.041922286,0.030824086,-0.052827533,0.015945775,0.07886887,-0.055847514,-0.018580284,-0.022084424,0.006530659,-0.004948989,0.028346807,0.079806715,0.03440396,0.11560493,0.0116573535,-0.031205319,-0.005711018,0.017631365,0.054792874,0.05657484,0.05945881,-0.004598228,0.020593755,-0.020079907,-0.018131465,0.0039438205,-0.015528103,-0.041132662,0.07385799,0.024852945,0.022076959,-0.059315693,-0.0068472237,-0.079281464,-0.011820684,-0.006696462,-0.031647637,0.056128945,-0.008843473,0.03716904,-0.0013801123,-0.015498978,-0.02961286,-0.036660925,0.025054079,-0.08662436,-0.03593243,0.044781666,-0.02134776,0.025181009,-0.030020934,-0.009642195,-0.0141983945,-0.07837665,0.07368437,-0.06569579,0.014627876,0.013393258,0.0039686123,-0.082517,0.016405733,-0.028068,0.018056065,0.0018572744,0.051487166,0.12302352,-0.02273751,0.019236557,0.06277384,-0.016126608,0.14094713,0.07430272,-0.018287152,0.009352093,-0.040830262,-0.009351421,0.03576671,0.074122645,-0.061004277,0.0006148949,0.07162727,-0.021836484,-0.033690423,-0.014729769,0.07708688,-0.078849375,0.035496343,-0.0371048,-0.014725191,0.011059262,-0.0074752932,-0.08958636,-0.0010769902,-0.0037866523,0.0063768234,-0.006134091,-0.016434263,-0.009948933,0.06797801,0.05634573,-0.0014832605,0.004501699,-0.027839117,-0.06862123,-0.0071503194,0.024886413,-0.032334603,0.015010043,-0.04546068,0.0057435497,-0.00039792646,0.037718665,-0.040301025,0.004470283,0.026875984,-0.037059374,0.07931865,0.05475703,-0.029405905,0.03170704,-0.002994675,0.03845696,-0.06930578,-0.038042247,-0.020127425,-0.05726109,0.003776727,-0.005186638,-0.035614695,0.022084748,-0.032326736,0.021242972,0.049674414,-0.006416177,-0.042098455,0.005885557,-0.04016935,-0.04453849,-0.08067093,0.032323815,0.055607107,0.022279505,-0.06601688,-0.047780067,-0.02635885,0.085479766,0.0413525,0.021745589,0.05578665,-0.012257579,-0.029356146,0.0047874693,-0.04755576,0.00086062535,0.031388078,-0.05424784,-0.045430325,-0.008801958,0.014206275,0.05981872,-0.0053457366,0.07066355,0.0011924085,-0.016128307,0.035130404,-0.039570462,-0.020970412,-0.0115197105,-0.03648013,-0.02934112,0.0006970047,-0.012953089,0.014585366,-0.015428484,-0.045516726,0.022570865,-0.00701206,0.065121666,0.021061158,-0.0021232017,0.0644416,0.06602064,0.024128273,0.05526257,-0.023694238,-0.033257645,0.0077524725,-0.016514933,-0.030563796,-0.041642446,0.058556136,0.034289163,0.059078258,-0.025558865,-0.09732177,0.0058780108,0.054935955,0.010589679,-0.03558932,0.021473443,0.030963978,-0.0058200657,0.02386372,-0.012832255,-0.032303866,0.046234153,0.011886477,-0.017120978,-0.03245812,-0.008229084,-0.0062601496,0.048171293,-0.012908561,0.061343513,0.020671077,-0.018769987,-0.008770106,0.011799967,-0.019438092,-0.012889031,-0.06368359,-0.04026913,-0.03992599,-0.038055543,0.019777844,0.0062797437,0.015442057,0.014562387,-0.013016215,-0.004558383,-0.021777209,-0.0077127856,-0.000795781,0.022039434,-0.026453767,0.06247504,0.0025273792,0.0007934936,-0.032325376,0.05706962,0.03929488,0.007188146,0.028913338,0.02673681,0.0548967,0.009037046,-0.02195758,-0.026372116,0.036312472,0.025601855,-0.020636458,0.024134096,-0.040946193,0.03187983,0.05872351,0.001608998,0.015176389,-0.012791374,-0.050801992,0.06559422,0.03110256,0.012015459,0.025467465,-0.0092735635,0.0110518765,-0.053226057,-0.0016360885,0.027699739,0.0049952473,-0.01970929,0.040158454,0.0062274807,0.042514287,0.026712107,0.012938531,-0.027397912,-0.025639787,-0.0006570759,-0.027536936,0.03417472,-0.03613534,-0.019515397,0.005091816,-0.04989405,-0.071792476,0.025522966,0.009442635,-0.015061067,0.012111351,0.013659306,-0.020645322,0.0165553,-0.011251872,-0.03403466,-0.0043842983,0.042037122,0.08719293,-0.05801949,0.010717761,-0.0073585087,-0.004734673,-0.006122483,-0.06873649,0.075942785,0.025919951,0.02834776,0.031039927,0.007279684,-0.022656333,0.03723998,0.06938968,0.06771343,0.042242542,-0.0044752136,-0.021643093,0.069061786,0.039353047,0.01763,-0.0050864695,0.041364033,0.06988704,-0.0058070174,0.031943813,-0.032874633,-0.014490046,0.06531265,-0.029939556,-0.021964844,0.08139534,-0.0422475,-0.024409583,-0.021643156,-0.034506537,0.014714091,0.01883536,0.028154874,0.01276443,-0.05239931,0.037167087,-0.003766418,0.014982586,0.07706522,-0.018201761,0.0022681064,0.0014348883,-0.024130821,-0.0157491,-0.012189149,-0.00591879,0.021305012,-0.007872421,-0.021793924,-0.020227024,-0.006911498,-0.016413175,-0.008001132,-0.00086540164,-0.07329481,-0.014506791,-0.01265348,-0.029600155,-0.02924891,-0.059068765,0.0362152,0.032820012,0.019220062,-0.02264698,-0.07215536,0.031636864,-0.027215946,0.049330514,0.0659003,-0.0078130225,0.0037266547,0.01598123,0.03850013,0.011731387,-0.018094178,0.015113527,-0.02819522,-0.023121443,-0.03137352,-0.0075965775,0.05946122,-0.07940976,0.066690095,-0.039658938,-0.0015996465,0.021678075,-0.00809842,-0.018007223,-0.074015036,0.007456619,0.033872183,0.011118632,0.018682595,0.012327017,0.027350454,-0.043932363,0.043930933,0.031037623,0.01693383,-0.031967785,0.0088503035,0.0671096,0.006183391,0.0121999765,-0.048944134,0.016472012,-0.014135327,0.024442269,-0.019802539,-0.027814269,0.019593682,-0.029713117,-0.040839765,-0.013752894,0.0023739017,0.07226083,0.0039251233,-0.002034725,-0.017197182,-0.020994466,0.06330753,-0.007586106,0.008694136,0.035494685,0.009308235,-0.0007706352,0.0014411446,0.07525469,-0.021294953,-0.023989497,0.04443004,-0.021050002,0.02365136,-0.010212917,-0.014940111,0.038134098,-0.020726442,0.07169594,-0.010543934,0.0235638,0.07268831,-0.046000835,-0.019816484,0.007983578,-0.019987281,0.02798502,-0.043366835,0.07395247,-0.032304037,-0.048227847,-0.054821454,-0.050783962,-0.036344156,0.010915941,-0.017654043,0.023168951,-0.04661125,-0.007079602,-0.0032427646,0.018548768,0.0015963509,-0.017917486,0.027321126,-0.014051112,0.012499723,-0.007372614,0.01135227,0.008793058,0.0032050307,-0.024427012,0.043106314,0.016458854,0.0043013524,-0.034656893,-0.012459274,0.04624611,0.0041473927,-0.010242661,-0.04846835,0.041951295,-0.04442683,-0.002706891,0.052865576,-0.018021217,0.018089058,0.00728553,0.05335678,0.02341995,-0.02110384,-0.0074548754,0.00089502236,0.015274146,-0.0527461,0.029814743,0.03038639,0.07036643,0.083503075,-0.020134283,-0.0140962135,0.010003042,-0.041444622,0.023383405,0.013094798,0.029032951,-0.06647324,-0.010425775,-0.012634193,-0.06654157,0.036846776,-0.04217445,0.021624424,-0.028167507,0.00514704,0.06423008,0.01029206,-0.0041281777,0.040106535,0.020191485,0.0087554585,-0.017094553,-0.016315496,0.019688403,-0.042285595,0.04225645,-0.029586352,-0.04585659,-0.001935026,-0.007079125,-0.032615557,0.015279142,-0.006666902,-0.0050763367,-0.008777761,-0.0010830445,-0.060425412,-0.0021737658,-0.007748744,0.034124006,-0.045014985,-0.014000557,-0.053943012,-0.0061748247,0.02642267,0.01717039,0.04358492,0.05394047,0.033469513,-0.0025183628,0.043450024,-0.0064044185,-0.00089520944,0.015664794,0.034160078,0.0063169478,0.011178869,-0.008745606,-0.025834575,0.019659312,-0.023744253,-0.008586347,-0.028116943,-0.037894104,0.029928304,-0.030076973,0.021263078,0.0032544562,-0.0049089217,-0.01766611,0.0655702,0.031347167,-0.008294906,0.005559521,0.0073677534,0.04391295,0.015769692,0.07845422,0.007989498,-0.0049394057,0.0104035,-0.01496474,0.07341746,0.010078429,-0.022744453,0.03100798,-0.012358675,-0.019084692,-0.046963908,-0.022850106,-0.04835999,0.047032326,0.012297059,0.033734947,-0.026698982,-0.05376635,-0.022253962,-0.01473006,-0.0048674624,-0.00656013,0.0130209355,-0.036422387,0.0044200695,-0.030853143,0.08983918,-0.02087798,0.00885996,-0.05439545,0.05136591,-0.03421626,-0.04533993,0.031124229,-0.03263189,-0.005235453,-0.012485845,0.04648402,0.027036263,-0.032975096,0.032811284,-0.010745451,0.052395567,0.01638793,-0.061188914,-0.06643377,0.014762195,-0.012739112,0.0029489612,0.08762486,-0.015185326,0.020462764,-0.017621765,0.018173395,-0.05298961,-0.0055476283,-0.0018707194,-0.019162131,0.028380932,-0.032713797,0.002988238,0.018408155,-0.052243307,-0.022139315,-0.06145775,-0.014929027,0.0019295791,0.024771312,0.007451292,-0.0076641142,-0.04358723,0.008153526,0.007357529,-0.008518211,0.03212513,0.036336552,0.045249593,0.051880307,0.037669297,-0.05177957,-0.05737919,-0.055811983,0.0006289328,0.020908175,-0.053459834,0.011161489,-0.04287881,-0.041737955,0.022681173,0.03040275,-0.03201849,-0.02301834,0.023897046,0.042306527,-0.031450327,-0.022533935,0.050609082,0.04206934,-0.028765934,0.024190756,0.028585985,0.043645106,-0.024070455,-0.016407829,-0.0904754,0.003463536,0.00024787016,0.002629577,0.013708289,0.0005339173,-0.02589645,-0.026744679,-0.018368583,0.025953813,0.015490538,-0.014498525,-0.007943257,0.016097223,0.017205823,0.0076048262,0.010993377,-0.06031305,0.060750004,0.031504586,-0.033543553,0.0415132,-0.017985106,-0.039592095,0.070118174,0.01903203,-0.054383792,0.03444668,-0.0067380606,-0.040068798,-0.038298257,-0.031608462,0.0021979623,-0.027708936,-0.0046284897,0.05363261,0.020042598,-0.006492248,0.031686306,0.019553991,0.009478929,0.042678785,-0.025475264,0.0458846,0.033337146,-0.010446602,0.061808363,-0.05394511,0.0019487196,0.019756688,0.045315433,0.03943511,0.017653726,0.0027961202,-0.039717387,-0.00485697,-0.009925284,0.043334216,0.020926025,-0.04762822,0.022562802,0.05904153,0.024470052,0.008698564,0.018718593,0.0043591247,-0.031198516,-0.022012524,-0.0520581,-0.0047106976,0.0006004814,0.0028873414,0.028067186,-0.055688154,0.0058704126,0.028524242,0.039991315,-0.033384047,0.04150303,-0.05340908,0.015013294]	Image title: Query Prototyping and Memory Compression\nTags: prototyping, compression, data-transformation, keys, values\nKey objects: Input, Keys, Values, Prototype, Compressed Values, Prototype Layer, Compression Layer\n---\nSummary:\nThis diagram illustrates two methods: query prototyping and memory compression. Both diagrams show a process where input data is transformed and processed, with the goal of representing or compressing information. The left side demonstrates query prototyping, while the right depicts memory compression.\nFull description:\nThe diagram shows two processes. In (a), 'Input' is processed and split into two branches labeled 'Keys' and 'Values'. The 'Keys' and 'Values' are then fed into a 'Prototype Layer', which generates a 'Prototype'. In (b), the 'Input' is also split into 'Keys' and 'Values'. These are then fed into a 'Compression Layer', which outputs 'Compressed Values'. Both diagrams showcase distinct approaches to data transformation  one for generating a prototype, and the other for compressing the data into a smaller representation.\nText found in image:\n- (a) Query prototyping\n- (b) Memory compression\n- Input\n- Keys\n- Values\n- Prototype\n- Compressed Values	{"tags": ["prototyping", "compression", "data-transformation", "keys", "values"], "title": "Query Prototyping and Memory Compression", "doc_id": "50683ed8-48c8-4195-9be2-90b468db6950", "source": "./images/a-survey-to-transformers/image_9.png", "summary": "This diagram illustrates two methods: query prototyping and memory compression. Both diagrams show a process where input data is transformed and processed, with the goal of representing or compressing information. The left side demonstrates query prototyping, while the right depicts memory compression.", "doc_type": "image", "key_objects": ["Input", "Keys", "Values", "Prototype", "Compressed Values", "Prototype Layer", "Compression Layer"], "parent_doc_id": "c1c11395-9c5d-4877-b686-6be0282fa997", "text_in_image": ["(a) Query prototyping", "(b) Memory compression", "Input", "Keys", "Values", "Prototype", "Compressed Values"], "contextual_description": "The diagram shows two processes. In (a), 'Input' is processed and split into two branches labeled 'Keys' and 'Values'. The 'Keys' and 'Values' are then fed into a 'Prototype Layer', which generates a 'Prototype'. In (b), the 'Input' is also split into 'Keys' and 'Values'. These are then fed into a 'Compression Layer', which outputs 'Compressed Values'. Both diagrams showcase distinct approaches to data transformation  one for generating a prototype, and the other for compressing the data into a smaller representation."}
d07379ac-a0d7-4bd3-8e7f-cfa96bf31763	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.014568769,0.05473358,-0.031560447,0.01529099,-0.019672876,0.038862,-0.040475383,0.07784657,0.04723347,-0.04643076,-0.030378787,-0.030689128,0.061855566,-0.031176902,-0.032413345,0.008600018,0.010444115,0.06120988,-0.005583992,-0.025215007,0.041590154,0.031520188,0.043640815,-0.017847963,-0.0004955476,0.02326731,0.00037164483,-0.03299078,0.008746565,0.0034476907,0.010151416,-0.017615756,0.03739051,0.0076258243,0.024717798,-0.009180129,0.017581686,-0.070876576,-0.0013024383,-0.016692482,0.023545293,0.024048647,-0.029768985,-0.030215617,-0.020462392,-0.042109672,-0.076571874,-0.005351023,0.029493896,-0.028461298,-0.02703447,-0.021484246,-0.0070728017,-0.03265807,0.003947792,0.010963033,0.004209213,-0.007700179,-0.023145795,-0.07984759,-0.00527749,-0.00030974732,-0.027133808,0.0075809387,0.045114566,-0.026646338,0.016530436,0.0028237666,0.042804435,0.13186046,-0.045301545,-0.012962138,0.026760062,-0.054371066,0.15124114,0.035866674,-0.038287483,-0.0039428547,-0.054202616,-0.00088763033,-0.011372343,0.051010385,-0.060698926,-0.034772184,0.098020315,-0.00552499,-0.0679227,-0.002466179,0.062771805,-0.06603008,-0.015932618,-0.024378931,0.0064557316,0.054267593,-0.021802906,-0.040713154,-0.030789407,0.030782592,0.008469691,-0.08195617,0.017270401,0.010007033,0.10887197,0.09544416,0.03677151,-0.05236134,-0.00031514323,-0.060579725,0.033285104,0.034149855,0.016958937,0.009042406,-0.0704407,0.008482567,-0.047184028,0.042772178,-0.039471947,0.010530011,-0.011739166,-0.019705739,-0.0010793685,-0.01070579,-0.018398715,0.007371747,0.06542038,0.0311832,0.026354486,0.004182132,-0.008152638,-0.0007617839,0.05855676,-0.022627078,-0.06266596,0.0546194,-0.009167186,0.0066287653,0.08459566,-0.0049736258,0.008751515,0.029754808,0.010711479,-0.08344301,-0.08971176,0.026899759,-0.040206477,0.0692555,-0.06426923,0.024021188,-0.043851294,0.07194938,0.034068268,-0.011912978,0.006824653,-0.02472567,-0.022086764,-0.01253386,0.010804148,-0.10765599,-0.008290389,-0.04176493,-0.067843065,-0.030788496,-0.014978613,0.05450895,0.00024523443,0.07116268,0.025338227,-0.03435565,0.017805163,0.04591182,-0.031942707,-0.0234968,0.002890904,-0.047134265,-0.0032738557,-0.0047147074,-0.019805165,-0.036348432,0.03343078,0.01590046,-0.009668746,0.049876004,0.03166267,0.062151693,0.000279641,0.025379198,0.024811057,0.0383956,-0.018584477,-0.0032010027,-0.00011632247,-0.008942683,-0.024208803,0.0013800334,-0.004275065,0.020677987,0.043155387,-0.08588415,-0.03011493,-0.050799694,-0.061046127,-0.008278019,-0.019491665,-0.00372827,-0.0008036544,-0.01602659,0.00274801,0.007567339,-0.01086777,0.05349697,-0.01268143,-0.020619188,-0.034636144,0.018656654,0.04373873,0.022572137,-0.019318284,0.09565665,-0.010366052,-0.047361355,-0.03109709,0.003889889,-0.0045853234,0.027767258,-0.027429001,0.004624799,-0.07043026,-0.0031231511,-0.029400183,-0.02329227,0.009800387,0.07037209,-0.0231651,-0.008539096,-0.04490333,0.017947737,-0.02964373,0.016114118,-0.00029181963,0.036109846,0.038453728,0.031992186,-0.012396294,0.11073612,0.016491326,-0.0088020805,-0.00050936243,0.058353513,-0.034069296,0.040758125,0.00418026,0.036683004,0.008964172,-0.0052610687,-0.0037671644,-0.057441287,-0.011623592,0.017315652,0.053104777,0.044850785,0.013709617,0.008012662,-0.015448902,0.005199012,-0.004321492,-0.0026997887,0.028136713,0.050256383,-0.019807747,-0.007965575,0.024522817,0.06096918,0.023963317,0.0075537316,-0.0040173596,0.055510182,0.021002177,0.0073163565,0.03188264,-0.011587479,0.005073126,-0.06705749,0.03899902,0.008200577,-0.0333702,0.007058612,-0.0017880858,-0.0366389,-0.007535201,-0.025628708,-0.015262513,-0.008671382,-0.02500015,-0.01322333,0.015470497,0.030816896,-0.013584066,-0.021588571,-0.036931466,0.06620069,0.10184792,0.019369273,-0.00742512,0.0057239393,0.017563688,-0.00088400324,-0.06502377,0.028426178,0.064923346,-0.03161578,0.03585055,0.0035459932,0.030326566,0.0026071544,0.06851265,0.047318242,0.034670603,-0.062135767,0.036588922,-0.01048485,0.024746446,-3.1017404e-05,0.026758105,0.0066745444,-0.0005482834,0.031284552,-0.011996755,-0.020125953,-0.058353566,0.05726212,-0.059246924,0.03311848,0.07414579,0.009358748,-0.011162951,0.0050733206,0.035022512,0.013836348,-0.0010737873,0.011318494,0.0047860104,-0.025967058,0.045797326,-0.0052091307,0.012705618,0.06150639,0.016027825,0.022512963,0.054857675,-0.01782239,0.008374767,0.021969248,0.02621802,-0.025361788,0.0045262533,0.007697853,-0.012296021,0.025690801,-0.008818007,0.004746312,0.04669996,-0.04570426,0.020115364,-0.059105802,0.049934056,0.022025144,-0.07499369,0.02327545,0.019852757,-0.042257495,0.0050730743,-0.04376297,0.006884852,-0.027063977,-0.008345057,0.014296116,0.06549527,-0.010487983,-0.035539877,0.0066930503,-0.001776607,-0.021503786,-0.03161169,-0.03783692,0.06958893,0.012808426,-0.029612055,0.09709635,-0.0005301272,0.05254318,-0.06845369,0.016706645,0.044704944,-0.016354147,0.0024692765,0.029006314,-0.0017897234,0.062288374,-0.020875052,-0.020193925,0.014414762,0.013711059,-0.07400285,0.027168516,0.0029205664,-0.0019487928,-0.06066737,0.0023929807,0.010101683,0.008001884,0.03281831,-0.053907245,-0.012252771,-0.025728026,-0.001355469,-0.0018890862,-0.034767132,0.03402009,-0.05988618,-0.016471773,-0.033911075,0.020901913,0.014916291,0.044462122,0.004179943,-0.018126361,0.008951583,-0.00032707534,0.030753564,-0.08539378,0.007911561,-0.07311427,-0.026901757,0.059945833,-0.006825147,0.04100791,-0.068930194,0.08192259,-0.007663584,0.033017922,0.049637303,0.0073425747,-0.0099849785,-0.033345655,0.02539259,-0.014374185,0.031165302,0.011863546,-0.018963888,-0.043628514,0.037002824,0.02717485,0.009391481,-0.05310432,0.03392262,-0.0061223246,-0.027969655,-0.034111913,-0.061112985,-0.003817407,0.022627998,-0.009369765,0.018148784,-0.03326715,0.01593138,-0.0013860748,-0.024313955,-0.03620118,0.0016682681,-0.009763981,-0.04241635,0.0012186802,-0.060386844,0.019764131,-0.008876914,0.029384173,-0.029287621,0.057860952,0.007665278,-0.01658162,-0.031212194,-0.03365066,0.016748894,0.020593522,0.015192575,0.0078018503,0.0017617431,-0.0153677575,-0.030089818,0.03463682,-0.015508065,0.05022926,0.012379457,0.02881341,0.03713044,-0.041628808,-0.010691702,-0.015124756,-0.019080548,-0.03478851,0.0046655843,-0.008092507,0.06683073,0.0074334503,0.00021496016,0.036395315,0.020340996,-0.021909077,-0.027825484,0.043469723,0.015120088,-0.07348011,0.004637934,0.013812352,-0.02105062,0.048683625,-0.015249202,0.005133565,-0.00809886,0.06445603,0.009612225,0.03532455,0.04105317,-0.028679162,0.029124187,0.025446849,0.0060520996,0.0063137487,0.029145733,-0.031303465,0.034709614,0.01563813,-0.05353971,0.0006433041,-0.048198055,-0.06364131,-0.0061651184,-0.06673928,-0.002122129,-0.03261455,-0.016607923,-0.0067001726,-0.0009797383,0.012417331,0.05840294,-0.045371607,0.0022791494,0.029957289,-0.0036524623,0.06756984,0.05018514,0.02702543,0.005186525,-0.0024668612,-0.01958098,0.032014105,0.09588835,0.011218644,0.021115523,0.048170928,0.027746672,0.009880445,-0.009373048,-0.035188667,-0.034288567,-0.019007307,-0.014905345,-0.009053504,-0.0411754,0.00077151397,-0.023609959,-0.011981741,-0.027714456,0.030595656,0.033346854,0.059600182,0.017373838,-0.008040784,0.040956296,-0.014996394,0.039274734,0.07019899,0.080921195,0.004920779,0.029563768,0.018288808,0.036713254,0.010079228,0.004405619,-0.047827598,-0.011986586,-0.03932652,0.013509804,0.0063975533,-0.020715743,-0.0285161,0.014994371,0.019044247,0.028623402,-0.041329756,-0.039755687,0.0037940696,-0.02041706,-0.035418343,0.022209939,0.030873287,-0.01613499,0.05761697,-0.030388862,0.039065458,-0.053477485,0.02424586,-0.011100126,0.029157612,-0.026463373,-0.032405525,-0.00017323457,-0.040152546,-0.016936664,-0.08647255,0.07144927,-0.027448978,-0.018220982,-0.012709098,-0.0031071377,-0.025941253,0.0050587677,-0.012920525,-0.0039964532,0.0049883877,-0.015764823,-0.006574966,0.07568233,-0.014063945,-0.044448692,0.036643773,-0.029234951,-0.060480934,-0.015904075,0.021951918,0.0226123,-0.024317032,-0.0099987555,0.062566705,0.05932525,-0.031759344,-0.0040039886,-0.032672945,-0.04688512,-0.035690658,0.0721583,0.040757827,-0.025630502,-0.05000219,-0.00029648968,0.0015742702,-0.010429147,0.036225654,0.02775311,0.07863845,0.034682088,0.035513185,-0.028917057,-0.061761882,-0.014513107,-0.060780667,0.017989678,-0.018651536,0.052750006,-0.06333574,-0.040374123,-0.008898028,0.0245584,-0.023938987,-0.011405689,0.06371784,-0.04657541,0.008321989,-0.030821236,0.0072374493,-0.01568986,0.027818253,-0.046682905,0.06533895,0.004312257,-0.025631523,-0.0041458122,-0.07406783,0.003492445,0.028199393,-0.016720466,0.036253057,-0.0054250793,-0.009618731,0.012006406,0.010770994,-0.0053138216,-0.044283774,-0.042994328,0.054655284,0.04212941,0.0034677275,-0.039732095,-0.01354666,-0.09825785,0.047586102,0.0015944728,-0.06656831,0.061630234,0.019764697,-0.036621492,0.025568403,-0.008473939,-0.012598416,0.003763363,0.012830141,0.04688012,0.023501081,-0.0059022275,-0.022046253,-0.052372243,-0.016873721,-0.008315112,0.025468372,-0.0028222653,0.039484076,-0.007828224,0.033431128,-0.009009711,-0.0017187431,0.036730707,0.008059252,-0.007940579,0.04297727,-0.02287771,0.042605765,0.001166113,0.05965372,-0.013579874,0.029595843,-0.012405878,0.021103144,0.01955181,0.003181259,0.016727366,-0.052766718,-0.028449448,-0.02288716,0.014822462,0.016350815,-0.025850762,0.006901841,-0.016810788,-0.020772833,-0.054136485,-0.026148228,0.026486887,0.025252301,0.027374696,-0.008080423,-0.049680017,0.0022281671,-0.026881788,0.026340535,-0.040987987,0.0023175736,-0.0067439503,0.029161815]	Keywords: attention mechanism, attention distribution, prior attention, Transformer\nKey Objects: Attention Mechanism, Attention Distribution, Prior Attention\nRefers to Images: ./images/a-survey-to-transformers/image_10.png\nHypothetical Questions:\n- What is the difference between a traditionally generated attention distribution and a 'prior' attention distribution?\n- Why might a model choose to use a 'prior' attention distribution instead of relying solely on input-derived attention?\n- How does the concept of a 'prior' attention distribution offer flexibility in designing attention mechanisms?\n---\nSummary:\nAttention mechanisms typically generate attended values as a weighted sum of vectors, using an attention distribution. Traditionally, this distribution is derived from inputs, but it can also originate from other sources, referred to as a 'prior'.\nOriginal Text:\n### 4.5 Attention with Prior  \nAttention mechanism generally outputs an expected attended value as a weighted sum of vectors, where the weights are an attention distribution over the values. Traditionally, the distribution is  \n$^{8}$The rank of A is far lower than input length T .  \nFig. 9. Attention with prior. This type of model fuse generated attention scores with pior attention scores, producing the final attention scores for attention computation.  \n\nContextualized Text:\nIn Transformer models, the attention mechanism often produces an attended value as a weighted sum of vectors. These vectors are weighted by an attention distribution, which typically originates from the input. However, this distribution can also be derived from a separate source, known as a 'prior', providing an alternative way to control the attention process.	{"tags": ["architecture", "NLP", "transformer", "attention"], "doc_id": "d07379ac-a0d7-4bd3-8e7f-cfa96bf31763", "summary": "Attention mechanisms typically generate attended values as a weighted sum of vectors, using an attention distribution. Traditionally, this distribution is derived from inputs, but it can also originate from other sources, referred to as a 'prior'.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["attention mechanism", "attention distribution", "prior attention", "Transformer"], "key_objects": ["Attention Mechanism", "Attention Distribution", "Prior Attention"], "contextual_text": "In Transformer models, the attention mechanism often produces an attended value as a weighted sum of vectors. These vectors are weighted by an attention distribution, which typically originates from the input. However, this distribution can also be derived from a separate source, known as a 'prior', providing an alternative way to control the attention process.", "mentioned_images": ["./images/a-survey-to-transformers/image_10.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["What is the difference between a traditionally generated attention distribution and a 'prior' attention distribution?", "Why might a model choose to use a 'prior' attention distribution instead of relying solely on input-derived attention?", "How does the concept of a 'prior' attention distribution offer flexibility in designing attention mechanisms?"]}
652d61f0-fa61-4850-a911-230eb2e62a31	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.030941458,0.01818123,0.026288327,0.026280815,-0.03394187,0.056391697,-0.022380305,0.0136697935,0.053007152,-0.021910222,-0.015842052,-0.038382888,-0.010382472,0.0015966194,0.016686711,0.031062003,0.008680898,0.057636812,-0.041600626,-0.03734838,0.037276987,0.045922864,0.014548334,0.045288764,-0.00799357,0.026186155,0.031132912,-0.0024830988,0.0073117614,-0.029600672,-0.011062849,-0.030080015,0.040490966,0.024856582,0.041445125,-0.04966383,-0.016668055,-0.09966139,0.005983126,-0.0018315845,-0.007048183,0.07965922,0.021780327,0.0041742343,0.024598451,-0.02946534,-0.073683135,-0.022544695,0.027373811,-0.0076289508,0.010841775,0.0075226547,-0.015193232,0.0012206624,-0.030839697,0.027735986,0.02227587,-0.06638345,0.040967297,-0.062678084,0.0008081892,0.048119586,-0.018785411,-0.020494314,0.03119323,-0.05154615,-0.014524569,-0.0018074597,0.087269545,0.13771975,0.0038372076,0.010638153,0.0017849875,-0.055550802,0.11610693,0.07664761,-0.023897806,-0.0010928572,-0.06687086,-0.019036388,0.03252991,0.069515176,-0.063383155,-0.024768269,0.047729705,-0.08103545,-0.016749341,0.027261706,0.03963708,-0.045981895,0.005516346,0.016554037,0.028361024,0.086721115,-0.04406985,-0.06309501,0.011066347,0.011056266,0.022573017,-0.07359743,0.04196236,0.0038903176,0.060754597,0.11568461,0.032928362,-0.021212412,-0.0038636364,-0.045894135,0.010538424,0.045247898,-0.0110260565,0.013550555,-0.07199359,0.0062241135,-0.043207765,-0.008976981,-0.057969827,0.0020124768,-0.011363709,-0.050695375,0.018765034,-0.0015023325,-0.012096536,0.03869375,0.024868624,0.043402947,-0.0061760675,0.00092648924,-0.0146402335,-0.027070941,0.049844496,-0.016754251,-0.050733842,0.014294032,-0.038831495,-0.01019775,0.037546497,-0.02435939,-0.02685384,-0.005204031,-0.012235027,-0.013491307,-0.04828596,0.017442008,0.037336003,0.013874644,-0.080024324,0.01685568,-0.067109965,0.057175826,0.041271947,0.01649009,0.023486046,-0.011545483,-0.038685303,-0.019470565,-0.016866455,-0.035601243,6.291054e-05,-0.027944377,-0.08710987,-0.020216506,-0.0142845595,0.06886081,0.01647529,0.114493504,-0.014127892,-0.014085021,0.023703523,-0.008810931,-0.01629706,-0.015267847,0.015380643,-0.019051006,0.026286146,-0.008707032,0.03301069,-0.023808163,-0.017031115,-0.017575318,-0.012672935,0.09230729,-0.005582701,0.0033256498,0.015221521,0.06384192,0.006338955,0.07480325,-0.051524106,-0.003254899,0.00213353,-0.055309325,-0.018403886,-0.02890146,0.011366601,0.03588983,0.03633256,-0.0343969,-0.024406115,-0.016482648,-0.010039862,-0.0035697788,-0.034253072,0.022861874,0.016498951,-0.006551067,0.04337458,-0.04622266,0.027883558,0.03063803,-0.005386182,0.0012381257,-0.051468957,0.035362724,-0.012616017,0.02584788,-0.0079114195,0.052125122,0.036959436,-0.0077654137,-0.020451864,-0.003987777,-0.016346905,0.04344601,-0.044656448,0.021065213,-0.0087124435,-0.0035170894,-0.0056657195,-0.008325831,0.0013323056,0.0135476,-0.034887247,0.06452504,-0.038591664,0.019655433,-0.0003190688,-0.002009668,-0.022997217,0.023130272,0.046834808,0.05472408,-0.02323077,0.07174105,-0.0046366984,0.033960976,-0.011573101,0.036034387,0.00795329,0.019702816,-0.019233275,0.03359097,0.0482009,-0.010888543,-0.0039205765,-0.014341861,0.027658088,0.0056706825,0.03751739,-0.038347177,0.033421844,-0.038561765,-0.021781653,0.035856485,0.0048477235,0.021368511,0.047846116,-0.016729407,-0.007975444,0.002898841,-0.008892757,0.056693025,0.014113819,0.0035467034,-0.0098739,0.036605377,0.046573874,0.003370399,0.030185834,-0.0050494145,-0.054394785,0.0051550684,0.026571576,0.01608614,0.0079350155,-0.0073303105,0.067899406,-0.0064269616,-0.038901445,0.05068468,0.018396467,-0.0073915683,-0.028877044,0.017837858,0.033172578,0.0007220601,0.009677413,-0.039606318,0.0043337424,0.0059992233,0.07902178,-0.018782357,0.017449379,0.057561707,0.062502354,0.009806265,-0.04921608,0.04773738,0.045576733,0.034110315,-0.017440086,-0.00055613363,-0.019844733,0.028486244,0.015951298,0.061520424,0.07375737,0.0064509264,-0.00010850937,0.017088382,0.041712437,0.004191584,0.024636352,0.033548344,0.046510182,0.036432143,0.02268705,-0.0001523522,-0.076765426,0.053033445,-0.06536424,0.016659068,0.074639514,-0.009476466,-0.030601611,0.0007700008,-0.015043871,0.03205122,0.039928474,0.03385486,0.01091569,-0.04376512,0.059919998,-0.01878348,0.006742902,0.06697262,-0.007585227,-0.007947776,0.034281258,0.03229243,0.028504474,-0.0026941572,-0.004004052,0.014832065,0.023507997,0.007397164,-0.022801995,-0.011059092,0.01848078,-0.005009994,0.06628503,-0.08512599,0.051884085,-0.05133537,-0.008499227,-0.02344268,-0.055981893,0.06525676,0.08734028,0.007442686,0.0023649114,-0.045637358,0.08324143,-0.06278409,0.008416075,0.07230183,0.046556935,0.023752317,0.007541558,-0.027101783,0.012590663,0.016683053,0.02826782,-0.050823137,0.018043824,-0.018296901,0.014272287,0.045340184,-0.07392842,0.060184766,-0.078497864,0.0034187825,0.024061412,-0.012059347,-0.012831332,-0.03578725,0.024378862,0.03172134,-0.020637624,0.009776876,0.012798964,0.015979724,-0.06208621,0.041971892,0.019092629,7.0912515e-06,-0.08879107,0.016926404,0.015255051,0.007305152,0.023394838,-0.041621037,-0.0068772202,-0.010116177,0.01411587,-0.040900055,-0.009799337,0.020499708,-0.053906653,-0.039429776,-0.007087976,0.04966994,0.06059965,0.028898275,0.047639534,-0.029672774,-0.04351056,0.0443937,-0.012559965,-0.017729064,0.053775247,4.07188e-06,-0.023328753,-0.01842657,-0.022412404,0.00039561844,-0.038086545,0.06517353,-0.036653217,-0.011027093,0.035152234,-0.034085985,-0.008092987,-0.04530762,0.026222384,-0.031476203,-0.009308942,0.042314652,-0.054076586,-0.023924654,0.0380437,0.023926163,-0.002000657,-0.0016441747,0.0795093,-0.029233089,-0.014817043,-0.034187168,-0.008703789,0.011307404,0.019836728,0.015536339,-0.0006326829,-0.022531858,-0.01198095,-0.010137085,0.034302752,-0.031382915,-0.032490976,-0.02502704,-0.0104737105,-0.0013739271,0.016554156,0.037916813,-0.030270027,-0.008727791,-0.016482355,0.04032175,0.04893908,0.018480686,-0.07569073,-0.041840274,0.02084515,0.0075629354,0.008238567,-0.0041976096,0.014930985,-0.057574693,-0.017502954,0.02460726,-0.010506243,0.024064878,-0.0062926537,0.03568624,0.019989243,-0.05778792,0.026709542,-0.027400473,0.05150806,-0.043470666,-0.01694436,0.033380184,0.049308356,0.040471394,0.00907056,0.025142254,-0.020507783,-0.02374506,-0.009773118,0.039967548,0.017447727,-0.0145556005,0.01056896,-0.0011631974,-0.08110144,0.036257584,-0.016590746,0.024454433,-0.01068893,0.012308188,0.04462665,0.013655535,0.043103185,0.021471433,0.005367791,0.038748592,0.0010175379,0.019383358,0.061853625,-0.076781094,0.007304944,-0.081351556,-0.033296872,0.012706467,-0.035538286,-0.024577588,-0.004437993,-0.069058314,0.00022413257,-0.013382668,-0.05170537,-0.0878987,0.016986407,-0.02640003,0.034205902,-0.060900718,-0.028436007,0.014753568,-0.0104271285,0.08822225,0.04232582,0.05173149,0.014236738,0.005829991,-0.017496776,0.013048761,0.030629767,0.0054221163,0.017368028,0.022041738,0.028550023,-0.0031898194,-0.010973734,-0.04437502,-0.0063238973,0.024297018,0.022598453,-0.0056494786,-0.021219917,0.024027096,0.010382536,0.0005313585,0.0032773314,0.0067159506,0.004273523,0.049587213,0.036232565,-0.029756008,0.015002622,-0.02329595,0.0067573367,-0.004228969,0.07185175,0.0033689,-0.0047381143,0.038388748,0.03805543,0.05394412,0.035443433,-0.0529188,0.001014538,-0.016007468,0.0065717483,-0.043777145,0.020146683,-0.06885538,0.026227757,-0.0037538246,0.039274417,-0.047231115,-0.041426927,-0.014761103,-0.011706044,-0.031859465,0.000120700075,-0.017023114,-0.012559593,0.061687846,0.0060614035,0.052087605,-0.066019885,-0.005168295,-0.041307084,0.010478686,-0.03501537,-0.02036173,-0.020699017,0.0011521779,0.036681358,-0.03588445,0.045394994,-0.028813368,-0.022557795,0.018316688,-0.04416845,0.010886025,-0.023549264,-0.03815192,-0.037537634,-0.008478377,-0.010249362,-0.008200966,0.087173544,-0.06134395,-0.02339426,-0.0061890436,0.02294299,-0.06836712,0.019570539,0.008094474,0.0041380315,0.002966258,0.013578897,0.02920346,-0.0057321223,-0.03705698,-0.03467721,-0.07592336,-0.018769667,-0.030940527,0.020892981,0.030667227,0.012146908,-0.030215692,-0.007722989,0.03333921,0.0076126023,0.050480563,0.08208208,0.04997632,0.052120876,0.03275165,-0.03808991,-0.036465276,-0.039994452,0.0056730625,0.035957817,-0.0031201942,0.03139202,-0.028714702,-0.022324616,0.041100286,0.008418201,-0.02011602,-0.005638947,0.05741053,-0.009851791,-0.033571593,-0.018770557,-0.0015279854,-0.005049265,0.00087791757,-0.038706917,0.07440152,0.01483889,-0.032049794,-0.013313761,-0.06288323,-0.020121535,0.021845844,-0.043629218,-0.0363228,-0.017340703,-0.056547962,-0.0045233783,0.0073702014,0.0077421996,0.007722412,-0.032265212,0.00018297075,0.05106905,0.034002863,-0.018449808,-0.024039054,-0.036902167,-0.0012969602,0.0033245299,-0.09112082,0.0036027443,-0.029792106,-0.02639206,-0.023104541,-0.019642575,-0.029100953,0.008491664,-0.012841895,0.004991244,0.017400755,-0.08193128,0.014459771,-0.07150052,0.0014587094,0.03331426,0.005538194,-0.054528248,0.066899754,0.030083554,-0.01320093,0.011879824,0.007304619,0.034967735,0.010362158,-0.022082498,0.034439016,-0.06289506,0.0140430685,-0.02324618,0.007722687,0.06023516,-0.017768076,-0.022642618,0.0004106179,0.012776576,-0.0070703593,0.025760973,-0.013784919,-0.018219156,0.017223973,0.0558275,0.052421283,0.01823161,0.01490563,-0.046234567,-0.033130758,-0.036476262,-0.030780062,-0.027303897,0.069314554,-0.011664943,0.035059102,-0.0656557,0.039382745,-0.014785037,0.06373747,-0.042916484,0.021198709,-0.051119845,0.027860079]	Keywords: query prototyping, query sparsity, Kullback-Leibler divergence, discrete uniform distribution\nKey Objects: queries, attention distributions, query sparsity measurement\nRefers to Images: None\nHypothetical Questions:\n- How is the Kullback-Leibler divergence used to measure query sparsity?\n- What are the advantages of assigning the remaining queries with discrete uniform distributions?\n- How does Informer's approach contribute to reducing the computational cost of attention?\n---\nSummary:\nInformer utilizes a query sparsity measurement, approximated by Kullback-Leibler divergence, to select prototype queries and assigns discrete uniform distributions to the remaining queries, reducing computational complexity.\nOriginal Text:\nInformer [170] selects prototypes from queries using explicit query sparsity measurement, which is derived from an approximation of the Kullback-Leibler divergence between the query's attention distribution and the discrete uniform distribution. Attention distributions are then only calculated for the top-u queries under query sparsity measurement. The rest of the queries are assigned with discrete uniform distributions.  \n4.3.2 Attention with Compressed Key-Value Memory . Apart from decreasing the number of queries with query prototyping, one can also reduce the complexity by reducing the number of the key-value pairs before applying the attention mechanism, as depicted in Fig. 8(b).\nContextualized Text:\nAs a query prototyping technique, Informer selects prototype queries based on a sparsity measurement derived from the Kullback-Leibler divergence between a query's attention distribution and a discrete uniform distribution.  Attention distributions are then calculated only for the top-u queries identified through this measurement, while the remaining queries are assigned discrete uniform distributions.	{"tags": ["attention", "optimization", "transformers"], "doc_id": "652d61f0-fa61-4850-a911-230eb2e62a31", "summary": "Informer utilizes a query sparsity measurement, approximated by Kullback-Leibler divergence, to select prototype queries and assigns discrete uniform distributions to the remaining queries, reducing computational complexity.", "doc_type": "text", "entities": ["Informer"], "keywords": ["query prototyping", "query sparsity", "Kullback-Leibler divergence", "discrete uniform distribution"], "key_objects": ["queries", "attention distributions", "query sparsity measurement"], "contextual_text": "As a query prototyping technique, Informer selects prototype queries based on a sparsity measurement derived from the Kullback-Leibler divergence between a query's attention distribution and a discrete uniform distribution.  Attention distributions are then calculated only for the top-u queries identified through this measurement, while the remaining queries are assigned discrete uniform distributions.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.3 Query Prototyping and Memory Compression"}, "hypothetical_questions": ["How is the Kullback-Leibler divergence used to measure query sparsity?", "What are the advantages of assigning the remaining queries with discrete uniform distributions?", "How does Informer's approach contribute to reducing the computational cost of attention?"]}
f5a29cce-2cde-4150-929f-17939a0d770f	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.027443524,0.012628436,0.021936378,-0.015753772,-0.01112562,0.023789823,-0.01010926,0.04263359,0.02303299,-0.0387669,-0.014108549,0.0011746283,0.0071730777,0.021979388,-0.013077328,0.00019270851,0.019658078,0.09278489,0.00550929,-0.032179356,0.023334045,0.014517459,0.015208042,-0.007116455,0.044624332,0.059970617,0.009130671,0.024272097,0.0012864701,0.022494996,0.036452655,-0.03522752,0.030727006,-0.0014051961,0.04376699,-0.021549337,0.023225045,-0.07205803,-0.0060171154,-0.0011591025,0.0019403124,0.04338635,-0.022992937,-0.02240584,0.021326317,-0.035492904,-0.06347833,-0.037397884,0.01953782,-0.02141524,-0.014687044,0.029391516,-0.05637423,-0.015747176,-0.0138489315,0.00969584,0.011665614,-0.076135784,0.045256887,-0.10591291,-0.030318538,0.011219804,-0.02765795,-0.036008924,0.039611828,-0.036737937,-0.011652666,0.026565269,0.007592242,0.14991549,-0.022880023,0.030940007,0.0042897663,-0.05278285,0.11429224,0.053845983,-0.026356837,-0.0171049,-0.07048344,-0.032167137,-0.006252689,0.05957574,-0.061982688,-0.0077471067,0.068515055,-0.045059677,-0.017322464,-0.008203672,0.06838442,-0.045605868,0.033049695,-0.019599332,-0.009829928,0.08810364,-0.03749702,-0.046991937,0.027618706,-0.014111592,0.02637416,-0.054034956,0.014301118,0.02112182,0.093638554,0.11298763,0.0582077,-0.026345469,-0.04180726,-0.064177044,-0.009046305,0.01120574,-0.016730543,0.01950211,-0.06287565,0.0029143263,-0.03887859,0.047367465,-0.05088859,0.019513339,0.009255177,-0.016405446,0.056918982,-0.035455093,-0.0016747396,0.03222093,0.043222222,0.045025855,0.016124869,-0.0043661725,-0.03763026,0.021117313,0.027730245,0.015401711,-0.03560585,-0.0040082913,-0.017854065,-0.014887934,0.0812177,0.0047882125,-0.014806094,-0.007040401,-0.0036882819,-0.039462127,-0.049434125,0.016395703,0.033268563,0.01000403,-0.07461266,0.03124276,-0.012593259,0.03902432,0.03183666,0.0118388,0.07059288,0.01805325,-0.018589972,0.014575669,-0.000102151935,-0.05039007,0.016149754,-0.051792223,-0.04465212,-0.028243748,-0.0031226885,0.07695023,0.027157389,0.119876534,-0.00823308,0.0009841153,0.017702712,-0.012547664,-0.050696686,-0.017522592,0.0027919381,-0.030071968,0.01654184,-0.002792953,0.028135797,-0.024820793,0.0046861568,-0.0010888285,-0.01280796,0.06207999,-0.020988915,0.01659152,-0.02166486,0.036978632,0.007295191,0.060634572,-0.019140247,-0.014525928,0.02079777,0.009327047,0.0051192762,-0.014589142,0.02309522,0.023713216,0.047512185,-0.09421979,-0.015907267,-0.022428483,-0.020539358,-0.006604009,-0.025876043,-0.004618953,0.028560264,-0.008255225,0.0022333912,-0.039143547,-0.014457536,0.037126854,-0.01710116,0.032479662,-0.033742223,0.017762015,0.0014823715,0.06085539,-0.025578462,0.010564326,0.03347935,-0.0592702,-0.048390582,-0.015270783,-0.051501755,0.052713368,-0.03495716,0.04457107,-0.004600728,0.03821788,0.040014133,-0.03038625,0.027986532,0.02627591,-0.046112724,0.025841372,-0.05589348,0.008618309,-0.011129861,-0.0009032644,-0.017438184,0.022562658,0.058268547,0.07892491,0.014927644,0.07611278,-0.029064432,-0.0088731,0.025847917,0.041514967,0.01407832,0.010387737,-0.014112619,0.033059783,0.030611832,0.023152951,-0.00047044488,0.0076848455,-0.0008825686,0.031162348,0.054635778,-0.013151403,0.033368785,-0.03998479,-0.003747771,0.04079474,0.012138374,-0.0180349,0.0322507,0.028664606,0.011408983,0.017901594,0.051976137,0.049725443,0.02545239,-0.006295617,-0.013864004,0.017430145,0.035139397,0.011214025,0.007910389,-0.020099923,-0.0035942975,-0.013985347,0.01239548,0.00492755,0.02013199,-0.02674956,0.02561925,-0.024420362,-0.023310628,0.03165387,-0.016320447,-0.024538787,0.044148028,-0.028875358,-0.012040272,-0.010615843,-0.008933755,-0.047960926,0.015241617,0.011389682,0.10582075,-0.0050710547,-0.0026979547,0.03911153,0.019640384,0.025910005,-0.03368439,0.06449418,0.007268315,0.011276202,-0.01443127,-0.020040756,-0.044653077,0.021489643,0.020060198,0.0857665,0.07734577,-0.0066789244,-0.016429672,0.022772577,0.03489107,0.015192167,-0.0015652559,0.04376795,0.039992545,-0.011153542,0.04352744,-0.035417434,-0.066076815,0.07146963,-0.07447821,0.044630103,0.07263975,-0.014094831,-0.031017538,-0.014114518,-0.010272199,0.02313226,0.022056336,0.0322729,-0.010634718,-0.040440414,0.0065451995,-0.011545751,-0.03141756,0.029056743,0.0030813883,-0.010599097,-0.000628809,0.0007858218,0.011707692,0.028516376,0.012644449,0.0390148,0.011238082,-0.017671987,-0.036048442,0.011739819,0.008816835,-0.017045293,0.055024274,-0.101765186,0.047243036,-0.024706487,-0.00012784262,-0.031558402,-0.07034284,0.02560388,0.056280673,0.016820002,0.015968366,-0.025523646,0.0696236,-0.03754739,-0.011280237,0.054358456,0.045689672,0.009649794,-0.016643865,-0.012361357,0.00071619503,-0.02525122,-0.031102952,-0.06492237,0.04789553,-0.008180256,0.049272668,0.066013224,-0.047824908,0.05753245,-0.049113527,0.028637843,0.04335014,-0.0055449875,-0.024654627,0.007637982,0.023651294,0.034764238,0.023235328,0.037407603,0.037876893,0.014638966,-0.04859867,0.01959194,0.034128044,0.032225728,-0.08511125,0.037803862,0.04278121,0.036708843,0.02122698,-0.035001893,0.002719315,0.019251693,0.048830487,-0.025606342,0.011357585,0.022767622,-0.051796895,-0.016480422,-0.028671285,0.026756775,0.021853724,0.03262612,0.029171972,-0.009184367,-0.05615544,0.045629222,0.0053798296,-0.071431436,-0.014164055,-0.039703574,0.011816926,0.025824605,-0.0018797232,0.003673213,-0.039947763,0.08902528,-0.029466657,0.0057396106,0.0558728,-0.017282298,0.0072519477,-0.032557156,-0.013198549,-0.03513333,0.03995218,0.06843249,-0.010427963,-0.024004526,0.023055302,-0.009903395,0.024291337,-0.031506862,0.06449521,-0.011425748,0.02151691,-0.02216713,-0.07658346,0.019820448,-0.017141217,0.033133708,-0.006057625,-0.016377626,-0.0143886125,-0.031553194,-0.04352599,-0.043508206,-0.02648008,-0.019834973,0.0033987304,0.040911227,-0.015391283,0.06996481,-0.0010962292,-0.010597886,-0.0235066,0.039703794,0.045361742,0.0016818937,-0.099993765,0.007425688,0.03118692,0.041319367,0.022069715,0.014745467,0.0024872085,-0.04326359,-0.007830496,0.0054491446,-0.002485684,0.027497273,-0.0021984268,0.034525968,0.011818436,-0.042944383,0.022136455,-0.013857965,0.045423314,-0.0344071,-0.024905546,0.011910935,0.034230307,0.037230447,-0.021863442,-0.010633807,-0.0114403665,0.01999157,-0.009816748,0.052795056,0.009470936,-0.04395473,0.034382947,-0.017960079,-0.05236452,0.0567053,-0.008043804,0.011037753,0.00042970464,0.04101004,0.042992733,-0.027246185,0.023443865,-0.00021552443,-0.026955305,0.012949313,0.0027360795,0.037894797,-0.008748106,-0.08976252,0.01928971,-0.03717378,-0.03805267,0.034342777,-0.07742806,-0.034459013,0.02876885,-0.06312015,0.017764902,-0.004497146,0.009204601,-0.060066354,0.026493296,-0.04141077,0.087039016,-0.039323837,0.01969301,0.0071649244,-0.014591407,0.08337554,0.01899292,0.02643759,0.001662498,0.015625779,-0.008331433,0.03408024,0.05503602,0.0037045116,-0.0155554535,0.0699575,-0.010808223,-0.024102503,-0.01074579,-0.053153172,-0.006476685,-0.005262363,-0.033154007,-0.01305407,-0.037457902,0.026203642,0.01953865,0.025398718,0.022730745,0.035651326,-0.0018671443,0.056411132,0.0293638,-0.005227982,0.008543574,0.03637948,0.048238564,0.021411935,0.08938173,0.022308236,0.0063796174,0.0191835,-0.011695834,0.012792368,0.0109955575,-0.03529754,-0.0071826144,-0.04789945,0.001386191,-0.018475581,0.008178712,-0.028787246,0.04797817,-0.009184086,0.028331008,-0.035493966,-0.0643404,-0.03811442,-0.043138023,-0.041077796,-0.03744244,0.0070744404,-0.023724884,0.01574123,-0.039126843,0.0715884,-0.095319435,-0.0029469188,-0.048990928,-0.0037932498,-0.017498698,-0.010014079,-0.045774896,-0.018019615,0.018458985,-0.021718739,0.027171567,0.030713173,-0.044371683,0.035961494,0.0008927083,-0.012026397,-0.017814947,-0.014045729,-0.03130017,-0.007567669,-0.030517174,-0.007197197,0.10357587,-0.029871536,0.005133623,0.022776179,0.023047613,-0.072375506,0.00987258,-0.029120836,0.017952051,-9.438696e-05,-0.021380322,-0.02619794,0.02238441,-0.020158468,0.006587861,-0.056821,0.012296213,-0.017256642,0.023822604,0.017167768,0.002210562,-0.028663281,-0.05099049,0.018688494,-0.008317078,0.04704595,0.071900845,0.049007483,0.07321765,0.03864814,-0.05065835,-0.048701502,-0.013234901,-0.019207312,-0.0020422272,-0.037993155,0.03220059,0.00074349024,-0.028527007,-0.00861175,-0.027650492,-0.032032464,-0.022006458,0.02904112,-0.0008237288,-0.044164713,-0.013430762,0.02650902,0.008511074,0.011244181,-0.015226101,0.06335095,0.015203633,-0.019480694,-0.06616659,-0.056646727,-0.036689162,0.03035198,-0.026136216,-0.012876022,-0.015416679,5.0392726e-05,-0.004835922,0.02382758,0.016545441,-0.04761825,-0.038028363,0.03353419,0.020059036,0.048108097,0.0115705095,0.0028034095,-0.10842036,0.031068088,-0.0110806255,-0.037137847,0.03703142,-0.02242557,-0.026573777,-0.010837937,0.015926864,-0.0019961365,0.03584255,0.0018423976,0.02274577,0.009574279,0.015042534,-0.035690676,-0.060295228,-0.05058063,0.012001723,0.016194362,-0.024624689,0.05724138,0.020185351,-0.0076913633,0.012763525,-0.008564296,9.660294e-05,0.009513229,-0.03241196,0.045300007,-0.035832856,0.014584438,0.014763226,0.017172497,0.08159588,-0.03362747,-0.0048730723,0.0011634907,-0.013309055,0.005881336,0.0038491825,0.0053910594,-0.03244654,-0.04001247,0.022612374,0.056918245,0.018499272,0.027812628,-0.0048322235,-0.021227395,-0.059301514,-0.049337666,-0.0004865337,0.035968505,0.037032362,0.01799904,-0.07364822,0.009298071,-0.02643,0.06427436,-0.00693302,0.036427684,-0.040286243,0.013752524]	Keywords: memory compression, strided convolution, global context, attention mechanism\nKey Objects: Keys, Values, Key-Value Memory\nRefers to Images: None\nHypothetical Questions:\n- How does MCA's use of strided convolution contribute to memory compression?\n- What are the benefits of using MCA in conjunction with local attention?\n- How does MCA compare to other memory compression techniques, such as those used in Set Transformer and Luna?\n---\nSummary:\nMemory Compressed Attention (MCA), proposed by Liu et al. [84], reduces the number of keys and values using a strided convolution to capture global context and process longer sequences.\nOriginal Text:\nLiu et al. [84] propose Memory Compressed Attention (MCA) that reduces the number of keys and values using a strided convolution. This modification is used as a complement to local attention proposed in the same work (as discussed in Sec. 4.1), in that it can capture global context. The mechanism reduces the number of keys and values by a factor of kernel size k and thus allowing to process significantly longer sequences than vanilla Transformer given the same computation resources.  \nSet Transformer [70] and Luna [90] use a number of external trainable global nodes to summarize information from inputs and then the summarized representations serve as a compressed memory that the inputs attend to. This reduces the quadratic complexity of self-attention to linear complexity w.r.t. sequence length.  \n$^{7}$The key-value pairs are often referred to as a key-value memory (hence the name memory compression).\nContextualized Text:\nTo reduce complexity in attention mechanisms, Liu et al. [84] introduced Memory Compressed Attention (MCA). MCA employs a strided convolution to decrease the number of keys and values, allowing the model to capture global context and process longer sequences compared to a standard Transformer.	{"tags": ["attention", "transformers", "compression", "NLP"], "doc_id": "f5a29cce-2cde-4150-929f-17939a0d770f", "summary": "Memory Compressed Attention (MCA), proposed by Liu et al. [84], reduces the number of keys and values using a strided convolution to capture global context and process longer sequences.", "doc_type": "text", "entities": ["MCA", "Set Transformer", "Luna"], "keywords": ["memory compression", "strided convolution", "global context", "attention mechanism"], "key_objects": ["Keys", "Values", "Key-Value Memory"], "contextual_text": "To reduce complexity in attention mechanisms, Liu et al. [84] introduced Memory Compressed Attention (MCA). MCA employs a strided convolution to decrease the number of keys and values, allowing the model to capture global context and process longer sequences compared to a standard Transformer.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.3 Query Prototyping and Memory Compression"}, "hypothetical_questions": ["How does MCA's use of strided convolution contribute to memory compression?", "What are the benefits of using MCA in conjunction with local attention?", "How does MCA compare to other memory compression techniques, such as those used in Set Transformer and Luna?"]}
c604e71f-4cdb-4da4-9db6-a5f0c02365c3	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.011876472,0.01313443,0.033082232,0.019833451,-0.018196154,0.028231721,-0.015250109,0.051486004,0.036601916,-0.04002253,-0.0012452989,-0.0017151368,-0.009511611,0.01219334,-0.0053280173,0.026968293,0.048019815,0.08560754,0.0068320585,-0.042248603,0.03319313,-0.0024432917,-0.009220652,0.015945,0.011023803,0.054357313,0.01828358,0.007940945,-0.029540934,0.0059149144,0.04417133,-0.050718963,0.009640513,-0.0033549971,0.014533447,-0.027532313,-0.0026560493,-0.100575075,-0.03368572,-0.0029269438,-0.0053905915,0.06759129,-0.056602396,0.0011358534,0.011186754,-0.062295713,-0.09285164,-0.026953995,0.04095975,0.008481787,-0.010060297,0.027653614,-0.04985372,-0.01673029,-0.016688485,-0.001225978,-0.0004388482,-0.080967516,0.054955997,-0.08250465,-0.013866945,0.020627035,-0.0353038,-0.025673974,0.04126102,-0.01629959,-0.009838978,-0.0011434163,0.04193714,0.13876691,-0.037516676,-0.0012902203,-0.021498261,-0.041996118,0.10854957,0.06795425,-0.020791844,-0.00647404,-0.061743498,-0.0069524483,-0.0059984154,0.03695103,-0.04025568,-0.043925483,0.07935969,-0.02516929,-0.0061512464,0.026701124,0.065554075,-0.056634985,0.013780077,-0.002281973,0.0066030254,0.08206324,-0.0018073383,-0.06656492,0.010095627,-0.028704438,0.00023829327,-0.031840026,-0.002298498,0.007430152,0.09049821,0.124545895,0.021653438,-0.02832799,-0.016397275,-0.027341992,0.014718054,0.034679458,-0.033734545,0.04444842,-0.06589442,0.0021109767,-0.022366865,0.052197948,-0.03436251,0.017342413,0.0049666264,-0.025971,0.06724886,-0.013492373,-0.011403165,0.0059333146,0.033197165,0.027894229,-0.016289007,0.0019471053,-0.04927822,0.013365043,0.04998339,0.015183084,-0.05175517,0.029782368,-0.0060631363,0.010175711,0.061276555,-0.013552317,-0.006989329,0.0063449973,0.0124673275,-0.009007766,-0.044097695,0.03008979,0.0485833,0.030163547,-0.056869082,0.028442089,-0.02814054,0.07390971,0.050238434,0.027106166,0.04891069,-0.017578164,-0.029009148,-0.0011536718,-0.0074278107,-0.036446273,0.010175089,-0.032074083,-0.051255293,-0.02766446,0.0100054005,0.077871025,-0.00198085,0.11450792,-0.006387807,-0.022913206,0.013452586,0.009241338,-0.022526478,-0.022743737,-0.012350638,-0.03371381,-0.0029541103,0.018219762,0.024996398,-0.035211112,-0.015137835,0.0076847994,0.008622391,0.059721343,-0.037750814,0.002409086,-0.029551595,0.012625444,0.037011642,0.03757598,-0.026416838,-0.03840437,0.009911186,-0.012724478,-0.019281084,-0.020777196,-0.002368277,0.051401492,0.044661395,-0.09035519,-0.04929931,-0.013930277,-0.009963228,-0.029675748,-0.056567777,-0.008910966,0.04740915,-0.009888365,0.007974818,-0.011644666,-0.01752294,0.022614906,0.016982121,0.026100082,-0.0013899682,0.03766978,-0.0070063276,0.010116114,-0.004319649,0.011678167,0.027074035,-0.020848516,-0.055764705,0.0118361525,-0.022507409,0.03765063,-0.03051163,0.04823453,-0.026832348,0.01841896,0.018915607,-0.034718186,-0.0062047755,0.014375083,-0.054492235,0.025767507,-0.039823055,-0.0007013218,0.010130466,0.035417054,-0.01922227,0.011566506,0.045659542,0.07705187,-0.033788905,0.0649083,0.0113778,0.008299537,0.012096878,0.051428195,0.012705422,0.033934046,-0.034420982,0.049929798,0.05032068,0.0013540022,0.01427615,-0.032223795,0.0023375324,0.017920153,0.031403422,-0.036767613,0.01883842,-0.016938372,-0.038777012,0.055247467,0.018276738,0.0007106581,0.01943486,0.013230047,0.008312303,0.018236673,0.03947516,0.05345776,0.012868863,0.0029842176,-0.021565678,0.0056169196,0.030303525,-0.006464308,-0.002346597,-0.004903391,-0.00839616,-0.009785565,-9.271141e-05,0.004083729,0.045787897,-0.034938652,-0.009330697,-0.0074551427,-0.024059387,0.022519764,-0.030901385,-0.026563387,0.030465096,-0.012881142,0.016450271,0.009944255,-0.0044340617,-0.051227026,0.0016997762,0.014805478,0.10323888,-0.012789371,0.020747244,0.030064698,0.06738227,0.0058036535,-0.03338474,0.057851553,0.029757861,0.034222778,-0.0070954086,-0.012743946,0.0025954917,0.024804976,0.046683185,0.07335037,0.04806829,-0.005951919,-0.0026890365,0.044592697,0.030555943,0.021443887,0.00367421,0.02646194,0.02708669,-0.015921906,0.079332836,-0.03985828,-0.065207995,0.0696737,-0.044898264,0.02603439,0.06438119,-0.013660896,-0.047415026,-0.0057091787,0.008131813,0.013791521,-0.000744792,0.00042721577,0.000498366,-0.050171718,-0.0032572104,0.010227303,-0.017796462,0.057586543,-0.0014054512,-0.039278243,0.0026413158,0.010980638,0.030764846,0.027052967,0.02070049,0.048171725,0.040211488,-0.0054447763,-0.019502757,0.03676306,0.007275618,-0.0027284725,0.04727371,-0.08486053,0.05566436,-0.03374605,0.01947533,-0.02745942,-0.039626203,0.02644762,0.05853297,-0.003049331,-0.0036582893,-0.029847937,0.039299507,-0.051207762,0.020327056,0.06945271,0.054744925,0.035735287,-0.0020594797,0.00068576605,0.0011990995,-0.010611795,-0.018390846,-0.052285466,0.0417187,-0.0042761266,0.041778307,0.06926165,-0.05084263,0.06889093,-0.043754112,0.014921997,0.029177573,-0.033758216,-0.04393085,-0.0037553562,0.0075473245,0.05091295,0.019137062,0.008004015,0.040584035,0.037505575,-0.06020887,0.022765033,0.038760398,0.010152275,-0.07468244,0.040134095,0.0643413,0.03535427,0.010703036,-0.026662845,0.0019754553,0.0200988,0.017962348,-0.027792698,-0.010744697,0.053888995,-0.05049813,-0.032693204,-0.01834718,0.039697733,0.05102097,0.024078308,0.0361205,0.0071912943,-0.007611235,0.047339085,0.015169816,-0.052576482,0.025198027,-0.013956103,-0.0050491826,0.0014797864,-0.01066028,0.007378133,-0.041674364,0.09010223,-0.034912985,0.014373505,0.046095736,-0.0135232005,-0.00079624425,-0.02653215,0.002994744,-0.052812442,0.029846676,0.06093962,-0.04617977,-0.027069602,0.019064292,0.038470127,0.014724817,-0.01876433,0.034776676,-0.043352418,0.01866366,-0.03406059,-0.06898486,0.010501378,0.028926967,0.016011452,-0.0056301164,-0.0039449884,-0.018106569,-0.0031231826,0.0014196696,-0.033164624,-0.011476399,-0.0041779806,0.00718476,0.04488389,-0.033164274,0.084836364,0.006332939,-0.03148148,-0.008557772,0.060380485,0.05959925,0.011722019,-0.077519506,-0.010581551,0.025819799,0.037981246,0.009781559,0.036931846,0.00036549574,-0.019328276,-0.017540764,0.024368221,-0.025182368,0.011184826,-0.015691282,0.040891178,0.019582076,-0.06338893,0.01828274,-0.016228609,0.047507454,-0.038054164,-0.044208445,0.003034009,0.056337878,0.038636617,0.0066198567,0.012798171,-0.042945225,0.03657762,-0.0077819615,0.08250324,0.017085068,-0.028248988,0.018344516,0.0023743233,-0.05122725,0.050195627,-0.021700861,0.021690562,0.040921744,0.042177014,0.04428423,-0.029077,0.029099263,0.030990852,0.010737867,0.014997698,0.0025928167,0.028796105,0.016503815,-0.08616461,0.013061554,-0.035172697,-0.015113616,0.009128577,-0.05735548,-0.03337424,0.010604133,-0.0496395,0.03142801,-0.0055384515,-0.009622735,-0.0726117,0.0118529685,-0.026254319,0.08943129,-0.04512285,0.017609354,0.026202122,-0.0038373042,0.09336855,0.021204462,-0.009346634,0.0071593826,0.0045462786,0.009674501,0.01703422,0.038846493,0.023760552,0.015383226,0.05270162,0.018067066,-0.033791035,-0.049981453,-0.039481446,-0.04775093,-0.02588735,-0.035180956,-0.025095664,-0.037165504,0.03154882,0.020980837,0.028226787,0.011158265,0.026590332,-0.00023430897,0.05820406,0.030099206,-0.009518328,0.02720721,-0.006229543,-0.003762102,0.0148894945,0.07161758,0.029365167,0.0089294035,0.025521198,-0.0057635503,0.021399228,-0.016054241,0.0024767527,-0.013277685,-0.04430353,-0.0099029355,-0.011432623,0.00016506675,-0.031659972,0.06634262,0.039011348,0.06405602,-0.07308948,-0.021767182,-0.022316782,-0.034013577,-0.035850555,0.005255623,0.011788089,0.0078008077,0.037499137,-0.019942626,0.08518151,-0.08489452,-0.022877658,-0.06369989,0.010827167,-0.03216668,-0.03399266,-0.044303786,-0.023010205,0.0009812015,-0.04896784,0.033193205,0.012485531,-0.032917287,0.01939083,-0.017571272,0.01489205,-0.018174188,0.012317411,-0.008959252,0.0054561654,-0.014257597,-0.0023630194,0.07822048,-0.054048542,-0.010142006,0.029954683,0.0063034883,-0.060832888,-0.0035378044,0.005202607,0.011074077,0.012604835,-0.015699044,0.0016654177,0.032164793,-0.048432965,0.01941737,-0.082383394,0.01637273,-0.00078205555,0.011228743,0.021974538,0.020163747,-0.052318495,-0.06764165,0.03513935,-0.01795536,0.038067836,0.03251724,0.012965487,0.08689038,0.0501985,-0.06617226,-0.04378422,-0.0244975,-0.03047607,0.03150986,-0.038868245,0.054171316,0.0051834183,-0.01066555,-0.036229968,-0.004234655,-0.029928844,0.0026654177,0.028546304,0.00086527073,-0.03023256,-0.0039652074,0.028717037,0.024696253,-0.0051833037,-0.007793514,0.034577083,-0.0028432787,-0.03061836,-0.03722941,-0.065199934,-0.051289275,0.023940628,-0.026500957,-0.007911575,-0.016720384,-0.023511406,-0.0072495784,0.04372469,0.023000147,-0.024977325,-0.04998586,0.058094293,0.032892667,0.06672997,0.0025117372,0.014766245,-0.07854195,0.023871714,0.0028312968,-0.06732172,0.028015161,0.007163986,-0.027675793,-0.01566957,0.0012461639,0.0072442996,0.023127962,-0.039424,0.04419039,0.011113055,-0.02489822,-0.0016047949,-0.070444554,-0.037860796,-0.001359062,0.004633605,-0.030910589,0.057309106,-0.0025979963,-0.0067356466,-0.021113519,0.014610894,0.02660059,0.03592933,-0.029962398,0.07485046,-0.023840997,0.055274498,-0.020422997,0.016023446,0.07667894,-0.019654825,-0.0028975331,0.0095898835,3.3727222e-06,0.012302019,0.005713849,0.0004505162,-0.03912025,-0.027964316,0.051075377,0.053401295,0.012806905,0.051000405,-0.012239037,-0.030625654,-0.06158814,-0.018675424,-0.027153429,0.0452013,0.030690957,0.013337732,-0.063964374,0.006424504,-0.031102916,0.0666294,-0.012318654,0.014260065,-0.006544147,0.026852883]	Keywords: key-value memory, linear projections, self-attention, memory compression\nKey Objects: key-value pairs, memory\nRefers to Images: None\nHypothetical Questions:\n- Why is reducing the size of the key-value memory beneficial?\n- What are the trade-offs associated with using linear projections or pooling operations for memory compression?\n- How does the assumption of a fixed input sequence length limit the application of methods like Linformer?\n---\nSummary:\nKey-value pairs in attention mechanisms are often called a key-value memory, and techniques like Linformer and Poolingformer compress this memory to reduce computational complexity.\nOriginal Text:\n$^{7}$The key-value pairs are often referred to as a key-value memory (hence the name memory compression).  \nLinformer [142] utilizes linear projections to project keys and values from length n to a smaller length n$\\_{k}$ . This also reduces the complexity of self-attention to linear. The drawback of this approach is that an input sequence length has to be assumed and hence it cannot be used in autoregressive attention.  \nPoolingformer [165] adopts two-level attention that combines a sliding window attention and a compressed memory attention. The compressed memory module is used after the sliding window attention to increase the receptive field. They explore a few different pooling operations as the compression operation to compress the number of keys and values, including max pooling and pooling with Dynamic Convolution [146].\nContextualized Text:\nIn attention mechanisms, the key-value pairs used in the process are frequently referred to as a key-value memory. To reduce computational complexity, techniques like Linformer and Poolingformer compress this memory. Linformer utilizes linear projections to reduce the size of keys and values, while Poolingformer combines sliding window attention with a compressed memory module using operations like max pooling.	{"tags": ["attention", "transformers", "optimization", "NLP"], "doc_id": "c604e71f-4cdb-4da4-9db6-a5f0c02365c3", "summary": "Key-value pairs in attention mechanisms are often called a key-value memory, and techniques like Linformer and Poolingformer compress this memory to reduce computational complexity.", "doc_type": "text", "entities": ["Linformer", "Poolingformer"], "keywords": ["key-value memory", "linear projections", "self-attention", "memory compression"], "key_objects": ["key-value pairs", "memory"], "contextual_text": "In attention mechanisms, the key-value pairs used in the process are frequently referred to as a key-value memory. To reduce computational complexity, techniques like Linformer and Poolingformer compress this memory. Linformer utilizes linear projections to reduce the size of keys and values, while Poolingformer combines sliding window attention with a compressed memory module using operations like max pooling.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.3 Query Prototyping and Memory Compression"}, "hypothetical_questions": ["Why is reducing the size of the key-value memory beneficial?", "What are the trade-offs associated with using linear projections or pooling operations for memory compression?", "How does the assumption of a fixed input sequence length limit the application of methods like Linformer?"]}
a5914a2d-0e87-4132-a797-81240da858ae	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.018945593,-0.008872562,0.006923185,0.080356024,-0.06225418,0.083425015,-0.00763867,0.034867365,-0.014520114,-0.033389706,0.028085873,0.02361558,0.010111885,0.024720252,-0.010466969,-0.018836996,0.02557664,0.048931424,-0.022794817,-0.03984769,0.01377021,0.029612528,0.017066598,-0.032055303,0.011083407,0.049770966,0.035647012,-0.0020921703,-0.026262416,0.006613371,0.044024374,-0.04194816,0.018076135,0.011250268,0.020492597,-0.01657417,0.012863952,-0.06055092,-0.00829818,-0.018590908,-0.058808375,0.071397915,-0.04375392,-0.014806988,0.005508419,-0.07402162,-0.073207326,0.009090871,0.00050230586,-0.023748504,-0.0072757485,0.01992182,-0.01094509,-0.048826803,-0.026027141,-0.0017929365,0.035649106,-0.056602396,0.0012825893,-0.08878196,-0.025727611,0.026518758,0.018912282,0.011931838,0.032462385,-0.024366042,0.045735896,0.02441348,0.0418782,0.11828166,-0.04134581,0.003434021,0.006252957,-0.060368296,0.102707066,0.008350398,-0.02028625,-0.031268045,-0.065443404,0.013757378,-0.018764261,0.074422866,-0.010458389,-0.005686814,0.084599055,-0.03577437,-0.01159046,0.0469678,0.056556985,-0.062576115,-0.028801877,-0.018617913,-0.010502803,0.05151733,-0.031125616,-0.03932663,-0.020812904,-0.043204382,0.008287182,-0.05467913,0.03941276,0.010078114,0.09855258,0.09556563,0.03703484,-0.018413704,-0.020114584,-0.012040503,0.013417257,0.025369648,0.010974139,0.003955939,-0.0538139,-0.007991219,-0.03357042,0.024301765,-0.03868421,0.017822623,-0.0130431205,-0.0122404825,0.012799953,-0.021021618,0.021042246,0.04095104,0.04380304,0.046921078,0.04636464,-0.018045265,-0.008881005,0.0050972924,0.02776204,0.00012911229,-0.025732573,0.033430446,-0.0010241857,0.016014619,0.078531094,-0.0015550074,-0.01483005,0.023178559,0.01681674,-0.009977506,-0.00309971,0.04414669,-0.0021150166,0.016460368,-0.030432628,0.036907084,-0.003710769,0.028566167,-0.010005338,-0.008948032,0.02282179,-0.0049977624,-0.026502417,-0.023242038,-0.014767347,-0.048396025,-0.019743707,-0.06116318,-0.053165525,-0.06830803,0.011326256,0.037100248,0.022466533,0.09296535,0.023246061,-0.026192317,0.017598132,0.014192738,-0.018116314,0.018606529,0.015852932,0.013910749,-0.0026445272,0.0050598807,0.032009825,-0.05620263,0.0050825034,-0.017064944,-0.029866457,0.06485234,0.011000228,-0.0053840624,-0.05090688,0.019156208,0.032751124,0.028243672,-0.032725584,-0.04633199,0.0020232028,-0.03655627,0.022185244,0.032611877,0.018574871,0.07043632,0.03502367,-0.071743496,0.027864136,-0.06268159,-0.004947517,-0.0030743305,-0.041139275,-0.008303176,0.010182768,-0.03571534,0.01615173,-0.0051818546,-0.010775282,0.033226274,-0.007536533,0.025125379,-0.037709415,0.03216959,0.0003463333,0.019987362,-0.026245017,-0.005457657,0.05943431,-0.015253455,-0.013805766,-0.007099985,-0.025461938,0.07394577,-0.017195951,0.01871381,-0.0037612978,0.009679493,-0.023199767,-0.017824322,0.030880433,0.05659278,-0.074666455,0.04151418,-0.052217994,0.002825512,0.01093929,-0.009160875,-0.022208652,0.019947985,0.018391998,0.04111976,0.0065402836,0.08338408,-0.02091264,0.03488676,0.023369202,0.048880614,0.03688363,0.024723522,-0.043550536,0.053438183,0.028851803,0.0024542958,0.017977891,-0.06196123,0.015947169,-0.0077043716,0.05019598,-0.03766418,0.06968492,-0.0181937,0.016441157,0.027471716,-0.0015044082,-0.02100376,0.0056231394,0.002155287,-0.06733525,0.0007650823,0.054764103,0.0817577,0.017551314,-0.0118040955,-0.035560105,0.014320795,-0.003116003,0.0010268511,0.014306582,-0.008656392,-0.020402426,0.017626856,0.012187979,-0.018343104,0.011436647,-0.008439647,0.048232395,0.01811161,-0.032792445,-0.0018945374,-0.013010502,-0.01618775,0.013492631,-0.030546268,-0.04001331,-0.00012213357,0.01617152,-0.059776146,0.021919841,0.028866598,0.07338137,0.0014800165,-0.011188714,0.056804538,0.0054855593,0.035431936,-0.042891536,0.02519742,0.05039767,-0.022432016,-0.008279893,-0.038854405,-0.03987296,0.0075629097,0.010372337,0.102756254,0.060981996,0.008928543,-0.017351655,-0.030713653,0.012286051,0.041563034,0.0051253974,0.037526157,0.03691001,-0.020531202,0.013170284,-0.0466636,-0.10399205,0.052040037,-0.030058265,0.035991702,0.05270062,-0.016642753,-0.00639598,0.018065587,-0.03249501,0.018813072,0.0108069135,0.014406988,-0.024748256,-0.0439831,0.01986024,-0.0059293997,-0.05931453,0.034882206,0.014808048,-0.009429954,-0.02145433,0.02193724,0.03501682,0.0006400322,-0.014810862,-0.0032492068,0.03761357,-0.020390216,-0.037884124,0.021222936,-0.006631311,-0.014611515,0.065214545,-0.09637234,0.065994196,-0.06082863,0.016652027,-0.042459536,-0.070198506,0.00026097515,0.07630861,-0.029210804,-0.022582904,-0.032009747,0.074243896,-0.049753178,-0.04697747,0.083185576,0.035565324,0.0714466,-0.036507763,-0.026584566,-0.028398477,-0.0013840613,-0.016236775,-0.040387068,-0.008657432,0.024977198,-0.019807488,0.090149894,-0.034545563,0.051338293,-0.04492152,0.009919609,-0.007892955,0.0047412002,-0.037971955,0.02903291,0.036556896,0.047709458,0.039076384,0.0052373535,0.0025939092,0.027004806,-0.04543173,0.03304608,0.023812233,0.014372673,-0.059162427,0.04809488,0.05789945,0.026000524,0.020896783,-0.039016142,-0.029516857,-0.016437618,0.06871026,-0.022197377,-0.006516983,-0.014849968,-0.04665475,-0.030185742,-0.01258816,0.059512753,0.02319563,0.05047538,0.014303959,-0.011222798,-0.03982305,0.043546576,-0.013292716,-0.049474854,-0.009825884,0.00899847,-0.035488144,0.03377536,0.0027081105,-0.011507422,-0.054443832,0.030543035,-0.050150122,-0.009387343,0.07431389,-0.060618654,0.025740966,-0.04711403,0.021918876,-0.07541533,0.051439956,-0.036328956,0.0010569934,-0.0144112455,0.034712423,0.0122437,0.022139432,-0.01775695,0.0768994,0.011630237,0.0029189105,-0.015454957,-0.065823644,0.021857215,0.0284768,0.04652481,0.022403415,-0.011214494,-0.022369616,-0.036554057,0.0020611521,-0.024892328,-0.02200579,-0.001394981,0.016618878,0.012722842,-0.027766634,0.056713916,-0.014312596,-0.038524892,0.004917192,0.0696178,0.020383993,0.014597599,-0.030718649,-0.04771739,0.0072001284,-0.023149656,0.050528806,0.039533485,-0.012940847,-0.01574414,-0.036816746,0.03894974,-0.03755976,0.06370513,-0.014125976,0.003558481,-0.020046527,-0.052001342,-0.00012255179,0.005768422,0.055733673,-0.01097084,-0.0383663,0.009887482,0.021157196,0.041128997,0.0035373445,0.05961887,-0.02408077,0.009218057,-0.01891499,0.048419744,0.038338605,-0.0343115,0.03115529,0.0015686181,-0.07586053,-0.0011603419,-0.013673238,0.047691323,-0.021282153,-0.010509779,0.004576673,-0.003215626,0.029432645,0.027096074,0.034531586,0.0056824354,0.0075525464,0.02392229,0.04312723,-0.08965448,-0.037962977,-0.058412716,-0.02443869,0.011553805,-0.026141955,-0.089196526,0.0071710073,-0.054096103,-0.0081870705,0.005842833,-0.023243286,-0.04350641,0.03997671,-0.011816654,0.05479892,-0.012225553,-0.0046969596,0.016597431,0.017062563,0.116772115,0.05241962,0.02135654,-0.023406608,0.0060953586,-0.017630925,0.009023086,0.06319956,0.03495967,0.032660007,0.041306887,-0.029776184,0.0063171284,-0.04588007,-0.019338936,-0.05318041,-0.0008503632,-0.035022013,0.021627361,-0.0006340016,0.015311592,0.009600061,0.011214947,0.019168671,0.013736332,0.01469479,0.03492143,0.008450078,0.035765603,0.039136805,-0.01490534,0.020771809,0.06871802,0.060929988,0.024087958,0.015801793,0.01946326,-0.021131644,0.008576347,0.004012557,-0.0016849582,-0.028753536,-0.040536765,-0.006698869,-0.008236932,-0.01623184,-0.03862531,0.03436846,0.061170813,0.032214638,-0.044782635,-0.02000411,-0.009126955,-0.00024062896,-0.038028795,-0.012161417,0.025375936,-0.037135284,0.03206351,-0.015436529,0.07507379,-0.08526129,-0.047402415,-0.021020109,0.00017132198,-0.013617176,-0.0032723958,-0.022666693,-0.008406086,-0.005250083,-0.0061769355,0.030987859,0.055825643,-0.03335433,0.034509055,-0.03372864,0.027949957,-0.017924756,-0.03690845,-0.020889705,-0.011918744,-0.0011421082,-0.023125684,0.0473439,-0.042598274,-0.0014754229,0.042287827,-0.017003413,-0.050835293,-0.050213493,0.0291176,0.02954642,-0.06219746,-0.009832824,-0.00084312866,0.026201919,-0.013772564,-0.021625115,-0.046792805,-0.002068704,-0.0029935588,0.023920806,0.040249333,0.019005673,-0.038523413,-0.015143653,-0.006346037,-0.033790007,0.030099424,0.027202033,0.03487169,0.07978681,0.05581226,-0.035310388,-0.03593069,-0.01941685,-0.064456604,0.020825008,-0.018265449,0.04653288,-0.0071247257,-0.008174731,0.013257948,-0.014120123,0.0012930392,-0.007093264,0.026662521,-0.026491169,-0.06491579,-0.00905857,0.013971349,-0.01042834,-0.02419453,-0.023574876,0.0579435,0.034415696,-0.045809004,-0.009368002,-0.078056194,-0.08452816,-0.01093737,-0.03279651,-0.020937746,0.064182185,-0.02096509,0.0040854695,0.04820827,-0.0047687856,-0.03130568,-0.02912392,0.010240859,0.016017504,0.04867932,0.019183977,-0.038271148,-0.10006609,-0.010306937,-0.0068982355,-0.05281121,0.02755042,-9.204972e-05,-0.009620695,-0.0026128122,0.007461143,-0.01654898,-0.004981357,-0.05159096,0.023174068,0.03112954,-0.0022014827,-0.0016559677,-0.06328127,-0.02993351,-0.040479008,-0.025429608,0.0042889966,0.076135926,0.04154636,-0.009069305,-0.023780094,-0.021207975,-0.0048142634,-0.006227976,-0.041991882,0.030197239,-0.038206406,0.022126403,-0.049656857,-0.0025972936,0.057881813,-0.06918083,0.0048686243,-0.008078914,0.031900734,-0.017171813,0.027559007,0.0056712944,-0.018476548,-0.039364997,0.015851514,0.037761908,0.008118996,0.0015494801,0.023795541,-0.049289815,-0.030022278,-0.030480446,-0.031247798,0.04029663,0.046870966,0.020418007,-0.043325976,0.035274193,-0.062994026,0.03241658,0.026465988,0.01858643,-0.026800908,0.01585719]	Keywords: self-attention, low-rank approximation, parameterization, inductive bias, long-range interactions, local dependencies\nKey Objects: self-attention matrix, low-rank approximation, parameterization, D_k\nRefers to Images: None\nHypothetical Questions:\n- Why is the low-rank property of the self-attention matrix advantageous?\n- How does limiting the dimension of D_k help prevent overfitting?\n- What are the differences between low-rank parameterization and low-rank approximation in the context of self-attention?\n---\nSummary:\nEmpirical and theoretical analyses suggest that the self-attention matrix, represented as A  R T  T, frequently exhibits a low-rank property, leading to two potential strategies: explicit modeling through parameterization or replacement with a low-rank approximation.\nOriginal Text:\n### 4.4 Low-rank Self-Attention  \nSome empirical and theoretical analyses [45, 142] report the self-attention matrix A  R T  T is often low-rank 8 . The implications of this property are twofold: (1) The low-rank property could be explicitly modeled with parameterization; (2) The self-attention matrix could be replaced by a low-rank approximation.  \n4.4.1 Low-rank Parameterization. The fact that the rank of the attention matrix is less than sequence length implies that, for scenarios where the inputs are typically short, setting D$\\_{k}$ &gt; T would be more than an over-parameterization and lead to overfitting. It is thus reasonable to limit the dimension of D$\\_{k}$ to explicitly model the low-rank property as an inductive bias. Guo et al. [45] decompose self-attention matrix into a low-rank attention module with small D$\\_{k}$ that captures long-range non-local interactions, and a band attention module that captures local dependencies.\nContextualized Text:\nResearch indicates that the self-attention matrix (A  R T  T) often has a low rank. This characteristic provides opportunities to either explicitly model this property through parameterization or to replace the self-attention matrix with a lower-rank approximation to enhance efficiency and prevent overfitting. Guo et al. demonstrated this by decomposing the attention matrix into modules that handle both long-range dependencies and local relationships.	{"tags": ["attention", "transformers", "optimization", "NLP", "deep-learning"], "doc_id": "a5914a2d-0e87-4132-a797-81240da858ae", "summary": "Empirical and theoretical analyses suggest that the self-attention matrix, represented as A  R T  T, frequently exhibits a low-rank property, leading to two potential strategies: explicit modeling through parameterization or replacement with a low-rank approximation.", "doc_type": "text", "entities": ["Guo et al."], "keywords": ["self-attention", "low-rank approximation", "parameterization", "inductive bias", "long-range interactions", "local dependencies"], "key_objects": ["self-attention matrix", "low-rank approximation", "parameterization", "D_k"], "contextual_text": "Research indicates that the self-attention matrix (A  R T  T) often has a low rank. This characteristic provides opportunities to either explicitly model this property through parameterization or to replace the self-attention matrix with a lower-rank approximation to enhance efficiency and prevent overfitting. Guo et al. demonstrated this by decomposing the attention matrix into modules that handle both long-range dependencies and local relationships.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.4 Low-rank Self-Attention"}, "hypothetical_questions": ["Why is the low-rank property of the self-attention matrix advantageous?", "How does limiting the dimension of D_k help prevent overfitting?", "What are the differences between low-rank parameterization and low-rank approximation in the context of self-attention?"]}
45741fab-21ae-455a-a007-2a6a53673c01	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017915877,-0.028493697,-0.0076207593,0.030977946,-0.07489512,0.08389206,-0.012241211,0.030564792,-0.038861986,-0.043965645,0.005694289,-0.012290802,0.021249166,-0.006666404,-0.020142792,-0.0441893,-0.017970793,0.06850443,-0.021798393,-2.1373673e-06,0.042159367,0.017220305,0.040531985,-0.023052573,0.03928987,0.02746881,0.037445474,0.005923737,0.029772831,0.033290073,0.029573921,-0.026821755,0.028103566,0.020971892,-0.012004349,-0.020751752,0.009909735,-0.08031936,-0.008303662,0.0022884991,-0.028945995,0.06849039,-0.040396728,0.024735902,0.041026015,-0.05591952,-0.058263,0.0056873946,0.002204403,-0.038573213,-0.023275781,0.06842815,-0.002581639,-0.036758453,-0.009718759,0.016409447,0.044421002,-0.016115518,-0.023587765,-0.09010096,-0.06658006,0.02484107,0.014635398,-0.008915728,-0.009209528,-0.02877674,-0.0049031526,0.019432126,0.07176179,0.13027863,0.0059027174,-0.012865216,0.008278905,-0.044691756,0.10106079,0.0045592543,0.016875722,0.003789788,-0.04865627,-0.01622413,-0.011714982,0.050109923,-0.02561655,0.01722747,0.07042677,-0.022792326,-0.025322609,0.0074798306,0.052123662,-0.05884989,0.0002382981,0.0025751535,0.0032715385,0.04772529,-0.025626844,-0.025643399,-0.0233549,-0.031818908,-0.012196704,-0.052169673,0.06374675,-0.008147141,0.074395284,0.107268184,0.04073527,-0.0067081847,-0.0050415723,-0.026274309,0.009922599,-0.019582989,-0.0025969318,0.016979083,-0.076917686,0.032544818,-0.026752206,0.04506646,-0.050934438,0.0040536453,-0.01906504,-0.004138734,0.0407865,-0.042405337,0.017310943,0.02902817,0.03392798,0.04661805,0.024279395,-0.009140215,0.0024770205,0.008704338,0.015203799,-0.03295064,-0.05714819,0.016968217,0.021713674,0.020231463,0.078574136,-0.048799817,-0.030970575,0.004473739,0.014878404,-0.02074513,-0.015479293,0.07126319,0.022530867,0.0139348935,-0.059149742,0.0065213996,0.0044311257,0.012247609,0.024927003,-0.0005611594,0.03497641,0.009809546,-0.045984834,-0.032028705,-0.06592995,-0.039527155,0.01001141,-0.05446258,-0.018722264,-0.023664312,0.002564083,0.028519088,0.024456058,0.0930328,-0.00023803946,0.007972223,-0.0004759618,0.005183652,-0.00074296235,-0.017190441,0.05360697,-0.025275035,0.012510869,0.00084646017,0.044147678,-0.06104918,0.014516866,-0.013614723,-0.008614001,0.07078262,0.01891215,-0.0073661944,-0.043972556,0.047886226,-0.015584269,0.04363874,-0.025270255,-0.040989026,0.009404135,-0.053216588,-0.0014438292,0.028068855,0.028468613,0.043567013,0.038350247,-0.053230606,0.024919895,-0.014334272,-0.010875441,0.020635838,-0.009750472,0.020894447,0.03195889,-0.02231182,0.034838974,-0.003859904,-0.026760524,0.06731229,-0.007884706,0.0032098817,-0.020609995,0.06385259,0.007845093,0.07262015,-0.03642842,-0.025983876,0.042213764,-0.01954756,-0.033848133,-0.02551072,-0.029438388,0.09355589,-0.017956268,0.02067016,0.009275807,-0.0009312298,0.011901072,8.463485e-05,0.035151664,0.048536874,-0.051984955,0.0053578503,-0.060674135,-0.011820846,0.03129418,-0.0035824373,-0.02149345,0.011739566,0.023919867,0.07188837,0.03774995,0.07214276,-0.021574331,0.04889484,0.04117687,0.05980468,0.023930613,-0.007715739,-0.037202485,0.049378585,0.04243799,-0.01713459,0.03946112,-0.04659589,0.01597829,-0.011051639,0.06296078,-0.0034565073,0.060318734,-0.021116793,-0.0038639659,0.011890356,0.012965703,0.0013639707,-0.010253865,-0.0070730676,-0.03691923,-0.013005099,0.051987715,0.07940585,0.009112473,0.022624524,-0.03967267,-0.0033754858,0.05276369,0.022375358,0.025875263,0.020587265,-0.0012647496,-0.008849617,0.007823977,-0.01231711,0.020804109,-0.0042691333,0.04813309,-0.023292484,-0.029170534,-0.050752494,-0.021657154,0.018614823,-0.00055175007,-0.059338942,-0.042369757,0.002137034,-0.0016896246,-0.052076723,-0.01683903,0.04675223,0.09201438,-0.0053659095,0.0043825535,0.071293764,-0.0010023746,-0.0061287703,-0.055689864,0.05505942,0.047604527,-0.00785098,-0.0630057,-0.0024720794,-0.015421311,0.0033072159,0.0111706555,0.07889013,0.07328182,-0.0015387004,-0.013922173,-0.036383998,0.046521485,0.0509242,-0.0033420927,0.05047056,0.03781322,-0.02191609,0.04551533,-0.02862796,-0.07811054,0.056296997,-0.010781908,0.052715216,0.06985529,-0.006584587,0.003388671,-0.006530141,-0.01425098,0.026180893,0.003434356,-0.010379732,-0.011976803,-0.04496921,0.062299885,-0.02129724,-0.060214527,0.026543824,0.031844564,0.012744275,-0.04146167,0.022019863,0.002386142,0.048642196,-0.0140257515,-0.0006313329,0.0061013657,0.025064714,-0.03840558,0.014040072,-0.0339985,-0.010775395,0.047256317,-0.07867488,0.051109016,-0.07191973,0.03874978,-0.03302327,-0.047391843,-0.003289562,0.06498705,-0.013815817,-0.013950537,-0.02174087,0.08371196,-0.017234763,-0.027990147,0.086777784,0.0286394,0.028154153,-0.000279442,0.0016205317,-0.0075824833,0.0071401596,-0.018549588,-0.058378477,0.017374055,-0.0065992745,-0.023918446,0.04708451,-0.055351768,0.05530614,-0.044684175,0.026502833,0.00091269426,0.017112726,-0.050070513,0.030266501,-0.003372051,0.056870922,0.020962762,0.011173616,-0.00058983825,0.031633127,-0.06502272,0.004252672,0.0363279,-0.011373852,-0.086530656,0.010585216,0.06648421,0.027338285,0.04228845,-0.016952427,-0.026521005,0.013318292,0.07521535,-0.0010575713,-0.002142439,-0.017575042,-0.060252532,0.00059636915,-0.002943658,0.046517733,0.059039243,0.016618405,0.02820925,-0.0073414207,-0.028136078,0.05046925,0.0049685524,-0.06373928,-0.006323845,0.0046203644,-0.04509208,0.0019180451,-0.006697401,-0.009583793,-0.054367714,0.05134774,-0.039384566,0.024410954,0.09490861,-0.05228701,0.030243475,-0.061278734,0.042459782,-0.049079996,0.040564965,-0.02789066,0.004821128,-0.018917497,0.043040786,0.010386957,0.028754191,0.008226776,0.07162621,-0.027993912,0.020846665,0.010525185,-0.033623993,0.0048412583,-0.0014845178,0.029383281,0.011458916,0.021084305,-0.0013092117,-0.037988614,-0.005699588,-0.024015812,-0.044941,-0.04828543,-0.016582841,0.027939096,0.0123909805,0.03726631,-0.011879194,-0.026229668,-0.012069816,0.04307432,0.019454408,0.011896739,-0.008278117,-0.060664643,-0.005523644,-0.030064614,0.032095335,0.05215666,0.016930185,-0.0031818566,-0.04500848,0.007986622,-0.026020464,0.06416427,-0.009325167,0.03591092,-0.043903437,-0.04382536,0.019025017,-0.0060770605,0.07657064,0.032405846,-0.03349662,0.017193295,0.04263073,0.0217738,0.003782198,0.060579944,-0.0077023674,0.0013786063,-0.019981053,0.044132065,0.041766334,-0.046552245,0.051897984,0.013104485,-0.054785572,0.014415703,-0.0037763226,0.04681639,-0.011289593,0.015037842,0.0026810681,-0.014944542,0.04739077,0.030243156,0.018887304,0.04713558,-0.013712205,-0.0060791923,0.0366559,-0.06585422,-0.03710959,-0.057521865,-0.014586937,0.011757359,-0.04194172,-0.06274082,0.0013755619,-0.04506535,-0.013344575,-0.00081258494,-0.037387352,-0.008837518,0.05062001,0.012943551,0.07754065,-0.015961172,0.005241237,-8.234195e-05,-0.009414378,0.083588295,0.09256215,-0.004184502,0.006997578,-0.029222589,-0.060439028,0.014214016,0.05357257,0.0205584,0.011274686,0.03911167,-0.0054768277,-0.023477796,-0.033071987,-0.019437464,-0.04312182,-0.01849337,-0.021443555,0.013916842,0.009444447,0.038718898,0.022268375,0.009169921,0.008375131,0.018247254,0.027871951,0.030090937,-0.010542988,0.00771643,0.016653074,0.007978726,0.023743758,0.06491658,0.072409995,0.006837041,0.018521069,0.07364717,-0.007078113,0.0061459146,-0.013224302,0.0076267608,-0.052732196,-0.06625338,-0.022396915,0.0019131958,-0.014573746,-0.03078313,0.048226368,0.024722325,0.04603683,-0.06305361,-0.018233495,0.03230889,-0.019164544,-0.014723448,-0.011374385,0.013051594,-0.030754264,0.032410085,-0.05490271,0.06865109,-0.0811904,-0.023829691,-0.0029300344,-0.048389114,-0.02881641,0.019674454,-0.033288963,-0.0052522565,-0.0080571985,-0.052259292,0.027365824,0.057318047,-0.02224929,0.038014345,-0.06690166,-0.006278902,0.0063042687,-0.04543552,-0.023025913,-0.0047186133,0.011352564,-0.013419418,0.042896196,-0.02116118,0.0075220647,-0.01851602,0.0042113047,-0.059160117,-0.05260751,0.039187092,-0.0042683594,-0.043246306,-0.00871937,-0.016272014,0.0063490537,-0.017048936,-0.017220045,-0.06041503,-0.030861553,-0.037943125,-0.008079192,0.02265856,-0.0034163028,-0.027622515,-0.033160083,-0.0032732547,-0.047825225,0.043105666,0.039779026,-0.0001328628,0.06084799,0.021606965,-0.061783105,-0.04936411,-0.0161864,-0.038377095,0.010539606,0.0010613155,0.04634764,-0.02234247,-0.016636532,0.037250526,-0.009390681,-0.01224729,-0.0055191927,0.040461987,-0.036193244,-0.042512175,-0.038278624,-0.0006493577,-0.023329003,-0.01985183,-0.02006142,0.0484636,0.035302296,0.0007763152,-0.02209317,-0.06194758,-0.06316837,-0.03401974,-0.013800529,-0.008980487,0.042046018,-0.022341497,0.021789277,0.070434034,0.013244851,-0.021806665,-0.019484038,0.015584146,0.017395018,0.048779335,-0.0172638,-0.026195848,-0.052394707,0.012903069,-0.005301391,-0.01942317,0.027019426,0.018433068,-0.017535126,0.0006705863,0.04783552,0.013933618,0.0120806815,-0.04965521,-0.0012279182,0.047204494,-0.007842796,0.004576863,-0.05743658,-0.0154537195,-0.018964862,0.014633571,-0.025055734,0.05638228,0.045063324,0.008009095,-0.014204969,-0.008788781,-0.012248388,0.03940312,0.0017841163,0.049093097,-0.030016333,0.01859132,-0.025671545,0.03454127,0.08069306,-0.04172304,-0.010701167,-0.0015249853,-0.0009095127,-0.0066362824,0.0014659928,-0.012786259,-0.0265975,0.0036748168,0.033344027,0.040147964,0.008719854,0.0033282076,-0.0053023617,-0.020164184,-0.018276485,-0.04066317,-0.031409778,0.027307577,0.05968268,0.025692463,-0.058198325,0.05374148,-0.017640263,0.018648332,-0.018012136,0.032359574,-0.038738728,0.010805412]	Keywords: low-rank approximation, self-attention, kernel matrices, random feature maps, Performer\nKey Objects: Attention Matrix, Kernel Matrices, Random Feature Maps\nRefers to Images: None\nHypothetical Questions:\n- How does the low-rank property of the attention matrix relate to the computational complexity of self-attention?\n- What is the connection between low-rank matrix approximation in self-attention and the approximation of kernel matrices?\n- Can you explain how Performer utilizes random feature maps to approximate Gaussian kernels in the context of self-attention?\n---\nSummary:\nThe low-rank property of the attention matrix suggests that low-rank matrix approximations can be employed to reduce the computational complexity of self-attention, drawing inspiration from kernel matrix approximation techniques.\nOriginal Text:\n4.4.2 Low-rank Approximation. Another implication of the low-rank property of the attention matrix is that one can use a low-rank matrix approximation to reduce the complexity of selfattention. A closely related methodology is the low-rank approximation of kernel matrices. We believe some existing works are inspired by kernel approximation.  \nSome of the aforementioned linearized attention methods in Sec. 4.2 are inspired from kernel approximation with random feature maps. For example, Performer [18] follows the Random Fourier feature map originally proposed to approximate Gaussian kernels. The method first decomposes the attention distribution matrix A into C$\\_{0}$G$\\_{CK}$ K where G is a Gaussian kernel matrix and the random feature map is used to approximate G .\nContextualized Text:\nRecognizing that the attention matrix often possesses a low-rank property, researchers have explored using low-rank matrix approximations as a means to reduce the complexity of self-attention. This approach is closely related to the low-rank approximation of kernel matrices, with methods like Performer drawing inspiration from random feature maps initially developed to approximate Gaussian kernels. Performer decomposes the attention distribution matrix by using a Gaussian kernel matrix and random feature map to approximate it.	{"tags": ["self-attention", "approximation", "complexity", "kernels"], "doc_id": "45741fab-21ae-455a-a007-2a6a53673c01", "summary": "The low-rank property of the attention matrix suggests that low-rank matrix approximations can be employed to reduce the computational complexity of self-attention, drawing inspiration from kernel matrix approximation techniques.", "doc_type": "text", "entities": ["Performer"], "keywords": ["low-rank approximation", "self-attention", "kernel matrices", "random feature maps", "Performer"], "key_objects": ["Attention Matrix", "Kernel Matrices", "Random Feature Maps"], "contextual_text": "Recognizing that the attention matrix often possesses a low-rank property, researchers have explored using low-rank matrix approximations as a means to reduce the complexity of self-attention. This approach is closely related to the low-rank approximation of kernel matrices, with methods like Performer drawing inspiration from random feature maps initially developed to approximate Gaussian kernels. Performer decomposes the attention distribution matrix by using a Gaussian kernel matrix and random feature map to approximate it.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.4 Low-rank Self-Attention"}, "hypothetical_questions": ["How does the low-rank property of the attention matrix relate to the computational complexity of self-attention?", "What is the connection between low-rank matrix approximation in self-attention and the approximation of kernel matrices?", "Can you explain how Performer utilizes random feature maps to approximate Gaussian kernels in the context of self-attention?"]}
536f117d-9ed2-490d-98dc-3b01f2d9f064	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0027224682,0.007268302,0.022837482,0.043408293,-0.03394747,0.07778309,0.012935308,0.04107473,0.02475173,0.006058764,-0.020288907,0.0003836829,0.024669433,0.047310896,-0.010046354,0.034929335,0.012527114,0.06765403,0.02327783,-0.056656346,0.021808462,0.00898088,0.011741051,0.015577823,0.03076544,0.03946699,0.036477443,0.02882792,0.020434063,-0.015210167,0.034031887,-0.10252241,0.044492386,-0.01968635,0.031302188,-0.026979102,-0.010393728,-0.08161214,-0.019842638,-0.018818405,-0.019025693,0.060600728,-0.010115476,0.01409067,0.005743627,-0.0335238,-0.034408152,-0.048024613,0.027938154,-0.023192396,0.016948309,0.05391764,-0.005621106,-0.020519514,-0.010318943,0.016739901,0.007975997,-0.0420246,0.034148272,-0.07719074,-0.02276777,0.021590952,0.002560075,0.024676574,0.025261737,-0.010586788,0.0018803927,-0.0003057728,0.03073493,0.10678148,0.00043746206,0.027696002,0.001999242,-0.06221939,0.08216651,0.052954223,0.024161818,0.010749359,-0.06263484,0.00064936216,0.03440408,0.039216004,-0.020113943,0.0073920535,0.062995695,-0.008350448,-0.011063945,0.003992162,0.040215965,-0.07314441,-0.025523419,-0.018158965,0.012493037,0.031951997,-0.035865586,-0.05057628,0.008204985,-0.055998243,0.01938078,-0.06186262,0.028105777,0.02208687,0.07455467,0.115025185,0.020262485,-0.038480364,0.014452448,-0.03760463,0.014691194,0.0046969764,-0.018052813,0.04926431,-0.07960587,-0.0070831743,-0.00878716,0.014116118,-0.038880274,-0.007259741,-0.026396168,-0.012556542,0.05231526,-0.024875453,-0.01692732,0.032769926,0.06428078,0.06859049,0.035605747,-0.028947143,-0.008440484,0.0024671205,0.04980542,0.018916542,-0.029552143,0.050390724,0.0058305175,-0.009137638,0.08156229,-0.05916083,-0.02969077,0.007271097,-0.029628001,0.007589655,-0.02707414,0.031563397,0.018657628,-0.0053845765,-0.046707205,0.01175136,0.0077809263,0.07780178,0.039058693,-0.007114728,0.08367643,0.013429081,-0.025650982,-0.049752876,-0.0017939452,-0.03835349,0.0012055441,-0.006092636,-0.040252104,-0.020006388,0.061315536,0.035648573,0.01948555,0.102417156,0.01942493,-0.016407734,0.03406547,-0.023538897,-0.030971926,-0.000119826465,0.015444896,3.7342397e-05,-0.0049553267,-0.046708994,0.032987323,-0.060942415,-0.034926303,0.051918935,0.023780815,0.06554811,0.0052239713,-0.023196867,-0.03689441,0.044152647,-0.007989214,0.03983624,-0.024429264,-0.036153086,0.02679354,-0.04940837,0.016764598,0.01616198,0.01935703,0.096201174,0.034822434,-0.07673701,-0.0068807737,-0.008708186,0.03397568,-0.01333594,-0.022948591,-0.0146457665,0.019768313,0.010878766,0.0030030068,-0.038983867,0.019623997,0.05334475,0.03717986,0.04564417,-0.026189428,0.06934812,0.029346151,0.021521112,-0.006512435,-0.02610232,0.026455766,-0.023378603,-0.019073285,-0.023315623,-0.01779229,0.059733607,0.012612343,-0.021033304,0.019013852,-0.009709927,0.010782052,-0.016566241,0.027608477,0.0256289,-0.06854024,0.039310627,-0.059800893,-0.00083982805,0.03710522,0.012767426,-0.045876868,0.047331695,0.038547985,0.068294816,-0.009861278,0.07099357,0.027787544,0.0072881347,0.035705693,0.048220366,0.05613604,0.009417243,-0.022774711,0.011037775,0.037076056,-0.017365627,0.026367957,-0.034603257,0.028963294,0.005392312,0.046321355,-0.02865203,0.015990227,-0.05192989,-0.0093080485,0.031428766,-0.0379281,-0.0032744547,-0.016301623,-0.002331992,-0.02208639,0.011474456,0.021459175,0.050676763,0.009941846,-0.022438385,-0.03979011,0.009901172,0.028854275,0.006269835,0.038018934,-0.025161624,-0.023729151,0.019854214,0.0013608569,-0.022667162,-0.009242268,0.008110326,0.07559462,-0.01132015,-0.0040122834,0.019937433,-0.0066769966,-0.0068126046,-0.015051682,0.003707474,-0.012637137,0.021310773,0.016718915,-0.038450144,0.02963376,0.046636883,0.10303683,0.021462746,0.021637445,0.033340808,0.02048812,0.0019055139,-0.015752058,0.04069985,0.040206287,-0.007137608,-0.03644199,-0.0027830203,-0.0038782998,0.028847327,0.015575174,0.088536136,0.08306769,-0.027514214,-0.037000746,0.015627446,0.030966552,-0.0190811,0.009624008,0.048751812,0.027481366,-0.031325642,0.034241598,-0.030883847,-0.049746472,0.055847015,-0.026910422,0.063982934,0.03377299,-0.005902744,-0.034576748,-0.001068437,-0.025891023,0.009128853,0.011136512,0.003993715,-0.029410446,-0.050124247,-0.0035241626,0.003015767,-0.024896976,0.015756631,0.017418189,-0.023862302,0.0040745903,0.015767034,-0.0103600975,0.048691094,-0.0064138896,0.025884591,0.04464961,-0.04362661,-0.0020794552,0.006245103,-0.019513005,0.008505537,0.04224959,-0.092700146,0.006129593,-0.062262405,0.031751927,-0.0036074712,-0.04369976,0.045970723,0.07338446,-0.004858763,0.009942076,-0.024689201,0.08869349,-0.023487955,-0.03582326,0.053425204,0.014097576,0.0034027097,-0.022945592,-0.011935396,-0.023317384,0.012882181,-0.05813965,-0.01979306,0.016355373,0.03049569,0.014059274,0.08945657,-0.01457483,0.006612653,-0.032822836,0.011294291,0.02342417,-0.007954309,-0.05513815,0.010759314,0.029640503,0.10437334,0.050104212,-0.0055103106,0.04342222,0.06130536,-0.047349185,0.0018893242,0.034963883,0.008219831,-0.068577126,0.0029133176,0.046558872,0.030511472,-0.0148059605,-0.008033649,-0.059157535,-0.039753255,0.05605777,-0.008744304,0.028016856,-0.012333093,-0.03771316,-0.018940937,-0.0024561042,0.082796626,0.055735476,0.027357206,0.04175847,-0.04036688,-0.035638407,0.037539456,-0.0031711787,-0.03577184,0.01973383,0.008345158,-0.027348163,0.01404091,0.003139562,-0.006179712,-0.0792381,0.06481513,-0.024977438,0.025634388,0.08239209,-0.059361193,0.037083283,-0.029434128,0.018764438,-0.049246766,0.033309154,-0.011588552,-0.0002774257,-0.0027135762,0.015594515,-0.015968988,0.01613926,-0.0025243938,0.08872227,0.017225632,0.04239143,-0.016889457,-0.03405169,0.0056279018,-0.010637996,0.0074408543,-0.01909034,-0.0054767565,-0.04477745,-0.012278878,-0.0028366332,-0.04267937,-0.013414397,-0.022666356,0.0055089127,-0.0027430623,0.0031938786,0.05142958,-0.017372672,0.0052527,0.031950813,0.041360546,0.028251546,0.04047486,-0.036212012,-0.031118425,0.0018137173,0.016695533,0.035805654,0.08018148,-0.0058744913,-0.022474354,-0.011936667,0.059709366,-0.049050782,0.072836876,-0.021650264,0.015483987,0.0046373783,-0.086959444,-0.013240247,-0.008216693,0.018731622,-0.027963182,-0.010654842,0.004325427,0.04187853,0.024084426,0.017580237,0.009571126,-0.013028199,0.034644783,-0.019990752,0.056307722,0.019753205,-0.0527858,0.027123908,-0.025671948,-0.06465509,0.048686918,-0.02470734,0.013289577,0.021399023,-0.016720496,0.015923657,-0.013964946,0.020237394,0.029328292,0.04234085,0.013887151,0.032685034,0.026891915,0.044318948,-0.05785932,-0.027752068,-0.029347084,0.020433048,0.042339105,-0.053028464,-0.06283483,0.018145563,-0.0784306,-0.007264271,0.00010378755,-0.025020057,-0.04805473,0.034509007,-0.0323322,0.056221917,-0.039671734,0.026253182,0.021822453,0.007955908,0.057011485,0.038646754,0.03596065,-0.003990066,-0.012771295,-0.006696197,0.046878822,0.052047372,0.02229689,0.024046943,0.03350731,-0.0009463284,0.016200889,-0.05502443,-0.059778456,-0.030505955,-0.018516948,-0.027735176,-0.034763575,0.003917764,0.0068651494,-0.005618,-6.212453e-05,-0.018700354,0.022669142,0.006234393,0.055505496,0.037962172,0.024985721,0.011021835,0.019736907,0.02168043,0.043295532,0.07624075,0.028280593,-0.008735173,0.05979965,-0.0019725778,-0.012098765,0.0075258347,0.009364642,-0.02998604,-0.025401043,-0.00884214,-0.0558997,0.01975055,-0.029555535,0.04322733,0.0005892625,0.029296093,-0.035752576,-0.031655215,-0.014539748,-0.044325747,-0.028085243,-0.0010024698,0.030323226,-0.03314238,0.047312725,-0.016408948,0.066526376,-0.05691472,-0.0578962,-0.047168437,0.014589724,-0.03702989,-0.01892818,-0.013291894,-0.050687175,-0.0076937703,-0.032357983,0.016309494,0.016996782,-0.06422224,-0.011008258,-0.028958304,0.004071271,0.0069956696,-0.004749171,-0.051325522,-0.0020156812,-0.011684247,-0.023745116,0.0834278,-0.029206092,0.009414114,0.046067756,0.047149703,-0.046057194,-0.04195408,0.06992321,0.018780954,-0.023163993,0.015183761,0.0017116951,0.032847095,-0.031606752,0.008145797,-0.046703752,-0.0006587966,0.024252232,-0.022040736,0.009054634,-0.024655456,-0.070208095,-0.06402382,-0.006213941,-0.0071579735,0.02646717,0.06461467,0.01379369,0.056626804,0.056396805,-0.06651341,-0.03675254,-0.033301763,-0.023164744,0.0069425222,-0.03555613,0.039691105,0.0296417,0.002522657,-0.014395748,-0.012753539,0.003668833,-0.027478239,0.019903714,-0.065648764,-0.062072493,-0.010237544,0.04742487,0.023239817,0.05044745,-0.003980071,0.058927476,0.040288053,-0.027803877,0.011914191,-0.0559452,-0.050346933,-0.0052231266,0.012405294,-0.030587675,-0.0013776415,-0.029362602,0.03043829,0.020242965,-0.022330679,-0.04466343,-0.046568133,0.030622618,-0.00039870216,0.033063065,0.024648087,-0.0050567086,-0.08112318,0.047159817,-0.021637928,-0.045687225,-0.0049525965,-0.006447624,-0.011710474,-0.025425445,-0.008169179,-0.010385555,0.046536867,-0.020033961,0.03710041,0.022505524,-0.010530361,0.003313306,-0.065708,0.013866501,-0.027878955,-0.04157323,-0.02182906,0.07182466,0.042176303,0.0034904983,-0.049057323,0.030064076,0.018817615,-0.0016968434,-0.014848053,0.018179545,-0.02768421,0.030542029,-0.03384991,0.010235375,0.07730286,-0.008296391,-0.033628732,0.00860344,-0.0046665217,-0.02242791,0.01611932,-0.0026193908,-0.055937603,-0.043034963,0.050282866,0.027673045,0.020255834,0.027147664,0.022502804,-0.058673948,-0.05056611,-0.03600862,-0.03836508,0.04663108,0.041743636,-0.025279162,-0.07732891,0.039762422,-0.030544244,0.038479976,0.012227109,-0.013179445,-0.006916916,-0.012406841]	Keywords: Nystrom method, landmark nodes, approximation, singular matrices, attention computation\nKey Objects: landmark nodes, attention matrix, approximation\nRefers to Images: None\nHypothetical Questions:\n- How does the Nystrom method reduce the computational complexity of self-attention?\n- Why is it important to handle cases where the matrix inverse does not exist in Nystrom-based methods?\n- What are the advantages and disadvantages of using the Moore-Penrose pseudoinverse compared to a regular matrix inverse?\n---\nSummary:\nNystrom-based methods approximate the attention matrix by selecting landmark nodes and using them to compute a simplified attention computation, which is particularly useful when dealing with singular matrices.\nOriginal Text:\nAnother line of work follow the idea of Nystrom method. These Nystrom-based methods [16, 152] first select m landmark nodes from the T inputs with down-sampling methods (e.g., strided average pooling). Let Q , K be the selected landmark queries and keys, then the follow approximation is used in the attention computation  \n$$\\hat { A } =softmax \\left ( \\mathcal { O } _ { K } ^ { T } \\right ) \\left ( \\softmax \\left ( \\mathcal { O } _ { K } ^ { T } \\right ) \\right ) ^ { - 1 } \\text {softmax} \\left ( \\mathcal { O } _ { K } ^ { T } \\right ).$$  \nNote that M - 1 = (softmax ( Q $^{T}$) ) K in Eq. (19) does not always exist. To mitigate this issue, CSALR [16] adds an identity matrix to M to make sure that the inverse always exists. Nystromformer [152] uses the Moore-Penrose pseudoinverse of M instead of the inverse so that the approximation can be made for cases where M is singular.\nContextualized Text:\nTo exploit the low-rank property of self-attention matrices, some methods employ the Nystrom method. These Nystrom-based methods first select a subset of inputs, termed 'landmark nodes,' and use them to approximate the full attention computation. This allows for efficient attention computation, especially when the original matrix might be singular, a situation addressed by methods like CSALR and Nystromformer which utilize techniques like adding an identity matrix or the Moore-Penrose pseudoinverse.	{"tags": ["self-attention", "approximation", "methodology", "optimization"], "doc_id": "536f117d-9ed2-490d-98dc-3b01f2d9f064", "summary": "Nystrom-based methods approximate the attention matrix by selecting landmark nodes and using them to compute a simplified attention computation, which is particularly useful when dealing with singular matrices.", "doc_type": "text", "entities": ["CSALR", "Nystromformer"], "keywords": ["Nystrom method", "landmark nodes", "approximation", "singular matrices", "attention computation"], "key_objects": ["landmark nodes", "attention matrix", "approximation"], "contextual_text": "To exploit the low-rank property of self-attention matrices, some methods employ the Nystrom method. These Nystrom-based methods first select a subset of inputs, termed 'landmark nodes,' and use them to approximate the full attention computation. This allows for efficient attention computation, especially when the original matrix might be singular, a situation addressed by methods like CSALR and Nystromformer which utilize techniques like adding an identity matrix or the Moore-Penrose pseudoinverse.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.4 Low-rank Self-Attention"}, "hypothetical_questions": ["How does the Nystrom method reduce the computational complexity of self-attention?", "Why is it important to handle cases where the matrix inverse does not exist in Nystrom-based methods?", "What are the advantages and disadvantages of using the Moore-Penrose pseudoinverse compared to a regular matrix inverse?"]}
77cfa270-30e3-4e0d-a0b4-239b75a9ef76	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.041348934,0.022217866,-0.008974055,0.04986397,-0.07188819,0.044735473,-0.042966407,0.017350607,0.09045934,-0.012041274,-0.025367903,-0.003255267,0.011173984,-0.0030228049,-0.01904539,0.01902511,0.03440525,0.018533112,0.013822077,-0.058359105,0.025692258,-0.012971583,0.011919631,0.0044338354,0.0047489433,0.030814873,0.020269774,-0.045592055,-0.037042275,0.017177762,-0.0029479545,-0.025924718,0.015305959,-0.001573416,0.01270818,-0.041457005,0.014052109,-0.050000023,-0.010378803,-0.023771517,-0.004067278,0.023126297,-0.018162135,0.009768905,-0.024657873,-0.011324043,-0.04991164,-0.0010363124,0.030992405,-0.009880977,-0.034190428,0.012150077,-0.022469653,0.01959172,-0.017763024,-0.043806706,0.005041209,-0.018769577,-0.0039518448,-0.090000086,-0.010095756,0.0058532064,0.007494777,-0.007199311,0.016954727,-0.021602027,0.041716684,0.015613796,0.05082235,0.15450506,-0.01546849,-0.014837488,0.037768494,-0.010461109,0.11541109,0.030252527,-0.04875448,-0.026847262,-0.035999577,0.0059734355,0.01567166,0.05354207,-0.07968419,-0.030808352,0.09407719,0.010427708,-0.088541284,-0.019842941,0.048751432,-0.07845978,0.03817809,-0.047912046,-0.0090864925,0.09403227,-0.04068896,-0.080941014,-0.05863132,-0.0097398795,-0.03149469,-0.03532955,0.029443111,0.0014485511,0.09159253,0.08132647,0.020113286,0.032337043,0.0038871937,-0.039898288,0.008443668,-0.04442657,-0.012954105,0.02282121,0.004247157,-0.0008569715,0.047815215,0.04130133,-0.0019061168,-0.003302742,-0.022558771,-0.025679352,0.049824525,0.023797981,-0.034983523,0.049906757,0.05201505,0.03451549,-0.01737252,-0.028771859,-0.052143287,-0.035176035,0.0768893,0.03336455,-0.02521899,0.034595992,-0.054316916,0.041973088,0.04832911,0.0047689644,0.0018554404,0.008156814,-0.015357256,-0.056742333,-0.054095384,0.0023458854,-0.03219083,0.06653351,-0.05290615,-0.008394402,-0.03603006,0.05457515,0.057028085,-0.027233496,-0.025019698,-0.06685482,-0.049310304,-0.006664979,-0.027858786,-0.031096842,0.016666101,-0.077508405,-0.07226828,-0.04469679,0.03633483,0.040200382,0.028272271,0.08549695,-0.02664298,0.00949103,-0.011451344,0.013770808,-0.046534676,-0.040393066,0.017020833,-0.0659406,0.048756808,-0.004367021,-0.0022586954,-0.018723672,0.0023419438,0.0040869717,-0.038301304,0.03533706,-0.0356878,0.02153545,-0.013098478,0.02641178,0.008632096,0.05894782,0.03668829,-0.012012057,-0.010491454,-0.0020824976,0.001534761,-0.02163129,0.025092844,0.01528062,0.05023829,-0.0590874,-0.025362786,-0.0289652,-0.0050269137,0.02824372,-0.022295896,0.052202288,-0.0051254295,-0.0077462406,0.009880643,0.030839004,-0.007684106,0.013206472,0.00025319256,-0.030507691,-0.032574106,-0.0222376,0.040413897,0.010199871,-0.06525108,0.031335372,-0.03295032,-0.026993955,-0.025137395,0.04825177,0.009068709,0.019209757,-0.030244866,0.00018756282,-0.045304623,0.024677057,-0.0005605213,-0.0041768295,0.016744966,0.049501292,-0.029883908,-0.0022497522,0.0002959632,0.018299354,-0.008019069,0.014877342,-0.020392278,0.04694825,-0.012522861,0.030488908,0.055111613,0.08160262,0.005213123,-0.015101998,0.029923296,0.059479382,0.011343082,-0.014173977,0.0065547163,0.029959459,0.043694306,-0.027289886,-0.041083634,-0.0074885297,-0.04706752,0.01256583,0.0176647,-0.054961585,-0.00017747167,-0.046594165,-0.011792881,0.066128075,0.019200414,-0.026677221,0.02350491,0.033832815,-0.03239764,-0.032958347,0.0067600906,-0.027863417,0.021251217,-0.03154174,0.009482094,0.022870405,-0.009967714,0.031971984,0.0046234163,-0.02347016,-0.004446939,-0.069007054,0.0018779652,0.02449268,-0.0007031917,0.006252463,0.049789682,-0.04474051,0.0005039486,-0.029807672,0.03966656,-0.030930819,0.018137438,0.0005526406,-0.01612266,0.014722461,-0.0052888314,-0.021051127,0.023905117,0.038505994,0.1083408,-0.045326304,0.020559415,0.010800995,-0.0046664895,0.027253056,-0.017225998,0.042249575,0.025028909,0.024402726,0.024682522,-0.045600023,-0.020457914,0.034219123,0.06984821,0.09247412,0.05005363,-0.00073092885,0.03911053,0.027505314,0.044599827,-0.011379779,0.0065550203,0.031012867,0.048642054,-0.02271088,0.03306684,-0.032316092,-0.050930876,0.08246,-0.051100414,0.019176643,0.07315495,-0.010631544,-0.017652035,0.011256473,0.0009310447,0.009479476,-0.003671417,-0.0022104203,-0.0354013,-0.03444021,0.029484687,0.03424796,-0.05246208,0.04922237,0.00080760365,0.013411148,0.01907602,-0.0051256325,-0.028630357,-0.038489196,-0.015691178,-0.0009748867,0.036325376,0.0035325726,-0.01276879,0.03741943,-0.019957077,-0.04674518,0.037903126,-0.06620298,0.035408057,-0.028866338,0.03125865,0.008311199,-0.09391295,0.0029525803,-0.041434802,-0.017072955,-0.010455225,-0.069269076,0.06847375,-0.021194061,-0.031671047,0.038038425,0.017383462,0.011475666,-0.021526653,-0.006102903,-0.014790914,0.005938553,-0.018026078,-0.031413656,0.019966638,0.037103772,-0.03690906,0.08203962,0.0036839088,0.05776907,-0.06738431,0.029062841,-0.0050711236,0.009773669,-0.041334342,-0.040470626,0.017530983,0.04748003,0.04099945,-0.0063513727,-0.004074455,0.03308906,-0.058772497,0.034382664,0.006912591,-0.023287453,-0.044740733,0.044183433,0.026400417,0.004310328,0.008013078,-0.051273536,0.03843023,-0.062168863,0.0019289908,-0.028355505,-0.040458445,0.03219104,-0.016317679,-0.004301531,0.0031487541,0.0019876014,0.017780175,0.0029520898,-0.02394743,0.011381547,0.0102004055,0.031879194,0.02502037,-0.044524234,0.0066174893,0.0037662515,-5.934763e-05,0.07073411,0.0010666116,0.025503125,-0.019889014,0.09008,-0.014361002,-0.01373708,0.03058556,-0.027263481,-0.015692139,-0.029444665,0.044163045,-0.02713703,0.033689238,0.022437984,-0.023095194,-0.05066426,0.06381734,-0.024792925,0.07322109,-0.018690271,0.042118303,-0.0036165642,-0.07302507,-0.044354238,-0.07079972,-0.04349257,0.03829469,-0.006313227,-0.0036064128,-0.007419483,0.0075653447,-0.025363715,-0.035332493,-0.05730108,-0.020874312,0.047169972,-0.061316956,0.0100543415,0.009183527,-0.007339778,-0.039150853,-0.020126527,-0.013667257,0.033141334,0.08114682,0.035517443,-0.061506033,-0.037874404,0.029542075,0.044165947,0.023714582,-0.026243685,-0.043468855,-0.011465246,-0.039558373,0.068208545,-0.0013082799,-0.0069382335,-0.006774636,0.019336505,0.023934042,-0.07366982,-0.02691436,-0.017091708,-0.025052361,0.0038691163,0.053301405,-0.028651228,0.050324965,0.017214732,-0.01812518,0.049134113,0.010795239,-0.035707362,-0.047757834,0.024148697,0.06451907,-0.053512577,0.009608915,0.036325198,-0.019143745,0.03745778,-0.016687118,-0.027881885,-0.019469691,0.047938697,0.030858396,0.0337669,0.033210807,0.0023696718,-0.015211676,0.002958547,-0.056537353,0.009633565,0.013480231,-0.04079879,0.030155685,-0.039174028,-0.05289898,-0.00786992,-0.028303469,-0.10709162,-0.020248232,-0.032313656,0.0017746368,-0.027831726,-0.0040658,-0.009186995,0.0010560207,0.024437204,0.050361138,-0.034188796,0.02187574,-0.029063359,-0.0016976503,0.015029673,0.051755216,0.027377442,0.02633695,0.016311826,0.023548752,0.06672433,0.033500105,0.030944696,0.029078059,0.019232417,-0.029388376,0.007044439,0.025290368,-0.034192767,0.011429499,-0.020958629,-0.02928902,-0.06658087,-0.03857566,0.03664685,-0.033387087,0.035760757,-0.013760763,-0.036051452,0.024658304,0.07449509,0.047557388,0.016923906,0.013731783,-0.023842722,0.031027358,0.08041392,0.038701702,0.01069472,0.0470344,0.04935892,-0.008566594,-0.027018324,0.016204007,-0.021706143,-0.05801793,0.008564262,-0.06324321,-0.054807223,0.02492443,0.0021285824,-0.0024804233,0.023408223,0.020801065,0.024806732,-0.09900918,0.011379153,0.005309401,-0.018564891,0.019177921,0.032730248,-0.0269612,-0.0011641618,-0.029702013,0.06615777,-0.035849057,-0.008454816,-0.020138595,0.05481717,0.012632547,-0.03954991,-0.025406137,0.03224227,-0.014056254,-0.022607047,0.06933538,-0.02162903,0.018120583,0.000109492066,0.012472821,0.011277866,0.0005385211,-0.0024296264,-0.021340927,0.03325088,-0.019372374,-0.033051934,0.03227498,0.030754974,0.012737253,0.009959898,-0.028151166,-0.049744222,-0.05147584,0.023895456,0.03536717,0.018054731,-0.004438224,0.011037971,0.029928146,-0.0818973,-0.029817058,-0.011497511,-0.009522546,-0.023813205,0.037257235,0.09417292,-0.02551993,-0.04516834,-0.011996758,-0.00998457,-0.010468939,0.029132392,0.025444977,0.018473223,0.0422886,0.034953672,-0.007940632,-0.09896886,-0.01242352,-0.049624473,0.039242323,-0.04681666,0.07938749,-0.050276544,-0.013065322,0.0029752336,-0.023344532,0.044998575,0.013575647,-0.011350734,0.06587769,0.021487646,-0.025407279,0.023844827,0.030868951,0.0040112073,-0.020688236,0.04511694,0.035161484,-0.02390318,-0.050379302,-0.052507013,0.008562265,0.027401254,-0.03071154,-0.0083943205,0.02317609,0.018664256,0.023406627,0.09838465,0.0041897087,-0.043400172,-0.05180441,-0.0033413984,0.06012289,0.032734964,0.009616945,0.000585689,-0.06451698,0.04075168,-0.000952406,-0.004709081,0.04027379,-0.013466045,-0.03874526,0.016276063,0.020774655,-0.03542293,0.038513348,-0.05455896,-0.001192294,-0.025771335,-0.016022194,0.036273584,-0.0475903,0.036242742,0.0077141738,0.012838837,0.024425955,0.05180233,0.042564113,0.009770658,-0.0035321421,-0.046891037,0.04610194,0.032417838,0.004564724,-0.013424923,-0.005771569,0.034156017,0.013546387,0.008009499,-0.0072133467,0.0328437,-0.002766438,0.0012870248,0.015619217,0.006968314,-0.0074759875,0.0023328704,-0.02390687,-0.030390685,-0.011666207,-0.0411465,0.024371631,-0.008396154,-0.02410738,-0.027516311,0.0004147999,-0.029334841,0.026896589,0.004888664,0.02461218,-0.010200131,-0.036037702,-0.024618318,0.0070108334,-0.015554763,-0.024380734,0.032521963,0.004180552,0.00872127]	Image title: Attention Mechanism Visualization\nTags: attention, mechanism, visualization, neural-networks, transformation\nKey objects: Pre-Attention, Circular Transformation, Final Attention, Grid Structure\n---\nSummary:\nThis diagram illustrates the evolution of attention weights within a system, showing the difference between pre-attention, an intermediate step represented by a circular transformation, and final attention. The visualization uses a grid-like structure to represent the attention scores at each stage.\nFull description:\nThe diagram displays three stages of an attention mechanism. The first stage, labeled 'pre-attention,' shows a grid with varying levels of intensity, representing initial attention scores.  An intermediate step is shown through a circular transformation. Finally, the last stage, 'final attention', displays a modified grid, showing how the attention weights have changed after the transformation. The color intensity in each grid represents the strength of attention at that particular point.\nText found in image:\n- pre-attention\n- Circular Transformation\n- final attention	{"tags": ["attention", "mechanism", "visualization", "neural-networks", "transformation"], "title": "Attention Mechanism Visualization", "doc_id": "77cfa270-30e3-4e0d-a0b4-239b75a9ef76", "source": "./images/a-survey-to-transformers/image_10.png", "summary": "This diagram illustrates the evolution of attention weights within a system, showing the difference between pre-attention, an intermediate step represented by a circular transformation, and final attention. The visualization uses a grid-like structure to represent the attention scores at each stage.", "doc_type": "image", "key_objects": ["Pre-Attention", "Circular Transformation", "Final Attention", "Grid Structure"], "parent_doc_id": "d07379ac-a0d7-4bd3-8e7f-cfa96bf31763", "text_in_image": ["pre-attention", "Circular Transformation", "final attention"], "contextual_description": "The diagram displays three stages of an attention mechanism. The first stage, labeled 'pre-attention,' shows a grid with varying levels of intensity, representing initial attention scores.  An intermediate step is shown through a circular transformation. Finally, the last stage, 'final attention', displays a modified grid, showing how the attention weights have changed after the transformation. The color intensity in each grid represents the strength of attention at that particular point."}
fe8c140f-4f59-4dbe-8bbd-4fbf74502256	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.005151167,0.023489581,-0.030159453,0.03328632,-0.012084963,0.055265315,-0.02284485,0.09078591,0.061157294,-0.024957772,-0.011656756,-0.029023284,0.052066773,-0.04204179,-0.047529932,0.018284893,-0.006073909,0.066721484,-0.008930216,-0.033938088,0.0339897,0.041333754,0.062273324,0.007968258,-0.0047014514,0.031103035,0.00897462,-0.033530682,0.007963024,0.015178937,0.014058385,-0.01642501,0.040091462,8.1882994e-05,0.007787701,-0.015983757,0.01753295,-0.08526623,0.032956716,-0.033480335,0.024561366,0.013068957,-0.036712524,-0.025032695,-0.0154729225,-0.030796286,-0.08484007,-0.017206816,0.038778905,-0.021733666,-0.023016289,0.002426533,-0.0089476025,-0.030501919,0.01081856,0.01153588,-0.0061537055,-0.015732296,-0.047714137,-0.0858389,-0.015778722,0.008794381,-0.041656703,0.00245525,0.012169044,-0.026731595,0.023480698,-0.0053070257,0.055555385,0.1089826,-0.037088618,-0.02210204,0.011465117,-0.027616989,0.12565216,0.038206887,-0.0428146,0.00069396925,-0.053727817,-0.0046125697,-0.013626073,0.054397527,-0.048231423,-0.05804059,0.08851259,0.0034499227,-0.07111138,-0.03465581,0.06444678,-0.052722357,-0.013248512,-0.032977983,-0.012727404,0.060019776,-0.016575987,-0.06294488,-0.044959944,0.018274918,0.027122734,-0.096182585,0.012228433,0.013792213,0.075789355,0.08849112,0.023648323,-0.057189345,-0.010147685,-0.027849732,0.02113237,0.027636284,0.011794413,0.020221578,-0.08156579,-0.008882363,-0.027264379,0.030713001,-0.032955386,0.0069150617,-0.0067164428,-0.04069398,0.0070353323,-0.026338223,-0.02397285,0.0044233715,0.06458769,0.045857787,0.03489646,-0.007858015,-0.030308945,0.010536062,0.06990288,-0.023528714,-0.046226438,0.040562924,-0.020890426,0.014805685,0.07529485,-0.013957396,0.004172622,0.055630114,0.011960876,-0.06993455,-0.074021086,0.035444602,-0.03802659,0.071418524,-0.07629845,0.022011496,-0.054283597,0.043354742,0.015844727,-0.0051256954,-0.058206446,-0.0028895983,-0.0051847743,0.010355596,-0.012280252,-0.10995194,-0.02312888,-0.0405907,-0.06330447,-0.03745475,-0.01177024,0.048349943,0.007069021,0.080223,-0.0001985727,-0.023787705,0.040006116,0.05771975,0.0010899759,-0.00021998321,-0.0009293873,-0.020627603,0.0064065414,0.0019583039,0.016789073,-0.006092384,0.022537714,-0.0027394386,0.004641833,0.02581497,0.031223126,0.07110534,-0.00053126557,0.03355238,0.015290353,0.03901531,-0.03129148,-0.03322824,0.011239511,-0.029748749,-0.013840227,-0.02820787,-0.024115609,0.031407554,0.044208333,-0.08485444,-0.020130225,-0.033608872,-0.0668871,-0.00044042847,-0.017951762,0.0075356173,-0.008162901,0.0064218096,-0.000374542,0.0011530749,-0.0017459962,0.054044563,-0.0103952885,-0.040236358,-0.047181956,0.022354724,0.035645336,0.030017428,-0.0039875074,0.102124386,-0.0056459396,-0.031928603,-0.0429741,-0.012636764,0.0010944996,0.009751539,-0.021498647,-0.0045898682,-0.07329495,0.023841972,-0.020077761,-0.0371385,0.020073099,0.05361819,-0.02672258,0.0039852154,-0.046176936,0.022980196,0.0067597297,0.010575011,-0.0018050544,0.020478949,0.038984556,0.016110025,-0.026144989,0.10243188,0.005638072,-0.00425337,0.0018511696,0.05497334,-0.014467904,0.041962612,0.0151946,0.055256028,-0.0020964008,0.015798738,-0.011140353,-0.05889575,0.0126303565,0.030191297,0.042241868,0.022118868,-0.0027244217,-0.0056978054,-0.029704683,0.006370241,0.0059068766,-0.008871088,0.033223204,0.039100587,-0.013237761,-0.024818802,0.014723727,0.04653597,0.030934146,0.0061783916,-0.02407492,0.052060325,0.04787023,-0.0071856924,0.05617344,-0.019655246,-0.00030434117,-0.060082622,0.055407025,-0.010987752,-0.01743173,0.01703942,0.030968763,-0.039275303,-0.011259147,-0.0043184264,-0.023485482,-0.02037723,-0.037119165,-0.016477466,0.016404038,0.023121303,-0.013265418,-0.026360989,-0.042513154,0.04871711,0.11233322,-0.013340277,0.017435333,0.011388074,0.013852155,-0.004079622,-0.06049414,0.035197213,0.0546293,-0.041710667,0.054668445,-0.020245427,0.027728247,0.012065517,0.052007273,0.058164343,0.046612322,-0.048681807,0.018938407,0.018027667,0.009255198,-0.013798499,0.021137657,-0.015667494,-0.0021051187,0.031880595,-0.009774066,-0.02759679,-0.058096588,0.025063349,-0.077100106,0.045193017,0.059449323,0.0026089498,-0.023687556,0.024180885,0.020993682,0.011666566,0.026994362,0.009110284,-0.0062936028,-0.028798627,0.05187105,-0.0041760956,0.016860299,0.049692996,0.0075635556,0.0065880455,0.07114821,-0.007761177,-0.017261352,0.013284772,0.02687591,-0.03774971,-0.0011783566,0.017022407,0.023159012,0.024919119,-0.018774489,-0.001993278,0.04949977,-0.03337934,0.01838586,-0.047570765,0.029257452,0.0069707646,-0.068792425,0.04233303,0.016286684,-0.029351652,0.010622862,-0.045971796,0.003938683,-0.040403333,-0.0071974476,0.032475125,0.03829646,0.012975246,-0.02121051,0.01443908,-0.012140434,-0.025907455,-0.052981112,-0.049350284,0.06577813,0.0033069544,-0.031534906,0.099571444,0.019556325,0.028541908,-0.046675734,0.019521138,0.03072014,0.0024565952,-0.011450318,0.0062609348,0.015952094,0.067030326,-0.021544173,-0.0149610825,-0.001622794,0.014930219,-0.07362938,0.040651128,0.017153632,0.010455287,-0.05609481,0.017921297,0.027647277,0.0021572923,0.040423892,-0.061135273,-0.00095525634,-0.005349806,-0.001318307,-0.0030898673,-0.012214997,0.028112134,-0.04664166,0.008789978,-0.037466496,0.028203474,0.016849069,0.0644237,0.015758164,-0.02530973,0.011830016,-0.012053216,0.017424487,-0.06340612,0.016824806,-0.05954174,-0.04318756,0.05693865,0.013768883,0.032544196,-0.04596682,0.08698068,-0.016647272,0.033958774,0.059788916,-0.01030591,0.006355831,-0.020200737,0.011014351,0.0013293194,-0.0058370987,0.021272793,-0.048702892,-0.036620993,0.03666515,0.028112382,0.04850073,-0.05084679,0.024147647,-0.014474542,-0.018107694,-0.02077199,-0.05406889,0.021678314,0.013600346,0.005048249,0.0038067107,-0.037933994,0.001456717,0.00768253,-0.018982485,-0.021812972,0.002778352,-0.0040702503,-0.03409092,0.0073391944,-0.050721545,0.026272135,0.018530916,0.033981763,-0.030688316,0.061726026,0.015751405,-0.024487903,-0.019399382,-0.03017413,0.005935323,0.03946242,0.026520088,0.03174033,-0.0060558994,-0.038553216,-0.015570681,0.03389004,-0.010239895,0.055307925,0.012177824,0.041422877,0.026692871,-0.043148514,-0.0016774173,-0.008116394,-0.00150875,-0.042017434,-0.0076720766,-0.009593013,0.056025665,0.0003751776,-0.007323888,0.026157713,0.003838731,-0.0009691106,-0.018644702,0.037940398,0.0034542682,-0.039355278,-0.011682729,0.014936915,-0.022521129,0.047578402,-0.0048678117,0.018373828,-0.022985414,0.057720445,0.0065201856,0.027752662,0.045456264,-0.014367719,0.037781138,0.024786279,-0.015832894,-0.004034924,0.033480152,-0.035306554,0.023647299,0.0019596794,-0.03923511,-0.0030128926,-0.05938813,-0.060968366,0.017612549,-0.053223953,-0.006578443,-0.034683138,-0.015212765,-0.021482155,0.01078253,0.014307484,0.052595943,-0.056241747,0.02381358,0.022330469,0.01884793,0.0687664,0.042109523,0.031102732,0.018844157,0.00988235,-0.011069231,0.045691643,0.08860927,0.020941209,-0.0044441735,0.04560665,0.025508307,0.03606581,-0.023578169,-0.03999602,-0.046718717,-0.0022696792,-0.020371543,0.0073497775,-0.042806134,0.012044,-0.034765415,-0.022624323,-0.0057622534,0.038116377,0.03168644,0.05545056,0.020585991,-0.02388671,0.053126875,0.012611149,0.06453793,0.054382704,0.0782985,0.02807842,0.036904786,0.010154046,0.027043251,0.00063885853,0.0049980697,-0.044761386,-0.03128802,-0.028886206,0.009649363,-0.0020536087,-0.024612464,-0.029163148,-9.70422e-05,0.039444655,0.036797572,-0.06078793,-0.034725066,0.0010609794,-0.038805135,-0.04901784,0.037358444,0.051961344,-0.031110717,0.057101995,-0.033930987,0.05913477,-0.06440216,0.019472232,-0.012126482,0.036098722,-0.02548844,-0.045848377,0.01578648,-0.017992476,-0.012364424,-0.07073945,0.053008806,-0.02298858,-0.028471516,-0.0045124716,0.0036807861,-0.03148082,0.0077725123,-0.023662152,-0.0028745206,0.003636325,-0.00085638446,-0.008242726,0.09071545,-0.02013481,-0.02860155,0.04263375,-0.027109414,-0.05987921,-0.004110967,0.02591671,0.052223507,-0.008393608,0.006880867,0.045375183,0.06007089,-0.04006899,-0.015312647,-0.007992115,-0.023179863,-0.03379068,0.050712377,0.0279703,-0.0409531,-0.04701205,0.0024158687,0.03471038,-0.011060442,0.018522581,0.023925353,0.087233335,0.040201526,0.03176204,-0.023150804,-0.069696866,-0.036489125,-0.068425015,0.028422283,-0.019286469,0.034671813,-0.03533977,-0.032800015,-0.019079255,0.006956218,-0.025616845,-0.014929825,0.07846897,-0.01737633,0.008587172,-0.056708083,-0.0062289867,-0.025963062,0.037950527,-0.037911683,0.08160667,0.03564065,-0.026524385,-0.013998392,-0.07502168,-0.00310196,0.028946215,-0.016970895,0.027331447,-0.005987177,-0.028454311,0.021474328,0.03799929,-0.00035134112,-0.036164235,-0.05004314,0.069896355,0.030811537,0.019622404,-0.0481012,-0.027801467,-0.0875362,0.05184686,-0.016171671,-0.0754951,0.038486023,0.015500392,-0.02612449,0.025466423,-0.0023119538,-0.010011716,-0.010612545,-0.0020553612,0.03346522,0.039418273,-0.013242284,0.0038423897,-0.04821223,-0.0059341597,-0.013557037,0.0077205445,-0.022742663,0.055050682,-0.01684033,-0.000791181,0.0054460242,0.023823954,0.031233748,0.027132798,-0.018853076,0.04591033,-0.0117530655,0.04724768,-0.022847088,0.060883768,-0.0072685326,0.01949658,-0.002777071,0.014407794,0.0066232826,0.011566826,0.029289426,-0.042845413,-0.005324032,-0.0051140254,0.012576727,-0.000774348,-0.023671439,0.024022315,-0.040958535,-0.0280948,-0.047765195,-0.0184356,0.07220148,0.026577504,0.050638802,-0.007563803,-0.04627439,0.030633215,-0.027055096,0.020907301,-0.039739702,0.009269525,0.029252062,0.0076683974]	Keywords: attention mechanism, prior, attention distribution, softmax, weighted sum\nKey Objects: Attention Distribution, Prior Attention, Attention Scores\nRefers to Images: ./images/a-survey-to-transformers/image_10.png\nHypothetical Questions:\n- What is the purpose of using a 'prior' in attention mechanisms?\n- How does 'attention with prior' differ from traditional attention mechanisms?\n- Why would one choose to supplement or substitute the distribution generated from inputs?\n---\nSummary:\nAttention mechanisms typically use distributions generated from inputs, but these distributions can also originate from other sources, termed 'prior.' This approach, called 'attention with prior,' allows for supplementation or substitution of input-generated distributions and often involves a weighted sum of prior and generated attention scores before applying softmax.\nOriginal Text:\n  \ngenerated from inputs (e.g., softmax( OK $^{T}$) in vanilla Transformer). As a generalized case, attention distribution can also come from other sources, which we refer to as prior . Prior attention distribution can be a supplement or substitute for distribution generated from inputs. We abstract this formulation of attention as attention with prior , as depicted in Fig. 9. In most cases, the fusion of two attention distribution can be done by computing a weighted sum of the scores corresponding to the prior and generated attention before applying softmax.\nContextualized Text:\nIn Transformer models, attention mechanisms often utilize attention distributions generated from inputs, such as through a softmax function. However, these distributions can also be derived from other sources, referred to as a 'prior.' This technique, known as 'attention with prior,' allows for supplementation or substitution of distributions derived directly from the inputs. A common implementation involves computing a weighted sum of the prior and generated attention scores before applying a softmax function.	{"tags": ["architecture", "NLP", "transformer", "attention"], "doc_id": "fe8c140f-4f59-4dbe-8bbd-4fbf74502256", "summary": "Attention mechanisms typically use distributions generated from inputs, but these distributions can also originate from other sources, termed 'prior.' This approach, called 'attention with prior,' allows for supplementation or substitution of input-generated distributions and often involves a weighted sum of prior and generated attention scores before applying softmax.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["attention mechanism", "prior", "attention distribution", "softmax", "weighted sum"], "key_objects": ["Attention Distribution", "Prior Attention", "Attention Scores"], "contextual_text": "In Transformer models, attention mechanisms often utilize attention distributions generated from inputs, such as through a softmax function. However, these distributions can also be derived from other sources, referred to as a 'prior.' This technique, known as 'attention with prior,' allows for supplementation or substitution of distributions derived directly from the inputs. A common implementation involves computing a weighted sum of the prior and generated attention scores before applying a softmax function.", "mentioned_images": ["./images/a-survey-to-transformers/image_10.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["What is the purpose of using a 'prior' in attention mechanisms?", "How does 'attention with prior' differ from traditional attention mechanisms?", "Why would one choose to supplement or substitute the distribution generated from inputs?"]}
adb8a305-ca80-43b6-8272-9e5fe0ed87c2	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.052447204,0.028117402,0.009582208,0.0502917,-0.07243701,0.025549168,-0.062313326,0.009360654,0.08416559,0.0048536444,-0.024399413,-0.008660343,0.019993104,-0.004014708,-0.0050204815,0.024508733,0.036402572,0.048070688,5.4378324e-05,-0.074752666,0.025090214,0.011716447,0.0033818097,0.01069811,-0.009650723,0.03564154,0.011868642,-0.013074894,-0.03405432,0.0019018669,0.033406943,-0.025445623,0.0018172023,-0.02396827,0.035119515,-0.07539923,0.040921655,-0.07133219,0.015189584,-0.03474655,0.006522878,0.019669743,-0.010899911,0.0054124044,-0.01650958,-0.020854102,-0.062387228,0.0164191,0.015665026,-0.022737518,-0.015943862,0.0038217283,0.009908387,0.019853137,-0.019294742,-0.026656235,-0.0025100168,-0.031732995,-0.01653565,-0.08176501,-0.0066625783,0.003329738,0.0039759977,0.0015816712,0.037399016,-0.014118459,0.03714317,-0.01115141,0.046200473,0.16027306,-0.019286422,0.009904044,0.038842954,-0.04791567,0.102526136,0.0048972503,-0.04891109,0.0038748228,-0.050450705,-0.008467934,0.009774833,0.07853845,-0.0628641,-0.017972961,0.082239375,-0.011775434,-0.061561476,0.0023350276,0.028618822,-0.063861266,0.028521564,-0.058908384,-0.0030186141,0.08303503,-0.03818157,-0.050659012,-0.07146148,0.012306307,-0.027014822,-0.069647856,0.026979694,0.022228487,0.077522,0.08948265,0.018547086,0.012768324,0.008946188,-0.036936197,-0.004755353,-0.030954737,-0.001844491,0.02665823,-0.03760653,-0.015945531,0.024975605,0.07138476,-0.009491922,0.0073799146,-0.0046938476,-0.006659227,0.04445851,0.0028829111,-0.044643518,0.036281023,0.05796848,0.043016363,0.005579857,-0.0049848245,-0.03036435,-0.04842479,0.084762305,0.050568964,-0.044112,0.026531892,-0.054505795,0.03489924,0.04544374,0.009594113,-0.011388642,-0.0035871454,-0.005643183,-0.06891174,-0.08798705,0.015267214,-0.013657706,0.044187244,-0.068072766,0.0053342916,-0.04120726,0.03955612,0.033351835,-0.037772536,-0.03401709,-0.05493776,-0.04786488,0.004952987,-0.023340328,-0.043341335,0.023677131,-0.077569574,-0.06344298,-0.041533038,0.04830835,0.06372437,0.040916134,0.07825185,-0.0075212764,-0.019078031,0.009803777,-0.0009024085,-0.014829827,-0.02762737,-0.0036436694,-0.028207058,0.025360873,0.0106677655,0.0080065755,-0.03512414,0.012348773,-0.024114942,-0.027417745,0.0784154,-0.023294905,0.012738133,-0.03582434,0.02472017,-0.005146101,0.090159856,0.04043384,-0.0129301855,-0.021928364,-0.0038651046,0.01882417,-0.024982348,0.024942063,0.016031867,0.046666615,-0.057129566,0.0014723405,-0.033649113,-0.025400832,0.021140855,-0.06227738,0.05789221,-0.026610203,-0.027334891,0.0058569,-0.0055774115,-0.011687783,0.019636763,-0.0073922807,-0.0073982743,-0.027198978,-0.008750015,0.033947833,0.012473524,-0.03053839,0.040952127,-0.020072341,-0.042437226,-0.020889862,0.052476343,-0.021465715,0.031003783,-0.040408477,0.0166843,-0.038962394,0.0155332545,-0.025902823,-0.002509191,0.02039041,0.06391208,-0.013760826,-0.009404707,0.0012126477,0.0018726642,0.011086629,0.028477801,-0.01896987,0.021394005,-0.0087337075,0.034404367,0.0429043,0.09206622,-0.008126153,-0.01681455,0.0077271326,0.0818737,-0.018310089,4.6082954e-05,0.021632291,0.03759739,0.029093873,-0.030944388,-0.045326278,-0.036418844,-0.009726308,0.022544326,0.02477442,-0.052012645,0.02105961,-0.04241777,-0.005201772,0.0607214,0.029294275,-0.028464565,0.03022371,0.039377198,-0.043452434,-0.031501744,0.0064506866,0.005606515,0.008365971,-0.01328081,-0.008263077,0.018521072,-0.0029055164,0.019916166,-0.024430957,-0.011990372,-0.0084102955,-0.04656279,0.01504281,0.013773949,-0.017078795,0.019907158,0.06299514,-0.03818599,0.0034487231,-0.045011062,0.028576579,-0.006613633,-0.0003436313,0.0060035586,-0.010024289,0.013888672,-0.0052077146,-0.009791365,0.03833776,0.045713037,0.083295815,-0.029291403,0.03265268,0.021926643,0.008372813,0.03171275,6.6358945e-05,0.05361891,0.023370307,0.023438089,0.026722599,-0.032812405,-0.0064903856,0.02964597,0.078415394,0.08284973,0.077573895,-0.032880068,0.013317306,0.010428591,0.04138437,0.0024648977,0.026398508,0.015222427,0.06272525,-0.007594641,0.056282375,-0.018800203,-0.030389681,0.045076076,-0.044712402,0.016064122,0.07360356,-0.012674367,-0.011472759,-0.016416494,-0.009262076,0.010411285,-0.0015646023,0.0022456364,-0.05255355,-0.011365682,0.042446353,0.021615772,-0.06504143,0.043776613,0.0135853,0.007354939,0.019750945,0.0053254873,-0.021365497,-0.042985357,0.0039587533,-0.020476803,0.016911613,-0.0134690525,-0.03356169,0.048063725,-0.017675389,-0.034978855,0.05247733,-0.08079157,0.038834125,-0.011053262,0.05187259,-0.0034644455,-0.07739484,0.015545991,-0.027243225,-0.0067109214,-0.013464427,-0.08781843,0.060810737,-0.030132988,-0.048168276,0.044953514,0.027059946,0.011160703,-0.029067622,0.020470718,-0.0100142285,-0.004611614,-0.026364855,-0.042560052,0.035079792,0.04179198,-0.023378478,0.10420293,-0.006711237,0.055059537,-0.06543653,0.005548236,-0.0031306595,0.008919395,-0.040560454,-0.0238423,0.0026660836,0.050082393,0.0136524765,-0.010204271,0.00038042967,0.031057855,-0.04794862,0.031333417,0.021035856,-0.032025993,-0.052263577,0.07114794,0.0048147924,0.013722456,0.009996963,-0.048752576,0.03819296,-0.055423345,0.029275129,-0.036418114,-0.032059148,0.01943129,-0.03202984,-0.030386476,-0.0037186553,0.0015633724,0.0050763288,0.009447374,-0.014888127,0.0127918385,0.004461286,0.052048475,0.018031597,-0.04963686,-0.004209054,0.012249413,-0.0069813943,0.03490815,-0.0050807144,0.04877487,-0.019183131,0.10206685,0.008692757,-0.028429883,0.0102851875,-0.037569474,-0.027582185,-0.035696246,0.033877105,-0.036624946,0.056500256,0.012739315,-0.02644003,-0.028133102,0.051605634,-0.01417581,0.06454753,-0.038257897,0.033533692,0.006204538,-0.055716272,-0.053147238,-0.063925646,-0.044033185,0.045221727,-0.011592768,-0.011025713,-0.023600988,-0.020510886,-0.04293965,-0.058601025,-0.03806093,-0.0210551,0.038233783,-0.057749722,0.025057388,0.0299387,0.00039499154,-0.018808194,-0.048870396,-0.02200089,0.033492588,0.06361734,0.036041092,-0.05727732,-0.041954193,0.025738,0.030184966,0.009428172,0.003914659,-0.019811764,0.00915974,-0.03692338,0.07306277,-0.015040929,-0.0027005277,0.00548064,0.0038919086,0.013489798,-0.06442141,-0.010997533,-0.03239547,-0.023296854,0.011249035,0.023751672,-0.00037982577,0.030994592,0.024350055,-0.014123597,0.047352184,-0.008080039,-0.03598183,-0.042597342,0.017170617,0.043739855,-0.034731485,0.0067273285,0.06651554,-0.04539867,0.034944113,-0.018056106,-0.0032860416,-0.03315702,0.053512737,0.043126587,0.042380918,0.057344474,-0.0060744192,0.004771972,-0.0019820419,-0.030056193,0.0127898175,-0.009753723,-0.05971431,-0.009079431,-0.040455036,-0.053486835,-0.016745644,-0.02237554,-0.12158737,-0.008401033,-0.039500218,0.004739779,-0.039259918,-0.010390377,-0.01584096,0.0002039114,0.0008968951,0.03376554,-0.015495071,0.015582603,-0.024550695,0.028141271,0.019798085,0.05632624,0.03329293,0.013161798,0.01990836,0.011555368,0.06906928,0.012535617,0.046993233,0.050653204,0.014808263,-0.01354123,0.0028237314,0.0055725886,-0.037530996,-0.019754266,-0.02612985,-0.0408759,-0.04837065,-0.0453547,0.028695302,-0.033577796,0.029079372,-0.008153195,-0.015161214,0.0070140143,0.07064689,0.04893365,0.010327012,0.03502649,-0.03882301,0.026071833,0.090231024,0.05868218,0.022907438,0.046899747,0.031302523,0.0012552191,-0.009094584,-0.0051308437,-0.0020542222,-0.05294082,0.007820738,-0.043000802,-0.03437468,0.040481545,-0.012362851,-0.0123297665,0.011677008,0.016028445,0.0017641372,-0.06273616,0.002151307,0.00338663,-0.015743542,0.02173632,0.040867995,-0.03940655,-0.006527474,-0.009053353,0.07166291,-0.04966933,0.009256701,0.020314997,0.04511951,0.00010870109,-0.045178372,-0.055033825,0.05281726,-0.01302115,-0.022141883,0.07666197,-0.0031341102,-0.0020724812,-0.0052048736,0.020202043,0.020868797,0.0003860674,0.0007373258,-0.013178076,0.046822805,0.0043603047,-0.026546631,0.056012142,0.008502221,0.0036042633,0.0238328,-0.045097757,-0.037956953,-0.06844208,0.01916541,0.03861659,0.0018828635,0.0038676914,0.031402163,0.012475027,-0.054198187,-0.020904493,0.012510757,0.01565293,-0.025631422,0.06425636,0.08989947,-0.018719437,-0.03556897,-0.022505201,-0.016651154,-0.018186055,0.051327735,0.034803696,0.01867595,0.05341254,0.04041301,-0.0047761155,-0.071877226,-0.020649575,-0.027161159,0.026922617,-0.025433633,0.044267524,-0.048991513,-0.023838056,-0.015682226,-0.026595816,0.03945631,0.029647287,0.034203775,0.024262704,0.013196664,-0.022381322,0.039512504,0.029234542,0.00036805242,-0.016791424,0.028884295,0.00945658,-0.022279201,-0.052242335,-0.030034684,0.008254769,0.0046314956,-0.037595697,0.002730754,0.0031537162,0.009700246,0.013685955,0.090958126,0.00870463,-0.048638098,-0.043250397,-0.032713234,0.07000507,0.03216481,0.0020198796,0.006880874,-0.020623686,0.030822363,-0.0077296724,-0.012982049,0.032398287,-0.003009212,-0.03802937,0.0061157765,0.0075547546,-0.0332608,0.048365623,-0.02570298,0.012067119,-0.022391189,-0.018711055,-0.0027291975,-0.08369355,0.01677327,-0.0030939085,0.011960669,0.029064244,0.059280686,0.046036474,-0.014883647,-0.023321059,-0.038842212,0.053733565,0.021715736,-0.028027873,-0.005355281,0.013336476,0.031636972,-0.032430008,0.005715451,-0.016202264,0.035306975,-0.012012258,0.027281998,0.02839492,-0.015491834,0.005065016,0.011914271,-0.009975109,-0.036569666,-0.036369614,-0.029534368,0.0036618833,-0.008211879,-0.05290568,-0.027760606,0.005087048,-0.046360992,0.0036735053,-0.004823555,0.018699527,-0.008691033,-0.049995143,-0.015837677,0.006051955,-0.0103472825,-0.033552498,0.037977546,-0.026319077,0.033234827]	Image title: Attention Mechanism Visualization\nTags: attention, neural-network, visualization, dataflow\nKey objects: Pre-Attention, Attention Process, Final Attention, Grid Representation\n---\nSummary:\nThis diagram illustrates a visual representation of the attention mechanism focusing on how attention weights change across different stages: pre-attention, during the attention process, and the final calculated attention. It showcases the evolution of the attention scores as the information flows through the model.\nFull description:\nThe diagram presents three stages of an attention mechanism visualized as a grid. The first stage, labeled 'pre-attention', displays an initial distribution of weights represented by blue color intensity in a grid.  The second stage, 'attention process,' shows how these weights are modified during the attention calculation. This stage depicts how blue intensity shifts and changes location on the grid.  The third stage, 'final attention,' showcases the final computed attention weights after processing, again shown as blue intensity within a grid. The arrows between stages show the data flow from pre-attention to the process and finally to the final attention stage.\nText found in image:\n- pre-attention\n- attention process\n- final attention	{"tags": ["attention", "neural-network", "visualization", "dataflow"], "title": "Attention Mechanism Visualization", "doc_id": "adb8a305-ca80-43b6-8272-9e5fe0ed87c2", "source": "./images/a-survey-to-transformers/image_10.png", "summary": "This diagram illustrates a visual representation of the attention mechanism focusing on how attention weights change across different stages: pre-attention, during the attention process, and the final calculated attention. It showcases the evolution of the attention scores as the information flows through the model.", "doc_type": "image", "key_objects": ["Pre-Attention", "Attention Process", "Final Attention", "Grid Representation"], "parent_doc_id": "fe8c140f-4f59-4dbe-8bbd-4fbf74502256", "text_in_image": ["pre-attention", "attention process", "final attention"], "contextual_description": "The diagram presents three stages of an attention mechanism visualized as a grid. The first stage, labeled 'pre-attention', displays an initial distribution of weights represented by blue color intensity in a grid.  The second stage, 'attention process,' shows how these weights are modified during the attention calculation. This stage depicts how blue intensity shifts and changes location on the grid.  The third stage, 'final attention,' showcases the final computed attention weights after processing, again shown as blue intensity within a grid. The arrows between stages show the data flow from pre-attention to the process and finally to the final attention stage."}
5d81ca15-9d7a-444a-ba72-7a26719b2923	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.009042282,-0.004029379,0.0038396954,0.02394281,-0.038850766,0.080573864,-0.0004992472,0.06907696,0.030891448,-0.028953938,-0.006548622,-0.0049415524,0.047714647,-0.0091018975,0.0074887127,-0.0007127567,-0.006454926,0.042322647,0.028981848,-0.021802913,0.048340276,0.011839109,0.027494047,-0.004321074,0.035779595,0.058290593,0.013603816,-0.006927877,-0.0021760298,0.02086565,-0.019488389,0.0013075813,0.03606561,-0.015725354,0.05640015,-0.034881335,0.04745597,-0.073326945,-0.02646175,-0.021398995,0.0022898617,0.07654953,-0.041614823,0.041436717,-0.01179223,-0.013836541,-0.081290126,-0.03698755,0.011994204,-0.02641218,0.044187676,0.028174229,-0.022272512,-0.015534842,-0.030701334,0.0018929881,0.07117093,-0.0364793,0.028130187,-0.091402195,-0.03489373,0.0055166944,0.0038577707,0.022950007,0.038177323,-0.033658285,0.00080850476,0.025858715,0.024788793,0.11435123,-0.05277665,-0.003006766,-0.039879724,-0.08239781,0.08832031,0.04627664,-0.045825176,-0.016034603,-0.060107153,-0.026982306,-0.027897308,0.06463105,0.0021850544,-0.031167608,0.06396663,-0.028661365,-0.05545083,-0.038798116,0.040956747,-0.05163447,0.006877558,-0.0074969726,-0.00817481,0.051753104,-0.035845958,-0.02965677,0.01393842,-0.013568014,0.010211431,-0.083838314,0.019805958,0.009688756,0.0836964,0.11675949,0.029915271,-0.043021858,0.022428712,-0.0036931573,0.03769082,0.028547036,-0.013333391,0.041400135,-0.09162995,-0.019123143,-0.048961878,0.004886705,-0.044517115,-0.019998629,0.005978682,-0.020796359,0.0065084253,-0.0079187425,-0.021908877,0.04268223,0.07570972,0.056150213,0.04389576,-0.0050375853,1.7242353e-05,0.017088428,0.07117872,0.025482517,-0.03379669,0.050526056,-0.01989486,0.011218286,0.08257054,-0.017353281,-0.036568534,0.037058827,-0.023344947,-0.036402453,-0.061405107,0.030499265,-0.0037762972,0.023368927,-0.069726266,0.027543906,-0.050722778,0.033111,0.036001723,-0.025185514,0.05248655,-0.02212628,0.007034142,-0.043892156,0.011621743,-0.01931607,0.03096369,0.0022683404,-0.059336815,-0.0316715,-0.023206387,0.054825332,0.02659528,0.08476558,0.011578233,-0.042773288,0.01776655,0.02872069,-0.026320474,0.008904044,0.027929828,-0.025434999,-0.004669374,-0.02699272,0.025973411,-0.049443826,-0.03222985,-0.0069544422,-0.018872755,0.06142179,0.013546786,0.050162446,-0.0504365,0.0014452608,0.017593708,0.048472103,0.0050460957,-0.027559966,0.011077603,-0.038607642,0.017761057,0.05230735,0.0126442835,0.044386603,0.024896301,-0.07323224,0.021817304,-0.054077778,-0.04766747,0.016361529,-0.034656778,0.0070228977,-0.0051746257,-0.029499108,0.03609474,-0.01038435,0.026163971,0.017478999,0.011551291,0.03405405,-0.037698735,0.02895249,0.009065101,0.0017277576,-0.021599557,0.017191065,0.06267299,-0.027520472,-0.02567036,0.032943115,0.006908689,0.07710679,-0.033703953,0.05452653,-0.031027315,-0.029934336,-0.007194186,-0.027120845,0.005493609,0.025523296,-0.0665136,0.041177202,-0.05954888,-0.024472449,-0.0075475713,0.009129382,-0.023884788,0.0052843946,0.054700226,0.028058121,-0.0078076166,0.073996276,-0.023280054,0.027049592,0.030665554,0.023050005,-0.013130172,0.058426566,-0.008477745,0.05519043,0.05559942,0.029750207,0.019774258,-0.040518783,0.03704183,0.031829167,0.037206385,-0.017582037,0.017178053,-0.012732092,0.0064413706,0.017634856,0.023663526,-0.015218985,0.028374117,0.013310052,-0.036420453,-0.018993078,0.037610624,0.062265962,0.019058885,0.009602263,-0.048799112,-0.004888002,0.04725289,-0.018627651,3.655746e-08,-0.039880104,-0.011291169,-0.043324478,0.012139998,-0.009519621,-0.0046365173,0.061411347,0.046812266,0.0124619845,-0.02130458,-0.0129435705,-0.0036497063,-0.028365202,-0.013686452,0.0033698734,-0.017369673,-0.01131701,0.031813156,-0.056789324,0.018680044,0.034027137,0.07929298,0.019122029,0.036566403,0.038224153,0.02930593,0.026436437,-0.044612877,0.04049184,0.0061959587,0.00017231793,-0.012239024,-0.013139873,0.0054826117,0.009199695,0.02550121,0.07241957,0.09999602,-0.027467282,0.011893673,-0.008384925,0.02402559,0.014596756,0.022269113,0.07375134,0.050583832,0.0026521299,0.030951018,0.014316911,-0.06186777,0.03560393,-0.035424557,0.033158313,0.048775606,0.02128503,0.0143051725,-0.009336132,0.026888018,0.017485986,0.020869877,-2.1412225e-05,-0.017052915,-0.014568601,0.058556486,-0.007130595,-0.04808664,0.0031964262,0.0091808755,-0.017693803,0.051050525,-0.0014315251,0.02516865,0.004488308,0.034607515,0.030209426,0.04661469,0.011172827,-0.05754906,0.05026336,-0.027723063,0.02227454,0.061454542,-0.04999482,0.010535116,-0.07328668,0.024598219,0.03164592,-0.05787576,0.028556399,0.058492277,-0.01661355,-0.0046018027,-0.024600096,0.04947265,-0.03788791,-0.03665063,0.023594784,0.0710984,0.027527243,-0.027815958,-0.040535554,0.004684767,-0.0047791023,-0.08389398,-0.018482681,0.022492344,0.0026064098,-0.006901311,0.08960782,-0.0059073507,0.035181634,-0.045929816,-0.005756134,0.0064598788,-0.011548048,-0.025196182,0.016152792,0.0526931,0.01701976,0.008889713,-0.027397046,0.036161803,0.0063397386,-0.055389047,-9.714419e-05,0.009266183,0.003266162,-0.056212377,0.042476032,0.003447684,0.012229128,0.0022482406,-0.047995333,-0.038096208,-0.014445523,0.035990164,-0.031041652,-0.0072435807,0.012943354,-0.051458683,-0.028354373,-0.024390334,0.04166241,0.038914308,0.061728597,0.042819783,-0.016303034,-0.06305933,0.018405,-0.011763605,-0.072700724,0.012380233,-0.0141219925,-0.013052969,0.047036856,-0.008623707,0.0309008,-0.055703484,0.097380705,-0.038961876,-0.016609682,0.038256608,-0.017793441,0.012442135,-0.00867653,-0.003175157,-0.018809717,0.037350517,-0.03296107,-0.0031511693,-0.011447855,0.021785492,0.035306297,0.015264723,-0.019562924,0.03583565,-0.0034168689,0.020447867,-0.021976542,-0.061242167,-7.496078e-05,-0.007103604,0.038281996,-0.008317927,0.014740808,-0.04405723,-0.022703005,-0.004296316,-0.022002544,0.020714825,-0.027587263,-0.014855275,-0.005422128,-0.0075516053,0.04283058,-0.009298265,0.00909864,0.004642218,0.062175345,-0.015036213,0.025819894,-0.00966938,-0.044930402,-0.016570415,-0.018662132,0.033746954,-0.009338701,-0.031153299,0.009036906,-0.03972906,0.018877398,0.02214304,0.011072483,0.0042463066,0.027089946,-0.013100007,-0.06038311,0.0062102405,-0.03140868,0.0051530893,-0.025407773,-0.039804485,0.035280675,0.05550714,0.0088901585,0.027704759,0.07070829,-0.00930329,0.007459259,-0.019439347,0.032179136,0.015323828,0.0044239275,0.014640546,-0.028446857,-0.0018963862,0.06630491,0.011080489,0.049637485,-0.011718659,0.04292173,0.06683333,0.052574504,0.08082093,0.012031548,0.03614699,0.027789187,0.029751249,0.016589459,0.03427581,-0.08337301,-0.017757803,-0.039507113,-0.01631279,-4.898467e-05,-0.09845013,-0.061603036,0.0066452804,-0.09440682,-0.01651312,0.007176778,-0.011598858,-0.04823084,0.013942425,-0.016686013,0.064185135,-0.067593515,0.009820108,0.038509604,0.022823146,0.08403867,0.044246458,-0.016721446,-0.021117518,-0.040861644,0.023913354,0.008036005,0.06193457,-0.004881292,0.02137621,0.03568498,0.0060058073,0.011188363,-0.0068076914,-0.030955484,-0.0342367,-0.019328812,-0.0012942593,0.017778337,0.0038451192,0.01767215,0.0299207,0.017857486,0.0008325502,0.029914714,0.008226833,0.05635061,0.026572142,-0.019588036,-0.005722911,0.023384005,0.03963695,0.036569804,0.06318206,0.030713078,0.020628463,0.028670056,0.0066820793,0.0063490155,0.020229133,0.009790322,-0.0042913393,-0.049412806,0.015490664,-0.00094137195,-0.008086628,-0.04621225,-0.017471012,-0.045298822,0.04246181,-0.056077134,-0.06103379,-0.013480273,-0.04232024,-0.03266246,0.03864891,0.0010898567,-0.03945615,0.042171415,-0.046242114,0.06346443,-0.07125669,0.017274521,-0.05397184,0.015937507,-0.01801211,-0.029691847,-0.0058908374,-0.025785133,-0.00075569254,-0.046276215,0.01450005,-0.004370546,-0.04395616,-0.008247976,-0.032398257,0.038584735,0.034629732,-0.02408833,0.008661538,-0.0086718425,0.0107980585,-0.039369244,0.088134445,-0.031698033,-0.042538658,0.049763534,0.0059836362,-0.044603273,-0.04152124,0.0297806,0.0054193516,-0.023134097,0.011494261,-0.017891902,0.030343149,-0.046736825,-0.008922041,-0.0398453,-0.03437826,-0.0216421,0.06359463,0.07113627,-0.0065548313,-0.064688966,-0.018047402,-0.02582444,-0.003313132,0.054265752,0.02937527,0.042115353,0.06893386,0.069735885,-0.045759864,-0.06127817,-0.04402554,-0.025328863,-0.00049428,-0.0063690194,0.038280986,-0.017090328,-0.03404668,-0.015040818,0.012557496,-0.021902472,0.015410219,0.059574224,-0.044437584,-0.014842987,-0.03342034,-0.0024298152,-0.012318946,0.03149557,0.011042532,0.088959746,0.034603424,-0.029728895,-0.03940828,-0.06841561,0.0059728585,0.019985482,0.023285499,-0.0033098403,-0.01624175,0.011329126,0.002089898,0.026544463,-0.0024513064,-0.044255283,-0.047539975,0.029919092,0.03791976,0.033892192,-0.012752574,-0.0104212,-0.07332524,0.013827776,-0.028016329,-0.021709267,-0.0045649507,0.00053712307,-0.009025166,-0.042866442,-0.01724328,-0.007854588,-0.0060315533,0.004103115,0.024864458,-0.00053134817,-0.042278487,-0.016210025,-0.053717095,-0.007008487,-0.03003095,-0.0069455793,-0.04748785,0.07602512,0.015888557,0.013656086,-0.019334916,-0.021500427,-0.013254083,-0.0021391208,-0.0533675,0.020593414,-0.016006485,0.0111465305,-0.032933943,0.013417673,0.036447376,-0.009408848,-0.02761334,0.0636516,0.059451018,0.02015016,0.0013092258,-0.012599028,-0.03905811,-0.013308573,-0.008064838,0.03805919,-0.016279168,-0.009176437,-0.0033424576,-0.06292788,-0.045712154,-0.06581083,-0.0038458852,0.058043938,0.023060506,0.03922065,-0.040897623,0.00047724776,-0.036226686,0.0004740121,-0.012989138,0.04812051,-0.007930733,0.03755108]	Keywords: locality, Gaussian distribution, prior attention, attention mechanism\nKey Objects: Gaussian distribution, positions, attention scores\nRefers to Images: None\nHypothetical Questions:\n- How does encoding locality as a prior attention improve the attention mechanism?\n- Why is a Gaussian distribution a suitable choice for representing locality?\n- What is the purpose of predicting a central position for each query?\n---\nSummary:\nSome data types, like text, often demonstrate a preference for locality, which can be incorporated into the attention mechanism by using a prior attention based on a Gaussian distribution over positions.\nOriginal Text:\n4.5.1 Prior that Models locality. Some types of data (e.g., text) can exhibit a strong preference for the locality. This property can be explicitly encoded as a prior attention. A simple method would be to use a Gaussian distribution over positions. Specifically, one could multiply the generated attention distribution with some Gaussian density and then normalize, which is equivalent to adding to the generated attention scores A a bias term G , where higher G$\\_{ij}$ indicates a higher prior probability that the i -th input attend to the j -th input.  \nYang et al. [156] proposes to first predict a central position p$\\_{i}$ for each q$\\_{i}$ using a simple feedforward network. The Gaussian bias is then defined to be  \n$$G _ { i j } = - \\frac { ( j - p _ { i } ) ^ { 2 } } { 2 \\sigma _ { 2 } }$$  \nwhere  denotes standard deviation for the Gaussian and can be determined as a hyperparameter or predicted from inputs.\nContextualized Text:\nTo account for the tendency of some data types, such as text, to exhibit locality, a 'prior attention' can be implemented. One method uses a Gaussian distribution over positions to encode this preference. Specifically, the generated attention distribution can be multiplied with a Gaussian density and normalized. This is equivalent to adding a bias term, G, to the generated attention scores, where higher G values signify a greater prior probability that the i-th input attends to the j-th input.	{"tags": ["attention", "locality", "transformer", "NLP"], "doc_id": "5d81ca15-9d7a-444a-ba72-7a26719b2923", "summary": "Some data types, like text, often demonstrate a preference for locality, which can be incorporated into the attention mechanism by using a prior attention based on a Gaussian distribution over positions.", "doc_type": "text", "entities": ["Yang et al.", "Gaussian Transformer"], "keywords": ["locality", "Gaussian distribution", "prior attention", "attention mechanism"], "key_objects": ["Gaussian distribution", "positions", "attention scores"], "contextual_text": "To account for the tendency of some data types, such as text, to exhibit locality, a 'prior attention' can be implemented. One method uses a Gaussian distribution over positions to encode this preference. Specifically, the generated attention distribution can be multiplied with a Gaussian density and normalized. This is equivalent to adding a bias term, G, to the generated attention scores, where higher G values signify a greater prior probability that the i-th input attends to the j-th input.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["How does encoding locality as a prior attention improve the attention mechanism?", "Why is a Gaussian distribution a suitable choice for representing locality?", "What is the purpose of predicting a central position for each query?"]}
d883d9df-4240-4ee9-b563-d9c76a9c74f6	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.00029674856,-0.001230601,0.02672713,0.053902965,0.01944071,0.064442635,0.021976873,0.07371994,0.03026595,-0.014371394,-0.042028736,0.004631245,0.025583414,-0.005457234,-0.014917833,0.03854277,-0.0051919646,0.058046307,0.010648108,-0.00387898,0.029969744,0.005928933,0.05018486,-0.022216782,0.07157109,0.038490154,0.019115243,-0.0014414137,-0.012202941,0.02071318,-0.013288008,-0.030115975,0.023482323,0.011396443,0.023746341,-0.012941558,0.025685498,-0.047017314,-0.026486618,-0.020345999,-0.010849734,0.09747454,-0.07805585,0.022907602,-0.0019171415,-0.016560657,-0.12811351,-0.014899117,-0.003787394,-0.035466615,-0.015092744,0.016872404,-0.03584392,-0.033956192,-0.01735108,0.00855649,0.032965947,-0.01971877,-0.0071380157,-0.08017335,-0.025183614,-0.020782897,-0.009321506,0.022345569,0.021430787,-0.040355045,0.038839843,0.025657963,0.014533334,0.10857811,-0.047920983,0.0001628508,-0.021164726,-0.06465762,0.09025761,0.047788925,-0.046050843,-0.014018642,-0.05067873,-0.047878362,-0.04934513,0.068428844,-0.0060278117,-0.04534531,0.07607783,-0.022698242,-0.04464534,-0.051769767,0.06665112,-0.041649774,-0.010565193,-0.005946312,0.009742363,0.025014805,-0.003485368,-0.032614417,0.0009650968,0.012880048,0.026280852,-0.05838953,0.0068699927,-0.019223744,0.089933924,0.112323605,0.0449821,-0.036022272,0.02183545,-0.01158674,0.01912312,0.027547128,-0.03137306,0.046800517,-0.094906546,-0.011733401,-0.034268968,0.011657224,-0.04556204,-0.01552573,-0.006749361,-0.011162049,-0.008132384,-0.032339215,-0.03193303,0.027090907,0.09520528,0.06802431,0.042650234,-0.02416157,0.010750607,0.028659424,0.06956488,0.0072558913,-0.024492389,0.041011002,0.0079534,0.0125758825,0.08059333,-0.011262566,-0.0027605002,0.035004247,-0.016378399,-0.026435344,-0.046728868,0.02385196,-0.05626326,0.022811662,-0.04262631,0.026412684,-0.042633187,0.033962432,0.062177464,-0.033962257,0.045348294,-0.010702381,0.0039563444,-0.026928516,0.02176253,-0.037980903,0.0041499278,-0.010864867,-0.041792873,-0.031938083,-0.015590263,0.04918823,0.031143185,0.08055387,-0.0006693679,-0.01363122,0.03698293,0.023757387,-0.049458385,-0.012004091,0.02157427,-0.036974408,-0.00510677,-0.05486226,0.028856361,-0.055229094,-0.045532092,-0.0315086,0.001573609,0.05794274,0.0155468555,0.08227232,-0.054260805,0.005498404,0.01939033,0.048589356,0.0039369683,-0.00894534,0.022183757,-0.04569066,0.0037525238,0.038975745,0.032305107,0.03720597,0.036189824,-0.081610635,-0.012431115,-0.051198788,-0.055554543,0.009936033,-0.006299279,-0.025221463,0.039618503,-0.01690196,0.050712425,0.015561219,0.025904905,0.043932073,0.0029429733,0.013333861,-0.012666663,0.023610765,0.028790027,-0.028370608,0.01878732,0.01970938,0.05045336,-0.062021058,-0.0355332,0.037962154,-0.006212006,0.04758873,-0.05092326,0.038177676,-0.051246326,-0.026989596,-0.019226436,-0.020067988,0.027873583,0.021572793,-0.083694726,0.04104534,-0.04271927,0.02168075,-0.011258229,-0.004099341,-0.0012356961,-0.0097116865,0.07109348,0.01585916,-0.04153789,0.07410945,-0.03213952,0.011262836,0.03445211,0.049944416,-0.02457236,0.05540225,-0.021530261,0.044870738,0.0028650472,0.0067071086,0.0129383225,-0.053622965,0.005515187,-0.005106823,0.032780338,0.008120067,-0.00093230396,-0.009895165,-0.0026989533,0.012435477,-0.0014940273,-0.013789645,0.032375637,0.02747591,-0.041977488,-0.03913926,0.039316863,0.0598912,0.017934259,0.013262683,-0.0368489,0.015323574,0.05133349,-0.01134359,-0.0066529554,-0.011567268,-0.016605696,-0.04461286,-0.0043857447,0.00088980247,0.026825624,0.03135843,-0.014354496,-0.0071188505,-0.053255673,-0.023423262,-0.010387274,-0.04634289,-0.016698305,0.016412964,-0.032352757,0.032651477,0.02390704,-0.032581627,0.012908831,0.037854355,0.08877988,-0.019479105,0.019779993,0.024083229,0.035545565,0.043419246,-0.030554311,0.052039873,0.045207568,-0.016801395,0.019517217,-0.0075614704,0.008236256,-0.004696154,0.0055636647,0.06408908,0.097020134,-0.025518397,0.0039127213,0.007293148,0.018030448,-0.015579081,0.010588385,0.051467225,0.019573811,0.01277999,-0.0032327704,0.006183082,-0.057056777,0.04360614,-0.04740619,0.070088714,0.049779765,0.037326347,0.02784276,-0.01680258,0.024407381,0.026602224,0.01594553,0.0013519559,-0.049665287,-0.013615406,0.024397641,-0.005395799,-0.0022819203,0.008770449,-0.0050737173,-0.024850499,0.043478817,-0.010564715,0.029717376,0.035206567,0.038936693,0.00034724385,0.027660953,0.0064662043,-0.029311083,0.027005743,-0.006710863,0.002440505,0.05053866,-0.015147703,0.005945172,-0.068420485,0.03195395,0.021691574,-0.09058486,0.019267207,0.03705884,-0.018498769,-0.0028626309,-0.039527547,0.045346465,-0.0322512,-0.040199704,0.04018126,0.06718898,0.034019455,-0.03416062,-0.028866239,-0.00041656097,0.0005421333,-0.051479112,-0.05063651,0.028020604,-0.0077588744,-0.024663873,0.09219248,0.023198249,0.06371657,-0.05050264,-0.013481143,-0.04303609,-0.014442796,-0.025832776,-0.0050429287,0.034617543,0.029139161,-0.040068503,-0.027967371,0.059280626,-0.0009365994,-0.047832828,0.013629249,0.013068913,0.00827927,-0.059601,0.05468883,0.0067283357,0.03154337,0.021173239,-0.041505847,-0.034880206,-0.01899078,-0.004978084,0.00712345,-0.03499276,0.0021338132,-0.06090995,-0.0071241823,-0.02949711,0.05180868,0.022536881,0.047610547,0.01690749,0.0015858451,-0.049111944,0.024175638,0.0035585056,-0.024641741,0.017568404,-0.04468201,-0.012253847,0.064012386,0.008290445,0.02833342,-0.064851075,0.10008175,-0.07227615,0.0068030613,0.037068587,-0.013865626,0.025066316,-0.022988155,0.009941683,0.0008076646,0.04388925,-0.03510544,-0.023552397,-0.0032221077,2.9948902e-05,0.007597078,-0.023778781,-0.01753977,-0.0040081725,0.00068974524,0.030369435,-0.018653609,-0.043163747,0.002325577,-0.0133673595,0.028484017,0.0034472942,-0.0014270622,0.0009861205,-0.015626881,0.012137971,-0.014400408,0.003794768,-0.010617672,-0.04029275,-0.008455551,-0.054363247,0.05003245,-0.033435337,0.00840904,-0.031844765,0.05544964,-0.011286132,0.026332418,-0.025069535,-0.015322724,0.00069370965,0.0013717059,-0.0018127852,0.019856852,-0.009214972,-0.005240413,-0.030477446,0.01678593,0.029891253,-0.004195328,0.042851698,0.005251776,-0.0012704484,-0.06847282,-0.015952425,0.0053915693,-0.015102211,-0.015394545,-0.009336981,0.025868017,0.06713512,-0.021343036,-0.009902754,0.054450892,-0.020811195,0.03152647,-0.018132681,0.045415927,0.022520674,-0.00569835,0.022719238,-0.012650984,0.014653905,0.053691823,0.005557139,0.0065923994,-0.012826023,0.07559288,0.020117877,0.034760013,0.06424626,-0.014772449,0.023407947,0.046690315,0.030790007,0.017788937,0.0204048,-0.05606457,0.0072220434,-0.011994458,-0.04494825,0.028710825,-0.061155878,-0.056066997,0.003656812,-0.08729245,0.027825125,0.015472149,0.0024836296,-0.0438406,0.006228095,-0.008627613,0.09116038,-0.061290357,0.019277643,0.03602094,0.0053288736,0.07861768,0.033652686,-0.015556029,-0.040114526,-0.033819567,0.022364398,0.011926593,0.08371123,0.0015728256,0.0044530453,0.072227456,-0.00064088625,0.0012680333,-0.0048787855,-0.027670814,-0.031737916,-0.02517231,0.010393387,0.041296702,0.000673445,0.027219115,-0.00035927194,-1.6913034e-05,-0.02101314,0.012512204,-0.026543222,0.056519575,0.042138614,-0.007803776,0.024383998,0.030181054,0.08550487,0.03954029,0.06442083,0.03618058,0.049983844,0.060204826,0.0046748505,0.0034534244,-0.0011990429,-0.017237974,-0.008259694,-0.024285685,0.0011397206,0.005615123,-0.041122403,-0.036817037,-0.03449392,-0.022735715,0.03054002,-0.040494718,-0.08921938,-0.034324028,-0.03933732,-0.036172695,0.022129906,0.016284363,-0.034645706,0.021758847,-0.046885204,0.09895588,-0.05330959,-0.0040809154,-0.041251022,0.027647909,0.025241889,-0.04638196,0.005126956,-0.031384584,-0.018837895,-0.027979916,0.0038025456,-0.011460343,-0.011938468,-0.03995408,-0.020771956,0.0029397993,0.00808421,0.0062189074,0.0027614105,-0.0075660474,0.006812809,-0.028221771,0.06542169,-0.00846949,-0.034780957,0.081380114,-0.025961475,-0.060865417,-0.01711244,0.022417955,0.02732026,-0.0017929745,0.023633966,-0.03708817,0.033774823,-0.012550913,-0.0133281415,-0.017644756,-0.027724367,-0.04189873,0.037897706,0.042724755,-0.006680421,-0.061980713,0.0036636833,-0.01887258,-0.029623765,0.012906941,0.05281191,0.041196045,0.057845462,0.048645433,-0.053679902,-0.07350092,-0.038427003,-0.050295603,0.049145415,0.0015820363,0.05854319,-0.005049579,-0.02358888,-0.02571555,-0.0054421937,-0.016136166,0.022898208,0.055644143,-0.042689625,-0.030588359,-0.049639925,0.0019598056,0.011235457,0.019916065,-0.009034474,0.09160597,0.04845556,-0.07581391,-0.03218435,-0.08245397,-0.009946992,0.016790086,0.002107353,0.013991465,-0.019923963,0.018475318,-0.012217522,0.0429756,-0.012311617,-0.040272668,-0.029161148,0.058247376,0.02818602,0.02817022,-0.023409652,0.010560201,-0.059740897,0.008959386,-0.009198273,-0.03261098,0.0082862135,0.014393377,-0.011786747,-0.054420736,0.024540972,-0.024666347,0.002264808,0.047410104,0.040765397,0.03554216,-0.0018006878,-0.013280204,-0.051644586,0.015935404,-0.03838832,-0.028671535,-0.018902939,0.066569656,-0.007961608,0.017135315,0.014831157,0.0123118255,-0.016818823,-0.0008163768,0.01037233,0.025568938,-0.043848213,0.0024780435,-0.032004613,0.025605455,0.046659146,-0.022380741,0.0049871053,0.06583144,0.059990175,0.022411728,-0.0027405012,-0.034017913,-0.0135269165,-0.04664271,0.012393945,0.0027472086,-0.018232612,-0.019829696,-0.0055031045,-0.049952548,-0.030954892,-0.020659393,-0.0037890046,0.0093821315,0.027039396,0.038637962,-0.042514537,0.0037073225,-0.024723506,0.030448368,-0.026074272,0.015387805,0.0011487709,0.029000187]	Keywords: Gaussian distribution, attention bias, central position, deviation\nKey Objects: Gaussian bias, central position, scalar parameters\nRefers to Images: None\nHypothetical Questions:\n- How does defining a Gaussian bias help the Transformer model focus attention?\n- What is the purpose of the 'w' and 'b' parameters in the Gaussian bias formula?\n- Why is it advantageous to assume the central position to be 'i' for each query?\n---\nSummary:\nGaussian Transformer models the central position of each query as itself and defines a bias to focus attention, using scalar parameters 'w' and 'b' to control deviation and reduce the weight for the central position.\nOriginal Text:\nwhere  denotes standard deviation for the Gaussian and can be determined as a hyperparameter or predicted from inputs.  \nGaussian Transformer [42] assumes the central position to be i for each q$\\_{i}$ and defines the bias to bes  \n$$G _ { i j } = - | w ( i - j ) ^ { 2 } + b |,$$  \nwhere w  0 , b  0 are scalar parameters that controls the deviation and reduce the weight for central position, respectively.  \n4.5.2 Prior from Lower Modules. In Transformer architecture, it is often observed the attention distributions are similar in adjacent layers. It is thus natural to provide attention distribution from previous layer as a prior for attention computation. The final attention scores can be defined as  \n$$\\hat { A } ^ { ( l ) } = w _ { 1 } \\cdot \\hat { A } ^ { ( l ) } + w _ { 2 } \\cdot g ( \\hat { A } ^ { ( l - 1 ) } )$$\nContextualized Text:\nTo explicitly encode locality preferences, some approaches utilize a Gaussian distribution as a prior attention. Gaussian Transformer assumes that the central position for each query is itself and introduces a bias to focus attention. This bias is defined by a formula involving scalar parameters 'w' and 'b', which control the deviation and reduce the weight for the central position, respectively.	{"tags": ["attention", "transformer", "architecture", "NLP"], "doc_id": "d883d9df-4240-4ee9-b563-d9c76a9c74f6", "summary": "Gaussian Transformer models the central position of each query as itself and defines a bias to focus attention, using scalar parameters 'w' and 'b' to control deviation and reduce the weight for the central position.", "doc_type": "text", "entities": ["Gaussian Transformer"], "keywords": ["Gaussian distribution", "attention bias", "central position", "deviation"], "key_objects": ["Gaussian bias", "central position", "scalar parameters"], "contextual_text": "To explicitly encode locality preferences, some approaches utilize a Gaussian distribution as a prior attention. Gaussian Transformer assumes that the central position for each query is itself and introduces a bias to focus attention. This bias is defined by a formula involving scalar parameters 'w' and 'b', which control the deviation and reduce the weight for the central position, respectively.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["How does defining a Gaussian bias help the Transformer model focus attention?", "What is the purpose of the 'w' and 'b' parameters in the Gaussian bias formula?", "Why is it advantageous to assume the central position to be 'i' for each query?"]}
9aa282ee-8470-4b85-9b82-729f9799eb51	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.009873872,0.019689925,0.011686957,0.03576193,-0.0043366537,0.07457346,-0.012139555,0.08331733,0.046394166,-0.024114763,0.00091570965,0.028872937,0.06024034,-0.018907033,-0.03502716,0.03447305,-0.012617006,0.07382755,-0.0052693393,0.0071638613,0.04960692,0.030575618,0.038618926,-0.0035141981,0.00071373885,0.05167762,0.034727376,-0.025600262,0.0035576785,0.028819283,0.008048332,-0.016170174,0.012964551,-0.011576676,0.006328958,-0.012904141,-0.0006732564,-0.049083464,-0.0019683125,-0.020612739,0.0054273293,0.047886897,-0.084414124,-0.03267091,0.005472612,-0.04708584,-0.0807071,-0.055697504,0.031608976,-0.03023931,-0.0402935,-0.0044501517,-0.042657785,-0.039405335,-0.0073412345,0.0057749255,0.012855099,-0.027850356,0.017443005,-0.094802886,-0.029409798,-0.013902037,-0.034591805,0.036178395,0.005423643,-0.026110785,0.03407051,0.011162651,0.04791276,0.10616058,-0.024348464,-0.006332863,0.010019141,-0.04042424,0.1055116,0.052775398,-0.06861029,-0.026766395,-0.057107802,-0.010593484,-0.015931483,0.06516614,-0.06344279,-0.035181522,0.07743714,-0.0264169,-0.039609738,-0.006068628,0.03697697,-0.035505705,-0.0022692515,0.009892295,-0.005153454,0.09435096,-0.005792381,-0.071972035,-0.023920363,-0.009254934,0.032603938,-0.04513271,0.029224336,-0.0050059087,0.123006,0.094573386,0.049019363,-0.03635261,-0.0007441389,-0.05932662,0.017682,0.035602804,-0.00028747265,0.008500853,-0.068192415,0.031744245,-0.008426771,0.0026395363,-0.031718343,0.015456969,0.0024091005,-0.0116098635,0.010567365,-0.033313215,-0.015731608,0.029376011,0.056771986,0.04188265,0.022973195,-0.024263844,-0.012143602,0.023144767,0.06556562,0.010554243,-0.0055660866,0.010506953,0.0063298363,0.022403004,0.054711007,0.03895897,-0.0017522381,0.0676608,-0.008828793,-0.056360982,-0.05722534,0.028895825,-0.034235936,0.046139717,-0.06150225,-0.012604034,-0.034720715,0.05177141,0.028982436,0.0039360975,0.0161916,-0.015951786,0.009165521,-0.013719383,0.014942747,-0.060441226,-0.01836061,-0.058021612,-0.06775363,-0.026455928,-0.054949254,0.038821705,0.029284442,0.093275614,-0.014558728,-0.02955055,0.038699158,0.037083946,-0.029423289,-0.019049034,0.006204974,-0.036904696,0.007629365,-0.036027417,0.00031218847,-0.012092124,-0.0060176323,-0.0050142976,0.01279026,0.019476706,0.0027442516,0.089507185,-0.02112363,0.0072939815,0.036466446,0.08584294,-0.009875232,-0.008349454,0.0031937722,0.008190689,-0.026180418,0.011224733,0.017560307,0.023777815,0.040631983,-0.08248206,-0.046213873,-0.038390778,-0.053892117,0.019924771,0.0075654257,-0.033909973,0.0433667,0.010089079,0.0022442266,-0.025406504,0.014439982,0.021534802,-0.0082960185,-0.019616336,-0.0029855783,0.02801586,0.03834702,0.006689482,0.0101507455,0.05596713,0.05356672,-0.046534654,-0.030761015,-0.0018482807,-0.035737116,0.044568624,0.0014901888,0.032352872,-0.069713935,0.037129324,-0.0083122095,0.000583692,0.03300262,0.012916254,-0.013698657,0.0043388554,-0.030075688,0.03516853,-0.016467966,0.013318259,0.0061340146,0.031307098,0.06677461,0.026116647,-0.03223993,0.09267389,0.050415203,0.016357483,0.04394552,0.05876052,-0.0069198143,0.006843614,0.028413322,0.036300197,0.042636987,-0.0078088576,-0.00039218378,-0.027813412,-0.0032377054,0.048650235,0.04952444,0.02375541,-0.011964426,-0.0021244423,-0.030934803,-0.029512635,-0.034188356,-0.018558457,0.0077530746,0.026003098,-0.014100719,-0.007793579,0.04573151,0.02012112,-0.0011132571,-0.011351924,-0.056219753,0.09492958,0.03227329,-0.012292566,0.05240271,-0.037474524,0.043050088,-0.017769381,0.030355673,0.00045264748,0.012006832,0.0094977645,0.029445438,-0.0023459026,-0.016432844,-0.018251548,0.0049315025,-0.052830495,-0.007390178,0.0044808886,-0.036550116,0.0121107595,-0.0073615136,-0.043950688,-0.0006852099,0.023065148,0.1163789,-0.025307512,-0.0031387482,0.017025702,0.004480318,0.034777317,-0.062073518,0.044101145,0.045490295,-2.6182119e-05,0.06552655,-0.03722383,0.019811628,0.012537846,0.048201066,0.056680445,0.03795588,-0.019557804,0.025944777,0.037617814,-0.01747001,-0.02667882,0.0051716776,0.03512155,0.0007334928,0.0034483692,0.0048761345,0.0050160405,-0.074702606,0.05497814,-0.04703061,0.05053723,0.06574181,0.0026527878,0.0031913496,-0.026013568,0.0127462,0.03358093,0.010329309,0.045775753,-0.029785773,-0.06160502,0.016996127,0.009729774,0.01240481,0.014723314,0.019364247,0.009426996,0.052173663,0.007564001,-0.016499532,0.053941257,0.01904821,0.0313222,-0.0047601005,0.009322878,0.00278812,0.015305595,-0.005955035,-0.005730958,0.067668624,-0.039285738,0.019745778,-0.035018295,0.018393062,0.027611857,-0.09086115,0.0010036908,-0.0050824136,-0.029375197,0.023653252,-0.030969484,0.025641687,-0.039068025,-0.0036721886,0.04878519,0.010728716,0.009989295,-0.018692875,-0.015693158,-0.0020502056,-0.030008012,-0.047282916,-0.023977432,0.0364828,-0.012191696,-0.03957322,0.10330517,-0.02640287,0.073972866,-0.027330708,0.03843433,0.008337565,-0.008562756,-0.015157469,0.008229917,0.005785294,0.05618661,-0.01504089,-0.0049250918,0.0820143,0.020895982,-0.069095634,0.0050880723,0.034051824,0.033340294,-0.06964353,0.044483062,0.017884403,0.046414286,-0.0139891375,-0.02530462,-0.007128686,-0.034644354,0.018498098,-0.0017153355,0.0003793977,0.045598406,-0.048389997,0.03373745,0.0018924773,0.020442627,0.015941357,0.039475344,0.019289406,0.0044984748,-0.016567357,0.020186339,-0.0060029095,-0.06797565,0.019267587,-0.058390293,-0.038818207,0.04591466,0.012109079,0.031627808,-0.061287384,0.067968026,-0.016486615,0.02311409,0.08208043,-0.013466299,0.004311115,0.017645182,-0.0028374933,0.024036432,0.008839307,0.011555131,-0.039177686,-0.038045157,0.026700724,-0.016095564,0.023610273,-0.017125616,0.008194317,-0.012944488,-0.00958644,-0.0017792071,-0.060764965,-0.011524245,0.020287396,0.028238446,0.017257797,-0.0033667854,0.0008358705,-0.032712683,-0.011873458,-0.025150143,-0.020722685,-0.011070756,-0.033416577,0.013844572,-0.08269043,-0.007240968,-0.013972335,0.018892918,-0.03743464,0.0422149,0.023281537,-0.029925518,-0.0001388588,-0.016381903,0.013923359,0.021616122,-0.007391334,0.04990486,-0.021319449,-0.014677283,-0.025547992,0.04718745,-0.032956716,0.042461965,0.0087409,0.012810048,0.03490538,-0.03309625,-0.017240485,0.00757447,0.022503994,-0.036191866,-0.015199811,-0.019102896,0.040050592,0.020992624,-0.011870851,0.008683375,0.009317298,0.04177406,-0.015006269,0.04242892,0.01841987,-0.05070435,-0.010833827,-0.029411342,0.014985975,0.0395068,6.252432e-05,-0.004843444,-0.006452963,0.05890409,-0.006308715,0.013725263,0.025858153,-0.050603036,0.018396556,0.022818074,0.044450507,0.014001189,0.04282953,-0.055481467,0.031529143,-0.033099104,-0.047958687,0.01803971,-0.048648205,-0.056725156,0.017398506,-0.052319836,-0.0010945648,0.015090939,0.011602144,-0.036694568,0.017488396,-0.041972343,0.077801086,-0.04256529,0.030801998,0.028655875,-0.010019972,0.07592767,0.02222403,0.0030292247,-0.020183248,0.0044177757,0.03322826,0.01636262,0.08845143,0.040197823,-0.025271377,0.06604464,0.022884736,0.02344231,0.0066894256,-0.06286828,-0.020236095,-0.018298546,-0.011769747,-0.03216474,-0.016489971,0.030846376,-0.0031749846,-0.016364513,-0.02609621,0.023573874,-0.009437157,0.06785693,0.04693404,-1.6742802e-05,0.039218135,0.005870173,0.08022289,0.020424668,0.05389841,-0.005475554,-0.001925729,0.031733193,0.020916065,-0.0064534917,0.04144715,-0.03713816,-0.0342729,-0.042724445,0.019357203,0.0016218069,0.00034657336,-0.020619681,0.03879613,0.0011106787,0.0037893385,-0.041150335,-0.07010308,-0.008809775,-0.05128058,-0.020338733,0.014841618,0.009313719,0.003995032,0.05671854,-0.040610395,0.06092968,-0.10193033,0.0047706165,-0.020730479,0.04175102,0.011044611,-0.018870492,-0.02792333,-0.04164298,-0.0046259793,-0.038280834,0.021542428,-0.036919504,-0.022471901,-0.013585275,-0.005657386,-0.017556759,0.009474871,-0.004889408,-0.009565996,-0.0010651468,-0.014695832,-0.01548961,0.05356535,0.018248249,-0.014776491,0.009123198,-0.03272787,-0.08174819,-0.008586958,0.015369426,0.02421693,-0.0094545195,-0.0019176287,-0.0021556672,0.052552726,-0.039065618,-0.0006459865,-0.028223703,0.00042268136,-0.044379782,0.026379384,0.07499879,0.007957468,-0.043670963,-0.0019313421,0.052631922,-0.046882212,0.04668125,0.013367888,0.09045075,0.02804703,0.04911466,-0.032542657,-0.11639655,-0.026601302,-0.022501754,0.0014602921,-0.020509312,0.047047514,-0.014440522,-0.016852835,-0.019715315,-0.013364685,0.003687565,0.030655658,0.02706693,0.0048106858,-0.01687642,-0.02165742,0.013653006,-0.00933791,0.029073613,-0.009193986,0.07276628,0.014596031,-0.016492281,-0.04610867,-0.061097383,-0.026878037,0.0057157925,0.0048049246,0.02086756,-0.039621808,0.034155965,-0.020011278,0.055576395,-0.021570753,-0.067020334,-0.0574489,0.05288832,0.020152034,0.017981663,-0.034078144,-0.024282556,-0.12186967,0.0212553,-0.003205549,-0.09073327,0.051323455,0.035070762,-0.02615305,-0.026174035,0.012142336,-0.01754306,-0.0036707025,0.039948445,0.018073026,0.034469515,0.017115671,0.005075652,-0.07985018,-0.036151018,-0.041199613,-0.018389124,-0.015082048,0.04593431,-0.010651793,0.010508195,0.018127108,0.00049186696,-0.018868655,0.03385833,-0.014526276,0.08588871,-0.03407739,0.059493367,-0.010172109,0.011991797,0.029218439,0.014531385,-0.03329676,0.010950287,0.028386619,0.025920527,0.016500808,-0.007972693,-0.039905436,-0.03470458,0.010091319,0.041735668,-0.009990709,0.014213001,-0.011755515,-0.025204752,-0.05989924,-0.010829647,-0.02397905,0.038015302,0.031137612,0.033418946,-0.045701742,0.009746414,-0.03409056,0.0500604,-0.00032176913,0.015784297,-0.0019022918,-0.013620278]	Keywords: attention mechanism, prior attention, convolutional layer, Transformer architecture\nKey Objects: Attention Scores, Transformer Layers, Convolutional Layer\nRefers to Images: None\nHypothetical Questions:\n- How does the function 'g' influence the final attention scores?\n- What are the benefits of using a convolutional layer as the 'g' function?\n- In what situations would a prior attention mechanism be most advantageous?\n---\nSummary:\nIn Transformer architectures, a prior attention mechanism can be introduced by combining attention scores from the current layer with those from a previous layer. This is achieved by weighting the current attention scores and transforming previous layer scores using a function, allowing for the incorporation of prior information into the attention process.\nOriginal Text:\n$$\\hat { A } ^ { ( l ) } = w _ { 1 } \\cdot \\hat { A } ^ { ( l ) } + w _ { 2 } \\cdot g ( \\hat { A } ^ { ( l - 1 ) } )$$  \nwhere A ( l ) denotes the attention scores of the l -th layer, w$\\_{1}$, w$\\_{2}$  R are weight applied to the scores from adjacent layers, and g : R n  n  R n  n is a function that translate previous scores to the prior to be applied.  \nPredictive Attention Transformer [143] proposes to apply a 2D-convolutional layer to previous attention scores and compute the final attention scores as a convex combination of the generated attention scores and the convolved scores. This is equivalent to setting w$\\_{1}$ = , w$\\_{2}$ = 1 -  and g (  ) to be a convolutional layer in Eq. (22). They experiment training such a model from scratch and finetune after adapting the pre-trained BERT model, and both sets of experiments show improvements over baseline models.\nContextualized Text:\nTo incorporate prior information, Transformer architectures combine attention scores from the current layer (A^(l)) with those from a previous layer.  The combined attention scores are calculated as a weighted sum where w and w are weights applied to the scores and g is a function transforming previous layer scores to act as a prior. For example, the Predictive Attention Transformer utilizes a 2D-convolutional layer (g) to transform the previous attention scores, creating a convex combination with the current scores.	{"tags": ["NLP", "deep-learning", "architecture", "attention"], "doc_id": "9aa282ee-8470-4b85-9b82-729f9799eb51", "summary": "In Transformer architectures, a prior attention mechanism can be introduced by combining attention scores from the current layer with those from a previous layer. This is achieved by weighting the current attention scores and transforming previous layer scores using a function, allowing for the incorporation of prior information into the attention process.", "doc_type": "text", "entities": ["BERT", "Predictive Attention Transformer"], "keywords": ["attention mechanism", "prior attention", "convolutional layer", "Transformer architecture"], "key_objects": ["Attention Scores", "Transformer Layers", "Convolutional Layer"], "contextual_text": "To incorporate prior information, Transformer architectures combine attention scores from the current layer (A^(l)) with those from a previous layer.  The combined attention scores are calculated as a weighted sum where w and w are weights applied to the scores and g is a function transforming previous layer scores to act as a prior. For example, the Predictive Attention Transformer utilizes a 2D-convolutional layer (g) to transform the previous attention scores, creating a convex combination with the current scores.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["How does the function 'g' influence the final attention scores?", "What are the benefits of using a convolutional layer as the 'g' function?", "In what situations would a prior attention mechanism be most advantageous?"]}
ea58ea56-5817-44ac-a1ab-eca5be052b83	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.031605516,0.0027406462,0.016776953,-0.01280193,-0.0351712,0.057035208,-0.013836279,0.03401363,0.08529571,-0.010747371,-0.007209712,-0.014334752,0.008466244,0.021700975,-0.03533733,0.043595456,0.008348907,0.07099426,0.002448417,-0.04706564,0.060544297,-0.004550446,-0.0033643793,-0.006110735,0.0109483525,0.057425782,0.04237674,-0.016579008,-0.024317471,-0.0045736614,0.035091385,-0.011890658,0.0050440584,0.0041393996,0.022567537,-0.044608038,0.017341204,-0.05660001,0.03482363,-0.031875655,-0.020838726,0.051902287,-0.04040172,-0.05000702,0.014527972,-0.036501013,-0.06301867,-0.0042655864,0.02768772,-0.014541714,-0.009178758,0.0015371754,-0.044104293,0.023690175,-0.02025099,-0.007803024,0.02945162,-0.061058044,-0.0063610794,-0.074572295,-0.04879141,-0.0046094563,-0.02515709,0.0016007137,0.0093423985,-0.027104955,0.019843813,-0.003755652,0.037199195,0.13114068,0.007886396,0.008480845,0.021734854,-0.0560521,0.1107543,0.028126575,-0.011729382,-0.04204285,-0.06557846,-0.031129403,-0.013083697,0.06398064,-0.031095784,-0.0551332,0.07628992,-0.038959306,-0.038909167,0.015299453,0.0302856,-0.040808387,-0.008882128,-0.03739209,0.011414541,0.065288685,0.010151004,-0.041575987,-0.013518528,-0.02200391,0.051916983,-0.05784396,0.019669868,-0.03478788,0.110417746,0.07472351,0.04729067,0.0015886547,-0.02906688,-0.023126913,0.007867511,0.03603083,0.011992277,-0.022036856,-0.08355497,-0.005702736,-0.07208243,-0.018808063,-0.052608695,0.030948082,-0.0103006875,0.007462674,0.047337484,-0.0021060146,0.004264925,0.036173172,0.05996878,0.07600787,0.006512702,-0.06749282,-0.023270566,0.021829877,0.044886954,0.0259286,-0.036426213,0.040459692,-0.0009183933,0.010019137,0.05726707,0.023594145,0.010662782,0.014232853,-0.017036542,-0.03544407,-0.04898535,0.009570462,-0.046786252,8.082847e-05,-0.061362293,-0.00022508187,-0.058564626,0.06863706,0.015960684,-0.040677376,0.031876452,0.0064669903,-0.002876569,0.008520768,-0.042401735,-0.022748156,0.0034118579,-0.034751475,-0.069215015,-0.025801206,-0.022826808,0.06611055,0.017739674,0.09872777,0.0016856119,0.0045911577,0.02260418,0.043647896,-0.014888926,-0.018513981,0.037779182,-0.009189898,0.009813549,-0.03213493,0.008580777,-0.0334982,0.0021223344,0.057738427,-0.0052959714,0.011444954,-0.01997695,0.027113885,0.0004878708,-0.002842164,-0.039451357,0.081179716,-0.017288724,-0.012688165,0.024693545,0.025901405,0.0150169805,0.019310419,0.00047746926,0.0437378,0.03401789,-0.05213949,-0.024216877,-0.063414395,-0.017375031,-0.041604098,-0.043535925,-0.018352648,0.01702061,0.03201654,-0.019440757,0.036896076,-0.009601317,0.017213605,0.01805758,0.0150117185,0.012421885,0.04782313,0.01807666,-0.005130824,-0.026127273,0.038273368,0.04237147,-0.032169633,-0.022498159,-0.029301545,-0.04292713,0.058291014,-0.043489207,0.015991045,-0.05344322,0.005518071,0.012805003,-0.0028978076,0.03544911,-0.016855378,-0.009580875,0.042919777,0.016148848,0.055799507,-0.006029298,-0.014051597,-0.00624399,0.04969679,0.08671862,0.05096544,-0.01792851,0.063280806,-1.0372311e-05,0.0049214843,0.014896214,0.07529862,0.019192165,0.023791214,-0.038252875,0.0145437,0.046888784,-0.026317898,-0.006452941,-0.020506354,0.010501008,0.019637192,0.058075327,0.032580115,0.0036526627,-0.017423091,-0.013436656,-0.05444182,-0.0059847976,-0.0524075,0.007918202,0.041041285,-0.050597314,0.00481669,0.039820623,0.031187747,-0.0076251384,-0.053993486,-0.093848765,0.0508887,0.028771436,-0.024523498,0.00014913284,0.015531058,-0.0355637,0.017180229,0.03199076,-0.0024259507,0.02140686,0.00486316,6.502437e-06,-0.014874382,-0.009855612,-0.015187891,0.017132746,-0.027976062,-0.0020062244,0.024733458,-0.02201151,-0.0024175958,-0.017028552,-0.034318253,0.008351371,-0.0031982565,0.10322966,-0.06391777,0.013545965,0.016572809,0.050274752,0.0089070555,-0.059536334,0.03202058,0.007907809,0.03438568,0.053414304,-0.016041527,-0.002942317,0.016677454,0.071576625,0.06319781,0.065283686,-0.026069285,0.017201468,0.024466628,0.009652371,0.020104486,0.015806552,0.015259249,0.009362058,-0.005897834,0.033327654,-0.008889819,-0.10536061,0.055336524,-0.042015746,0.010793,0.060055837,-0.01198649,-0.0012692623,-0.01436945,0.041360084,0.09086789,0.035119854,0.03495483,0.0039530452,-0.07595941,0.033171453,0.035701852,-0.038991544,0.047271863,-0.016297469,-0.013214843,0.05706861,0.011624923,0.013322198,0.0050706463,0.0036340037,0.009505292,0.0326667,-0.031464465,-0.037141044,-0.0016980853,-0.03244678,-0.0078018387,0.05445383,-0.06937295,0.030408539,-0.054885395,-0.0056535606,-0.01752553,-0.049578663,0.046614278,0.016322456,-0.050454482,0.020439558,0.0010167292,0.048277467,-0.02304397,0.0035049391,0.056806188,0.00023490185,-0.025291456,-0.01944805,0.005553178,-0.04922208,-0.016432313,-0.043079954,-0.02883245,0.037070747,-0.006163305,-0.001994084,0.10700852,-0.056404218,0.014203249,-0.039851177,0.043671843,0.051006734,-0.008271287,-0.01900779,0.025831806,-0.017009625,0.048114855,0.017474946,-0.017762475,0.032359444,0.040555857,-0.065400064,0.02385074,0.028801356,0.011702528,-0.056661494,0.030106645,-0.025021259,0.05189122,0.0717244,-0.03982154,-0.041203085,-0.016063202,-0.014300204,-0.01270176,0.0151308095,0.009763104,-0.035824455,-0.011528375,0.0028675136,0.019829307,0.017569581,0.06733099,0.05947509,-0.007983716,-0.033252727,0.0069037173,-0.0043544965,-0.08498382,-0.01689802,-0.038136013,0.005284073,0.03845605,-0.008752687,-0.012615327,-0.062171157,0.08200785,-0.027512424,0.02295836,0.018967144,-0.017184265,0.016392948,-0.007382131,0.037135355,-0.030734748,0.026799887,-0.016154325,-0.003972556,-0.009569489,0.016573668,0.018122014,0.038384758,-0.01109185,0.024007782,0.010016998,-0.046184298,-0.041337397,-0.07673784,-0.018627416,0.0052767224,-0.00792019,0.014143239,-0.02998884,0.016107956,-0.07925153,-0.01220182,-0.0033706117,-0.018128516,-0.03943868,-0.011051672,-0.0006192149,-0.054861926,0.03875807,-0.009787505,0.009043738,-0.012132452,0.03183555,0.026592297,0.008089019,-0.06778583,-0.017516874,0.041135024,0.0094785215,-0.00904534,0.006628449,-0.01955872,-0.029333586,-0.03896829,0.03500532,-0.014979787,0.08366865,-0.025519568,0.006537509,0.040347535,-0.030586053,-0.0035334374,0.0142737245,-0.00494459,-0.00886412,-0.02032083,-0.03680884,0.060983267,0.016270028,0.01072067,0.06892822,-0.01640148,0.030881811,-0.017253656,0.016373964,0.02423386,-0.086028256,-0.00049759174,-0.0059940848,-0.011338038,0.011185743,0.018942859,0.04380956,-9.679661e-05,0.050737187,-0.036666866,-0.008815524,-0.011331315,-0.024910271,0.038464103,0.047365814,-0.0034026732,-0.0011443417,0.0068389378,-0.03718544,0.01265667,-0.016928092,-0.015700085,-0.018811561,-0.06148621,-0.00536743,0.011850529,-0.025950192,0.0032703811,0.043739293,0.002649902,-0.04587496,0.02114738,-0.011733915,0.035970148,-0.035546828,-0.0108754635,-0.011822331,0.059195127,0.04109259,0.01216561,-0.0034880643,-0.0180587,-0.018097188,0.02108281,0.029087938,0.04535498,0.04173446,0.01722983,0.049946234,0.0048673428,0.010433445,-0.039936718,-0.024025422,-0.004805814,-0.0051995586,-0.05766331,-0.030455982,-0.021715038,0.005091872,0.01598018,-0.02421707,-0.03775356,0.030229375,-0.021428093,0.026335575,0.021955801,0.010933685,0.03349212,0.00036710672,0.041803002,0.017256847,0.050357405,0.012877328,0.0021323164,-0.0021038703,0.005945915,0.0013949145,0.02263077,-0.01070083,0.027995696,-0.048793167,-0.012744056,-0.021003732,-0.00458953,-0.0056527825,0.0320835,0.0037456793,-0.0022710005,-0.060611207,-0.050010063,0.0092794625,-0.07886017,-0.02866107,0.011509524,0.007818264,0.034084484,0.08292922,-0.00823637,0.06737545,-0.0753385,-0.009924028,-0.0033784048,0.061086908,-0.040053718,-0.020175437,-0.030842744,-0.033430796,0.002396101,-0.032866087,0.034615386,0.013821436,-0.0434693,-0.0043506487,-0.032967713,0.010944639,0.0072459364,-0.013644566,-0.035166685,0.02465132,-0.0050899056,-0.0056675524,0.08023522,0.023379125,-0.0069280574,-0.0017564712,0.0011817184,-0.09960516,-0.036272332,0.08041732,0.026660219,-0.008828419,-0.020479197,0.026694862,0.016747778,-0.03573984,-0.011718553,-0.012813588,0.0042414796,-0.04244204,-0.0016961081,0.057524975,-0.0011606588,-0.061049987,-0.017990185,0.028434008,-0.00060880155,0.06718781,0.03688159,0.034461074,0.034122277,0.053148516,-0.025503047,-0.11519464,-0.047808938,-0.040764373,0.0029611096,0.002952368,0.02215169,-0.00080367574,-0.040694788,-0.00041456963,0.011779429,0.0069024563,0.00013529118,-0.0028720903,-0.050572675,-0.0039233733,-0.037488483,-0.02714379,0.038684327,0.011932377,-0.060001668,0.046446476,0.016768003,-0.029653322,-0.06745773,-0.021389224,-0.0476707,0.0029786893,0.022214416,-0.016452288,0.009893781,0.006549992,0.002304511,0.04081846,0.030297974,-0.07452255,-0.025796937,0.063143715,0.0034373407,0.039752908,0.009552638,-0.033206634,-0.07776321,0.028488927,-0.015342644,-0.055692222,-0.026222004,0.002255883,-0.030892173,-0.01872707,-0.013199354,-0.0127880275,0.0028295985,0.034065697,0.01675254,0.018355457,-0.011053947,-0.012527543,-0.053524055,-0.0135288155,-0.0027204915,0.02824956,-0.0030592403,0.10153348,0.008299884,0.026003646,-0.038671512,0.027114334,-0.009783395,0.037021987,-0.04041955,0.06567432,0.010571259,0.0698368,-0.012552527,-0.032721803,0.058171768,0.010775126,-0.037637576,-0.037651125,0.030804563,0.0022697588,0.006883519,-0.048007295,0.023698369,-0.049317177,-0.0240779,0.0110637285,0.01608157,0.03460899,-0.0383716,-0.028903373,-0.053905107,-0.0601681,-0.011026992,0.056943968,0.07993903,0.0135041755,-0.03355703,0.025189484,-0.014816982,0.057233345,0.0027601419,-0.006714396,-0.015862802,0.020475198]	Keywords: attention mechanism, residual connection, Realformer, BERT\nKey Objects: attention scores, attention maps\nRefers to Images: None\nHypothetical Questions:\n- Why might adding previous attention scores improve performance?\n- How does the residual skip connection on attention maps contribute to Realformer's efficiency?\n- What are the benefits of Realformer surpassing the baseline BERT model with reduced pre-training?\n---\nSummary:\nRealformer utilizes a technique where previous attention scores are directly added to the current, generated attention scores, functionally acting as a residual skip connection on the attention maps.\nOriginal Text:\nRealformer [51] uses adds the previous attention scores directly to the generated attention scores, thus resembles a residual skip connection on attention maps. It's equivalent to setting w$\\_{1}$ = w$\\_{2}$ = 1 and g (  ) to be identity map in Eq. (22). They conduct pre-training experiments on this model. The results show that this model outperforms the baseline BERT model in multiple datasets and surpasses the baseline model even when pre-training budgets are significantly lower.\nContextualized Text:\nTo leverage information from previous layers, Realformer [51] directly adds the previous attention scores to the generated attention scores. This is analogous to a residual skip connection on the attention maps, allowing the model to benefit from prior contextual information. This approach was demonstrated through pre-training experiments, revealing that Realformer outperformed baseline BERT models, even with reduced pre-training budgets.	{"tags": ["architecture", "transformers", "attention"], "doc_id": "ea58ea56-5817-44ac-a1ab-eca5be052b83", "summary": "Realformer utilizes a technique where previous attention scores are directly added to the current, generated attention scores, functionally acting as a residual skip connection on the attention maps.", "doc_type": "text", "entities": ["Realformer", "BERT"], "keywords": ["attention mechanism", "residual connection", "Realformer", "BERT"], "key_objects": ["attention scores", "attention maps"], "contextual_text": "To leverage information from previous layers, Realformer [51] directly adds the previous attention scores to the generated attention scores. This is analogous to a residual skip connection on the attention maps, allowing the model to benefit from prior contextual information. This approach was demonstrated through pre-training experiments, revealing that Realformer outperformed baseline BERT models, even with reduced pre-training budgets.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["Why might adding previous attention scores improve performance?", "How does the residual skip connection on attention maps contribute to Realformer's efficiency?", "What are the benefits of Realformer surpassing the baseline BERT model with reduced pre-training?"]}
ac5ba16d-ed33-4ab0-8749-37e205983a7a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03533875,0.03359292,0.044162646,0.006000642,-0.06470396,0.04113054,-0.017533362,0.037013505,0.017517569,0.018076742,0.017528892,0.00398539,0.0038734963,-0.017466659,-0.044243168,0.012462849,0.009983562,0.08776523,-0.002657863,-0.031036323,0.010901149,0.042543765,0.013251906,0.0050638975,0.01688668,0.05801436,0.04689653,0.0052060564,-0.004011045,-0.01900202,0.032260224,-0.06116551,0.0028636123,0.020579083,0.042537786,-0.027872534,-0.0048547536,-0.05682533,0.025882212,-0.016975628,0.018047916,0.050605632,0.005952207,-0.023526348,-0.02775094,-0.034055345,-0.07399945,-0.023177672,0.041373916,0.006027,-0.030817999,-0.0018383957,-0.024809182,-0.046460245,-0.03195467,0.011685563,0.00071796164,-0.079279535,0.013201925,-0.088962734,-0.025304036,0.05479094,0.0025387679,0.027412513,0.00399952,-0.052990012,0.01116276,0.015274645,0.00669102,0.13160457,-0.0064930716,-0.004131838,-0.014992941,-0.06602577,0.10891709,0.020492347,-0.05035584,-0.008650094,-0.055362124,-0.021075482,-0.003419217,0.03611617,-0.03574546,-0.04140379,0.063291304,-0.016268749,-0.030345162,0.021682922,0.042730127,-0.048266813,-0.020924965,0.004451745,-0.040139504,0.048628498,-0.009838653,-0.07922777,-0.04678718,-0.049026154,0.06311047,-0.049665727,0.030962775,0.027556852,0.061242413,0.10876767,0.05380209,-0.025873816,-0.015865307,-0.029169507,0.016124887,-0.0076828916,-0.005502472,0.026444774,-0.0506187,0.02790656,-0.0034664632,0.007571064,-0.040037822,-0.0064083585,0.028964028,-0.011183634,0.03884529,-0.039094664,-0.018342521,-0.001255726,0.066570096,-0.0020812913,0.022416076,0.0022521843,-0.017996082,0.02688015,0.04025272,0.016791565,-0.019843908,0.029526751,-0.009943316,-0.045339428,0.0394043,0.006871936,-0.025347384,0.0035127685,-0.0017296376,-0.038114164,-0.055485602,0.026907345,0.004737512,-0.010480138,-0.0843636,0.014639427,-0.03464735,0.026748437,0.039075624,-0.031027315,0.04828823,-0.017946603,0.003696058,-0.030443752,-0.013484999,-0.034239333,0.0027289973,-0.008130786,-0.033747375,-0.050907157,0.031164575,0.07642691,0.06104254,0.08797719,0.019200897,0.011473369,0.014250864,-0.02753229,0.00875346,-0.01129391,-0.0040382072,-0.031549226,0.0068839137,-0.010136941,0.05124974,-0.086099885,0.017819684,-0.036119398,0.015939722,0.07462596,-0.0051399977,0.006007653,-0.035672903,0.030455958,-0.01035788,0.06149882,0.0032176645,-0.03477113,-0.028766438,-0.021811986,0.030611381,-0.018444277,-0.023141012,0.02943359,0.047116563,-0.109689504,-0.0058941757,-0.060413208,0.005052859,-0.033543095,-0.034596607,-0.026757022,0.019129546,0.00091509876,-0.0155351525,-0.046973515,0.0010772055,0.0042111347,-0.029498724,-0.0060654487,-0.02282311,0.041454885,0.018155303,0.03321775,-0.019647101,0.02583579,0.05944728,-0.05013444,-0.046957947,-0.0084792,-0.09990389,0.032490406,-0.016799543,0.057482176,-0.0060084183,-0.005066316,-0.010850869,0.023571674,-0.0006029334,-0.019513655,-0.040074896,0.03028462,-0.010034797,0.0021506615,0.0072358223,0.0036232078,-0.060526285,0.048505038,0.021170523,0.03921535,0.019067353,0.08596812,-0.008305282,0.014445364,0.025519133,0.010482222,0.019735496,0.010731846,0.0049977587,0.07091002,0.059415106,-0.035006423,-0.016164491,-0.026952019,0.021478385,0.01737652,0.06291182,-0.0054341205,0.021790737,-0.023848891,-0.015839202,0.0018503873,-0.018505376,0.033691023,-0.019223671,0.012657524,-0.026809549,0.0046626497,0.013924265,0.028447023,0.018734308,-0.001617152,-0.066327535,0.043309692,0.014982066,-0.024965143,0.019988948,-0.0017297199,-0.013163278,-0.029598156,-0.009195727,-0.00399373,0.02670728,-0.019661471,0.0144063225,0.0045167627,-0.029804995,-0.015862714,-0.035551008,-0.029392412,0.03175649,0.0030640813,-0.017710634,-0.002103378,-0.0027961787,-0.058199927,-0.018833417,-0.02083712,0.11275629,0.008122984,-0.0040508183,0.010214646,0.08082851,-0.0034651577,-0.05396065,0.05979502,0.0030116704,0.01681512,0.026935514,-0.022471745,-0.015232724,0.0009876577,0.05320991,0.054728415,0.07292396,-0.018239753,-0.0027009682,0.047990743,0.02648089,-0.01561825,0.0072658546,0.04031581,-0.0037997654,-0.040324196,0.02230943,-0.012112706,-0.080084085,0.08512906,-0.06764475,0.02899777,0.06421772,0.0020389857,-0.013909191,-0.029182514,-0.023015084,0.027613692,0.026839757,0.04161907,-0.033023518,-0.029661492,0.029205102,0.020901024,-0.0464737,0.04415135,-0.012165799,-0.05847893,0.027258558,0.036305536,-0.02029252,0.00735072,0.028597338,-0.013621802,0.0039333506,-0.024850292,-0.029044202,0.013258445,0.010721915,0.011228277,0.043875113,-0.08048844,0.042015452,-0.022364307,0.035637062,-0.021156097,-0.09596073,0.026159232,0.06431164,-0.013239759,-0.023249593,-0.030478356,0.05843859,-0.030983897,0.008358908,0.077500336,0.049036764,0.07261003,-0.021475168,-0.01133058,0.008080469,-0.0022449642,-0.043735586,-0.0018746365,0.049189433,-0.00079987966,-0.0007309278,0.08598283,-0.0364337,0.04772335,-0.021471107,0.03802112,0.024642745,0.030019782,-0.013263348,-0.011433162,0.009489395,0.045177735,0.03641974,-0.01929913,0.035928335,0.03061335,-0.04374572,0.020264108,0.051129617,-0.0072797476,-0.086432844,0.0403704,0.012619399,-0.011965084,-0.007233192,-0.04143482,-0.02514861,-0.007775382,-0.0026547944,-0.012113997,-0.017484747,0.020839494,-0.035370342,0.020222863,-0.023259234,0.050873484,0.0056015025,0.050870113,0.044051725,0.008721281,-0.016922476,0.037738975,-0.0103808865,-0.0016542991,-0.0049955216,-0.022632886,-0.012041524,0.04506551,-0.013758911,0.015415005,-0.03933293,0.06940677,-0.04201697,0.008864089,0.03421729,-0.021161418,-0.01107737,-0.02089623,0.009073775,0.0024230909,0.0029925497,-0.00977341,-0.027855877,-0.04434685,0.03520814,0.008103412,0.012177397,-0.020480089,0.045454,0.014158842,0.016730163,-0.042300742,-0.08358055,-0.0049199318,0.07485499,0.02111213,0.042204157,-0.008029327,-0.013084496,-0.020521257,-0.022471428,-0.015811548,-0.028464576,0.015335852,0.044143163,0.062046647,-0.017065147,0.06748861,-0.023315324,-0.014682916,-0.02344632,0.026585206,0.07671212,0.056326423,-0.06945559,-0.06189901,0.01353485,-0.00797005,-0.022418763,-0.0021271112,-0.011125574,-0.0342471,0.01391727,0.021056084,-0.016970107,0.0299466,0.00962777,-0.00036044093,0.017367838,-0.036534075,-0.01660704,-0.013595502,-0.001695944,-0.004724564,-0.025375163,0.005713723,0.060015265,0.0073467735,0.024373718,0.0012107014,0.014631032,0.077742755,-0.031902555,0.053552315,0.016350513,-0.039341133,0.024687994,0.040508974,-0.03818823,0.025704613,-0.009320129,0.032710247,-0.026801137,0.0058690044,0.012610428,-0.0007116513,0.013025151,0.014122942,0.005612645,0.03159334,0.016618831,0.003859642,-8.307686e-05,-0.10171384,-0.0029436515,-0.02495,-0.029834865,0.033591088,-0.08565454,-0.05167746,0.0022083002,0.006418583,0.018524608,-0.0042727194,-0.011400588,-0.07979425,0.023536801,-0.02223723,0.08439174,-0.07069938,0.005667746,0.023215115,-0.0040741726,0.09811083,0.022511572,0.022086252,0.0020975557,0.026715156,-0.024382442,0.04614455,0.057414923,0.029384367,-0.003294631,0.06936269,0.007955267,0.0030610466,-0.031456728,-0.054663617,0.0009613619,0.00027913947,-0.023686536,-0.055867393,-0.010603985,0.041322872,0.014130728,0.0057202927,0.0062116645,0.011630185,-0.014254353,0.05554474,0.009795733,0.015629569,0.0487285,0.010290823,0.034979615,0.019285126,0.08288151,0.0029424964,0.01691792,0.022676608,0.017724644,-0.0017134581,0.008348773,-0.0075006154,-0.0036542492,-0.07500465,0.0091558285,0.0029349832,-0.020879343,-0.02333968,0.059221905,-0.008722003,0.075120166,-0.023741947,-0.05747981,-0.028967148,-0.045782667,-0.012855561,0.02552913,0.00069046987,0.010638171,0.030119078,0.0057087652,0.057455573,-0.115252554,-0.008701588,-0.015233098,0.008640812,0.010708287,-0.03851389,-0.022666924,-0.027495721,0.007516559,-0.024282943,0.028515324,0.0037063789,-0.015215936,0.014596592,-0.008876491,0.0060170586,0.00964883,-0.008373029,-0.030390633,0.00017605507,0.013751853,-0.006282744,0.098688975,-0.013053879,-0.014294153,0.057428733,0.0005849724,-0.058351334,-0.01028017,0.039383356,0.049672812,-0.03629806,0.034745295,0.031795,0.003418749,-0.06361444,0.010734192,-0.022749608,-0.010891659,-0.023375817,0.04214196,0.05424774,0.0011824489,-0.032888938,-0.034392647,0.021379769,0.026903078,0.035514545,0.021356825,0.014998516,0.04769017,0.047941227,-0.023913035,-0.034315668,-0.00141933,-0.0074830456,0.022285294,-0.03970869,0.07460492,-0.00066114444,-0.009807725,-0.023632016,-0.025777379,-0.024824418,-0.0024003037,0.02413212,-0.027835403,-0.04710047,0.026092969,0.020873476,0.023686271,0.046572,-0.020221097,0.079165906,0.018607736,-0.014253873,-0.07576363,-0.07595649,-0.026564928,0.00040301267,0.017870698,-0.015341119,0.012920448,0.023824465,0.007622712,0.077141136,0.018681211,0.0022115016,-0.03959352,0.01104785,0.021281691,0.05134221,-0.018729638,-0.040944755,-0.022272022,0.063189484,-0.018384326,-0.04460303,-0.0003110025,-0.017958185,-0.032605402,-0.027779156,0.011686403,-0.010206218,-0.028413577,0.043377217,0.034412857,0.05619566,0.030053176,-0.0013244866,-0.067665145,-0.0064026807,-0.015476254,-0.019468678,-0.011864521,0.052246004,-0.02104177,-0.019036748,-0.06693096,0.037457746,-0.005932643,0.008874758,-0.022394286,0.043624446,0.009369521,0.03149301,-0.019890025,-0.040162988,0.07346834,-0.0110633215,-0.012710204,0.020462438,-0.0046338695,0.0037003157,-0.046229,-0.011533903,-0.031896573,0.03586624,-0.00026411374,0.063130416,-0.012796798,0.020940907,0.0005585069,-0.03915514,-0.04892704,-0.0154832555,0.018457973,0.05259671,0.03992884,0.009236247,-0.07082121,0.028266266,-0.032803964,0.054381635,-0.007395498,-0.011069492,-0.037018314,0.014577246]	Keywords: attention maps, computation cost, lazyformer, layer sharing\nKey Objects: attention maps, layers\nRefers to Images: None\nHypothetical Questions:\n- How does sharing attention maps impact the model's ability to capture complex relationships?\n- What are the potential drawbacks of sharing attention maps across layers?\n- In what scenarios would sharing attention maps be most beneficial?\n---\nSummary:\nLazyformer proposes a method to reduce computational cost by sharing attention maps across adjacent layers, effectively reusing calculated attention maps.\nOriginal Text:\nAs an extreme case, Lazyformer [159] proposes to share attention maps between a number of adjacent layers. This is equivalent to setting g (  ) to identity and switch the settings of w$\\_{1}$ = 0 , w$\\_{2}$ = 1 and w$\\_{1}$ = 1 , w$\\_{2}$ = 0 alternatingly. The benefit of this approach is that the attention maps are computed only once and reused several times in the succeeding layers, thus reducing the computation cost. Their pre-training experiments show that the resulting model remains effective while being much more efficient to compute.  \n4.5.3 Prior as Multi-task Adapters. Adapters are task-dependent, trainable modules that are attached in specific locations of a pre-trained network for cross-task efficient parameter sharing [108]. Pilault et al. [98] propose a Conditionally Adaptive Multi-Task Learning (CAMTL) framework that uses a trainable attention prior M ( z$\\_{i}$ ) that depends on task encoding z$\\_{i}$  R D$\\_{z}$\nContextualized Text:\nTo further optimize computation, Lazyformer introduces a technique where attention maps are shared between adjacent layers. This approach involves setting specific weights (w1, w2) and a function 'g' to allow for the reuse of calculated attention maps across subsequent layers, ultimately reducing the computational cost.	{"tags": ["efficiency", "architecture", "transformers"], "doc_id": "ac5ba16d-ed33-4ab0-8749-37e205983a7a", "summary": "Lazyformer proposes a method to reduce computational cost by sharing attention maps across adjacent layers, effectively reusing calculated attention maps.", "doc_type": "text", "entities": ["Lazyformer"], "keywords": ["attention maps", "computation cost", "lazyformer", "layer sharing"], "key_objects": ["attention maps", "layers"], "contextual_text": "To further optimize computation, Lazyformer introduces a technique where attention maps are shared between adjacent layers. This approach involves setting specific weights (w1, w2) and a function 'g' to allow for the reuse of calculated attention maps across subsequent layers, ultimately reducing the computational cost.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["How does sharing attention maps impact the model's ability to capture complex relationships?", "What are the potential drawbacks of sharing attention maps across layers?", "In what scenarios would sharing attention maps be most beneficial?"]}
a6cc485f-6332-4882-b555-a1ff3fb05cf5	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0124967825,0.015527938,0.006217607,0.04951145,-0.039083693,0.048604958,0.02498272,0.042440496,0.047737505,-0.009427266,-0.0020149478,0.0025527368,0.035155386,-0.020775031,-0.012871455,0.024050044,-0.0076119867,0.1010066,-0.017447723,-0.027688159,0.030254958,0.019616323,0.06658258,0.003620586,0.0119426735,0.032970138,0.04197273,0.011845321,-0.0161316,0.005154413,0.011205512,-0.068741985,0.03537788,-0.0046034376,0.03214623,-0.011538244,-0.0173309,-0.04888881,-0.0058546592,0.016057387,-0.01430657,0.04728058,-0.06118145,-0.014755732,-0.027389798,-0.03784269,-0.108279526,-0.0029606645,0.024053037,-0.016036319,-0.02052362,0.008224728,-0.034531146,-0.011589649,-0.055636387,0.01110996,0.004612713,-0.029188203,0.0034715137,-0.118788056,-0.00085471565,0.0148488125,-0.0061143166,0.030095309,-0.02059356,-0.015142457,0.02592096,-0.0011076503,0.021090388,0.118713565,-0.022304155,0.00115941,0.006285235,-0.022309672,0.11248043,0.050646603,-0.07204045,-0.005617304,-0.06446958,-0.01407696,-0.027404446,0.06709969,-0.028640263,-0.026100056,0.082514904,-0.024449095,-0.041522432,0.009421522,0.05722398,-0.059581485,-0.025298933,0.0014212915,-0.010252747,0.058205944,-0.045393668,-0.080112465,-0.036828194,-0.022503436,0.04457642,-0.040172704,0.0154335145,0.026209608,0.10929086,0.10220887,0.03598661,-0.036443472,-0.007195857,-0.042556826,0.014733072,0.026303213,0.0071910024,0.031934865,-0.05202913,0.004148716,-0.0318071,0.010386647,-0.035701476,-0.0074908533,0.025408033,0.005601552,0.0059533617,-0.010311112,-0.008111245,0.029262226,0.07447254,0.041445088,0.026500665,-0.019381734,-0.042222615,0.014801577,0.07272537,-0.02194077,-0.010767607,0.034602694,-0.02373614,0.02438257,0.082735546,0.0013565129,-0.01699667,0.018428372,0.01148065,-0.04016535,-0.053268883,0.014412387,-0.041291572,0.03589553,-0.060982026,0.008272634,-0.042181477,0.03868313,0.04153465,-0.026784679,0.01875955,-0.014835046,0.0011252048,-0.0066863624,-0.0049989754,-0.066838734,-0.01692663,-0.018506896,-0.034566667,-0.069090866,-0.0040890663,0.061581157,0.021347858,0.11782379,0.015284816,0.0008162539,0.024971964,0.044429872,-0.012744383,0.0044974075,0.0053979224,-0.045279033,-0.0016483975,-0.019873792,0.0044353595,-0.049705576,-0.005236186,-0.009695211,0.023194708,0.041390724,0.01187885,0.064808145,-0.045705415,0.027469061,0.038118575,0.04802241,0.00064677506,-0.050967537,0.009403584,-0.018610433,0.009067704,0.04384079,-0.017940426,0.033774696,0.035754185,-0.10391278,-0.028241672,-0.03381755,-0.014277176,-0.018919978,-0.020271989,-0.012119682,0.0252647,-0.014588005,0.023926819,-0.0064332993,-0.010454038,0.04706377,-0.027968554,-0.037719663,-0.0346356,0.054969102,0.05069378,-0.0191722,-0.003724777,0.038426045,0.020865392,-0.022712588,-0.031082172,-0.033604916,-0.04317877,0.016394932,-0.019416807,0.028655594,-0.019486476,-0.0048593343,0.021001808,-0.0024092686,0.034212615,-0.0044592996,-0.04993277,0.044790372,-0.024994219,0.031194568,-0.017156199,0.03931902,-0.033958387,0.03721035,0.042244606,0.061380118,0.0014629272,0.08991664,0.034069158,-0.0030577783,0.038060367,0.016208509,0.023510013,0.022971762,-0.0017196309,0.053230852,0.009146697,0.004003115,-0.007031164,-0.05217041,-0.0054258727,-0.00035558827,0.038830236,0.01314685,-0.00024460084,-0.026122583,-0.010625654,0.0012612651,-0.010000928,0.012947293,-0.0027605325,0.025239201,-0.021285051,-0.024228958,0.029358355,0.054720726,0.0042360076,-0.022292946,-0.036042888,0.026337838,0.014724869,-0.03751433,0.054052502,-0.030685589,-0.025310976,0.0071426057,0.03764234,-0.0029501647,0.029432729,-0.013342214,0.017389337,-0.021878777,-0.023924908,0.010813326,-0.024590477,-0.026539866,0.004767342,-0.014326361,-0.032431126,0.02224554,-0.007692275,-0.024723195,-0.026401369,0.025893606,0.107315466,-0.015279471,-0.0039956532,0.00880603,0.026151283,-0.005181192,-0.051549036,0.048140593,0.06571927,0.00045707394,0.024882931,-0.009147422,0.014401686,0.0058178846,0.04854363,0.08294811,0.0710106,-0.017800307,0.010302349,0.02990645,0.017380042,-0.010134807,0.00880249,0.013801384,0.052632652,-0.008377218,-0.02254425,-0.036373086,-0.08800426,0.082040615,-0.07755244,0.03275906,0.04781532,0.020637821,-0.029119126,-0.013589978,-0.00081264164,0.027188612,0.0027135499,-0.00743348,-0.019575149,-0.06797499,0.036580425,0.00018851984,-0.008368264,0.04060456,0.00035699617,0.0060474207,0.034427032,0.0065812333,-0.005473945,-9.555195e-05,0.01758973,0.032265127,0.024016572,-0.027545135,-0.02235355,-0.0018091052,0.004328694,0.009918397,0.043196395,-0.044094615,0.058813978,-0.046882175,0.051025216,0.0014549814,-0.08800954,0.013925836,0.057939447,-0.033073217,-0.0351126,-0.010934631,0.040635776,-0.050355352,-0.03303265,0.030864779,0.072972745,0.021632012,-0.02603848,-0.022994887,-0.024451643,0.004172566,-0.05219583,-0.016257398,0.067423336,0.0039919256,-0.008294398,0.09728543,-0.007651662,0.041095197,-0.06227588,0.027516698,-0.008221219,0.0002218248,-0.030916622,0.005242345,0.0108280005,0.04093674,-0.010841471,-0.031171847,0.032760806,0.0016364149,-0.055413563,0.032835804,0.03984248,0.015309196,-0.06581627,0.037760247,0.03787684,-0.0011288486,-0.0013225559,-0.046223026,-0.0098158885,-0.027780289,0.0053401627,-0.03045894,-0.02313613,0.020546053,-0.04345681,0.020354878,-0.005251973,0.032033354,-0.0074479203,0.06908268,0.058288537,-0.038372047,-0.0445082,0.005034036,-0.012746544,-0.041002322,-0.0014892421,-0.036464874,-0.006661494,0.049780756,0.002917385,0.013040181,-0.071394056,0.08573726,-0.024251716,0.0070514358,0.071109295,-0.019649537,0.0055726795,-0.023587676,0.02219688,-0.0028828122,0.031373315,-0.008703621,-0.035674375,-0.021078963,0.04351867,0.036410563,-0.01835261,0.00050157064,0.067192204,0.008273062,0.010308135,-0.007275855,-0.09218942,-0.014549951,0.006592009,0.03351546,0.019578202,-0.0010171761,-0.00277834,-0.01889792,-0.033071425,-0.031842936,-0.017529583,-0.028652607,-0.0087340465,0.025751907,-0.037004672,0.03722492,0.013354824,0.0054030754,-0.016799124,0.04284905,0.03472925,-0.0002844731,-0.0637698,-0.026917653,0.015450689,0.026281592,0.0030310438,0.00053377345,-0.042246666,-0.03697612,-0.038033105,0.018963667,-0.008818827,0.041843127,0.04225592,0.032213327,0.037784755,-0.05525399,0.015478724,0.009572118,0.002658205,-0.0076099806,-0.02895471,0.0016530991,0.07673528,0.018786265,-0.0064814403,0.054032803,0.015624971,0.031243237,-0.0013525754,0.078079104,0.011757505,-0.044797245,0.027969368,-0.015091844,-0.013838286,0.013006468,-0.003785929,0.0070904293,-0.024354275,0.039584402,0.006191712,0.022594038,0.03334287,0.02696379,-0.01776884,0.031780884,0.017725762,0.01544983,0.017046003,-0.072199374,-0.012068463,-0.008147445,-0.024185302,0.02677154,-0.06378387,-0.03473375,0.0071994164,-0.057546314,0.012579579,-0.020530844,-0.013507463,-0.027694771,-0.0053789252,-0.042245403,0.094209164,-0.07280296,0.011376887,0.012528194,-0.0069883144,0.07103356,0.020230317,0.016234858,0.014342846,0.023677303,0.010928162,0.043690074,0.056110032,0.023516832,0.004272549,0.04976323,0.04224512,0.021819847,-0.025787637,-0.042093616,-0.03616919,-0.035353333,0.036265455,-0.015043479,-0.028917197,0.01745304,0.0053935847,-0.012069899,-0.0009750995,-0.003296292,-0.010562148,0.06203331,0.032998398,0.017446045,0.06503918,-0.022726614,0.06683263,0.05022134,0.0737948,0.019764356,0.0057166014,0.016739305,-0.004870206,-0.005960348,-0.0032959338,-0.019593123,-0.03175667,-0.030831289,-0.0058310684,0.014372064,-0.027717784,-0.022760136,0.027443063,0.008025996,0.03660325,-0.041666683,-0.03854073,-0.03332544,-0.037619907,-0.052136183,0.035816357,0.008157306,0.003808967,0.028953426,-0.03768415,0.061964355,-0.07176107,-0.0098334495,-0.037864238,0.029309245,-0.0037233145,-0.020333752,-0.017541997,-0.012385573,0.014116229,-0.04311742,0.033098716,0.007623699,-0.0335321,0.009883936,-0.0009194362,-0.01707817,-0.012394148,-0.008231465,0.028911436,0.010373338,-0.0049787527,-0.020468164,0.07675755,-0.012069274,0.014387149,0.03320006,-0.020764988,-0.06971962,-0.038181,0.045883086,0.050377823,-0.038360443,0.028569523,0.038467404,0.011999984,-0.07670835,0.018046279,-0.010730186,-0.016794402,-0.008523799,0.05491539,0.070402965,-0.03111453,-0.06310681,0.01057411,-0.00095649983,0.0010621661,0.040138528,-0.001326686,0.06571806,0.043091502,0.057979625,0.00022521506,-0.058672965,-0.02995544,-0.026511032,0.0073471186,-0.038298175,0.065420866,-0.016479969,-0.01223582,-0.044422697,0.023248477,0.0071417014,0.049190663,0.03838768,-0.0067159547,-0.023477936,-0.029926462,-0.006980155,0.032195043,0.004765955,-0.03000984,0.0791767,0.005856747,-0.057624828,-0.06325974,-0.06726219,-0.0077434476,0.010617233,-0.010006551,-0.0038474796,0.009242114,0.002712658,0.019275328,0.07540198,-0.007185598,-0.035545766,-0.06499968,0.027253605,0.0423535,0.043072518,-0.017301317,-0.028625654,-0.083488636,0.0423716,-0.019468542,-0.05695187,0.04352567,-0.0014668098,-0.02587478,-0.032839328,0.006862042,-0.014104689,0.011770894,0.012992196,0.057691302,0.018583043,-0.001735461,0.009276181,-0.076889485,0.027582686,-0.027783299,-0.029825194,0.009352455,0.06822357,0.014958287,0.033568766,-0.021213563,0.0096826535,-0.019696087,-0.007642365,-0.008444059,0.07063921,-0.020483907,0.03728998,-0.043801967,0.0025323718,0.06116737,-0.0015482298,0.004787788,-0.007501216,0.024347845,0.019479433,0.008819728,-0.022467593,-0.066471584,-0.032322474,0.032505654,0.02069209,0.0014870218,-0.017926667,-0.034040052,-0.04660703,-0.038521893,-0.0076095383,-0.022974769,0.036901157,0.03968301,-0.008172378,-0.07332748,0.028892392,-0.03489166,0.031517148,-0.027385794,0.030898284,0.028247388,0.021313239]	Keywords: adapters, multi-task learning, attention prior, parameter-efficient, inductive knowledge transfer\nKey Objects: Attention Prior, Adapters, Task Encoding\nRefers to Images: None\nHypothetical Questions:\n- How do adapters contribute to parameter-efficient multi-task learning?\n- What is the role of task encoding in defining the attention prior?\n- Why is the attention prior formulated as a block diagonal matrix?\n---\nSummary:\nSome approaches utilize task-dependent, trainable modules called adapters to introduce a prior attention distribution based on task encoding. These adapters, formulated as a block diagonal matrix, are added to the attention scores of upper layers in pre-trained Transformers to enable parameter-efficient multi-task inductive knowledge transfer.\nOriginal Text:\n$$M ( z _ { i } ) = \\bigoplus _ { j = 1 } ^ { m } A _ { j } ^ { \\prime } ( z _ { i } ), \\ \\ A _ { j } ^ { \\prime } ( z _ { i } ) = A _ { j } ^ { \\prime } ( z _ { i } ) + \\beta _ { i } ( z _ { i } ),$$  \nwhere P denotes direct sum, A$\\_{j}$  R ( n/m )  ( n/m ) are trainable parameters, and y$\\_{j}$ , $\\_{j}$ : R D$\\_{z}$  R ( n/m )  ( n/m ) are are Feature Wise Linear Modulation functions [96]. A maximum sequence length n$\\_{max}$ is specified in implementation. The prior is formulated as a block diagonal matrix and added to the attention scores of upper layers in pre-trained Transformers to serve as an adapter for parameter-efficient multi-task inductive knowledge transfer.  \n4.5.4 Attention with Only Prior. Some works have explored using an attention distribution that is independent of pair-wise interaction between inputs. In other words, their models exploit only a prior attention distribution.\nContextualized Text:\nTo facilitate parameter-efficient multi-task inductive knowledge transfer, some models employ trainable modules called adapters. These adapters introduce a prior attention distribution based on a task encoding (z). The adapters, formulated as a block diagonal matrix, are then added to the attention scores of upper layers within pre-trained Transformers.	{"tags": ["NLP", "deep-learning", "transformers", "adaptation", "transfer learning"], "doc_id": "a6cc485f-6332-4882-b555-a1ff3fb05cf5", "summary": "Some approaches utilize task-dependent, trainable modules called adapters to introduce a prior attention distribution based on task encoding. These adapters, formulated as a block diagonal matrix, are added to the attention scores of upper layers in pre-trained Transformers to enable parameter-efficient multi-task inductive knowledge transfer.", "doc_type": "text", "entities": ["Transformers", "Feature Wise Linear Modulation"], "keywords": ["adapters", "multi-task learning", "attention prior", "parameter-efficient", "inductive knowledge transfer"], "key_objects": ["Attention Prior", "Adapters", "Task Encoding"], "contextual_text": "To facilitate parameter-efficient multi-task inductive knowledge transfer, some models employ trainable modules called adapters. These adapters introduce a prior attention distribution based on a task encoding (z). The adapters, formulated as a block diagonal matrix, are then added to the attention scores of upper layers within pre-trained Transformers.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["How do adapters contribute to parameter-efficient multi-task learning?", "What is the role of task encoding in defining the attention prior?", "Why is the attention prior formulated as a block diagonal matrix?"]}
e87835a2-c10d-40fb-b3c0-e846fbc1a6a6	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.015480785,0.023142247,0.033327665,0.026961885,-0.032899737,0.061682574,-0.011150189,0.0228174,0.01425656,-0.0305138,-0.0019353749,-0.015539417,0.0044371895,-0.009255925,-0.0108833555,0.020714022,0.057530735,0.067576006,0.010991675,-0.026088543,0.03880215,0.017802665,0.021832054,-0.04096612,0.0010226644,0.034114387,0.042131923,0.0053115115,-0.023357967,-0.025300432,0.05790776,-0.042507228,-0.012513577,-0.015904035,0.0075207264,-0.00012534241,-0.0032998489,-0.04205429,-0.034854773,0.001956134,-0.0044196458,-0.0068370034,-0.07433776,-0.017975215,-0.0024551891,-0.046262413,-0.056809817,-0.008844922,0.009795803,0.005738991,0.03616063,0.005839983,-0.005224234,-0.01946734,0.017871467,0.01608911,-0.029852418,-0.023911057,0.0045630676,-0.08945554,-0.031407405,0.013103503,-0.024521355,-0.046234228,0.026724303,0.015794955,0.014735277,-0.014666913,0.049562354,0.13391171,-0.012438855,0.027250743,0.0056536165,-0.04245673,0.08626269,0.028531164,-0.05592995,-0.042555045,-0.053099506,0.016556347,-0.026732009,0.08071304,-0.03966471,-0.06703518,0.0659894,-0.029826183,-0.056940503,-0.006542799,0.050873794,-0.05668384,-0.0410209,-0.019609032,0.03324161,0.09487973,-0.05814509,-0.062905066,2.3959872e-05,-0.0067307744,0.024548486,-0.117128305,0.02213941,-0.010550293,0.0632014,0.12450911,0.026695274,-0.02356657,-0.017162584,-0.046704642,0.01796925,0.020985527,0.0131053915,0.022573069,-0.07257898,-0.017267512,-0.02361547,0.02103493,-0.03109106,0.034875035,-0.0060721207,-0.011164056,0.016739037,-0.027693622,-0.00881494,0.03139963,0.087149546,0.026801258,0.051295564,-0.024260676,-0.030620616,0.0023430847,0.04057829,0.0057217395,-0.012436692,0.058031436,-0.025178216,-0.025581967,0.09090038,-0.0009146184,0.0073969243,-0.01590395,-0.022985287,-0.021894602,-0.038258877,0.024645776,-0.04266142,0.009306546,-0.064293936,0.023353172,-0.008491474,0.06474437,0.027942572,0.009751436,0.014861637,-0.016212689,-0.021397647,0.016039,0.01261078,-0.086635284,0.041983098,-0.023570836,-0.022516266,-0.058571257,0.016158191,0.07201516,0.045041416,0.078030325,0.020947231,-0.048844744,0.011672834,-0.003927362,-0.021730395,-0.020516546,-0.010169057,-0.01090461,0.00681264,-0.0064300774,0.019453682,-0.014792142,0.010065544,-0.025094312,-0.041456904,0.031266905,-0.029906213,0.03918979,-0.022848196,0.004971681,0.022692192,0.09826038,-0.0266776,-0.023918247,0.002277405,-0.041077677,-0.0039944104,-0.00017909543,0.00010260147,0.022243302,0.031695418,-0.055314474,-0.029266734,-0.036463562,0.017341334,-0.014832647,-0.04285179,0.01750289,0.012723478,-0.033255268,0.027469909,0.010299677,-0.013704203,0.022642065,0.008259406,-0.030393759,-0.0133125875,0.027019072,0.015544636,0.014040308,-0.008082308,0.034585554,0.019591276,-0.034264095,-0.06652129,0.021349354,-0.005204541,0.05065987,-0.034461513,0.05452035,-0.015537721,-0.00454812,-0.017023968,-0.0015753856,0.028019717,0.044226054,-0.04161944,0.011691802,-0.021215308,0.048434146,0.029449478,0.024266677,-0.024835223,0.01655678,0.091189735,0.014799602,-0.0020662996,0.07035169,0.0024046218,-0.02957029,-0.021629535,1.0529043e-05,-0.01744841,0.003477723,-0.016380586,0.04878952,0.034683403,0.022748131,-0.028719533,-0.038613584,-0.044044394,0.020198015,0.08669235,0.006905805,0.032385457,0.002262243,-0.009068575,0.011722972,-0.0007003348,0.0032430245,0.014610852,0.006737932,-0.016920673,-0.03283365,0.016056722,0.049214613,0.035679284,-0.018091764,-0.023639236,0.04425154,-0.021781983,-0.0075578224,0.026102494,-0.055739004,0.030732894,-0.043116946,0.018249206,0.004609232,0.031944454,-0.0066752583,0.012991065,-0.013905298,-0.03013024,0.008864847,-0.022670336,-0.044129614,0.028162044,-0.00625639,0.013487806,0.031384025,-0.0041999184,-0.04982608,0.0097162295,0.030637179,0.09134321,-0.025990462,-0.021967823,0.013841015,0.035467662,0.021015806,-0.013860314,0.028296795,0.06178197,0.050210938,-0.0054053585,-0.015410221,0.02579862,-0.007725499,0.02443245,0.04775518,0.08061538,-0.051571473,-0.00870569,0.013825208,0.034068838,0.012366931,-0.004128333,0.024786646,0.015813615,0.0041237366,-0.0017764639,-0.037822682,-0.075684346,0.07799188,-0.10350406,0.026786797,0.05024635,0.009544441,0.033124782,-0.020048458,-0.015820993,0.0034682236,0.010643728,-0.021653503,-0.017159712,-0.05252333,-0.002174458,0.009786377,-0.0434261,0.019710401,-0.0269311,0.03309989,0.036848936,-0.006469422,-0.021667285,-0.03145431,0.023373928,-0.010192303,0.0066202804,-0.019567335,-0.0059742643,0.021846255,0.007888264,0.0016895109,0.067796774,-0.08882393,0.03609178,-0.037270676,0.008521214,-0.01761976,-0.039250355,0.029080393,0.043205004,0.019130008,-0.022174323,-0.020183355,0.06532685,-0.05748787,-0.043466754,0.03425497,0.055644583,0.022662029,-0.00047735262,-0.004802511,-0.01059367,0.0058088605,-0.029487101,-0.054867927,0.027167682,0.005193129,0.0031837586,0.12328309,0.017463297,0.07009693,-0.044552214,0.02542163,0.015927553,0.027996913,-0.070563465,0.021880966,0.023743577,0.06260086,0.026483303,-0.0037109314,0.032402445,0.006611112,-0.047341987,0.018584987,-0.003539218,0.044542555,-0.10214249,0.03964172,0.0145905465,0.014683304,0.013372203,-0.02699018,-0.033283908,-0.023970073,-0.015419877,-0.023035955,-0.0010872438,0.05237898,-0.03646732,0.016800072,-0.010646204,0.035931986,-0.0060090525,0.045650855,0.019624103,-0.031718325,0.007518146,0.056647237,-0.016218847,-0.03871922,0.024151122,-0.031471897,0.008167593,0.01730782,-0.02634869,-0.008543587,-0.05287922,0.0764883,-0.049265202,0.030432511,0.035010483,-0.015650652,-0.0055869287,-0.0026592321,0.033206984,-0.012099519,-0.0060434625,0.047065437,-0.02440244,-0.00435217,0.0071116,0.021549832,0.034178317,-0.015328504,0.011682972,-0.038287263,0.0080058575,-0.0012924139,-0.02548053,0.011454507,-0.00017672413,0.005807345,0.03667211,-0.010155122,0.0050898762,0.0010939185,-0.0009068224,-0.045673683,-0.030541567,-0.016144618,0.0026232544,0.06534929,-0.040890634,0.0458251,-0.00033477496,-0.02075802,0.02565039,0.0074518067,0.03104216,-0.0058434876,-0.090398364,-0.027124409,0.01793925,0.055982415,0.009460482,0.017540392,-0.01032149,-0.04515608,-0.034638055,0.019183276,0.0021690368,0.0038153364,0.06438927,0.030198317,0.04241038,-0.05013577,0.013253126,0.0024515323,-0.013667706,-0.05441118,-0.03297722,-0.0048640477,0.032452352,0.015677894,-0.034445208,0.02239488,0.006234799,0.034234025,-0.014060543,0.08437643,0.0010902851,-0.042532455,0.019428607,0.02619235,-0.074230514,0.04733987,0.01479149,-0.011601319,0.013579242,0.06493214,-0.015518384,0.018480118,0.025450025,-0.0069779446,-0.0012587466,-0.018393278,0.017639171,-0.002970234,0.026918221,-0.07100908,0.0046902765,-0.039539468,0.017943611,0.013346417,-0.06353596,-0.07784354,0.030770032,-0.041207206,0.028508255,0.02504549,-0.012230071,-0.016183166,0.0110818315,-0.031197816,0.07647083,-0.024032671,0.031920828,0.024056317,-0.0018690617,0.11174821,0.043848876,0.022270754,-0.015129954,-0.015633894,0.017835572,0.02133012,0.06615717,0.025951032,-0.014812819,0.06054363,-0.01292565,-0.003508593,-0.03741454,-0.02639385,-0.042686522,-0.029893676,-0.019096628,-0.03397929,-0.020143423,0.030237071,0.006189558,0.00862699,-0.029219856,0.024448002,0.011131089,0.06345178,0.037099753,-0.014662888,0.009194647,0.005708199,0.04669193,0.027618889,0.056165427,-0.008137387,0.01798513,0.040281065,0.024438113,0.0059870197,-0.017701566,-0.04785492,-0.015133375,-0.03634731,0.019567477,-0.033709195,-0.018916482,-0.04915754,0.02953704,0.034142006,0.033186067,0.0072462293,-0.05597006,-0.050887898,-0.037472066,-0.008583427,0.034558747,0.06018251,0.00456788,0.0048829336,0.0036931608,0.053099617,-0.082682155,0.026060846,-0.042407982,0.07728574,0.0030737084,0.019083574,-0.025116872,-0.021464985,0.029117988,-0.038922742,0.036378875,-0.022997024,-0.014221002,0.03348359,0.040105,0.0040114806,-0.02011178,0.009779061,-0.022304755,0.004941674,-0.0033161372,-0.021608697,0.09310246,-0.047936693,0.0016265607,0.041259054,-0.002308643,-0.08925222,-0.027671244,0.009620387,0.053848717,-0.017618733,0.017217478,-0.0026580244,0.004255993,-0.019935189,-0.01838007,-0.08534041,0.035720915,0.0035513688,0.0009953366,0.034333274,-0.05118104,-0.059316695,0.0041682483,-0.00721798,-0.022534827,0.021803703,0.059253342,0.051999107,0.05113971,0.052706346,6.506139e-05,-0.09724049,-0.011676246,-0.031497743,0.069076315,-0.06150841,0.06273215,0.022320287,-0.046739966,-0.03009061,-0.00433828,-0.026980931,-0.0430145,0.007277505,-0.014549754,-0.019852245,-0.008018274,-0.009338716,0.01971394,-0.0037819822,-0.03301368,0.10689144,0.0016754587,-0.03354319,-0.04316792,-0.08140746,0.014484552,0.009378609,0.0077542304,-0.013310243,0.02317911,0.015678328,-0.021721054,0.056712262,0.0032434564,-0.039543595,-0.0045774616,0.007240794,0.015668338,0.012122064,-0.013446531,0.035165723,-0.12493436,0.0073373066,-0.038671643,-0.029427283,0.017388975,-0.045055855,-0.019735426,-0.022032518,-0.009349994,-0.018513735,0.017543543,0.0048854775,0.048663437,0.029518919,0.0042782146,0.011680562,-0.07498175,-0.0021536483,-0.04028199,-0.017808001,-0.04214882,0.061470073,0.0131035065,-0.006696385,-0.035615567,0.018982766,0.0063107125,0.0029788911,0.030357618,-0.0044606724,0.018956967,0.008792601,-0.026520127,-0.026387196,0.04585348,0.0036667113,0.030184448,0.026444562,0.048278946,0.042433623,-0.00018982335,0.0069310986,-0.043556463,-0.09450172,0.00722856,0.01253225,-0.038869858,0.029955767,0.015765592,-0.04071731,-0.0035883887,-0.0033575157,0.01875307,0.022991166,0.014395149,0.012900862,-0.03051406,0.0043185884,-0.009107415,0.03443287,-0.004738122,-0.02639486,0.027058924,0.0066070007]	Keywords: average attention network, discrete uniform distribution, Transformer decoder, parallel training, RNN\nKey Objects: attention network, discrete uniform distribution, Transformer decoder\nRefers to Images: None\nHypothetical Questions:\n- Why would using a discrete uniform distribution for attention simplify the decoding process?\n- What benefits does a cumulative average attention mechanism offer over traditional pairwise attention?\n- How does the addition of a feed-forward gating layer improve the expressiveness of the average attention network?\n---\nSummary:\nZhang et al. introduced the average attention network, a Transformer decoder variant that utilizes a discrete uniform distribution as the only source of attention, which allows for parallel training and RNN-like decoding, avoiding quadratic complexity.\nOriginal Text:\nZhang et al. [164] design an efficient Transformer decoder variant called average attention network that uses a discrete uniform distribution as the sole source of attention distribution. The values are thus aggregated as a cumulative-average of all values. To improve the expressiveness of the network, they further adds a feed-forward gating layer on top of the average attention module. The advantage of this approach is that the adapted Transformer decoder can train in a parallel manner as usual Transformers do and decode like an RNN, thus avoiding the O ( T $^{2}$) complexity in decoding.\nContextualized Text:\nTo explore attention distributions completely independent of input interaction, Zhang et al. [164] designed an efficient Transformer decoder variant called the average attention network. This network uses a discrete uniform distribution as the sole source of attention, aggregating values as a cumulative average. To enhance expressiveness, a feed-forward gating layer is added. A key advantage is that this adapted Transformer decoder can be trained in parallel and decodes like an RNN, thereby avoiding the O(T) complexity common in decoding.	{"tags": ["architecture", "efficiency", "decoding", "transformer"], "doc_id": "e87835a2-c10d-40fb-b3c0-e846fbc1a6a6", "summary": "Zhang et al. introduced the average attention network, a Transformer decoder variant that utilizes a discrete uniform distribution as the only source of attention, which allows for parallel training and RNN-like decoding, avoiding quadratic complexity.", "doc_type": "text", "entities": ["Transformer", "RNN"], "keywords": ["average attention network", "discrete uniform distribution", "Transformer decoder", "parallel training", "RNN"], "key_objects": ["attention network", "discrete uniform distribution", "Transformer decoder"], "contextual_text": "To explore attention distributions completely independent of input interaction, Zhang et al. [164] designed an efficient Transformer decoder variant called the average attention network. This network uses a discrete uniform distribution as the sole source of attention, aggregating values as a cumulative average. To enhance expressiveness, a feed-forward gating layer is added. A key advantage is that this adapted Transformer decoder can be trained in parallel and decodes like an RNN, thereby avoiding the O(T) complexity common in decoding.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["Why would using a discrete uniform distribution for attention simplify the decoding process?", "What benefits does a cumulative average attention mechanism offer over traditional pairwise attention?", "How does the addition of a feed-forward gating layer improve the expressiveness of the average attention network?"]}
e7d81646-c373-4f7f-b917-243f1620f9b9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.00070968515,0.036806315,-0.0021483304,0.029586129,0.0064850356,0.056601003,0.0032708284,0.049743902,0.03764436,-0.017029533,-0.006988922,-0.017763665,0.0033509163,-0.010150621,0.012920015,-0.005776417,-0.01189818,0.06994586,0.0029080615,-0.03670514,0.04742125,-0.00013392638,0.023975987,0.009131162,0.039389726,0.05043415,0.012574707,-0.0058491644,-0.028412621,0.012534081,0.06448831,-0.044063274,0.002246607,0.0034543762,0.035067543,-0.028414544,0.04860363,-0.05167808,0.022492882,-0.028476687,0.0058862627,0.059650876,-0.042943824,-0.038351845,0.015814906,-0.0139509365,-0.07056368,-0.02670103,-0.01299927,-0.029446986,0.015073964,0.032765068,-0.009786435,-0.03531929,0.018079067,-0.016374277,0.024965275,-0.007510897,-0.002645253,-0.10063097,-0.011739947,-0.0029150625,-0.029957656,0.0073576733,0.04848456,-0.038728237,0.026270341,-0.016947595,0.04244619,0.1362003,-0.010779601,0.034935094,-0.0110277925,-0.08735794,0.07527931,0.043354433,-0.031648632,-0.0041366327,-0.06979033,-0.042682715,-0.02263986,0.05941628,-0.0470727,-0.056902334,0.064714886,-0.03356813,-0.044548526,-0.013628552,0.041011803,-0.03018518,-0.016767146,-0.011421013,-0.001108976,0.059393298,-0.07404231,-0.054169916,-0.023655359,0.026725944,-0.0072007566,-0.101598576,0.03674377,-0.00418891,0.09238967,0.1350117,0.026010916,-0.031049516,0.0029622777,-0.025363853,0.004628851,0.013430113,-0.013080797,0.024701938,-0.081243046,-0.037525937,-0.028195988,0.009883184,-0.06257305,-0.0032347369,0.0151606565,-0.01423856,0.032860577,-4.7864778e-05,-0.031275682,0.051397588,0.08974491,0.032357328,0.06501167,0.0029874567,-0.0063589523,0.015474225,0.05171505,-0.0150466915,-0.029720785,0.049484342,-0.018401822,-0.02673973,0.07452362,-0.04321216,0.008664791,0.03260761,-0.017604947,-0.016848944,-0.042864755,0.028704546,-0.033687096,-0.012491373,-0.06410443,0.032686844,-0.051193126,0.022042532,0.044851683,-0.01611273,0.0072687585,-0.012824252,0.0048213713,0.0015797,-0.0022670026,-0.054110076,0.0028563177,-0.015457575,-0.049830567,-0.037685733,0.013364637,0.042682074,0.049644627,0.068407245,-0.018072916,-0.03945855,0.0037216896,0.04238842,-0.047134317,-0.0019709908,0.012004731,-0.012181216,0.02009022,-0.015359198,0.014309981,-0.040604044,-0.01169223,-0.033982784,-0.021803247,0.06018855,0.0062792446,0.02077894,-0.05055006,0.029036231,0.007186416,0.059354845,-0.011200214,-0.015964882,0.017536497,-0.044136573,0.013972113,0.0069726696,0.027921427,0.049011502,0.0393723,-0.06767318,0.006640233,-0.04952859,-0.02869735,0.002407683,-0.036138233,-0.0029309664,-0.013641936,-0.016914694,0.05161007,0.00040927794,0.012124415,0.024805708,0.012511166,0.0209071,-0.0112640485,0.048256177,0.019126967,0.01774765,0.0041384213,0.028380806,0.03188987,-0.053122006,-0.042029068,0.0036162236,-0.034644894,0.0461374,-0.013400466,0.040283125,-0.011847907,-0.005179764,0.0033524923,-0.026984842,0.023572067,0.03783804,-0.0445412,0.035284534,-0.027018044,-0.0066674063,-0.003389276,0.0062744715,-0.021696806,0.017991947,0.053545866,0.0014601927,-0.009000634,0.081192166,-0.024062907,0.014976583,0.027111959,0.060684327,-0.050453577,0.035164144,-0.031752743,0.060318563,0.047224943,-0.0153683135,0.0012619756,-0.071164384,0.021389132,-0.014913796,0.033608742,-0.01247645,0.013040011,-0.0064506326,0.016228097,0.023425305,0.022560595,-0.0063530705,0.028096609,0.021520821,-0.035541907,-0.006772109,0.028643552,0.06331105,0.011065571,0.00043261985,-0.050553273,0.017256232,0.039809734,-0.0036419327,-0.0013767536,-0.024174027,-0.035210468,-0.074499,0.018408827,-0.002909401,0.012390724,0.040555254,0.027160225,0.0066978703,-0.04071688,-0.008243637,-0.026533652,-0.04853982,-0.007748104,-0.0042269356,-0.014514101,0.017506747,0.007288084,-0.04189573,0.024060793,0.04781865,0.07202458,-0.012604798,0.028355254,0.029953683,0.018154144,0.03101753,-0.027308539,0.06542518,0.041025944,0.009860079,0.00062783546,0.010855662,0.0068627265,0.017228734,0.02238098,0.057104316,0.101002075,-0.022658136,-0.02891873,-0.009909955,0.035519075,-0.023602733,0.0022567066,0.031567108,0.02260286,-0.01771916,0.005332748,-0.0071508316,-0.083577126,0.04330793,-0.072477214,0.075216666,0.036371797,0.022324841,0.021696854,-0.023371708,-0.017349435,-0.022541113,0.034663986,0.015454982,-0.007637926,-0.019167906,0.025121668,-0.018088054,-0.02592035,0.030977469,0.00549889,-0.03403163,0.031510565,-0.014082891,0.015483124,0.033086315,0.034035224,-0.0062092547,0.027761504,-0.030416267,-0.028156947,0.028957056,-0.022773545,0.007284464,0.07466634,-0.05359673,0.0531661,-0.06982387,0.033279482,0.015991624,-0.08169002,0.055667035,0.06312702,-0.016396448,0.00043059015,-0.058793347,0.058377065,-0.042490624,-0.028894851,0.033073694,0.0446743,0.04341031,-0.01799064,-0.011745851,-0.016475558,-0.025513707,-0.032062862,-0.0206915,0.035841458,-0.010556684,-0.0014462715,0.11753417,-0.0038630865,0.017487908,-0.06394274,-0.008391464,0.010536096,-0.0012950313,-0.02170532,0.026023405,0.03184358,0.04951483,0.0086556785,-0.028563596,0.017059904,0.013087261,-0.05038204,0.015908755,0.044501916,0.030283729,-0.058456372,0.04882529,0.016726352,0.031474907,0.02885831,-0.02919539,-0.03934025,0.0069361962,0.010223896,-0.025650539,0.007828873,0.0012082191,-0.057809263,-0.008505591,-0.038546156,0.031936742,0.012485527,0.04831961,0.036336947,-0.032680564,-0.037650004,0.03088404,-0.059079923,-0.070692465,0.017621852,-0.04159724,-0.009490221,0.022458548,-0.020484675,0.02774901,-0.061502323,0.09747598,-0.07693223,-0.0048417165,0.043664835,-0.013008417,0.012893686,-0.03335567,0.007967561,-0.01799844,0.05785697,0.016650515,-0.029710002,0.007462106,0.045812465,-0.0007888983,0.033403922,-0.050653424,0.019070989,0.006249535,0.02156294,-0.022319177,-0.043836843,0.019707484,-0.003665522,0.01843631,0.010080788,-0.00071664795,0.0009506585,-0.033095967,0.008929138,-0.0335676,-0.013992466,-0.03648,0.013009939,0.03728067,-0.02652673,0.04362819,0.0044477987,-0.018919414,0.0043047555,0.04447377,-0.02508789,0.04021024,-0.080965,-0.040628843,0.0004836528,0.01619714,-0.014147214,0.0062750685,-0.027950216,-0.011495897,-0.045501743,0.062621914,0.016257914,-0.0012001406,0.034693114,0.0038253837,-0.0044631762,-0.08816332,0.018451728,-0.0046621016,-0.006163018,-0.024288746,-0.04353199,0.029839734,0.05764886,0.0063359435,-0.0155297555,0.052356053,0.007774968,0.007569163,-0.027059648,0.024347374,0.046703335,-0.017258931,0.024745803,0.041652665,-0.023331145,0.050510988,0.001501122,0.021472322,-0.012582375,0.06795262,0.021283615,0.0388932,0.06011949,-0.012777166,0.002035688,0.039504394,-0.0058284686,0.03710013,0.04754208,-0.08425452,0.029397292,-0.028886754,-0.024593769,-0.0001246855,-0.055741776,-0.08100374,0.025833372,-0.062298138,0.01939508,0.009165256,-0.016033465,-0.054729406,0.026038691,-0.05272268,0.0492551,-0.06683019,0.018161435,0.042675603,0.010525,0.10055237,0.041944075,0.013405198,-0.015413546,-0.007579245,-0.0133051425,0.008624199,0.08051307,0.015045423,0.00039736924,0.061393503,-0.024605673,0.017546184,-0.031199267,-0.02141086,-0.04041863,-0.004461003,-0.008238873,0.017805796,-0.018505407,0.034116738,0.01046354,0.016996268,0.008568591,0.05644359,0.01900681,0.07168416,0.00019408691,-0.028847087,0.0075063654,0.033049647,0.05644025,0.037259623,0.07242818,-0.0071217916,0.034456376,0.009863508,0.0047465228,-0.014831234,-0.0016929253,0.0018660019,-0.027746867,-0.004254568,0.025994005,-0.011323887,-0.018166235,-0.041372694,-0.021445677,0.010089949,0.03771066,-0.026261494,-0.07861064,-0.026790358,-0.043937963,-0.03268069,0.037250645,-0.0037012894,-0.014104666,0.07082114,-0.030436706,0.08398698,-0.07236721,0.020316564,-0.029997278,0.023179688,-0.029632783,0.0042890664,-0.0040773256,-0.0004490131,-0.0141864475,-0.05527326,0.038234677,-0.012967034,-0.031152464,0.0030330215,-0.0051762466,0.003966633,-0.012823358,-0.012867659,0.0050302204,-0.008774396,0.017161755,-0.035469066,0.08456573,-0.056026563,-0.03285807,0.04730073,-0.0066639856,-0.05112725,-0.045741707,0.03675737,0.04825554,-0.03487701,0.02679664,-0.0132120885,0.021622818,-0.036590107,-0.00653192,-0.034491442,0.019592036,-0.013628562,0.04896448,0.057219543,-0.015888782,-0.045715015,-0.006092688,-0.022191558,-0.0074146683,0.04721617,0.06665363,0.013794676,0.07482154,0.04025585,-0.03813783,-0.05375632,-0.042196553,-0.0525279,0.025849853,-0.024403887,0.038259063,-0.02371661,-0.041287515,-0.001443934,0.0064364085,-0.010665039,-0.018109117,0.04906519,0.004543147,-0.03788684,-0.022805722,-0.021358997,-0.015204887,0.023135526,-0.03456061,0.08463175,0.0514571,-0.06777133,-0.042864975,-0.07795479,0.008603525,-0.006898193,-0.016807416,-0.0042424346,0.0045858803,-0.0022892796,-0.018107275,0.021323258,-0.009527486,-0.03917758,-0.017635992,0.01166434,0.0412421,0.040594332,-0.013765842,-0.021190781,-0.07393583,0.0026391465,-0.03336767,-0.036919594,-0.015995858,-0.031078886,-0.0010880319,-0.03566503,0.010497585,-0.008791759,-0.017960485,0.024883535,0.038771585,0.022155568,0.0013532046,0.006439729,-0.07252807,0.015094666,-0.047163267,-0.022648592,-0.004168808,0.06860483,0.029317524,-0.01599875,-0.021528076,0.0113332905,0.008758128,0.010718768,-0.04389544,0.02297797,-0.02599874,-0.019037904,-0.04495072,0.013716844,0.036139593,-0.022317868,0.0011760578,0.036388233,0.036752027,0.03851318,0.008708045,0.015025544,-0.030106612,-0.034846567,-0.012758808,0.01245626,-0.02519892,0.00874028,0.002466863,-0.04390335,-0.03221032,-0.04465275,-0.0021049096,0.008547836,0.04404717,0.034267377,-0.06913193,0.021918891,-0.011798916,0.012801321,-0.0062439013,0.008522133,0.011133194,-0.0027717948]	Keywords: Gaussian distribution, attention distribution, machine translation, self-attention\nKey Objects: Gaussian distribution, attention distribution, self-attention\nRefers to Images: None\nHypothetical Questions:\n- Why would researchers choose to eliminate generated attention altogether?\n- How does the use of a Gaussian distribution contribute to focusing attention on a local window?\n- What are the potential advantages of using hardcoded attention compared to traditional attention mechanisms?\n---\nSummary:\nYou et al. [161] explored using a Gaussian distribution as the sole attention distribution, eliminating the need for generated attention, and demonstrated comparable performance to baseline models in machine translation tasks.\nOriginal Text:\nYou et al. [161] utilize a Gaussian distribution as the hardcoded attention distribution for attention calculation. The intuition is very similar to Yang et al. [156] and Guo et al. [42] in that attention distribution should be focused on a certain local window. Distinctively, they drop the generated attention completely and use only the Gaussian distribution for attention computation. In this approach, the mean (central position) and variance are designed to be hyperparameters. The experiments show that the hardcoded attention, when applied only to self-attention, can achieve comparable performance to the baseline model in machine translation tasks.\nContextualized Text:\nResearchers, such as You et al. [161], have investigated using pre-defined attention distributions to enhance Transformer models. Specifically, they utilized a Gaussian distribution as the sole attention distribution, dispensing with the typically generated attention. This approach, drawing on the intuitions of earlier works by Yang et al. [156] and Guo et al. [42], posits that focusing attention on a localized area can be beneficial. The study found that this hardcoded attention method, applied solely to self-attention, achieved results comparable to those of baseline models in machine translation tasks.	{"tags": ["NLP", "transformers", "attention mechanism", "machine translation"], "doc_id": "e7d81646-c373-4f7f-b917-243f1620f9b9", "summary": "You et al. [161] explored using a Gaussian distribution as the sole attention distribution, eliminating the need for generated attention, and demonstrated comparable performance to baseline models in machine translation tasks.", "doc_type": "text", "entities": ["You et al.", "Yang et al.", "Guo et al."], "keywords": ["Gaussian distribution", "attention distribution", "machine translation", "self-attention"], "key_objects": ["Gaussian distribution", "attention distribution", "self-attention"], "contextual_text": "Researchers, such as You et al. [161], have investigated using pre-defined attention distributions to enhance Transformer models. Specifically, they utilized a Gaussian distribution as the sole attention distribution, dispensing with the typically generated attention. This approach, drawing on the intuitions of earlier works by Yang et al. [156] and Guo et al. [42], posits that focusing attention on a localized area can be beneficial. The study found that this hardcoded attention method, applied solely to self-attention, achieved results comparable to those of baseline models in machine translation tasks.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["Why would researchers choose to eliminate generated attention altogether?", "How does the use of a Gaussian distribution contribute to focusing attention on a local window?", "What are the potential advantages of using hardcoded attention compared to traditional attention mechanisms?"]}
db59608c-cf5c-4328-ad24-20bbacd5c4b0	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02540162,0.05042312,0.00528247,0.069856,0.0026745144,0.07675768,0.007927921,0.069696225,0.024311077,-0.026850846,-0.038523,-0.00978946,0.008941924,0.019195283,-0.0045411596,-0.0032058228,-0.0058608656,0.07852121,3.5807625e-05,-0.019223128,0.016818376,0.039557572,0.008318661,0.008707858,0.0073760874,0.038399607,0.056244556,-0.015586207,-0.03309179,-0.03164446,0.042269066,-0.030737966,0.0119298315,0.0029934093,0.018058704,-0.05170796,0.0619793,-0.02074904,-0.02668719,0.0069952975,-0.007775126,0.043863513,-0.02597283,0.027372317,0.0202841,-0.04633701,-0.08891698,-0.05125345,0.02146478,-0.015998425,0.005916738,0.003770129,-0.040760662,-0.029387498,0.02221619,0.017701274,0.036190014,-0.028818248,-0.007500928,-0.11176343,-0.03636547,0.0046827164,-0.028187817,-0.008584293,0.0660573,-0.033566095,0.00086260977,-0.030894008,0.023306862,0.1408995,-0.00065389444,0.044952158,-0.017245723,-0.029134087,0.12708122,0.016564095,-0.032219127,0.0032692135,-0.07785677,-0.004065474,0.004077293,0.056094937,-0.069341004,-0.016408484,0.07483158,-0.0027849502,-0.07972968,0.0129375355,0.04616375,-0.04612906,-0.01752368,-0.040987145,-0.014395807,0.015337023,-0.039104026,-0.056672186,-0.00021165286,-0.020402793,0.023695482,-0.08108565,0.024655271,-0.010002589,0.10454307,0.09547921,0.001781055,-0.02600291,0.0071739377,-0.054047484,0.012110689,-0.0030366573,-0.01725517,0.05516619,-0.042705335,-0.0060877954,-0.049110837,0.0073697995,-0.022650039,0.016272489,0.0136007015,-0.0020532825,0.026179496,-0.03486915,-0.01030533,0.01221214,0.04776872,0.043832544,0.030685449,-0.005681682,-0.03715705,0.018480387,0.04189535,0.020810489,-0.008409397,0.009944977,-0.043541722,0.018618062,0.045503575,-0.038949028,0.00602576,0.012169797,0.02028503,-0.05121456,-0.055050153,0.01829074,-0.03063046,-0.004762393,-0.07348503,0.019928144,-0.042708308,0.06494978,0.025743384,0.037227165,0.04066055,-0.031074729,0.00025521856,-0.0013879063,-0.0040547797,-0.040169053,0.0029809375,-0.04838806,-0.033967976,-0.010284107,-0.04407077,0.03066086,0.02722089,0.10815627,-0.0012133732,-0.010729635,0.037400484,0.03153803,-0.021799598,0.007204154,0.019034285,-0.013897185,-0.015859967,0.0012833909,0.0117794,-0.03383424,0.023271073,0.0061590103,-0.0008886598,0.010055341,0.012075009,0.027470736,-0.052478872,0.0328726,-0.002787971,0.053106256,-0.045216486,-0.011177219,0.034304008,-0.0092679495,-0.03895454,0.022304982,-0.0046336134,0.05009628,0.069439895,-0.07628442,-0.036437258,0.008388884,-0.020305326,0.0025991432,-0.025023464,-0.011791567,-0.018805189,-0.042916,0.006450832,-0.056708932,0.00083087233,0.0053862804,-0.023303397,-0.007173751,-0.015587922,0.043255102,0.025218632,0.03483084,-0.012569065,0.05811678,0.0014066652,-0.04363959,-0.0592098,0.0044888314,-0.04614898,-0.02158479,-0.0002987451,0.011851866,-0.025707534,0.022576774,-0.004481091,-0.04985136,0.035776958,0.049919073,-0.020983629,0.018322265,-0.04316263,0.058097538,-0.024470601,0.040318776,-0.012068059,0.02879129,0.046659872,0.04537063,-0.032941885,0.067399815,0.02789922,-0.018447252,0.052289516,0.07064527,-0.02675983,-0.026374973,0.0068914485,0.054634493,0.03700375,-0.02130748,-0.010007136,-0.040771205,0.0075388392,-0.0058218576,0.03626298,-0.013026437,-0.0152583765,-0.014560049,0.0020444666,0.040551227,-0.0039255903,-0.019634407,0.043066412,-0.007376044,-0.0355987,0.007566728,0.055950653,0.01477111,0.03488623,-0.016352303,-0.05045989,0.05534675,-0.014924525,0.0036444657,0.044620827,-0.026881125,-0.03973868,-0.026319839,0.011578258,0.0076319138,-0.009187786,0.015938401,0.024075449,-0.03141905,-0.045406565,-0.003976876,0.0011839798,-0.040416215,0.03138664,-0.00548809,0.01637071,-0.0050569065,-0.02049656,-0.04316102,-0.049659792,0.029966589,0.10447047,-0.0070600454,0.02232691,0.006907818,0.026848132,0.016839253,-0.055556726,0.042836837,0.0095590735,0.0041794716,0.02589009,0.006046847,-0.03297404,0.0058045746,0.049427718,0.06680602,0.067285925,-0.0067264074,-0.013502288,0.0032520753,0.05121556,-0.013153388,0.012782512,0.020658806,0.039695077,-0.04766726,0.028425097,-0.011553838,-0.086682886,0.049247917,-0.07094722,0.039844193,0.056662783,0.0022281513,0.005968808,-0.008553422,-0.030301899,0.053719174,0.009932162,0.026758013,-0.012775291,-0.0396241,-0.004645462,-0.003931047,-0.009892604,0.009854114,-0.038036574,-0.01600311,0.011944952,0.01553897,-0.010154457,0.039256085,0.029076891,0.016367953,-0.023435168,-0.07200664,-0.02269682,0.029561918,0.017696584,-0.018034989,0.047711845,-0.092769474,0.08560146,-0.054299805,0.024630489,0.0031031864,-0.043974902,0.051293544,0.03229727,-0.014313063,0.008332666,-0.02445905,0.060882233,-0.061393566,-0.040846422,0.033571526,0.040102538,0.019499622,-0.040023707,-0.006776443,-0.04615614,0.02335894,-0.03635903,-0.037854746,0.011841196,0.009938654,0.010024766,0.043623075,-0.009488418,0.013783252,-0.072370104,-0.039398402,0.059035208,0.0030659805,-0.044427235,-0.014261392,-0.008817948,0.046870813,-0.014432365,-0.023710823,0.010526451,0.017032797,-0.051671598,0.059931837,0.053456392,0.03479824,-0.05047199,0.02923669,0.033590056,0.052993327,0.015222648,-0.023765175,-0.02270529,0.0071909605,0.01632214,-0.03142577,-0.05999887,0.05559027,-0.03318497,0.009486288,-0.028903669,0.0304213,0.048995074,0.06211134,0.008712064,-0.0015238815,-0.0032662281,0.008874623,-0.04658531,-0.05887334,-0.009224782,-0.05349714,-0.014208582,0.060756113,-0.00022498642,0.027077448,-0.040200274,0.08371801,-0.047879666,0.050484587,0.031793203,-0.042615194,-0.02241358,-0.0034256016,0.044600084,-0.038263924,0.028031461,0.036679555,-0.08009759,-0.023743946,0.038200885,0.0014718617,0.027678479,-0.02616823,0.03954444,0.026720814,-0.008763977,-0.038496777,-0.06790994,0.034369487,0.026117077,0.021556512,0.0079721585,-0.024339044,-0.016328963,-0.014120054,-0.027366573,-0.050747607,-0.023237485,0.03993219,0.008805881,0.02668491,0.005884224,0.04485818,-0.021412928,-0.050708294,0.002889791,0.030729773,0.007315681,-0.0075510014,-0.05487227,-0.049314085,-0.018470094,0.01176853,-0.017380623,-0.006828913,-0.021966733,-0.035218447,-0.042537756,0.02946959,-0.0020995901,0.048434775,0.020109529,0.01938248,-0.02052822,-0.035522975,-0.00091456954,0.018883565,-0.000533758,-0.014087652,-0.01857377,-0.0023101731,0.007972336,0.05423016,-0.03565391,0.0037582028,0.005552215,-0.0046127904,-0.05752349,-0.014046895,-0.011106959,-0.01823163,0.0046664057,-0.0038082744,-0.04923076,-0.010734126,0.0058236625,0.012652419,0.007274703,0.055722643,-0.020962251,-0.008016509,0.039502237,-0.010752981,0.02095048,-0.00031409052,0.012473775,0.02331013,0.02727956,-0.064618506,-0.004906929,-0.013854119,-0.0012854261,0.013642955,-0.07084468,-0.028654423,0.00911298,-0.07431194,-0.022849701,-0.01591174,-0.031877454,-0.067320056,0.008872364,-0.040228195,0.09689843,-0.04980732,0.00432483,-0.0031440528,-0.0062036105,0.08094953,0.0214725,0.0023387333,-0.024234895,-0.029776936,-0.023782287,0.016603783,0.069710806,-0.025622876,-0.020492839,0.05584002,-0.032273903,0.021246301,-0.036432587,-0.017969865,-0.03922596,-0.03722714,-0.016809396,0.00032051306,-0.027608143,0.04499782,-0.013290407,-0.001524103,0.03106574,0.026256727,0.012761074,0.08297395,0.022493193,-0.02547544,0.018639125,0.015217099,0.029200757,-0.010591491,0.04865847,-0.011387926,0.03451339,-0.00097593723,-0.0004990502,-0.009572086,-0.016619211,-0.00604176,0.0039418936,-0.03014232,0.0038611153,0.014102856,0.007052015,-0.007627838,0.04205132,0.006477921,0.017905246,-0.037977118,-0.09565124,-0.026769135,-0.044803232,-0.061119787,0.0125418,0.002800102,0.008784747,0.026196599,-0.018499061,0.08324882,-0.06580456,0.026847681,-0.06552112,0.054039143,-0.00824751,0.008117819,-0.024567943,-0.035865214,-0.02004447,-0.06713989,0.05481453,0.029885663,-0.015999125,0.024268882,0.01802646,0.009165318,-0.0046709753,-0.013872832,-0.0041178344,0.0017788053,-0.016085932,-0.008657173,0.08176418,-0.04502524,0.0017729984,0.03550418,-0.031248087,-0.050545868,0.0091198385,0.04769071,0.028187547,0.01849884,-0.002720599,0.026797995,0.034946095,-0.02210923,-0.020600865,-0.013896438,-0.008119129,0.02448709,-0.009472668,0.049614754,-0.028940242,-0.04577158,-0.019610753,0.016055416,-0.017183771,0.026664408,0.023862073,0.06550461,0.0646676,0.020148527,-0.0042040753,-0.08982902,-0.061761927,-0.04384344,-0.032193385,-0.054659933,0.0053405524,-0.01325345,-0.015218702,0.0027763029,-0.04729051,-0.012060126,-0.028014382,0.005023328,-0.0089893015,-0.05523621,-0.033528026,0.016654158,0.061895788,0.002365054,-0.030765127,0.05253995,0.03451752,-0.033024937,-0.019165695,-0.062043656,-0.0013947506,0.018686062,-0.0048592836,0.033043772,0.017592188,-0.009715995,0.0071589327,0.059247665,0.009365703,-0.056162655,-0.055082403,0.042567793,0.029720828,0.03775104,-0.029004868,-0.035576917,-0.11920923,0.039528053,-0.008030109,-0.035907056,0.013163332,0.042285465,-0.02905734,-0.01482504,-0.033430822,0.044453494,0.0018547383,0.029612605,0.017156156,0.021412186,0.015699925,-0.011793974,-0.0757036,0.04593561,-0.025973778,0.004978277,-0.047066323,0.06028162,0.020937163,-0.012150884,-0.0122795515,0.009401099,0.049600426,0.015353428,-0.0331576,0.037641875,0.010849328,0.053447295,0.02618557,0.018631658,0.023671275,-0.042686895,0.029788842,0.0039723814,0.044224825,-0.015812123,-0.011728493,-0.007970305,-0.025807545,-0.0599644,0.012994657,0.010874724,-0.022231689,0.019248981,-0.015363889,-0.03446708,-0.03153998,-0.040200934,-0.0015724133,0.05975898,0.03576666,-0.02530498,-0.047548722,0.03141748,-0.046865486,0.019041665,-0.0068935435,0.020654969,0.018346483,0.013114112]	Keywords: attention mechanism, machine translation, language modeling, synthesizer\nKey Objects: attention scores, feed-forward network\nRefers to Images: None\nHypothetical Questions:\n- Why would randomly initialized attention scores be effective?\n- What are the potential benefits of using a feed-forward network to generate attention scores?\n- What might be the underlying reason for the effectiveness of these non-traditional attention methods?\n---\nSummary:\nThe Synthesizer model, as described in [131], explores replacing the usual generated attention scores with either learnable, randomly initialized scores or scores from a feed-forward network conditioned only on the querying input, demonstrating competitive performance in machine translation and language modeling.\nOriginal Text:\nSynthesizer [131] proposes to replace generated attention scores with: (1) a learnable, randomly initialized attention scores, and (2) attention scores output by a feed-forward network that is only conditioned on the querying input itself. The experiments on machine translation and language modeling show that these variants can achieve competitive performance with vanilla Transformer. It is not explained why these variants work but the empirical results are intriguing.\nContextualized Text:\nAs part of exploring attention mechanisms, the Synthesizer model [131] investigates an unconventional approach. It proposes replacing the typically generated attention scores with either learnable, randomly initialized scores or scores output by a feed-forward network that is only influenced by the input query itself.  Remarkably, this alternative demonstrates competitive results in tasks such as machine translation and language modeling, despite the underlying reasons for its success remaining unclear.	{"tags": ["NLP", "transformers", "attention"], "doc_id": "db59608c-cf5c-4328-ad24-20bbacd5c4b0", "summary": "The Synthesizer model, as described in [131], explores replacing the usual generated attention scores with either learnable, randomly initialized scores or scores from a feed-forward network conditioned only on the querying input, demonstrating competitive performance in machine translation and language modeling.", "doc_type": "text", "entities": ["Synthesizer", "Transformer"], "keywords": ["attention mechanism", "machine translation", "language modeling", "synthesizer"], "key_objects": ["attention scores", "feed-forward network"], "contextual_text": "As part of exploring attention mechanisms, the Synthesizer model [131] investigates an unconventional approach. It proposes replacing the typically generated attention scores with either learnable, randomly initialized scores or scores output by a feed-forward network that is only influenced by the input query itself.  Remarkably, this alternative demonstrates competitive results in tasks such as machine translation and language modeling, despite the underlying reasons for its success remaining unclear.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.5 Attention with Prior"}, "hypothetical_questions": ["Why would randomly initialized attention scores be effective?", "What are the potential benefits of using a feed-forward network to generate attention scores?", "What might be the underlying reason for the effectiveness of these non-traditional attention methods?"]}
b33b8f9e-db44-48e2-8492-ac8dadddfbd4	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.021623872,-0.01880388,0.054038122,0.035833545,-0.020818964,0.047322948,0.012634343,0.072397426,0.017659286,-0.011827935,0.014717134,-0.018593738,-0.017230922,-0.046916313,-0.015987253,0.013525271,0.03257066,0.09985067,0.0058211624,0.0016728173,0.047489617,0.023945665,0.04450285,-0.009157363,-0.02821276,0.033117663,0.030916812,0.00456534,-0.04274491,0.022634482,0.057213068,-0.06280054,0.047654163,0.02395018,0.06587441,0.015718287,0.0253873,-0.059810057,-0.0060203164,0.014431128,0.0067268885,0.035381973,-0.03726524,-0.04577156,-0.002552376,-0.04534011,-0.11886947,-0.021592246,0.025933797,0.009443854,-0.026805304,0.040061638,-0.041097816,0.007340901,-0.005675518,-0.014208816,0.014549244,-0.046383064,0.0048614936,-0.12157264,0.00335101,0.008858631,0.016945839,-0.0061303084,0.01869601,0.0013797702,0.002798554,0.019386137,0.041882977,0.1340647,-0.021423716,-0.006629762,0.012968263,-0.069776155,0.11127944,0.046637166,-0.030869722,-0.03572952,-0.04045306,-0.033423383,-0.05291996,0.051070824,-0.057247072,-0.05524986,0.043261398,0.044428784,-0.0574086,0.01317327,0.062750295,-0.055601563,-0.0089511415,0.0217119,-0.0032849256,0.05885912,-0.016816657,-0.055152766,-0.019659929,-0.032231756,-0.031084102,-0.088169,0.016987853,-0.00831318,0.08260905,0.1065922,0.027870174,-0.02652884,-0.004097519,-0.0557868,0.033776816,0.032255158,0.0012115502,0.031036245,-0.016466612,0.019790221,0.0025505165,0.032004464,-0.029923439,-0.019350475,0.010874158,0.008591099,0.018124966,-0.039408095,-0.0023103473,0.013551645,0.038306322,0.02401119,0.057216283,0.01228048,-0.04058212,0.01747005,0.048390664,-0.02139804,-0.01838993,0.0020657904,-0.052665543,-0.012454325,0.07149714,0.015645709,0.011805823,0.0294821,-0.0063604885,-0.048179783,-0.03743944,0.032550327,-0.028942792,0.06430964,-0.05969857,0.043859452,-0.02661191,0.064641386,0.0758271,-0.037208747,0.019726785,0.001308529,-0.008113154,-0.020800343,-0.013077633,-0.065736294,0.041464858,-0.04298199,0.0014612289,-0.044077154,0.014966241,0.05819996,0.069683075,0.1060659,0.04252042,0.009583601,0.008523725,-0.0001987279,-0.0071868845,0.0030760856,0.007009303,-0.041414376,-0.013374827,0.0332669,0.0035078118,-0.037438445,0.04535575,-0.0034702355,0.007771968,0.026422545,0.014483439,0.03923702,-0.030001275,0.020286325,-0.02812113,0.017037423,-0.004751404,-0.044633795,0.036054432,0.016933173,0.02658633,-0.00500554,0.031168174,0.03636269,0.03816517,-0.079188794,0.0018947915,-0.009942443,0.018838251,-0.038221493,0.049708273,-0.023633175,-0.008316429,-0.025346987,0.014240069,-0.041851364,-0.02149686,0.041126132,-0.0307047,-0.03128582,0.005153593,0.01980542,0.007472531,-0.0046654157,0.013680083,0.040651254,0.016120417,-0.022892574,-0.057134178,-0.00050086965,-0.08853445,-0.006550581,1.2630728e-05,0.056284834,-0.022469217,0.031174418,0.016639888,-0.027273698,0.0044985577,0.0027165157,-0.040490385,0.0023578142,-0.014285026,0.012236356,0.005585706,0.022781419,-0.028556075,0.020639036,0.055387843,0.04812541,0.0012472739,0.0856533,0.016305339,-0.004692882,0.036119718,0.013370827,0.010995751,0.023308856,-0.027174773,0.08564837,0.008523618,-0.030190542,-0.030541267,-0.05378276,-0.027446195,0.0064201285,0.009082206,-0.012016045,-0.01799842,-0.01690124,0.032533597,-0.01318482,0.038475588,0.030351123,-0.012980317,0.010567829,-0.041815866,0.015280739,0.009213237,0.041051358,0.021042569,-0.03069149,0.009511353,0.06335681,-0.0047832425,-0.002157991,0.01820028,-0.021433584,-0.026883826,-0.015268723,0.023292013,0.0032229568,0.04524605,-0.015963055,0.03056194,-0.01664153,-0.090426706,0.00023630659,-0.039173342,-0.013624056,0.00812803,-0.0099389,-0.0032359953,-0.02930817,-0.00947402,-0.011901626,-0.019570023,0.039444193,0.12226516,-0.02381172,-0.018183913,0.023766814,0.008359332,0.013547375,-0.056615513,0.012607876,0.03917626,-0.012909641,0.04344372,0.02478394,-0.026438762,-0.011664402,0.012008733,0.08837954,0.0861574,-0.007886513,-0.007344458,0.031894118,0.0086843325,-0.0047667623,0.01306791,0.037990343,0.00018434339,-0.00661822,0.033446506,0.0016846842,-0.09262988,0.06971858,-0.052511975,0.052397456,0.048406884,-0.04189252,0.0362863,0.0007357215,0.0150825335,0.02433863,-0.003247696,-0.00012637567,-0.028850405,-0.03318244,0.025258781,0.005889985,-0.011704409,0.029575467,-0.022140399,0.014278966,0.03223538,0.025876086,0.024040665,0.052680034,0.016566701,-0.021685055,-0.03625253,-0.038578965,-0.029371517,0.04807484,0.023683954,-0.0042124554,0.0804802,-0.101906076,0.0778129,-0.028201096,0.06386444,0.014656561,-0.04207582,0.035560116,0.0019862775,-0.020280711,0.0040479396,-0.04150966,0.027991533,-0.025623543,-0.04667509,0.011021675,0.039992083,0.020424658,-0.027775489,-0.009012716,0.014516412,-0.0013288439,-0.048694063,-0.028402261,0.017920347,0.0403003,0.0036354328,0.08144987,-0.015858592,0.011763082,-0.025126964,0.0072234753,0.025715232,0.006507841,-0.03336889,0.02001417,0.019213878,0.028606687,0.019670578,-0.05192845,0.019600624,0.03429563,-0.059487004,0.056198895,0.030267302,0.020941596,-0.07650344,0.013870931,0.04033292,0.036653887,0.03546839,0.007435551,-0.02799045,0.014512124,0.022630451,-0.02093257,-0.023360126,0.03351467,-0.03673458,0.034179673,-0.017484497,0.06692738,0.022908162,0.058288764,0.027776724,-0.019605817,-0.0021754105,0.06133727,-0.008002299,-0.040157266,0.01918008,-0.056339066,-0.008120713,0.05403059,-0.026990581,0.03383628,-0.06183287,0.06636965,-0.02021129,0.02384395,0.06856315,-0.039581135,0.053184707,-0.015168432,0.003876888,-0.053860977,0.034854505,0.026949001,-0.02162428,0.00610313,0.026505176,0.039683044,0.026583401,-0.0015354375,0.053361,-0.0051112287,0.012247074,-0.016640522,-0.049781673,0.016809061,0.02253976,0.051248748,0.0372816,0.008688966,0.0066116406,-0.006698977,0.009287466,-0.08485529,-0.023757445,-0.007781084,0.03020974,0.04430303,0.0022712143,0.04642798,-0.008673575,-0.025693502,0.012493589,0.04904356,0.048046228,0.017855454,-0.09950429,-0.0014230561,0.013159708,0.008427862,0.02476153,-0.0025362368,-0.01767605,-0.02744329,-0.03235977,0.027482795,-0.005535562,-0.005713617,0.0048923357,0.06275706,0.025459772,-0.033479504,0.019451655,0.03466458,-0.002139159,0.018180795,-0.025899084,-0.0150981955,0.08857332,0.05586947,-0.03286746,-0.026613304,0.020955991,0.01623756,-0.010826861,0.07869591,0.004585077,-0.01364499,-0.0009495915,0.026095392,-0.039010487,0.02610792,-0.027630338,0.013310626,0.0137986615,0.038011003,-0.025286987,0.048174948,0.032806765,0.022012701,-0.004983554,-0.025597982,0.027361754,0.0012166923,0.03837281,-0.07051538,0.01223602,-0.030128088,-0.01876208,0.011233251,-0.06995687,-0.06680757,0.007841263,-0.0040911525,0.0013229499,0.043705586,0.0023748137,-0.019500853,0.00981104,-0.02057568,0.0631317,-0.062529996,0.028859172,0.0215097,0.015270343,0.07723156,0.018802935,0.0059920563,-0.001834216,0.026605615,-0.0017109532,0.023647718,0.06736284,-0.0006524704,-0.003935297,0.038737472,0.016763767,0.036071185,0.004552802,-0.04915456,-0.021700617,-0.01051688,0.0041660583,0.0035312774,-0.010639922,0.0014327335,-0.007793107,0.027146466,0.026655417,-0.027397122,0.015961729,0.061000228,0.031150844,0.004962694,0.071271636,-0.030048937,0.049974743,0.017090987,0.028617952,0.0029927818,-0.0115543585,-0.009022402,0.02594209,-0.052909106,-0.039020684,0.0027890175,-0.026935318,-0.04796012,0.040099647,0.028039614,0.004861673,-0.046757992,0.023178434,0.019355478,0.026587244,-0.0535844,-0.05951547,-0.03406854,-0.03541775,-0.04601897,0.0058308523,0.0015589513,-0.0007148506,0.015097129,-0.016998582,0.04769089,-0.06527527,0.0112380255,-0.053467978,0.009932296,0.021815458,-0.017480046,-0.0052794977,-0.020588405,-0.012323789,-0.04066002,0.094296224,0.003942548,-0.010440722,0.0018343036,0.016406177,-0.014582973,-0.010820976,0.005939929,0.005295258,-6.639463e-05,-0.008683394,-0.02516078,0.06418323,-0.01688763,0.021732733,0.045888998,-0.0050116302,-0.058342922,-0.007216246,-0.0081872055,0.06966922,0.006931913,0.058391143,0.04632796,0.027898032,-0.046804987,0.029475873,-0.047652293,-0.017187357,0.0076094293,0.031280905,0.07011032,-0.055948548,-0.06088661,0.0045352774,0.03488046,0.02423495,-0.012914436,0.03231515,0.049297586,0.06958158,0.029819699,0.022852838,-0.05466882,-0.018340217,-0.04799343,-0.008341565,-0.04430042,0.021657458,0.008432842,-0.015672067,-0.01076467,0.016533224,0.008958341,0.0053244997,0.009142362,0.01558475,-0.04495878,-0.033193003,0.0058289915,0.0043781428,-0.006075505,0.0054326034,0.05590278,0.019562423,-0.049305838,-0.03279287,-0.08279267,-0.013407344,0.030102942,-0.01818098,0.013158375,-0.010810711,-0.026338695,-0.033595517,0.092634216,-0.022894215,-0.019683179,-0.012892334,0.020096151,0.04659923,0.03318696,0.019417206,-0.031555638,-0.052218534,0.007207021,-0.041117232,-0.02527738,0.039788377,-0.03323181,-0.023654617,-0.009143357,0.011726936,0.009390759,0.005272359,0.013791828,0.019634813,0.03338903,0.038101386,0.015417039,-0.103893146,-0.008225154,-0.0056087435,-0.036754195,-0.01151106,0.052754864,-0.021773232,-0.032909475,-0.008133385,-0.0028517402,-4.2383614e-05,0.014777635,-0.024251858,0.041264333,0.009311645,0.034147516,-0.012067196,0.019457456,0.06756231,0.018460838,0.0062013716,0.00039833444,0.037274975,0.03491447,-0.005792979,-0.0018893724,-0.042392764,-0.02213548,0.02509528,0.028552558,-0.030741332,0.019782705,-0.0040709055,-0.05264847,-0.020530628,0.019178193,-0.017379096,0.05126144,0.058875278,0.015634095,-0.027034046,0.020070784,-0.007901936,0.026292419,0.016597442,0.0254528,0.015656153,-0.0018072305]	Keywords: multi-head attention, representation subspaces, attention heads\nKey Objects: attention heads, representation subspaces, multi-head attention\nRefers to Images: None\nHypothetical Questions:\n- Why is it important to ensure that attention heads capture distinct features?\n- What are some potential drawbacks of the vanilla Transformer's multi-head attention mechanism?\n- How might introducing sophisticated mechanisms improve the behavior of attention heads?\n---\nSummary:\nMulti-head attention is designed to allow models to attend to information from different representation subspaces, but there's a challenge in ensuring that each attention head captures distinct features.\nOriginal Text:\n### 4.6 Improved Multi-Head Mechanism  \nMulti-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. However, there is no mechanism to guarantee that different attention heads indeed capture distinct features.  \n4.6.1 Head Behavior Modeling. A basic motivation for using multi-head attention is to allow the model to jointly attend to information from different representation subspaces at different positions [137]. However, in vanilla Transformer there is no explicit mechanism to guarantee different behavior across attention heads, nor is there any mechanism for heads to interact with each other. A line of work is dedicated to improving multi-head mechanism by introducing incorporating more sophisticated mechanisms that guide the behavior of different attention heads or allow interaction across attention heads.\nContextualized Text:\nIn multi-head attention, a key goal is to allow models to attend to different representation subspaces. However, in the vanilla Transformer, there's no built-in mechanism to guarantee that each attention head behaves distinctly or interacts with others. This has led to research focused on improving the multi-head mechanism by introducing more sophisticated approaches.	{"tags": ["NLP", "deep-learning", "architecture", "transformer"], "doc_id": "b33b8f9e-db44-48e2-8492-ac8dadddfbd4", "summary": "Multi-head attention is designed to allow models to attend to information from different representation subspaces, but there's a challenge in ensuring that each attention head captures distinct features.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["multi-head attention", "representation subspaces", "attention heads"], "key_objects": ["attention heads", "representation subspaces", "multi-head attention"], "contextual_text": "In multi-head attention, a key goal is to allow models to attend to different representation subspaces. However, in the vanilla Transformer, there's no built-in mechanism to guarantee that each attention head behaves distinctly or interacts with others. This has led to research focused on improving the multi-head mechanism by introducing more sophisticated approaches.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["Why is it important to ensure that attention heads capture distinct features?", "What are some potential drawbacks of the vanilla Transformer's multi-head attention mechanism?", "How might introducing sophisticated mechanisms improve the behavior of attention heads?"]}
96c4be5d-d5d9-48e5-b892-f8fbebbe6a91	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.028002203,0.032399435,0.006100498,0.012801578,-0.035417553,0.08481697,0.030498631,0.04320828,0.0063965153,-0.038876276,0.014720934,-0.00031469596,0.022160577,-0.021189729,-0.025158428,-0.033049006,0.0133671155,0.064182594,0.0031454964,-0.029284647,0.10280016,-0.006294416,0.009940504,-0.0033747952,0.014474557,0.047338266,0.0390524,-0.024078254,-0.013984885,0.0019762013,0.028079432,-0.022783792,0.0017214749,0.004066987,0.032229573,-0.012918115,0.0062170145,-0.08320971,0.02419793,-0.015908295,-0.040151898,0.013417353,-0.036561176,-0.010814294,-0.0022698364,-0.0564073,-0.08086902,0.0024298332,-0.028524484,-0.0031303568,0.01048455,0.051931385,-0.027844368,0.002347221,-0.042482935,-0.0026266598,0.021386743,-0.031610355,0.012622851,-0.111155935,0.0027934413,0.04385679,-0.010779125,0.0129072815,0.025083557,-0.045576878,-0.015752709,-0.0013432768,0.05606515,0.1405915,-0.005150027,-0.007884517,-0.005547137,-0.08304884,0.11305281,-0.0012418346,-0.03237691,-0.011094223,-0.012900394,0.023309134,-0.018410051,0.040826175,-0.0515889,-0.019112108,0.09184464,0.03534548,-0.025771512,0.001475264,0.039029445,-0.06220201,-0.019264584,-0.02781826,-0.023605147,0.060411457,-0.020293595,-0.015294421,0.004942907,-0.034466747,-0.024995554,-0.08907467,0.0041334163,0.03620829,0.08349927,0.119086966,0.024139926,-0.022301598,0.003077458,-0.022982372,0.0058315285,0.011471082,0.022948075,0.006407768,-0.055757012,0.013665032,-0.015583268,0.018226316,-0.06453833,0.02292924,0.018075466,-0.028055247,0.018462194,-0.008446947,0.011491133,0.021300022,0.07151591,0.04727227,0.06768005,-0.040004406,-0.009782596,-0.008202433,0.068962686,0.003663299,-0.005060709,0.012111985,-0.007262444,-0.00027229392,0.0643939,0.017936658,0.009405587,0.0042690863,-0.028085718,-0.029155714,-0.027198346,0.016259767,0.010905714,-0.0050020823,-0.05379162,0.005302992,-0.03949545,0.054882064,0.031110974,-0.026685255,0.039027147,0.00019862229,-0.059232723,0.010951442,-0.0019679826,-0.028235562,0.023096846,-0.046426233,-0.07527721,-0.030694583,0.0068244888,0.05168964,0.07061866,0.063130766,-0.015688129,0.015541323,-0.0043020328,0.037626293,-0.05362048,-0.026187386,-0.0009452402,-0.049634334,-0.0301324,0.018509392,0.0034802305,-0.026956106,0.007404976,-0.017080044,-0.027139567,0.077146575,-0.015370126,0.006241128,-0.015545611,-0.005500689,-0.010279183,0.05371267,-0.014869709,-0.022709804,0.0010303612,-0.036515124,0.022765772,-0.016801735,0.0110350475,0.03579839,0.042995255,-0.04889878,-0.00013382274,-0.04606341,-0.021424064,0.0054496364,-0.011961522,0.000434059,-0.0049656387,-0.033712253,0.03635343,-0.007153379,-0.03990479,0.0008805094,0.015654247,0.0163747,0.012389639,0.037147302,-0.032552276,0.03787109,-0.020220468,0.043296434,0.007953105,-0.04551247,-0.051399358,0.007423888,-0.027716348,0.041141182,-0.025634868,0.06612232,0.00023878495,-8.0026744e-05,-0.0068745636,-0.0029121854,0.03609075,0.0459847,-0.025880527,0.04155441,-0.027532887,-0.029177653,0.018314898,0.00036297724,-0.038931582,0.010806752,0.065809995,0.04975241,0.009711156,0.08716755,-0.032668874,-0.004903094,0.007059094,0.041194085,-0.025097474,-0.00873922,-0.02271231,0.05915154,0.034106895,-0.017360527,-0.02935312,-0.037255388,0.02285399,0.02307248,0.028147632,-0.024822323,0.04923525,0.022154598,0.016844844,-0.010473919,0.03163248,0.021733131,0.05535691,0.001974578,-0.029709194,0.020753253,-0.004351288,0.058813438,0.04440438,0.0010135969,-0.03693268,0.045582652,0.014245792,0.027081786,0.006065318,-0.010996051,0.026620457,-0.018102191,0.015209757,0.020650145,0.023884658,0.005088603,0.053070933,-0.015803652,-0.03287565,-0.022131186,-0.022995463,-0.06130575,0.026728231,0.00025794774,-0.014036291,0.019824049,0.0067159384,-0.055175245,-0.023169376,0.018976761,0.07560211,-0.031040285,-0.019923704,0.008984732,-0.018702444,0.029678963,-0.07164038,0.042600844,0.032561265,0.028507117,0.008021268,-0.005498571,-0.0010006792,0.015548058,0.06956714,0.067834355,0.061340835,-0.01708787,0.012782957,-0.011665422,0.06189647,0.0065676994,-0.008849171,-0.013800513,0.042852793,-0.014268359,0.0014513455,-0.06434754,-0.05671455,0.04855667,-0.11191906,0.052035976,0.0738373,-0.046581376,0.0012457372,-0.0034475704,-0.0012590404,0.035078518,-0.013812888,0.013628803,-0.019874314,-0.016482404,-0.009912023,0.0048002526,-0.04020671,0.05537751,0.00063178135,0.0018000662,0.04124033,0.051787544,0.05788776,0.032826778,0.0024352982,-0.044400305,0.013581355,0.004631277,-0.016757071,0.021996424,-0.02882492,0.0013108978,0.10864478,-0.08511349,0.06888327,-0.06884905,0.046456337,0.022087915,-0.042874686,0.020397775,0.029434063,-0.023457415,0.01634527,-0.029522829,0.045622032,-0.050583787,-0.05736443,0.03005056,0.053274687,0.013392988,-0.04373223,-0.019577231,-0.040944293,-0.015981903,0.0044937604,-0.024547812,0.050670113,0.016608788,-0.009068495,0.084992364,-0.010656252,0.02602433,-0.0539685,0.034698203,0.037516642,-0.0025915136,-0.031144792,0.06459722,0.021715868,0.031139525,0.029108481,-0.06785947,0.034955885,-0.009429121,-0.042002384,0.018232869,0.009746549,0.035003576,-0.08393248,0.03025099,0.014445139,0.044337355,0.045209024,-0.06176198,-0.0041742995,-0.030554118,-0.022727983,-0.04480201,-0.0027604306,-0.000881157,-0.045274813,0.002277353,-0.078678,0.043998055,-0.027033407,0.012611014,-0.0041991933,-0.028710704,-0.03755115,0.07321012,-0.043228623,-0.014897871,0.026371425,-0.009494702,-0.010475597,-0.0033602864,-0.05581515,-0.0038934425,-0.0008001889,0.08737996,0.005912905,-0.023420943,0.054089125,-0.002368208,0.016552357,-0.0042118565,0.041975103,-0.045092784,0.032203294,-0.007158522,0.022282626,0.019444456,0.028922245,0.03947984,0.040156953,-0.007986014,0.035873912,0.016669173,-0.0015707099,-0.018290337,-0.017132547,0.009773673,0.004763526,0.020236839,0.03075038,0.002444593,-0.03240758,-0.01670272,-0.02898788,-0.045241475,-0.062055867,0.0013090599,0.0043081166,0.045574546,0.028144058,0.052925564,0.02318001,0.027535133,-0.0009214143,0.031044288,0.047644567,0.031303395,-0.08919554,-0.001714236,-0.010194037,0.01757703,0.027494956,0.0015833627,-0.009590618,-0.021876669,-0.028759211,0.028570602,-0.016466469,0.029116346,0.02330003,0.033936083,0.0020082167,-0.032363143,0.06384239,-0.030842835,0.002670182,0.008373491,-0.024262825,0.022718243,0.058599584,-0.0039316393,0.022936188,0.036462966,0.01114524,0.024524126,-0.027376834,0.05395498,0.039440375,-0.009703991,0.05236387,0.022684032,0.004508153,0.03759896,-0.035871286,-0.0031097052,0.033317268,0.025716593,0.012607911,0.005273926,0.044601172,0.00061362155,0.0024994202,-0.032114778,0.03287311,0.0021379679,0.009420275,-0.08068545,-0.012637706,-0.028815117,-0.010973782,0.022632143,-0.07210631,-0.07844143,0.044907216,-0.036046118,0.019414552,0.02562593,-0.04146667,-0.020879138,-0.020803347,-0.036253415,0.07031578,-0.011570149,0.014124292,0.03656229,0.034697875,0.097308464,0.036770258,0.022227747,-0.004150706,0.0048944918,-0.023792323,0.0015056763,0.06105346,0.028361311,-0.02553807,0.021355718,-0.0049922774,0.020564355,-0.015681475,-0.031142883,-0.06849517,-0.01219142,-0.018374445,-0.0006828235,-0.055017237,0.034030832,-0.027266318,0.056177534,-0.0119705545,0.028453523,0.043371856,0.038573515,0.015494276,0.009112987,0.011420486,0.0027698886,0.016028943,-0.012931869,0.015015854,0.02447395,0.020951912,0.01654094,0.008650694,0.005529544,0.027563049,0.0023443247,0.00091976085,-0.03579814,-0.028048037,-0.032823153,-0.0045960257,-0.04460072,0.0053337067,0.016863842,0.04406694,-0.046318613,-0.055263694,-0.0074606696,0.0062382394,-0.0038840754,0.057991225,0.08791343,0.013766646,0.021406196,-0.004000854,0.010460303,-0.10171502,0.012210201,-0.041550897,0.015178767,-0.067155086,-0.028454134,-0.014246964,-0.016035736,0.014410943,-0.018724324,0.05431296,0.028627086,-0.022758752,0.0224506,-0.028713716,0.017725756,-0.027240656,-0.045406558,-0.01058323,0.014244923,0.015790524,-0.013210971,0.08876492,-0.027669258,0.00077127723,0.0117077865,-0.0074514435,-0.11343128,0.011244705,0.011095718,0.041298684,-0.027965058,0.069944665,0.010895093,-0.009821056,-0.013466197,-0.019730292,-0.05214703,-0.012377067,-0.0009310763,0.009303777,0.04439439,-0.008719175,-0.053264055,-0.034697186,0.010150074,-0.008661709,0.042232666,0.031358343,0.024269583,0.06731968,0.07807669,0.008380312,-0.07319972,-0.007917287,-0.026809722,0.05953908,-0.019696582,-0.025703212,0.009285482,-0.033993788,-0.03025026,0.009526602,-0.016542036,-0.0045301057,0.027716303,0.021233045,-0.06958821,-0.039464943,-0.01391389,0.03808692,-0.045863345,-0.033566304,0.07048263,-0.0010351106,0.01632848,-0.029029226,-0.057044536,0.004041893,-0.014408546,-0.011948187,0.009454657,0.0071983635,-0.006344141,-0.0044751796,0.049184266,0.015661232,-0.038562577,-0.025088936,0.015989559,-0.00740948,0.046939023,0.035209224,-0.023236992,-0.072590664,0.0016972608,-0.014984171,-0.031671982,0.023085175,0.018250681,-0.043069024,0.009178244,0.006739457,0.05850736,-0.00030032458,-0.017369257,0.02318854,0.002446142,-0.045152213,0.038376812,-0.095943816,0.008941835,-0.026061585,-0.017890338,-0.019891474,0.052033335,0.04367731,0.013153926,-0.025382014,-0.016772347,0.010659029,-0.01705823,-0.038863808,0.015499084,0.026190808,0.054417975,-0.02310967,0.030001996,0.07793082,0.019179555,0.007712279,0.0054311496,0.0047891606,0.027017603,-0.030116769,-0.031327035,-0.035141118,-0.053293664,0.0037922743,0.008531947,-0.014190034,0.027763586,-0.030135192,-0.03552037,-0.03990468,-0.011279047,0.030663857,0.04533724,0.031068983,0.009427682,-0.03864874,0.0026510346,-0.010091095,0.044557575,0.018571004,0.03727662,-0.011271491,0.039416037]	Keywords: attention heads, regularization, cosine distances, diversity\nKey Objects: attention heads, loss function\nRefers to Images: None\nHypothetical Questions:\n- Why is it important to encourage diversity among attention heads?\n- How does maximizing cosine distances contribute to diversity in attention heads?\n- What is the purpose of dispersing the positions attended by multiple heads?\n---\nSummary:\nLi et al. [73] introduced a regularization term to the loss function that encourages diversity among attention heads by maximizing cosine distances and dispersing attended positions.\nOriginal Text:\nLi et al. [73] introduce an auxiliary disagreement regularization term into loss function to encourage diversity among different attention heads. Two regularization terms are respectively to maximize cosine distances of the input subspaces and output representations, while the last one is to disperse the positions attended by multiple heads with element-wise multiplication of the corresponding attention matrices.\nContextualized Text:\nTo improve multi-head attention, Li et al. [73] introduced an auxiliary disagreement regularization term into the loss function. This term encourages diversity among different attention heads by maximizing cosine distances between input subspaces and output representations, and by dispersing the positions attended by multiple heads using element-wise multiplication of their attention matrices.	{"tags": ["architecture", "optimization", "NLP"], "doc_id": "96c4be5d-d5d9-48e5-b892-f8fbebbe6a91", "summary": "Li et al. [73] introduced a regularization term to the loss function that encourages diversity among attention heads by maximizing cosine distances and dispersing attended positions.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["attention heads", "regularization", "cosine distances", "diversity"], "key_objects": ["attention heads", "loss function"], "contextual_text": "To improve multi-head attention, Li et al. [73] introduced an auxiliary disagreement regularization term into the loss function. This term encourages diversity among different attention heads by maximizing cosine distances between input subspaces and output representations, and by dispersing the positions attended by multiple heads using element-wise multiplication of their attention matrices.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["Why is it important to encourage diversity among attention heads?", "How does maximizing cosine distances contribute to diversity in attention heads?", "What is the purpose of dispersing the positions attended by multiple heads?"]}
f70af643-7637-4cc9-bf4d-407ded33e403	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.035887383,0.0014357226,0.04267105,0.035201482,0.0039930455,0.051937018,0.0025998377,0.047496617,0.0023995189,-0.044567537,0.026730238,0.024089418,0.015342704,-0.05314541,-0.037879456,0.025641916,-0.015005401,0.08787632,-0.015544067,-0.031764753,0.033277705,0.011812683,-0.022728903,-0.02019564,-0.002736539,0.015367347,0.0059415004,-0.051038414,-0.0067609427,0.02960484,0.011193692,-0.030419707,0.03841128,-0.027081734,0.04953474,-0.009986355,0.016315818,-0.011515963,0.03516605,-0.028131418,-0.021426503,0.062057287,-0.07902003,-0.021897882,0.014776561,-0.07166492,-0.114574775,-0.0129388245,0.0115376515,-0.031869575,0.0032401017,-0.005974783,-0.012216357,-0.017019447,0.0016075997,0.0046856366,0.023650328,-0.019054113,0.0020759082,-0.08598472,-0.025390005,0.01623846,-0.036742877,0.0034189718,0.024377631,-0.041614693,0.0324852,-0.016858123,0.019558737,0.106013075,-0.017376017,0.028932657,-0.0067057097,-0.0476211,0.09487146,0.013824805,-0.027044801,-0.043715883,-0.055892926,-0.0021025203,-0.0046143807,0.079677686,-0.013268069,-0.07387529,0.06259796,0.00069474237,-0.055816848,0.056448713,0.034703467,0.006466867,-0.0045087505,-0.008545203,0.0291856,0.09202853,-0.011638463,-0.08491705,-0.021724112,0.025509786,-0.009298743,-0.04066708,0.018002119,0.01452088,0.09122679,0.08167836,0.06314781,-0.025210371,0.00231007,-0.034522034,-0.012589276,-0.010669806,0.035837077,-0.021794159,-0.040688034,-0.025025174,-0.010259449,0.008545979,-0.056117073,-0.007746811,0.012680655,0.0001623425,0.005452702,0.0037757633,0.010839436,0.024331,0.04344845,0.07351675,0.022083795,-0.0009649818,-0.02029102,-0.011791184,0.04249601,0.0069247913,-0.056843407,0.07677548,-0.025880964,0.014853542,0.08317279,-0.025074974,-0.008352227,0.028085478,0.0009020469,-0.033884462,-0.058859568,0.06652895,-0.027176576,0.022868982,-0.059352234,0.014314963,-0.04801604,0.013191009,0.0036908307,-0.00033159647,0.015083927,0.013113361,-0.00698842,0.020977594,-0.008522117,-0.04438742,0.019372156,-0.025264578,-0.06113627,-0.029938854,-0.009059502,0.09173156,0.04491243,0.086988434,0.012101454,-0.00042565796,0.031076754,0.015049134,0.001296996,0.022818523,0.011054325,-0.04527303,0.00086825463,0.0013748921,0.014095944,-0.03850202,0.014960374,-0.040377524,-0.027479958,0.064058505,0.030864958,0.049922496,-0.027764888,0.002815224,0.040407762,0.06529057,-0.041960333,-0.021657933,0.01461921,-0.0074857054,0.006025335,-0.028886583,-0.029523412,0.046461303,0.045142204,-0.056849267,-0.01265422,-0.06189286,0.022570405,-0.011742986,0.007439195,-0.013504499,0.019125734,-0.024612613,0.012660411,-0.00012954316,0.00712604,-0.020693624,0.008041875,0.030665493,-0.014842236,0.081860736,0.00037263735,0.017258704,-0.009124891,0.037500616,0.0534124,-0.037387464,-0.03702274,-0.011926559,-0.034268983,0.0062572802,-0.0023558587,0.026886707,-0.023206666,0.006271119,-0.0553616,-0.027936663,0.0029242826,0.01660872,-0.0068699652,0.0324839,-0.0350514,0.040198427,-0.014485363,0.007173293,-0.012225517,0.034594107,0.0587592,0.0005570628,0.022806002,0.08036647,-0.03875109,0.019841976,0.018806752,0.039585378,0.018760797,0.015743313,-0.02919518,0.047916498,0.01759687,0.016581656,-0.025546525,-0.05008161,0.007496055,0.010936026,0.018526642,-0.023633616,0.019835347,-0.022772465,0.013072491,0.012778664,-0.013060901,0.0009039419,0.025073912,0.024862431,-0.026871353,-0.018777205,0.034317963,0.028236622,-0.026622972,-0.07916006,-0.09832038,0.04586195,-0.032110583,-0.0048187207,0.024495987,-0.02390829,-0.0127526205,-0.015965085,0.036404666,-0.016913265,0.034245905,0.0013814428,0.046851724,0.0045447955,-0.09463601,0.013679879,-0.029612184,-0.041007213,-0.0049997424,-0.0037176446,-0.0719847,0.041550767,0.01017444,-0.0045438195,0.017124197,0.030294094,0.07342703,-0.046401612,-0.0019833446,0.05090753,-0.0023125226,0.048941057,-0.06580135,0.0027678192,0.013377132,-0.016205087,0.021090474,-0.007666138,-0.0052501075,0.013510295,0.021863574,0.06394431,0.081758425,-0.0061321477,-0.0062343306,0.004502524,0.015614179,0.025847638,-0.0023716248,0.010584302,0.07210073,-0.03459423,0.006087402,-0.05453072,-0.098040566,0.024411239,-0.053613424,0.057424426,0.037784103,-0.008899437,-0.00087373465,-0.006714467,-0.017614253,0.036105987,0.012548981,0.025269361,-0.017237704,-0.048715927,-0.01147683,0.013750711,-0.012961113,0.04907553,-0.016877187,-0.010993188,0.051817235,0.03238063,0.0061034146,0.037915777,0.0025957462,-0.0021847717,0.011412795,-0.03552881,-0.03649646,-0.0050292835,-0.0073447363,-0.0044450117,0.069346964,-0.070089296,0.05602217,-0.033147924,0.028180206,0.012561006,-0.075297706,-0.001601631,0.02888723,-0.01568164,0.009412724,-0.039161786,0.046792127,-0.022962213,-0.03278988,0.027854742,0.04103974,0.03383567,-0.060206857,-0.027924415,0.011426157,0.018449876,-0.035919003,-0.0514854,-0.0015636088,0.013786463,0.019635025,0.10435508,-0.00024818283,0.07753092,-0.05237928,0.03172598,0.042553425,0.004844618,-0.025555516,0.018430408,0.006257638,0.043237533,-0.022378862,-0.03682171,0.009020039,0.039393485,-0.03687598,0.04717918,0.06711348,0.020476846,-0.07759717,0.05883281,0.0710788,0.058395177,-0.009227342,-0.026806949,-0.04682797,-0.0014702021,-0.002356718,-0.010906593,-0.050425287,0.028515274,-0.033469524,-0.04020256,-0.027681649,0.031541523,0.025771243,0.062040403,0.026445003,0.0069329133,-0.032190092,0.0023563292,-0.022956474,-0.029721674,0.031124383,-0.025642576,0.017516771,0.016062934,-0.030315125,0.025673391,-0.06346057,0.018803824,-0.06609583,-0.006765649,0.056084782,-0.027299682,-0.04925838,-0.008845382,-0.00081476604,-0.024298156,0.00025331782,-0.006773708,-0.027280074,-0.026055746,0.014147754,0.054010104,0.012106292,0.013409207,0.02990247,-0.028462892,0.0082521,-0.018986376,-0.06462202,-0.014015238,0.02169979,0.029517412,-0.042568438,0.007376271,0.013964838,-0.02794822,-0.034640476,-0.017521128,-0.050509017,-0.017249022,-0.014168468,-0.0036060128,-0.03944977,0.042449564,-0.035899077,-0.031199878,0.006524874,0.048272323,0.020797154,-0.0005841446,-0.056208838,-0.07994666,-0.013452885,0.015598978,0.0050122947,0.024713159,0.012980073,-0.04615416,-0.035627715,0.06311656,-0.004377718,0.010993865,0.031949215,0.027647963,0.01746185,-0.043625705,-0.028306209,0.010023707,-0.0021295568,-0.024448045,-0.006354644,-0.0042149457,0.08761291,0.007299914,0.03133459,-0.007642939,0.001286099,0.003979107,-0.021254055,0.0028730365,-0.0050062686,-0.04012366,0.04882952,-0.0044781854,-0.04838728,0.025403231,-0.00134946,0.0050251433,-0.012284965,0.06222464,-0.0035408004,-0.0008102028,0.007955929,0.001344186,0.0017636234,-0.0070040296,0.03963956,0.040373445,0.0030538794,-0.06231101,-0.012637236,-0.038484294,-0.031443354,0.011045393,-0.026784867,-0.025864532,0.037857573,-0.041807886,-0.031089954,0.05484748,-0.002936805,-0.031465568,0.03698304,-0.03806245,0.044132184,-0.07746687,-0.005131507,0.018010331,-0.030378988,0.09437147,0.016501267,-0.012995551,-0.058745418,-0.021033268,-0.028983487,-4.0018316e-05,0.07730752,0.042185903,-0.0022884407,0.07323668,0.0016392597,0.033492792,-0.04303169,-0.013861394,-0.026572252,-0.040079024,0.006458851,0.013250408,-0.007398578,-0.0195326,0.0074189417,-0.007267711,0.023878612,0.019278688,0.009920965,0.024802871,-0.0074062524,-0.00940793,0.043072805,-0.04718491,0.02419548,0.05976743,0.012788359,0.033089746,0.016289053,0.0068572173,-0.010887051,-0.034160104,-0.016294777,-0.005176902,-0.060890917,-0.014908541,0.027515072,-0.025395991,0.029504895,-0.00037729437,0.020953937,0.031900264,-0.009967028,-0.047642358,-0.055096865,-0.02913509,-0.05274931,-0.07513511,0.04630401,0.0066697565,-0.009794276,0.026726073,0.0008247,0.085683145,-0.0958857,0.005194901,-0.038036972,0.024078732,0.007633199,-0.027063042,-0.018978808,-0.012894878,-0.028987333,-0.033114873,0.047971606,0.025498478,-0.023212334,-0.012092473,0.026885718,0.011137239,-0.01685122,-0.04921531,-0.017105054,-0.010450677,0.0012924955,-0.020807479,0.075145744,-0.025204603,-0.002701806,0.04430718,-0.03086293,-0.071320206,-0.02527365,0.04770387,0.02506958,-0.018047996,0.0099532,0.021551015,0.047177877,-0.050298482,0.016513778,-0.033312008,0.025466554,-0.041066524,0.005236931,0.054611403,-0.0039289934,-0.031782445,-0.003592897,0.035424955,-0.037682608,0.013236562,0.068868585,0.05462775,0.05467958,0.05455593,-0.032848194,-0.09030882,-0.0007066772,-0.047472473,-0.0022852954,-0.027230186,0.03760215,-0.006543224,-0.023289941,-0.007044326,0.042805076,0.032098386,0.00441144,0.02953026,0.01543526,-0.038531188,-0.022050602,-0.030621232,-0.019954808,-0.011186995,-0.025283754,0.057636898,0.012493248,-0.039764553,-0.048684917,-0.060951207,0.0038015733,-0.027791737,-0.011318525,0.010485207,0.00960034,0.0013560657,0.028402887,0.0864096,0.028686473,-0.058554675,-0.05951342,0.0076216245,0.058932718,0.03658578,0.032611668,-0.049143612,-0.09887582,-0.023986919,-0.02022705,-0.07947908,-0.010486268,0.043469466,-0.010167014,0.01489256,0.016754903,-0.022327214,-0.019706331,0.021340111,-0.017060919,0.031214675,0.003192103,-0.009026978,-0.053716782,-0.01802027,-0.05427749,-0.022751143,-0.023760242,0.04665998,0.024667772,-0.02764514,-0.023168076,0.021602059,-0.0011151765,-0.051651444,-0.049933016,0.035262734,-0.047194235,0.033551212,0.019959258,0.015311794,0.021695336,-0.0024001037,0.040661994,-0.0011363309,0.035288975,-0.015034054,0.02865577,-0.017824415,-0.05963594,-0.07304799,0.006768721,0.013370497,-0.011348174,0.0139452135,-0.05160487,-0.04587021,-0.019430177,-0.015436272,-0.011952273,0.028387997,0.05750721,0.02439657,-0.043541886,0.025857419,-0.015737407,0.01759068,0.012995809,0.020554718,0.0033617811,0.03900255]	Keywords: self-attention, Transformer, BERT, auxiliary loss, Talking-head Attention\nKey Objects: attention heads, attention patterns\nRefers to Images: None\nHypothetical Questions:\n- Why do self-attention patterns in pre-trained Transformers sometimes lack linguistic backing?\n- How do auxiliary losses help improve the training of Transformer models?\n- What is the core motivation behind Talking-head Attention?\n---\nSummary:\nSome studies have shown that pre-trained Transformer models exhibit self-attention patterns that do not always align with linguistic principles. To address this, researchers have introduced constraints, such as auxiliary losses and specific attention mechanisms like Talking-head Attention, to improve Transformer training.\nOriginal Text:\nSeveral probing works have revealed that pre-trained Transformer models exhibit certain patterns of self-attention that are of little linguistic backing. As a representative work, Kovaleva et al. [68] identify several simple attention patterns in BERT. For instance, many of the attention heads simply pay attention to special BERT tokens [CLS] and [SEP]. As a result, some constraints can be introduced to boost the training of Transformer models. To this end, Deshpande and Narasimhan [27] propose to use an auxiliary loss, which is defined to be the Frobenius norm between attention distribution maps and predefined attention patterns.  \nTalking-head Attention [119] uses a talking head mechanism that linearly projects the generated attention scores from h$\\_{k}$ to h$\\_{k}$ heads, applies softmax in that space, and then projects to h$\\_{k}$ heads for value aggregation. The motivation is to encourage the model to move information between attention heads in a learnable fashion.\nContextualized Text:\nResearchers have observed that pre-trained Transformer models sometimes exhibit self-attention patterns that lack strong linguistic grounding. To rectify this, methods like introducing auxiliary losses and utilizing techniques such as Talking-head Attention have been proposed to refine and improve the training process of Transformer models.	{"tags": ["NLP", "transformers", "attention", "training"], "doc_id": "f70af643-7637-4cc9-bf4d-407ded33e403", "summary": "Some studies have shown that pre-trained Transformer models exhibit self-attention patterns that do not always align with linguistic principles. To address this, researchers have introduced constraints, such as auxiliary losses and specific attention mechanisms like Talking-head Attention, to improve Transformer training.", "doc_type": "text", "entities": ["BERT", "Transformer"], "keywords": ["self-attention", "Transformer", "BERT", "auxiliary loss", "Talking-head Attention"], "key_objects": ["attention heads", "attention patterns"], "contextual_text": "Researchers have observed that pre-trained Transformer models sometimes exhibit self-attention patterns that lack strong linguistic grounding. To rectify this, methods like introducing auxiliary losses and utilizing techniques such as Talking-head Attention have been proposed to refine and improve the training process of Transformer models.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["Why do self-attention patterns in pre-trained Transformers sometimes lack linguistic backing?", "How do auxiliary losses help improve the training of Transformer models?", "What is the core motivation behind Talking-head Attention?"]}
f7991b73-b9a6-45a7-8fe9-7c3985081ebd	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017910117,0.013230603,0.0217111,0.034410857,-0.032367352,0.06015085,-0.0019169716,0.072025836,0.010451942,-0.008488702,0.009967523,-0.029017579,-0.021410547,-0.015883617,-0.029506298,0.018919175,0.027702464,0.083722696,-0.011733066,-0.030701753,0.055366717,0.042513046,0.04150782,0.0021738817,-0.021701751,0.032695647,0.02855544,0.014267415,-0.033165734,0.035599753,0.026805924,-0.06893009,0.045309577,0.024446417,0.06822823,-0.030577406,0.012382793,-0.08818441,0.038187396,-0.013375829,-0.0016645872,0.059058134,-0.037545815,-0.04736673,-0.019496685,-0.024497509,-0.09348862,-0.027861916,0.016386501,-0.030518789,-0.019338436,0.02194441,-0.035572883,-0.0062901494,-0.052912455,0.0110245785,0.0063058473,-0.04285386,0.026900712,-0.12750077,0.0068238275,-0.0020373224,-0.0006426661,0.0049402495,-0.010903654,-0.036277376,-0.017265683,-0.005128129,0.042790297,0.12552172,-0.029702934,-0.006913958,0.035814278,-0.077289656,0.10256779,0.05396863,-0.0063114073,0.0013819439,-0.05968085,-0.022539118,-0.011604873,0.03535712,-0.04335574,-0.029026141,0.08348338,0.004206948,-0.02731965,0.022406537,0.05414152,-0.04706438,0.0045046173,0.0055833687,-0.016266128,0.07585817,-0.006312197,-0.053327527,-0.013766473,-0.020277034,-0.00052286923,-0.04443265,-0.004316336,0.029397901,0.09499265,0.11205247,0.035036378,-0.033708714,-0.02481185,-0.05538021,0.030256284,0.011450164,0.00051418546,0.03947581,-0.05687535,0.009018164,0.012883551,0.028687464,-0.026793877,-1.681127e-05,0.01904162,-0.0025844527,0.04584453,-0.018501226,-0.0025972058,0.025146298,0.061530095,0.03541465,0.052870106,-0.004872942,-0.043117385,-0.0111183645,0.038044047,-0.0074264314,-0.02185011,-0.009409254,-0.029898021,-0.0091238525,0.07195339,-0.027453924,-0.01724985,0.042022485,-0.015053604,0.0013301063,-0.03583085,0.01604261,-0.016774306,0.040370777,-0.07349336,0.032898117,-0.02241875,0.048875183,0.066902794,-0.038437687,0.034114722,-0.028652111,-0.013899655,-0.019586442,-0.010235054,-0.051873885,0.014400742,-0.032102413,-0.021679642,-0.013288159,-0.015142446,0.0794728,0.070768505,0.09979322,-0.011989364,0.025746448,0.00085657655,0.010477468,-0.0105280215,-0.011396587,-0.019637637,-0.03707573,0.0050026965,0.0018405141,0.016091745,-0.041747387,0.01724244,0.0050914306,0.012822608,0.057138655,-0.00021796135,0.029539965,-0.035522852,0.041370258,-0.0068181353,0.023193816,0.0059984853,-0.03360774,-0.00047877,0.017031996,0.0094904285,0.0006301656,0.039910138,0.047803897,0.03763699,-0.071149945,-0.0071963784,-0.031749353,-0.03515223,-0.0065710805,0.006640413,-0.042905748,0.01953059,-0.017383184,0.024853058,-0.039821,0.002262942,0.023089817,-0.009427624,-0.023887577,-0.016113982,0.045602653,0.008658742,-0.003866611,-0.02091355,0.037650432,0.026759136,-0.052223064,-0.031028818,-0.01934785,-0.08343627,0.029466512,-0.012831579,0.0626448,-0.019556256,0.031233024,0.020359904,-0.0147630945,-0.011758074,-0.007672698,-0.04629368,0.024854213,-0.042947475,0.016027767,0.0055214227,0.022149159,-0.055661436,0.015827434,0.045607176,0.08339662,0.028719064,0.098262735,0.026181158,0.009676062,0.028593464,0.0043054637,0.023620648,-0.0032784254,-0.048939217,0.050030343,0.023942625,-0.02127424,-0.0139097655,-0.011618504,-0.004515493,0.018337926,0.0073073446,0.0014588465,0.02419706,0.00037575068,0.0015826018,-0.005438305,0.022525702,0.03591466,0.013085732,0.023677144,-0.043768756,0.0017117664,0.013651032,0.05069465,0.01746479,-0.008391091,-0.054018985,0.042509373,0.035269283,-0.027003933,0.005928181,-0.027619438,-0.017505262,-0.010712996,0.030234715,-0.0005808601,0.020745102,-0.0012836589,0.037650213,0.00040797796,-0.013086351,0.006585593,-0.016592063,-0.025496192,-0.014503594,-0.018915096,-0.009189885,-0.020953316,0.01036338,-0.03882155,-0.014986271,0.03254526,0.13487162,-0.051654153,0.017863961,0.03927494,0.026541399,-0.003983467,-0.082951434,0.04862372,0.028791122,0.012765968,0.017443756,0.003188914,-0.0060089715,0.0077329013,0.031414796,0.104332894,0.06587324,0.014492983,0.0023293546,0.0048563927,0.003611073,0.016779773,0.009769513,0.022636123,0.033849496,0.0014857235,0.01738942,-0.011563045,-0.054400075,0.064507656,-0.06412951,0.028153444,0.06173542,-0.0273577,0.029823797,0.019848088,-0.015450712,0.025957804,0.0188082,0.035552397,-0.03577187,-0.044850588,0.028550688,-0.005380837,-0.039242305,0.02806937,-0.0061381417,-0.0060195033,0.02269116,0.0015892762,0.031277888,0.03234931,-0.018068241,-0.010453718,-0.030923417,0.0010394166,-0.031838454,0.062155,0.004699552,-0.0012843859,0.080961496,-0.099978186,0.0886305,-0.07801744,0.050812352,-0.02161684,-0.069849186,0.016336206,0.058795523,-0.02344009,0.0019761214,-0.014233454,0.034986354,-0.01592451,-0.050177597,0.040930226,0.058969986,0.038875263,-0.026086364,-0.022812506,0.017747167,0.005841881,-0.009104081,-0.043002415,0.042259764,0.006023127,-0.00405467,0.071938634,-0.009270539,0.020553311,-0.011814302,0.030214792,0.0037007278,-0.021946607,-0.0321222,0.020661237,-0.0010493827,0.028041733,0.032878328,-0.06302966,0.05295505,0.028578257,-0.047411565,0.003072455,0.024688546,-0.001822115,-0.09049404,0.03195328,0.03417158,0.013166994,0.035608586,-0.022610726,0.003300755,0.0038338464,0.025275417,-0.032863777,-0.0054412475,0.0020347692,-0.045272358,0.0079359105,-0.0053339144,0.06675414,0.0052372967,0.032303333,0.043453913,-0.00093054917,-0.058967784,0.03148233,-0.021571752,-0.039704904,0.04071458,-0.013847834,-0.016519312,0.0023170945,-0.01790717,0.034922004,-0.04622166,0.06895155,0.008976162,0.026415398,0.05810952,-0.03246663,0.027140617,-0.022853402,-0.008610271,-0.02270667,0.03908958,0.017142816,-0.052128922,-0.019839311,0.036344018,0.023169845,0.014957109,-0.0033917846,0.07098808,-0.012875517,-0.010737341,-0.044395603,-0.031971376,0.007526035,0.022594742,0.02383929,0.04751017,-0.011650996,-0.0026771491,-0.021610854,-0.017548477,-0.047347542,-0.012465117,-0.031708024,0.0036597305,0.050549526,0.0016085097,0.03628293,0.018505184,-0.0410334,-0.019019319,0.046335343,0.0663632,0.0012690354,-0.07873214,-0.005102995,0.0035895952,-0.00010710208,0.008030584,0.011154683,-0.02455091,-0.014887067,-0.0036966903,0.026024066,-0.0025825412,0.0059184646,-0.012481884,0.044150613,0.03346132,-0.07147013,0.0020438572,-0.014756487,0.018740257,0.025369003,-0.046070796,-0.004618064,0.093360215,0.06055887,0.02491287,0.023753708,0.017409101,-0.0011488654,-0.015634865,0.08339998,0.007889237,-0.03214031,0.03617162,0.019084461,-0.051289257,0.026088115,-0.010180499,0.03474266,-0.034324627,0.0008797841,0.0066345497,0.0022502595,0.035522826,0.030746603,0.016181374,-0.0033586775,0.026359724,-0.015670046,0.022726288,-0.09835454,0.009635956,-0.024654677,-0.052231543,0.017333116,-0.06244759,-0.08277588,-0.018499374,-0.070046015,0.028154628,-0.018587241,0.0038460013,-0.050518338,0.0015080816,-0.034243926,0.049523275,-0.027208146,0.026368545,0.0013833799,-0.0006757424,0.06392188,0.0037105943,0.017371228,-0.013102898,0.027034529,0.0010091165,0.05689555,0.05501349,0.028232682,0.00984876,0.07803547,0.01678758,0.023659064,-0.015477486,-0.049385067,-0.02527373,-0.00633519,-0.029908357,-0.027609242,-0.0033086082,-0.008678556,-0.018576879,0.014956442,0.0059771044,-0.0029810197,0.009405251,0.033289418,0.015490146,0.001386009,0.04325586,-0.0037482448,0.039593577,0.035052653,0.05015254,0.022178698,-0.030443598,0.02722028,-0.028121045,-0.019178698,-0.0071429187,-0.0034048373,-0.023273788,-0.03100404,0.027303567,0.028956594,0.01124278,-0.04139282,0.036911808,-0.012525706,0.015447137,-0.06823055,-0.037430163,-0.03304306,-0.07321646,-0.011712574,0.030522093,0.021994427,-0.008858192,0.001307866,0.015260965,0.07353454,-0.09062297,-0.0062362854,-0.052068323,-0.023447277,-0.023491004,-0.0270123,-0.022490349,-0.015933754,0.02114623,-0.027491925,0.06996058,-0.0005503546,-0.023446541,-0.016053911,-0.008204639,-0.017032081,0.004220458,-0.012727732,0.005580404,-0.014563398,0.005265163,-0.014296905,0.061127957,-0.03269585,-0.0060254433,0.026082447,0.016471025,-0.1018643,-0.03172534,0.029352898,0.047974385,-0.017750654,0.03717674,0.0101120705,-0.012329472,-0.05868627,0.026909798,-0.02950281,-0.036568932,0.0111948615,0.027762076,0.04008986,-0.028982164,-0.060484704,-0.016297018,0.0042024804,-0.0010287336,0.022726133,0.064988405,0.038048364,0.062197015,0.055780597,0.012664033,-0.071087345,-0.03332626,-0.037811674,-0.016353354,-0.016448274,0.046743736,0.03323455,-0.023800801,-0.0061188815,-0.033972636,0.011955614,-0.0062278993,0.020397078,-0.010337517,-0.027620848,-0.010098941,-0.0030458325,0.021745458,0.022224275,0.014024292,0.07591689,0.012160236,-0.008980508,-0.055285092,-0.10114954,-0.012314153,0.04045096,-0.010570516,0.012113704,-0.00046165785,-0.03795638,0.0018974895,0.06777573,-0.007049966,-0.048013385,-0.025247863,-0.004992053,0.029111251,0.048285846,0.013900551,-0.03210456,-0.07448568,0.0114000505,-0.014444499,-0.07166041,0.02203682,-0.018872935,-0.031145817,-0.005058,0.012423796,0.021382872,0.019134277,-0.0013996112,0.037085235,0.0046962006,0.0009713178,0.02857508,-0.095237136,-0.021368574,-0.0011736866,-0.00881791,-0.008384052,0.050958563,0.03586058,-0.006740518,-0.035887938,0.014585181,0.0046886015,0.009671309,-0.016555276,0.04914114,-0.0019653884,0.025768882,-0.024376364,0.01790712,0.07214973,-0.015423618,-0.027673136,-0.005199056,0.012799463,0.004617129,-0.008439496,-0.016341552,-0.048224192,-0.023524681,0.019770496,0.0025124087,-0.037461847,-0.003955039,-0.037903003,-0.0485483,-0.02860128,-0.022658085,-0.03080071,0.047569644,0.02229085,0.010338648,-0.048116844,-0.0016936025,0.0018457364,0.08918362,-0.0020692388,0.02167998,-0.036364313,0.039413426]	Keywords: multi-head attention, query projection, key projection, mixing vector, collaborative attention\nKey Objects: Query Projection, Key Projection, Mixing Vector, Attention Heads\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of sharing query and key projections in Collaborative Multi-head Attention?\n- How does the mixing vector contribute to the filtering of projection parameters?\n- What potential benefits might arise from adapting Eq. (3) in this manner?\n---\nSummary:\nCollaborative Multi-head Attention [21] introduces a method where query and key projections (W Q and W K) are shared across all attention heads, along with a mixing vector (m) to filter projection parameters, adapting Eq. (3) to achieve a specific attention outcome.\nOriginal Text:\nCollaborative Multi-head Attention [21] uses shared query and key projection W Q and W K and a mixing vector m$\\_{i}$ for the i -th head to filter from the projection parameters such that Eq. (3) is adapted to  \n$$\\mathbf h _ { k } = \\mathbf A \\mathbf t e t { \\mathbf T i n a n d } ( Q W ^ { Q } \\partial g _ { i } ( m _ { i } ), K W ^ { K }, V W ^ { V } _ { i } ),$$  \nwhere W Q and W K are shared by all the attention heads.  \n4.6.2 Multi-head with Restricted Spans. Vanilla attention adopts full attention spans assume, where a query can attend to all of the key-value pairs. However, it is often observed that some heads focus their attention distribution mainly in a local context while some other heads attend to broader contexts. It could thus be beneficial to restrict the attention spans:  \n-  Locality. Restricting attention spans induce explicit local constraints. This is advantageous in cases where locality is an important prior.\nContextualized Text:\nTo improve upon standard multi-head attention, Collaborative Multi-head Attention [21] uses shared query and key projections (W Q and W K) and a mixing vector (m) for each head to filter projection parameters. This adaptation of Eq. (3) aims to refine the attention process by encouraging heads to focus on relevant features.	{"tags": ["architecture", "attention", "transformers", "NLP"], "doc_id": "f7991b73-b9a6-45a7-8fe9-7c3985081ebd", "summary": "Collaborative Multi-head Attention [21] introduces a method where query and key projections (W Q and W K) are shared across all attention heads, along with a mixing vector (m) to filter projection parameters, adapting Eq. (3) to achieve a specific attention outcome.", "doc_type": "text", "entities": ["Collaborative Multi-head Attention"], "keywords": ["multi-head attention", "query projection", "key projection", "mixing vector", "collaborative attention"], "key_objects": ["Query Projection", "Key Projection", "Mixing Vector", "Attention Heads"], "contextual_text": "To improve upon standard multi-head attention, Collaborative Multi-head Attention [21] uses shared query and key projections (W Q and W K) and a mixing vector (m) for each head to filter projection parameters. This adaptation of Eq. (3) aims to refine the attention process by encouraging heads to focus on relevant features.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["What is the purpose of sharing query and key projections in Collaborative Multi-head Attention?", "How does the mixing vector contribute to the filtering of projection parameters?", "What potential benefits might arise from adapting Eq. (3) in this manner?"]}
0a2b6342-2fee-4555-939a-e30b6c63e21b	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.029239876,0.018529579,-0.0009006232,0.024917232,-0.027978688,0.03570463,0.06599696,0.07726413,0.028185671,0.0037520376,0.029063476,0.014051317,0.0109993685,-0.037217736,-0.057968084,-0.016981734,0.03616574,0.106686644,-0.0042986963,-0.03035909,0.034441352,0.011791666,-0.0032132342,-0.056091476,0.00029979172,0.04139102,0.014617812,-0.0129150655,-0.03183103,0.000859868,0.032278758,-0.026359178,0.044773005,0.008562652,0.03358205,-0.040555816,0.01489361,-0.06826886,0.008746527,0.013735528,-0.04531053,0.013234482,-0.022310376,-0.03214163,-0.01668012,-0.058098778,-0.1164151,-0.012128285,0.009317337,0.0143340845,0.0002575036,0.008867233,-0.024894712,0.025997108,0.017315974,0.008302834,-0.023483472,-0.01742639,-0.022568677,-0.07338141,-0.00937215,0.007911299,-0.033080872,0.004194431,-0.010835048,-0.0043594725,0.026127242,0.006500637,0.03882911,0.12591882,-0.05866647,-0.021654021,-0.019657275,-0.05929262,0.10723645,0.022644816,-0.017838549,-0.023464177,-0.077870615,-0.0045603993,-0.017126229,0.05631984,-0.054287888,-0.02246665,0.089193225,-0.00521756,-0.033897884,-0.010150329,0.037673015,-0.06527235,0.023741033,-0.0073968023,-0.02746696,0.055019043,-0.03444734,-0.08080443,0.013550812,-0.017357113,0.012024347,-0.060786556,0.04013814,0.022450171,0.07229021,0.08957778,0.034586485,-0.042406373,-0.01877211,-0.020852868,0.008616945,0.010978721,0.028654244,-0.002742628,-0.03967574,-0.005576674,-0.033369973,0.031453855,-0.013999553,-0.011081488,-0.007966912,-0.0443769,0.025309127,-0.020292712,-0.029968137,0.014735999,0.07741484,0.046633244,0.021348331,0.0010654754,0.001377173,0.0013978934,0.06833797,-0.007632988,-0.048992157,0.0065445052,-0.0022525156,-0.008095235,0.0714066,0.014033775,-0.007344206,0.020886753,0.023949277,-0.06378408,-0.028319389,0.043417405,-0.012987093,0.0006100002,-0.07810785,0.028725095,-0.016063416,0.038801357,0.028509747,-0.024725767,0.022626715,-0.029336043,4.7653055e-05,-0.013666875,-0.037181273,-0.036683284,0.0015039636,-0.06323379,-0.05149685,-0.015934767,-0.03368025,0.075702086,0.07437631,0.09276589,0.00016143399,-0.016800761,0.028064683,0.019057527,-0.04640472,-0.007535405,0.0019566766,-0.026488416,-0.011976461,-0.036643725,0.02964027,-0.043182194,0.018058533,-0.030794637,-0.03865774,0.02505839,0.0052183513,0.030841395,-0.011742509,0.00566461,-0.020255182,0.0327797,-0.047536418,-0.0019061712,0.0070627765,-0.027606793,0.0044926777,0.007870308,-0.0122287525,0.024883363,0.040129118,-0.063588604,0.006711824,-0.04559555,-0.009853789,0.025583267,0.007097459,-0.044340592,-0.010171592,-0.028232347,0.005618268,-0.00946565,-0.016112013,0.014182593,-0.0024731928,-0.028761804,-0.051030263,0.039135188,-0.0045072646,0.010666349,-0.05966837,0.04104303,0.05546115,-0.00070545817,-0.0681347,0.022546288,-0.0021311487,0.080166526,-0.046215694,0.041014746,-0.011879644,-0.019833641,0.022357292,0.00614806,0.007158019,0.025103826,-0.014977508,0.068619065,-0.047266494,0.034200817,-0.010727055,-0.007011857,-0.028378861,0.024500813,0.054544576,0.051242117,0.031104648,0.10360316,0.0020993873,-0.009384029,0.005716507,0.03860908,-0.02215724,0.026340172,-0.026208961,0.029987419,0.06447409,-0.0029390915,-0.01327439,-0.038455836,0.048715066,0.037940223,0.008327073,-0.039614182,0.014192253,0.0203208,0.01245042,0.018849198,0.0007460564,-0.044146407,0.030107211,0.010473428,-0.053250056,-0.034931272,0.021378811,0.046730086,0.004742457,-0.015728664,-0.065673515,-0.017075049,0.061389472,-0.053330973,0.018998893,-0.017245587,-0.02879175,-0.024923235,0.03846586,0.00039335672,-0.006607685,-0.013973254,0.030731207,-0.042594597,-0.03099688,0.010551753,-0.018961309,-0.04386775,0.023509545,0.0051432783,-0.022127332,0.023035642,-0.0033920691,-0.021576174,-0.0025949536,0.021446943,0.13116722,-0.025478851,0.017310232,0.008374143,-0.0010603971,-0.0011722937,-0.0813177,0.049255945,0.030455036,-0.02162292,-0.030014172,0.014119368,-0.06556093,0.008252262,0.023919318,0.052938957,0.036724746,-0.042951956,-0.022808904,0.01813942,0.032511804,0.037032295,0.013646918,0.021453703,0.06276525,0.020296313,-0.0032260958,-0.008826589,-0.07022775,0.045960113,-0.08257043,0.04389189,0.07196131,0.0060149063,-0.009433181,-0.016621398,-0.0331358,0.027226333,0.005831365,-0.017321475,-0.043128196,-0.03777405,0.029946998,-0.016434219,-0.020887759,0.04901705,-0.020623019,0.017452095,0.030726034,0.0047983634,0.029833592,0.019914873,0.007368455,0.026119746,-0.0052167694,-0.029342715,-0.049620893,0.0003669066,0.00022209287,-0.022475233,0.0548619,-0.06441644,0.069302455,-0.05263543,0.009657391,0.004442097,-0.070042945,0.008348109,0.053735953,-0.070106685,0.005135771,-0.022576945,0.04869931,-0.067340724,-0.05201974,0.03692247,0.08542834,0.031315524,-0.024768177,-0.004753465,0.018492546,-0.024033014,0.011500738,0.021895226,0.06825378,-0.009837467,0.0050219144,0.07444454,-0.0072940663,0.037843734,-0.04203209,0.014186002,0.014363732,-0.0025244625,-0.037961196,0.020483626,0.009731004,0.027265584,-0.02174192,-0.04648,-0.04621297,0.018175097,-0.058778346,0.06199637,0.011159727,0.042269964,-0.08044705,-0.021955168,0.033515625,0.048294965,0.0099968165,-0.026406141,-0.035830162,-0.0073224013,0.016407557,0.024027662,0.010368619,0.026587803,-0.047301408,-0.01911925,-0.021268567,0.02422698,0.0061561903,0.06869887,0.07730426,0.0010211084,-0.04703719,0.036045723,-0.022676026,-0.046878017,0.012309945,-0.04883913,-0.043237112,0.051342312,-0.009754268,0.009809417,-0.013935027,0.07294048,-0.035008833,0.011411897,0.04509565,-0.01658362,0.014643848,-0.020021442,0.040322322,-0.011100644,0.019124351,0.025251126,-0.03718811,-0.014780558,0.036243178,0.03933519,0.022209836,-0.040087067,0.030200329,-0.014255864,0.002662279,-0.058552008,-0.0008674765,-0.0015854995,0.016131286,0.02515591,0.0036740948,0.008725461,-0.0016723756,-0.043151945,-0.0026424753,-0.032841124,-0.031165102,0.033898376,0.010313837,0.0063478225,-0.026909554,-0.0123334015,0.04490407,-0.025739426,0.041477695,0.06104543,0.026248494,0.010471777,-0.07796692,-0.0007580812,-0.009926339,-0.011374398,-0.010658931,-0.016924882,-0.024163935,-0.013566825,-0.039396252,0.030155443,-0.0069788545,0.05132227,0.015901303,-0.024022294,0.03895548,-0.050698943,-0.014150374,-0.016665632,0.05213714,-0.018583449,-0.0005080496,-0.016761394,0.024584804,0.033289857,-0.018203106,0.0505877,-0.0074238684,-0.010185942,-0.014037936,0.06872461,0.016435655,0.013117047,-0.008525625,0.054666042,-0.008103823,0.021571338,0.029692123,-0.0052059884,-0.017057711,0.03833653,-0.024860632,0.004882638,0.044601075,-0.025354307,0.0037975404,0.012285454,0.045121387,0.052033547,0.0053288215,-0.08035311,0.020076515,-0.016325122,-0.054615606,0.0056832647,-0.09321109,-0.04076827,-0.014515086,-0.06865613,0.016109494,0.014150761,-0.024517814,-0.017571788,0.050245292,-0.039949458,0.033868045,-0.07262104,-0.00897457,-0.009718893,-0.008780866,0.101412565,0.016765263,0.02368137,-0.05009586,0.0064900783,-0.039743118,0.02228785,0.057520866,0.038153592,0.0028970253,0.07275409,-0.026530422,-0.019302435,-0.015984762,-0.044755694,0.003124454,-0.018074693,0.013878974,-0.013433375,-0.03385604,0.026426177,0.002858402,-0.014703248,-0.021880668,0.0004358174,0.033268906,0.019973183,0.014677728,-0.019706301,0.028136835,0.029812224,0.044300206,-0.0053189206,0.035500947,0.019247135,-0.02560499,0.0042310436,-0.0021354978,-0.031762954,-0.030175088,-0.0059003667,-0.050750222,-0.0494663,-0.0013747239,-0.011263416,-0.01761027,-0.033848092,-0.020426651,-0.03608036,0.03250644,-0.03533888,-0.054906555,-0.018729834,-0.038853113,-0.029485064,0.039459996,0.024367962,0.027998775,0.031536542,-0.026877517,0.039522693,-0.0698528,0.038201373,-0.0011420883,0.032124765,0.034738615,-0.053056035,0.019733781,0.0058331117,0.0014018244,-0.04135569,0.03809637,0.041829966,-0.043299258,-0.015477223,0.023942254,-0.0005464955,-0.0060932776,-0.009505506,0.02350902,-0.010764078,0.03567442,-0.011420768,0.0330823,-0.016980967,0.021212883,0.027438354,-0.0011838186,-0.05383549,-0.01632189,0.014559149,0.06605145,0.0007531091,0.05167301,0.008907449,-0.001370377,-0.03543989,-0.014237647,-0.0199633,-0.007202292,-0.007945722,0.021446671,0.009881727,4.541376e-05,-0.022982163,0.008328687,0.006007634,-0.0009484008,0.054745074,0.050355367,0.06362467,0.06982334,0.037048347,0.034164783,-0.052990276,-0.016899727,-0.04300375,0.00773701,-0.0060992464,0.03371056,-0.012553769,-0.010199112,-0.03882465,-0.0046061585,0.024215758,0.011124092,0.048297554,-0.019061645,-0.047836684,-0.02248782,-0.021213982,0.00779935,0.012361695,-0.03295526,0.08380533,0.07051767,-0.0009802227,-0.071225755,-0.044986956,0.018307978,0.018042546,-0.011146255,0.022576364,0.02984436,0.004970713,-0.030447325,0.06402926,0.06845451,-0.026075533,-0.06724234,0.016681954,0.022166338,0.06516323,-0.0121783335,-0.018168667,-0.11683689,0.029956983,0.012706309,-0.038177665,0.034585886,-0.004341448,-0.04428272,-0.025603041,0.016060606,-0.016958926,0.0171849,-0.0021012488,0.039101414,0.006818921,0.010925758,-0.003919826,-0.0636284,-0.015206406,-0.030839212,-0.007343031,0.0024026788,0.04951846,0.044721827,-0.018421048,0.026837151,0.028547583,0.030296843,0.0056815688,-0.047298577,0.07930853,0.0028454813,0.034601655,-0.011616227,-0.008109574,0.092330135,-0.008915671,0.009506097,-0.038557447,-0.0017223653,0.023730854,0.005309452,0.009376512,-0.035887126,-0.05921905,-0.00933231,0.015566516,0.0063505387,0.029150307,-0.056627564,-0.027475031,-0.021967258,-0.017933767,-0.00012208894,0.03864408,0.009133279,0.045545522,-0.08231476,-0.03244844,-0.020795139,0.048908714,0.0010180519,0.024140513,-0.018833632,0.0037725435]	Keywords: attention spans, locality, efficiency, masking\nKey Objects: attention spans, mask\nRefers to Images: None\nHypothetical Questions:\n- Why is locality an important consideration in attention mechanisms?\n- How does restricting attention spans contribute to the efficiency of Transformer models?\n- What is the role of the mask value in restricting attention spans?\n---\nSummary:\nRestricting attention spans in Transformer models can improve locality and efficiency, allowing for scaling to longer sequences. This is achieved by multiplying attention distribution values with a mask, which can be learned or fixed.\nOriginal Text:\n-  Locality. Restricting attention spans induce explicit local constraints. This is advantageous in cases where locality is an important prior.\n-  Efficiency. If implemented appropriately, such a model can scale to very long sequences without introducing additional memory footprint and computational time.  \nRestricting attention spans can be expressed as multiplying each attention distribution value with a mask value and then re-normalize, where the mask can be expressed as a non-increasing function that maps a distance to a value in [0, 1]. A vanilla attention assigns a mask value of 1 for all distances, as depicted in Fig. 10(a).  \nSukhbaatar et al. [126] propose to use a learnable attention span, as depicted in Fig. 10(b) . The mask is parameterized by a learnable scalar z and a hyperparameter R . The experiments on  \nFig. 10. Three types of span masking function m ( x ) . The horizontal axis represents distance x and vertical axis the mask value.\nContextualized Text:\nTo improve performance and scalability in Transformer models, techniques restricting attention spans are used. Restricting attention spans induce explicit local constraints and allow models to efficiently process longer sequences. This approach involves multiplying attention distribution values with a mask, which can be either learned or fixed to control the attention range.	{"tags": ["NLP", "deep-learning", "architecture", "efficiency"], "doc_id": "0a2b6342-2fee-4555-939a-e30b6c63e21b", "summary": "Restricting attention spans in Transformer models can improve locality and efficiency, allowing for scaling to longer sequences. This is achieved by multiplying attention distribution values with a mask, which can be learned or fixed.", "doc_type": "text", "entities": ["Transformer", "BERT"], "keywords": ["attention spans", "locality", "efficiency", "masking"], "key_objects": ["attention spans", "mask"], "contextual_text": "To improve performance and scalability in Transformer models, techniques restricting attention spans are used. Restricting attention spans induce explicit local constraints and allow models to efficiently process longer sequences. This approach involves multiplying attention distribution values with a mask, which can be either learned or fixed to control the attention range.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["Why is locality an important consideration in attention mechanisms?", "How does restricting attention spans contribute to the efficiency of Transformer models?", "What is the role of the mask value in restricting attention spans?"]}
67fb171b-6622-4fb3-a9c0-e0c2e44e1d2d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.025051614,-0.00943568,0.018832628,0.01718794,-0.049534656,0.029203538,0.018284354,0.060729187,0.035630737,-0.0072569945,0.036003646,0.023682278,0.013258034,-0.03537812,-0.0474594,-0.02522582,0.013250324,0.08899453,-0.0034979614,-0.037298474,0.047283776,0.020020477,0.008994118,-0.03413417,0.009396766,0.04201971,-6.245114e-05,-0.043620516,-0.02286189,-0.00087754946,0.035708074,-0.033578373,0.037950084,0.012266458,0.021882793,-0.025173757,0.007795357,-0.039050672,0.012771013,-0.024992801,-0.05916138,0.043968838,0.013845074,-0.034566823,-0.0074615562,-0.05195892,-0.08784317,-0.025544517,0.005969381,0.0012972226,-0.018317813,0.008667302,-0.022768578,-0.00030412248,0.014295407,-0.013823888,-0.026568247,-0.031396296,-0.02034162,-0.08901063,-0.034032226,0.054056104,-0.028851254,0.020497862,0.032457203,-0.028600046,0.03264332,0.0077801263,0.025201442,0.13810611,-0.04918103,0.020362899,-0.012299685,-0.05817375,0.11036322,0.03979119,-0.016323356,0.0139332265,-0.051096562,0.008643443,-0.017654698,0.039747752,-0.05554716,-0.033020232,0.08113894,0.0025184518,-0.044090677,0.010898383,0.044748016,-0.03364999,-0.00957109,-0.011210592,-0.033911668,0.08000445,-0.0576538,-0.046728794,-0.021863548,-0.03110006,0.0024904262,-0.04202842,0.022201797,0.0070930272,0.11384794,0.10707519,0.025219908,-0.047321536,-0.01730998,-0.010119959,0.016047914,0.0068575367,0.04428753,0.004307522,-0.029417194,0.01429872,-0.027909227,0.024793476,0.0017636465,-0.021257848,0.002492318,-0.003942206,0.033271853,-0.005269818,-0.04552186,0.0051481766,0.053955175,0.054666184,-0.0022646873,-0.026970249,-0.0020838012,0.015738742,0.046099126,-0.013354814,-0.028635124,0.037210874,-0.015694672,0.005488866,0.06377472,0.0045601474,0.00043713045,0.00652167,0.013778724,-0.042169157,-0.034848046,-0.0011093011,-0.032250058,0.026027322,-0.09361412,0.032552674,-0.0068002827,0.059328236,0.0027907088,-0.008868683,0.03803139,-0.02960434,-0.030110363,0.007393584,-0.047362074,-0.03539896,-0.011517107,-0.037600264,-0.054387294,-0.04349712,-0.03923921,0.056083154,0.06390197,0.10976844,0.051219556,-0.022927698,0.015747877,0.01690592,-0.055960998,0.00652805,0.026607586,-0.003945386,-0.007129731,-0.009150671,0.011222838,-0.03714289,0.0073439195,0.0067017362,-0.016589617,0.020395743,-0.009524954,0.019945357,-0.037949033,0.020156566,-0.028451787,0.04968844,-0.047054786,-0.028870927,-0.036525466,-0.03747131,-0.024081195,-0.015570142,0.01351969,0.061634775,0.03531668,-0.07673809,-0.021433126,-0.00079043704,0.0027106907,0.0021306905,-0.0014015287,-0.045271613,-0.006686896,-0.018802723,-0.0116042225,-0.0070552495,-0.014212278,0.030516954,-0.023884904,-0.048029736,-0.0057130475,0.046209384,0.042746928,-0.011741837,-0.046401978,0.029819164,0.06750352,-0.0023549176,-0.040495124,0.005478135,-0.031597085,0.057874423,-0.025971305,0.0029344968,-0.042139675,0.019806596,-0.017955756,0.016168404,0.018661367,0.018688083,-0.03180689,0.04612478,-0.045013912,0.035525043,0.03314602,-0.0045565083,-0.007507718,0.027096888,0.058119234,0.05579422,0.0109617775,0.08374957,-0.017064162,-0.00044261635,0.034644753,0.03529574,-0.018353363,0.025385726,3.078036e-05,0.05426141,0.09952913,-0.009054847,-0.01096088,-0.0102272015,0.023639139,0.0075686,0.03997512,-0.017669968,0.028283216,0.02256065,0.0032937187,0.024439467,0.016130561,-0.011212237,0.016717207,0.013111979,-0.03811331,0.003377968,0.026914751,0.04860709,-0.023233924,-0.0153998975,-0.082039826,0.015445377,0.023257283,-0.013127096,0.039470676,-0.022904126,-0.028209362,-0.004191191,0.054958683,-0.009395449,0.0030534049,-0.017105766,0.049409214,-0.027143486,-0.037449975,0.0023431745,-0.005899611,-0.039434504,0.06087491,-0.0070216265,0.0034100176,0.0054304875,-0.0020980032,-0.016519839,-4.3286662e-05,0.032682456,0.10575337,-0.037003398,-0.0013203324,0.0324359,0.029359398,0.002570037,-0.06812038,0.020120423,0.029566986,-0.0068440097,0.013165862,0.009940447,-0.082302704,0.024477508,0.03399607,0.025636205,0.040109836,0.020760197,0.0023508875,0.03785451,0.059537474,-0.00954314,0.02878565,0.024924185,0.035220396,-0.0021981217,0.0057593384,-0.017918196,-0.07441348,0.0680151,-0.073185995,0.056988597,0.06186518,-0.019132493,-0.026481919,0.006700909,-0.037613396,0.021919828,0.008080951,-0.017858796,-0.020305242,-0.039462082,0.041010235,-0.0025958796,-0.039798617,0.03957164,-0.023348596,-0.0022205214,0.029181598,-0.01220404,-0.002182597,0.029012945,-0.020511208,0.01374569,0.023575185,-0.07424489,-0.037215978,-0.013842547,0.011374108,-0.030916179,0.05881004,-0.07992355,0.09557652,-0.05213629,0.027870417,-0.008588707,-0.08644311,0.010679684,0.04150183,-0.047024816,-0.029304348,-0.038696896,0.065188535,-0.039435487,-0.020391492,0.03476815,0.06026062,0.009014388,-0.027920913,-0.020958498,-0.011332192,-0.009630097,-0.0047419365,-0.01157337,0.06082975,0.004747557,-0.004069327,0.04957721,0.002873524,0.030718861,-0.031626895,0.0092759095,-0.014386813,0.022374216,-0.04576244,0.02102417,-0.00045703576,0.035493325,0.0076351874,-0.03375474,-0.039460804,0.02211409,-0.055825364,0.04426792,0.059630327,0.037336074,-0.063620746,0.03355154,-0.0067794533,0.041775115,0.021596316,-0.009095292,-0.024976425,-0.008511703,-0.0031330178,0.015208371,0.0022072275,0.02951651,-0.045163326,-0.030145401,-0.047351558,0.039489295,0.009627805,0.04535887,0.06991596,-0.0068536587,-0.07204445,0.06922351,-0.027936283,-0.046690863,-0.011618879,-0.060365032,-0.03947257,0.04768086,-0.008664972,0.010789979,-0.0327008,0.074840076,-0.017489502,0.055302937,0.04904451,-0.0015006105,-0.011590265,-0.0068619605,0.005017385,-0.05831283,0.04443582,0.005194621,-0.0214232,-0.022580022,0.039840993,0.019154815,0.032686163,-0.02719495,0.0466952,-0.016285766,5.399365e-06,-0.057569496,-0.02612207,-0.012423015,0.012695638,0.03951194,0.01355127,0.012795668,0.0026383076,-0.022932313,-0.0073767514,-0.04021836,-0.038425997,-0.000664889,-0.014578065,0.020027423,-0.0310957,0.0067080236,0.029067233,-0.006507871,0.013833194,0.07124971,0.035390064,0.030312277,-0.047469668,-0.022303624,-0.019431721,0.032232344,-0.035669338,-0.024233032,-0.012750236,-0.03381213,-0.020797847,0.024950705,0.0061228015,0.032082725,0.023045087,-0.00740694,0.01126494,-0.032293342,-0.030458422,0.021872249,0.022631066,-0.019055072,-0.016771106,0.01626407,0.02349363,0.03033834,-0.017636104,0.056640334,-0.028528677,-0.0024865095,-0.02587649,0.049625184,-0.0042231493,-0.01723203,0.0037280803,0.037293144,-0.036096714,0.016522003,-0.0055590263,-0.026459787,-0.007823941,0.040562727,-0.013813486,0.013067935,0.025519412,-0.022461526,-0.0070150723,-0.0013349783,-0.0013124571,0.041656036,0.048154693,-0.093911976,0.023154806,-0.028995082,-0.04216306,-0.0047247913,-0.07129686,-0.04933159,0.0053188005,-0.072363615,0.018550457,0.0038732837,-0.023777422,-0.028098397,0.021752117,-0.016722403,0.051552627,-0.07469283,0.0011092707,-0.013036251,0.023998171,0.09325991,-0.011364491,-0.011899616,-0.045676254,-0.0002157546,-0.034320485,0.034419615,0.04139315,0.04714874,-0.019782396,0.067553185,0.0026933555,0.012099699,-0.020636005,-0.028981112,-0.012257877,-0.020730093,-0.014682121,0.008601278,0.012563712,0.042978574,-0.007373169,-0.02298729,0.016112952,-0.016114974,0.013380873,0.03710272,0.039323915,0.0042223404,0.05763772,0.009022809,-0.006031502,0.024469377,0.029136235,-0.0017290139,-0.04022222,-0.01224372,-0.006463327,0.01493335,-0.049112145,0.03202203,-0.048188116,-0.03844638,-0.021690695,0.00036739293,-0.023214528,-0.034606148,0.01557036,-0.016183248,-0.008942859,-0.048902463,-0.0728385,-0.028630234,-0.043660488,-0.027260836,0.059865713,0.0122521315,0.025493307,0.0516664,-0.04727619,0.071758136,-0.0959027,0.012546358,-0.0039594863,0.048126895,0.013269166,-0.024796953,-0.009074466,0.00073460565,0.019911524,-0.05569207,0.06128862,0.036550365,-0.01931369,0.01129193,0.007877501,0.0047670836,-0.018961491,0.022381036,0.010564442,-0.00022163377,0.02371524,-0.014779327,0.054637186,-0.0047408887,0.011315513,0.030472029,-0.01765131,-0.06949266,-0.045507416,0.033387806,0.036438026,0.01367885,0.026437122,0.02487953,0.02854913,-0.06519666,-0.0059283567,-0.019093925,-0.027284628,0.00614165,0.01105643,0.061226554,-0.0050477725,-0.029908884,0.002412727,0.0037653842,-0.034231152,0.068246864,0.04481235,0.032872815,0.09204467,-0.020808052,0.022979435,-0.057509568,-0.0019062432,-0.036439955,-0.013262865,-0.00560206,0.025451059,-0.009900815,-0.024294088,-0.0061460664,-0.008059723,0.007466328,0.008590894,0.049419947,0.00902725,-0.07361603,-0.047267012,-0.00987947,0.058982816,0.025110569,-0.007153172,0.08356992,0.05671324,-0.02079816,-0.067353874,-0.028152207,-0.011456271,0.039255913,-0.01867256,0.013229546,0.056661654,-0.009171912,-0.0013708313,0.06435085,0.07440342,0.00044197415,-0.065385364,0.034269165,0.05668182,0.040587354,-0.020762201,-0.05635344,-0.07295623,0.03735492,-0.0061461623,-0.038664404,0.041101005,0.017957464,-0.022275874,-0.024228428,0.022319017,-0.006748952,0.0019958988,0.00015184219,0.009418315,0.034613814,0.021207841,0.004082489,-0.057824865,-0.019398516,-0.026496977,-0.0065284576,0.002401696,0.050203964,0.020664513,-0.007097026,-0.025973182,0.0114994785,0.051969647,0.0010706767,-0.03888993,0.03815012,-0.0068175998,0.015378887,-0.0007102355,-0.024045724,0.081021585,0.0074089775,0.022286676,-0.03665714,-0.006428804,-0.021008313,0.028128877,0.0016354822,-0.02899921,-0.072905675,-0.014583436,0.04817439,0.012677697,0.036785804,-0.007346994,-0.029888567,-0.015258169,-0.008229146,-0.012103133,0.084412456,0.026111389,0.019262327,-0.08816249,-0.03122253,-0.033900373,0.07003745,0.024972932,0.011100022,-0.036882855,-0.009188706]	Keywords: attention span, language modeling, hierarchical features, adaptive span\nKey Objects: attention span, adaptive-span models, language models\nRefers to Images: ./images/a-survey-to-transformers/image_11.png\nHypothetical Questions:\n- How does using a learnable attention span improve language modeling performance?\n- What is the significance of the observation that lower layers have smaller learned spans and higher layers have larger spans?\n- Why is it beneficial to reduce the number of floating-point operations (FLOPS) during language modeling?\n---\nSummary:\nAdaptive-span models, which use learnable attention spans, have been shown to outperform baseline models in character-level language modeling while requiring fewer floating-point operations (FLOPS). These models also exhibit a hierarchical composition of features, with lower layers having smaller spans and higher layers having larger spans.\nOriginal Text:\nFig. 10. Three types of span masking function m ( x ) . The horizontal axis represents distance x and vertical axis the mask value.  \n  \ncharacter-level language modeling show that the adaptive-span models outperform baseline models while having significantly fewer FLOPS. It is also observed that lower layers generally have smaller learned spans and higher layers otherwise. This indicates that the model can learn a hierarchical composition of features.\nContextualized Text:\nSeveral techniques have been explored to restrict the attention spans within Transformer models. Adaptive-span models, which utilize learnable attention spans, demonstrate improved performance in character-level language modeling, achieving better results with fewer FLOPS.  Furthermore, these models reveal a hierarchical feature composition, indicated by smaller learned spans in lower layers and larger spans in higher layers.	{"tags": ["attention", "transformers", "NLP", "architecture", "modeling"], "doc_id": "67fb171b-6622-4fb3-a9c0-e0c2e44e1d2d", "summary": "Adaptive-span models, which use learnable attention spans, have been shown to outperform baseline models in character-level language modeling while requiring fewer floating-point operations (FLOPS). These models also exhibit a hierarchical composition of features, with lower layers having smaller spans and higher layers having larger spans.", "doc_type": "text", "entities": ["BERT", "FLOPS"], "keywords": ["attention span", "language modeling", "hierarchical features", "adaptive span"], "key_objects": ["attention span", "adaptive-span models", "language models"], "contextual_text": "Several techniques have been explored to restrict the attention spans within Transformer models. Adaptive-span models, which utilize learnable attention spans, demonstrate improved performance in character-level language modeling, achieving better results with fewer FLOPS.  Furthermore, these models reveal a hierarchical feature composition, indicated by smaller learned spans in lower layers and larger spans in higher layers.", "mentioned_images": ["./images/a-survey-to-transformers/image_11.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["How does using a learnable attention span improve language modeling performance?", "What is the significance of the observation that lower layers have smaller learned spans and higher layers have larger spans?", "Why is it beneficial to reduce the number of floating-point operations (FLOPS) during language modeling?"]}
0b7b6821-f4ab-432e-bcc5-d9fb3a87cacb	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.045187756,-0.011924586,-0.0025045814,0.022093266,-0.040111806,0.029525774,-0.018038593,0.035539422,0.07984904,-0.048624974,0.027055223,-0.006132941,-0.03343673,-0.027358288,-0.07636331,0.014544025,0.048588637,0.026359648,-0.034573317,-0.034172475,0.050304234,-0.040738307,0.03682555,0.037964247,0.016606828,0.053025376,0.0002623016,0.0034132851,-0.026097123,0.05371885,0.039962657,-0.0043522893,0.03160776,0.044471283,0.023550857,0.010298839,0.035211723,-0.06308975,-0.0106506925,0.005038031,-0.006207851,0.0051141866,0.014527081,-0.01647188,0.015045876,-0.030956782,-0.09915605,-0.017324112,0.0058625783,0.015664484,-0.034730446,0.03027177,-0.037494533,0.001678879,0.0042646886,-0.009927652,-0.05129326,-0.0065253526,-0.015970541,-0.08907118,-0.01112578,0.05758686,0.020421201,0.021834504,-0.034245946,-0.044511043,-0.000558382,0.01612561,0.014614982,0.12251283,-0.043609783,0.006273268,-0.03188231,-0.03406027,0.11775774,0.075152926,0.002006994,0.05393141,-0.050500304,-0.01728511,0.033987477,0.004128425,-0.0645226,-0.043711934,0.08430623,0.004893952,-0.06251328,-0.023232348,0.02177394,-0.058022268,0.03380575,0.003094132,-0.031274613,0.07972191,-0.028132552,-0.09366376,-0.0211984,-0.0010314679,-0.0112578655,-0.09369926,-0.0004528308,0.0016905015,0.04524084,0.09469703,0.024517281,0.009094548,-0.019943265,0.0008344881,-0.013923307,0.04020457,-0.020746926,-0.02817862,-0.034152597,0.0076959766,0.021726253,0.067893386,0.016270326,-0.020258393,0.024843788,-0.022118244,0.046426255,-0.0006355026,0.026607102,-0.001481161,0.038555536,0.051665716,-0.035938233,-0.006470356,-0.045408446,-0.028152574,0.052802097,-0.0028143125,-0.04054417,0.023635687,-0.016184954,0.024189828,0.044543523,-0.019167336,-0.019205874,0.0052444777,0.03012495,-0.034548305,-0.01580894,-0.048753433,0.03209218,0.04891518,-0.10056021,-0.013434552,-0.024839574,0.06278212,0.026433548,-0.02560079,0.008884128,-0.032614887,-0.05616344,0.057069886,-0.048678122,-0.042904895,0.031772103,-0.072953284,-0.02884666,-0.041886777,-0.016732145,0.0354023,0.060595445,0.04966486,0.004651505,0.015524996,-0.020552972,-0.009391027,-0.046468303,-0.028916469,-0.03192356,0.014617951,0.037674427,-0.0056313523,0.009946999,-0.011855654,0.0048360904,-0.014600546,-0.016824765,0.016816601,0.0021698205,0.060917355,-0.0014217701,0.06939886,-0.06317503,0.05774127,0.017022861,-0.021694569,-0.024134275,-0.03469495,-0.011553615,-0.02937267,-0.024466626,0.017149528,0.03538286,-0.03940018,-0.017690439,0.03500278,-0.014420722,-0.0041604103,-0.03436057,0.024861753,0.0023832773,-0.0033589974,0.00096117426,0.011451751,-0.037487376,0.012106008,-0.003761316,-0.057659402,-0.04128447,0.042066928,-0.006403451,-0.037801776,-0.05159798,0.05019962,-0.0020682323,-0.016904198,-0.044095602,-0.011179997,-0.03014403,0.069646545,-0.030978054,0.01089942,-0.078277506,0.016027616,0.012365301,-0.017829346,-0.004248111,-0.003992779,0.012700075,0.020718168,-0.011563687,0.0028103385,0.03963324,0.009665452,-0.011245951,0.015674546,0.004894122,0.06016122,0.03848588,0.062231716,0.025734333,-0.019766249,0.0030746683,0.04019451,0.0030715386,0.030552996,-0.025053496,0.024035087,0.04064797,-0.01355993,-0.03608185,-0.024399985,-0.00941722,0.022228146,0.050709408,-0.018016186,-0.031144029,-0.024739567,-0.054576684,0.0670655,0.06057472,0.006125958,0.04650028,0.046869714,-0.035626538,0.020222595,0.0071725924,0.00052245596,-0.0062059304,-0.009629381,-0.061160386,-0.025926799,0.046708334,-0.016326552,-0.008615794,-0.024541473,0.03934011,8.247965e-05,0.037239607,0.018439403,0.00926339,0.014725564,0.0091552455,-0.081777714,-0.032552946,-0.010773658,0.039302368,-0.057013966,-0.028882591,-0.012689597,0.03126136,0.04008795,0.015775416,-0.026280934,0.029051343,0.018848382,0.10529344,-0.03772158,-0.027787061,0.008820093,0.022591246,-0.025152884,-0.055569004,0.014626576,0.052522853,0.03416054,0.0077951243,-0.03728502,-0.02226053,0.009071666,0.04953835,0.041767713,0.02126027,0.00076637685,-0.0039002995,0.08164931,0.05382608,-0.018376015,-0.00066359324,0.01606182,0.09840478,-0.019956693,0.014641148,-0.057875678,-0.006867073,0.028046057,-0.03665546,0.09015775,0.053112112,-0.030497335,-0.017167076,0.0016785138,-0.029043246,0.043168277,-0.013434359,-0.021776302,-0.06411153,-0.03229345,0.04304514,0.006939601,-0.001694248,0.018916933,-0.013558672,0.040545847,0.013818148,0.003105654,0.016849438,0.004812702,0.0016240666,0.021458698,-0.0029808725,-0.039790563,-0.017248904,0.050885055,0.025168536,-0.030825008,0.042427644,-0.07125802,0.11081187,-0.037949856,0.004060285,-0.03534644,-0.050648764,0.047670383,0.044792704,-0.022152519,-0.03130616,-0.058021974,0.048885703,-0.017481228,-0.033643063,0.03363668,0.019380815,-0.028650919,0.0011132172,-0.022630766,-0.0414325,-0.013002306,-0.008440664,0.0026752204,0.015065658,-0.016159732,-0.035211973,0.052912947,-0.022354886,0.044015102,-0.04386134,0.023617111,0.0035168084,0.019907849,-0.061558846,0.012598904,0.022730328,0.046965223,0.00023331639,-0.04019818,-0.050743792,0.07139502,-0.036364328,0.04502489,0.035922688,0.011148005,-0.017045077,0.015633624,0.017871134,0.009003885,0.03612189,-0.04085966,0.020290418,-0.023930155,0.017163143,-0.02716213,-0.0035433276,0.014543075,-0.024877455,-0.009822685,-0.027981082,0.047798637,-0.005357274,0.02239005,0.053308018,-0.025024803,-0.032661643,0.026516676,0.0081160655,-0.045486182,0.035919257,0.009780526,-0.031942703,-0.011363653,-0.0039996216,-0.03461672,-0.04175409,0.1052143,-0.023043707,-0.016870264,0.03766877,-0.023980737,0.018487599,0.015837764,-0.0033507177,-0.015118307,0.025495125,0.0007463065,-0.03839239,0.046160605,0.045222715,0.06470359,0.03698825,-0.023141261,0.0036136983,0.005627541,-0.022383535,-0.053041633,0.0006998938,-0.0040089907,0.015297765,0.035964992,0.00031647464,0.010163774,-0.01439053,-0.05486762,0.012828019,-0.03073287,-0.020286066,0.024593977,-0.027286382,0.002670826,-0.044051338,0.04032465,-0.019076595,0.010490672,-0.02748125,0.00029165554,0.055882927,0.039959878,-0.07830243,-0.041426457,0.034510512,0.01568181,-0.036576733,-0.01182259,-0.016502703,-0.033227906,-0.020525321,0.02815187,-0.005663608,0.013839502,0.024445824,0.015544015,0.00017377819,-0.08052391,0.009040493,-0.0012305425,0.046391472,-0.03804169,-0.026671603,0.025971645,0.037949074,0.009450567,-0.04482147,0.06768628,-0.006517682,-0.0034872515,0.0021061301,0.082999885,-0.00026522193,-0.00029145498,-0.016241908,0.06871782,-0.024630414,-0.011048396,0.010531203,-0.035796758,-0.010625264,-0.003069024,0.027666872,0.022796249,0.050803553,0.009534962,0.015708571,-0.034541838,-0.046370976,0.044059567,0.015654724,-0.047363896,0.018427463,0.0039966796,-0.0382437,0.020419776,-0.021802284,-0.030810518,-0.000121036,-0.030783664,0.018141806,0.033814315,-0.043420795,-0.055045858,-0.029433422,0.021237306,0.03735501,-0.02765535,0.012727245,-0.040340498,0.03074434,0.062242642,0.039534345,0.011265732,-0.04681303,-0.026835088,0.023275713,0.07765275,0.037677098,0.020226996,0.02720674,-0.0066694953,0.01002483,0.0037697984,0.00075557106,-0.022098556,-0.023415444,0.010689734,-0.02445569,-0.02569803,-0.028040407,0.01066497,-0.0072058653,0.006385856,0.035223063,0.03786244,0.021582833,0.00035532858,0.045821976,0.018402837,0.005821681,0.028791472,0.027299063,-0.031437356,0.026388666,0.030275753,0.0026502523,0.067673475,-0.019799147,-0.013077742,-0.027094737,0.049488377,-0.034402702,-0.066776186,-0.03736877,-0.041622363,0.006416859,-0.029426044,0.017576337,0.014055041,0.011568772,-0.037079025,-0.0915385,0.008060741,-0.02675994,0.021328323,0.03219365,0.051485445,0.0025650598,0.010071978,-0.044922654,0.039461024,-0.04372775,-0.016396856,0.022357944,0.027354972,-0.017782567,-0.06635003,0.008109021,0.016226673,-0.012099368,-0.013975454,0.084098585,0.039521296,-0.00882223,-0.0033909949,0.0010749915,-0.00514619,-0.0059123235,0.011893235,-0.006567431,-0.018741021,0.03554076,-0.008406206,0.022597257,0.02470148,0.014959904,0.044622187,0.010263074,-0.025964592,-0.055310067,0.039019063,0.02631625,-0.020140504,0.025787754,0.019409116,-0.0059827412,-0.072181456,-0.02725297,-0.029691072,-0.01904938,0.01589673,0.015255282,0.02365188,-0.05496976,-0.037977967,0.000846536,-0.019338863,-0.021458883,0.050887097,0.07380353,0.04045252,0.06457647,0.037100255,0.103351146,-0.030424763,-0.07747315,-0.05537597,-0.011088506,0.011968496,0.04431322,0.017100882,-0.059172828,-0.012445486,0.014334042,0.01899696,0.028053852,0.015586804,0.07443149,-0.052006315,-0.032080967,-0.046725523,0.029703414,0.012315427,-0.00015079648,0.05704079,0.052285943,-0.02180944,-0.028038917,-0.050209865,0.026433853,0.020931548,0.06125379,-0.013622291,0.036564708,0.0033735859,0.008036105,0.08465399,-0.0005959298,-0.00354019,-0.089869045,0.04565101,0.049313042,0.04710373,-0.020156872,0.0014957256,-0.08071456,-0.0023716742,0.0063306987,0.008527339,0.0562679,-0.043790564,-0.032576814,0.0050319918,0.00610268,0.018932614,0.08620995,-0.02328341,0.0109521,-0.02696416,0.029122459,-0.016671805,-0.041332874,-0.03798578,0.0032143407,-0.0029109584,0.019864567,-0.020922124,-0.014026055,0.0255031,-0.037104834,-0.001047379,0.0083205495,0.046105944,-0.007861746,0.015342991,-0.03762889,0.030651802,-0.01168515,-0.020319894,0.014849311,0.022319704,0.043810856,-0.05501061,0.030968126,-0.021748733,0.08193188,-0.010988898,-0.018906016,-0.035325974,-0.0010304903,-0.0030380117,-0.0038148388,0.03471305,-0.023835631,-0.02473241,-0.024560466,-0.024256522,-0.008602499,0.049064346,0.013676748,0.0040618707,-0.08634422,-0.018523518,0.010709399,0.018985245,-0.007749934,0.04781499,0.0067219,0.04613075]	Image title: Mask Functions for Attention Mechanisms\nTags: attention, masking, sequence, neural-network, NLP\nKey objects: Vanilla Attention Mask, Adaptive Mask, Fixed Span Mask, Input Sequence, Output Sequence, Masked Area, Unmasked Area\n---\nSummary:\nThis diagram illustrates three different mask functions used in attention mechanisms. The first shows a mask for vanilla attention, the second depicts a mask function for adaptive mask, and the third shows a mask function for fixed span.\nFull description:\nThe diagram presents three different masking functions applied to sequences. The first plot, labeled 'vanilla attention', shows a constant value (representing a mask) followed by a sharp drop to zero, indicating that the sequence up to that point is masked and cannot be attended to. The second plot, 'adaptive mask', starts with a high value, then linearly drops to zero. The third graph, 'fixed span mask', depicts a flat line at a high value and then goes flat again, representing a region where the sequence is fully unmasked and then re-masked.\nText found in image:\n- mask function for vanilla attention\n- mask function for adaptive mask\n- mask function for fixed span\n- input sequence\n- output sequence	{"tags": ["attention", "masking", "sequence", "neural-network", "NLP"], "title": "Mask Functions for Attention Mechanisms", "doc_id": "0b7b6821-f4ab-432e-bcc5-d9fb3a87cacb", "source": "./images/a-survey-to-transformers/image_11.png", "summary": "This diagram illustrates three different mask functions used in attention mechanisms. The first shows a mask for vanilla attention, the second depicts a mask function for adaptive mask, and the third shows a mask function for fixed span.", "doc_type": "image", "key_objects": ["Vanilla Attention Mask", "Adaptive Mask", "Fixed Span Mask", "Input Sequence", "Output Sequence", "Masked Area", "Unmasked Area"], "parent_doc_id": "67fb171b-6622-4fb3-a9c0-e0c2e44e1d2d", "text_in_image": ["mask function for vanilla attention", "mask function for adaptive mask", "mask function for fixed span", "input sequence", "output sequence"], "contextual_description": "The diagram presents three different masking functions applied to sequences. The first plot, labeled 'vanilla attention', shows a constant value (representing a mask) followed by a sharp drop to zero, indicating that the sequence up to that point is masked and cannot be attended to. The second plot, 'adaptive mask', starts with a high value, then linearly drops to zero. The third graph, 'fixed span mask', depicts a flat line at a high value and then goes flat again, representing a region where the sequence is fully unmasked and then re-masked."}
fefb97d3-640a-4444-a9e4-d9dae197051d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.022828434,-0.018005975,0.019768428,0.036958985,-0.033941656,0.027682647,0.028971834,0.08628677,0.008956118,-0.008321011,0.037864663,0.0042351764,-0.0010191508,-0.023271887,-0.010428414,0.01854944,0.01969058,0.082690984,0.010874867,0.0001116295,0.041245017,0.013361879,0.024728356,-0.021785168,0.0025028337,0.05566524,0.047234897,0.0018687719,-0.03953025,0.03541685,0.047027845,-0.04847783,0.037652735,-0.0026857685,0.02645652,-0.018038893,0.026486775,-0.047416925,-0.02403065,-0.0019497299,-0.024460562,0.050226226,-0.04501151,-0.0587351,0.003287265,-0.053259827,-0.07544573,-0.009020822,0.0067561,-0.008997208,-0.038332988,0.037508145,-0.03392585,0.0399689,-0.006460261,0.030209003,-0.018418444,-0.023263259,-0.010383324,-0.08614465,-0.027585143,0.0146410335,0.00436294,0.035752133,0.031102074,-0.0027340609,0.053416606,0.01463081,0.006808409,0.14691906,-0.05077539,0.015910627,0.028480649,-0.039086375,0.123143524,0.03595079,-0.04365993,-0.004927162,-0.05016933,-0.022912174,-0.025886483,0.05279859,-0.057437584,-0.01949502,0.10297264,0.0045957896,-0.049607016,-0.004511698,0.06774701,-0.036369767,0.0033025146,-0.015253784,-0.004679651,0.05849286,-0.032971356,-0.028602295,0.0031854229,-0.013957798,0.027328223,-0.051101625,-0.0022882624,-0.014806086,0.071662344,0.10914983,0.012512362,-0.03611208,0.009460995,-0.04375546,-0.021113368,0.007926791,0.015036868,0.006613417,-0.045119423,0.017659485,0.008339845,-0.012153495,-0.013304655,0.011487215,0.01097575,-0.009772809,0.03211569,0.009756495,-0.0033877315,-0.015433131,0.0398047,0.07439716,0.025564559,0.005535105,-0.03722676,0.04948015,0.04284766,-0.003125133,-0.0025745633,0.014829273,-0.05325762,0.0036474904,0.04300953,0.007693589,0.013282832,0.0044700378,0.018251274,-0.047311638,-0.061660096,0.0033558966,-0.0364585,0.025327118,-0.07398271,0.044239644,-0.0145317465,0.04424878,0.07200395,-0.045560464,0.019937672,-0.014465632,-0.033516414,-0.0034346892,-0.044129044,-0.06699328,-0.012706305,-0.0417311,-0.041222084,-0.036516793,-0.014821386,0.0635744,0.08117401,0.12160638,0.03229126,0.00882323,-0.018813372,0.019201655,-0.024663141,-0.02365036,0.0064419326,-0.0684838,-0.021908872,0.009561101,0.014163224,-0.05704096,0.025213145,0.028199235,-0.014828033,0.04874713,-0.0060798014,0.042035162,-0.041014545,0.044562552,-0.021512503,0.042678148,-0.022205504,-0.020438809,0.014887429,-0.014120194,0.008758075,0.008822226,0.036257774,0.030059136,0.06828272,-0.081631176,-0.026591519,-0.017287752,0.014171767,-0.014026231,0.04076977,-0.027838947,0.015945258,-0.014010962,-0.0018890104,0.019034607,0.009636207,0.03249204,-0.012347967,-0.008281042,0.006020559,-0.00014535723,0.036692966,0.0015689002,-0.023231968,0.027536472,0.049013294,-0.057707664,-0.04353631,-0.013715147,-0.06095301,0.0057815327,-0.011001625,0.021975031,0.0066849394,0.0059416196,0.030000761,0.017680498,0.0101792365,-0.003122978,-0.01473082,0.052442923,-0.04540586,0.04913531,0.003714514,0.015793553,-0.032830827,0.06328247,0.041604526,0.054739404,0.013891236,0.092559524,-0.005205782,-0.022312002,0.017594073,0.016975962,0.0061854534,0.014653947,-0.03337462,0.047508284,0.025236784,-0.0352556,-0.037420567,-0.028837219,-0.007842518,0.011653492,0.02209871,0.0034981135,0.032393213,-0.009744904,0.01683373,0.004763896,-0.019152233,-0.00084425195,0.014909032,0.043569792,-0.067149825,-0.00029015745,0.015555227,0.03965669,0.008700145,-0.039768748,-0.02537018,0.035982065,0.05533943,-0.028514516,0.039993327,-0.046808884,0.0022946158,-0.04101812,0.024813738,0.007218996,0.0004266952,-0.0034601039,0.05297158,-0.021358445,-0.04838975,-0.01375652,-0.00012267705,-0.012342021,0.050955854,0.017261827,-0.009342476,0.013339752,-0.02403822,-0.008353239,-0.029814975,0.043697625,0.121566705,-0.020948188,0.008334316,0.021797048,0.015397887,0.013268237,-0.06108445,0.037519332,0.01600073,0.014133793,0.040564075,0.029371189,0.008913696,0.026246298,0.025706278,0.052099403,0.03152816,0.0050795604,-0.036694307,0.049611922,0.01119458,0.004630782,0.03411696,0.043761935,0.021406949,0.0010297991,-0.014768403,-0.029981267,-0.08778012,0.081378944,-0.059857138,0.04500879,0.079490416,-0.040518854,-0.0044120234,-0.009439696,0.02666996,0.028946465,0.029700994,-0.00031911445,-0.032850143,-0.07006677,-0.01196929,0.010267765,-0.030992594,0.03724057,-0.0034770481,0.007891158,0.034459375,-0.0040622535,0.0039643217,0.04473742,0.010130578,0.0067421664,-0.02475893,-0.040119924,-0.020848915,0.012717349,0.01744721,0.034338027,0.07616305,-0.07138899,0.09196054,-0.06906758,0.06024347,0.018013116,-0.064371794,0.041681945,0.041192714,-0.051400214,-0.00462687,-0.036754362,0.03229896,-0.030921245,-0.023613557,0.060475875,0.03450551,0.020928513,-0.02606535,-0.004782107,0.006825041,-0.0062075704,-0.013744378,-0.04191024,0.047836285,0.010816712,0.032517575,0.091495484,-0.006031104,0.03834343,-0.045180976,0.018889805,-0.027315302,0.01808693,-0.04296753,0.004982785,0.0029951297,0.0445515,0.0032763614,-0.03284503,0.009224056,0.032860097,-0.062329516,0.05706041,0.0112339,0.010823591,-0.093339674,0.03444766,0.028408991,0.056805063,0.03820156,-0.013511199,-0.013984982,-0.0055792965,0.011957797,0.016302422,0.004396494,0.0071362536,-0.04520181,0.016512973,-0.04578066,0.032977715,0.02664952,0.062114842,0.059208464,-0.004514189,-0.04769309,0.068703935,-0.007681231,-0.05247031,0.02169208,-0.059487943,-0.047485333,0.06392561,-0.015520034,0.017424693,-0.025419027,0.09471403,-0.010755447,0.058578204,0.02380615,0.003638375,0.03497182,-0.005988838,-0.0049379747,-0.025018543,0.036399536,0.031467132,-0.0115235755,-0.03316654,0.045124434,0.025420697,0.02975594,-0.040931225,0.04573325,0.018625164,-0.01751271,-0.035283342,-0.04534332,0.016795699,0.024609912,0.04671335,0.04637122,-0.006085785,-0.011051177,-0.036674663,0.004243389,-0.06804133,-0.046323493,-0.030255368,-0.02585929,0.036668196,-0.004709163,0.0053417375,-0.008024041,0.0036182613,0.01048247,0.06824433,0.048489805,0.017972097,-0.07168896,0.0041161417,0.029949142,0.018378265,-0.03088931,-0.01745472,0.00046721165,-0.042470068,-0.050252844,0.039583787,-0.03577151,0.054979205,0.023955526,0.0019930697,0.0045862203,-0.016592437,-0.009012079,0.031768207,0.0054727765,-0.0021383252,-0.012342551,-0.035863694,0.028377868,0.0006523744,0.0038805413,-0.021469463,0.016855972,-0.010459684,-0.007891173,0.03570867,0.006339375,-0.0184988,0.021689748,0.032971576,-0.016964383,0.02641631,0.01437994,-0.011056263,-0.04549344,0.060158245,-0.019927965,0.015269736,0.00898699,-0.01286368,-0.0071955896,0.026351962,0.014912963,0.028043123,0.041456662,-0.066042,0.008106476,-0.02364829,-0.03088376,0.014553915,-0.06354133,-0.04597378,-0.009295578,-0.060586624,0.009596397,0.011205726,-0.015207315,-0.025042715,0.0142984595,-0.017746331,0.0824153,-0.05105531,0.010237657,0.0074406443,0.008273289,0.08250061,0.026787624,-0.017712792,-0.018587084,-0.0042471853,-0.008274944,0.028541716,0.043290045,0.02389953,0.0021474021,0.0648123,0.004308958,0.008462339,0.0058382484,-0.04302143,0.00794211,-0.033132896,-0.02052756,0.0008855321,-0.008839486,0.010433449,0.003128419,-0.010308006,-0.007954682,-0.016698329,0.014316834,0.045196492,0.035327133,-0.0041214763,0.034678835,0.0061814208,0.03691334,-0.0007283883,0.022494206,-0.011744798,0.0037600836,-0.014967577,0.02539535,-0.044887234,-0.04077166,-0.007607714,-0.0176067,-0.028510183,-0.0014076707,0.004498607,-0.0563229,-0.0462258,0.0044545284,0.0055118394,0.013812111,-0.029544901,-0.10112101,0.00032064685,-0.030896008,-0.003035072,0.026963511,-0.0013479696,0.016129592,0.05367189,0.016137796,0.06308346,-0.044489335,0.0073456527,-0.03069699,0.038205206,0.04476097,-0.01097548,-0.011620474,-0.0055024773,-0.0026444457,-0.04908598,0.10842989,0.04766955,-0.0071893763,-0.0049367086,0.05695606,0.0029692205,-0.028715342,0.017062983,-0.010888326,-0.00042275782,0.010435649,-0.0025842714,0.057522763,0.039400455,0.039691593,0.043517355,0.00044863892,-0.07986794,-0.0026474528,1.9739067e-05,0.053345833,0.0091100875,0.042491663,0.04888183,0.009994438,-0.041805808,-0.013643256,-0.032363515,-0.010468849,-0.025951134,0.06881303,0.06801169,0.004557869,-0.022090405,0.02116804,-0.0019140216,-0.018481847,-0.0034887164,0.057897855,0.012223174,0.09750007,0.044542894,-0.0061484957,-0.08332182,-0.037424468,-0.029013792,-0.030465772,-0.00943541,0.0106758205,-0.003947552,-0.00601292,-0.016182689,-0.0063745137,0.030304724,0.0080681,0.034519363,0.00023433802,-0.060673643,-0.04127162,-0.0069348468,0.035347097,-0.02468303,0.008439982,0.06125902,0.034524906,-0.023559887,-0.05429047,-0.03863317,-0.00488955,0.0062299343,-0.056475405,0.0140833575,0.016231071,0.0039185667,-0.020566175,0.07386429,0.029211335,-0.05897846,-0.040520664,-0.011796553,0.031321492,0.04968709,0.017828185,-0.017362671,-0.08265893,0.02346701,-0.004540553,-0.045779783,0.053129353,-0.029708328,-0.04671791,-0.010768803,0.03133758,-0.031357817,0.024944624,0.008776251,0.030949991,0.0039100237,-0.018337516,0.010028523,-0.042382132,-0.021167887,-0.006123507,-0.054140724,-0.018451303,0.057592575,0.009847966,-0.0127252545,-0.016105393,0.017569391,0.011473233,0.0066169263,-0.0061094975,0.06024647,0.007224583,0.022529367,0.034364156,-0.042093255,0.06028646,0.01962296,0.0042597335,-0.027610073,0.00587344,0.0081320815,0.0017996887,0.01728436,-0.036035165,-0.066570684,-0.0043828944,0.021401942,-0.029771136,0.0027331458,-0.0116818445,-0.017374557,-0.04770369,-0.009479203,-0.031119833,0.056119017,0.054726746,0.0008422059,-0.072035365,0.006277407,0.0036407795,0.03344772,0.022269886,0.045073524,-0.032904577,-0.010115706]	Keywords: attention span, fixed attention, scale value, Multi-Scale Transformer\nKey Objects: attention spans, layers, scale value\nRefers to Images: None\nHypothetical Questions:\n- How does varying the attention span across layers contribute to the Transformer's ability to process information?\n- What is the linguistic rationale behind using larger attention spans in higher layers?\n- How does limiting the attention span improve inference speed on long sequences?\n---\nSummary:\nMulti-Scale Transformer introduces a method of using fixed attention spans, varying the maximum span for different heads across layers, to control the attention window and optimize performance.\nOriginal Text:\nMulti-Scale Transformer [44] proposes to use a fixed attention span, with different heads in different layers using a different max span. The fixed attention span is depicted in Fig. 10(c). The attention is restricted within a fixed window which is controlled by a scale value w . They design the scales from an intuitive linguistic perspective and empirical observation from BERT such that higher layers tend to have more large scales (e.g., large span size), and lower layers should be confined with a smaller scale. Their experiments on several tasks show that the model can outperform baseline models while accelerating inference on long sequences.\nContextualized Text:\nTo manage the scope of attention, Multi-Scale Transformer uses fixed attention spans. These spans vary across layers, with higher layers generally utilizing larger spans (larger scale values) and lower layers confined to smaller spans, allowing for a hierarchical composition of features.	{"tags": ["architecture", "transformer", "attention"], "doc_id": "fefb97d3-640a-4444-a9e4-d9dae197051d", "summary": "Multi-Scale Transformer introduces a method of using fixed attention spans, varying the maximum span for different heads across layers, to control the attention window and optimize performance.", "doc_type": "text", "entities": ["BERT", "Multi-Scale Transformer"], "keywords": ["attention span", "fixed attention", "scale value", "Multi-Scale Transformer"], "key_objects": ["attention spans", "layers", "scale value"], "contextual_text": "To manage the scope of attention, Multi-Scale Transformer uses fixed attention spans. These spans vary across layers, with higher layers generally utilizing larger spans (larger scale values) and lower layers confined to smaller spans, allowing for a hierarchical composition of features.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["How does varying the attention span across layers contribute to the Transformer's ability to process information?", "What is the linguistic rationale behind using larger attention spans in higher layers?", "How does limiting the attention span improve inference speed on long sequences?"]}
105c28b4-afc6-413e-ae73-0d070559a875	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.007990922,0.018389855,0.022296214,0.037819333,-0.0229918,0.067129746,0.0010409154,0.063150875,0.036020324,-0.027699124,0.020229176,-0.021976225,-0.02221718,-0.023576682,-0.012842701,0.04538578,0.027628269,0.059930872,-0.007655084,-0.034432642,0.040652376,0.012832148,0.047012284,-0.003489824,0.016353022,0.033827074,0.026696667,0.04605864,-0.05328963,0.041889805,0.025721174,-0.048649598,0.06174459,0.0153030185,0.059341405,-0.014996217,-0.0048443247,-0.07390974,-0.0032761705,-0.008493323,-0.024722377,0.031569794,-0.04011685,-0.051666692,0.0038132255,-0.052132037,-0.083673544,-0.041906472,0.01508728,0.0014822176,-0.035703108,-0.010849585,-0.052468657,-0.00035780936,-0.048737735,0.0038800729,0.015390046,-0.036769748,0.011012185,-0.116160475,-0.010406767,-0.004531248,0.036360808,0.015596815,-0.0001286693,-0.01722314,-0.0037547476,0.0026785624,0.047043048,0.12689118,-0.03603245,0.007690219,0.061494324,-0.058219787,0.09980275,0.04503665,-0.041255854,-0.0058571114,-0.06448986,-0.019195976,0.006205665,0.037438963,-0.06431711,-0.040240444,0.061241336,0.013557653,-0.02420973,0.008946302,0.031505194,-0.077497154,0.0019584193,0.011748709,-0.0036230537,0.047405437,-0.01634507,-0.044202812,-0.012780122,-0.033943214,-0.009755752,-0.07437612,0.024368513,0.011947069,0.09347342,0.11128838,0.0016078881,-0.044757728,-0.011929069,-0.03128788,0.022320028,0.039000187,-0.012434078,0.05482638,-0.044115964,0.02928864,-0.0037002587,0.027718658,-0.0023774465,-0.014884522,0.019359626,-0.013655953,0.049328566,-0.012464927,0.018138623,0.016545916,0.042427663,0.024098048,0.048830487,0.0027201069,-0.05354537,-0.003768149,0.050195966,-0.005544245,-0.020124616,-0.0064356504,-0.048210904,0.005377347,0.07469475,0.01275686,-0.009018917,0.028166031,-0.018822424,-0.020333195,-0.039481558,0.034871783,0.012718647,0.07069477,-0.049269643,0.04235848,-0.006230603,0.0671129,0.06292649,-0.014865856,0.018135615,-0.020199247,-0.014788984,-0.011972868,-0.014886521,-0.05905527,0.030896984,-0.0023521117,-0.03167435,-0.012575973,-0.0050344416,0.089863405,0.07140525,0.10395437,0.019150585,0.017374616,0.0012312044,0.008497172,-0.01635838,-0.014965605,0.002683245,-0.034690406,-0.010689363,0.0052182917,0.0025866698,-0.0524761,0.022944162,0.0169322,0.024781385,0.055666666,0.007869895,0.028880479,-0.027577609,0.036036424,-0.0011784744,0.022754604,0.01691723,-0.043231808,0.012919109,0.015875788,0.019224267,-7.109881e-05,0.026717268,0.03816436,0.033757843,-0.07509626,-0.009835135,-0.016309505,-0.016565355,-0.017320197,-0.01048148,-0.022343518,0.005499181,-0.03282302,-0.011355287,-0.016743267,0.015175205,0.03457741,0.003668016,-0.039315637,-0.016113847,0.0366604,0.024068989,-0.01135573,0.015862502,0.030773241,0.008863123,-0.04214782,-0.062004767,0.018650608,-0.09375814,0.02387683,0.0016314598,0.039759997,-0.019073054,0.025693836,0.01785575,-0.018509183,0.0028519735,-0.00801864,-0.052325446,-0.0022985844,-0.020866012,-0.0016294465,0.01747842,0.014970477,-0.04866281,0.026786385,0.04869675,0.062026154,0.0036622705,0.08882234,0.022749007,-0.0013401036,0.039370727,0.031610414,0.016395476,0.004195206,-0.053125776,0.078681976,0.0003704402,-0.030303067,-0.0074503603,-0.056022946,-0.004793783,0.014219332,0.010658557,0.0071522067,0.016024536,-0.011307905,-0.005734291,-0.00933077,0.030708179,0.03342295,0.0039222753,0.043744694,-0.06029737,0.027053101,0.027350076,0.03912469,0.02483554,0.0001994611,-0.012514065,0.048541676,0.00092830666,-0.009759074,0.011947208,-0.01629783,-0.009554441,-0.036790174,0.05073076,-0.0046252958,0.035367653,-0.0054054796,0.05481192,-0.010116149,-0.017049966,0.0039562816,-0.021446561,-0.011668819,0.011970187,-0.031364836,-0.0067592203,0.0011962136,-0.00075702905,-0.029543497,-0.0143504245,0.029002305,0.12084447,-0.029155042,-0.0011034376,0.0043976638,0.008123247,0.004790923,-0.06366985,0.047087282,0.031823535,-0.015853465,0.044390425,0.0053062965,-0.011940689,0.0019804877,0.007349291,0.101072796,0.07072192,-0.0044678375,-0.003038917,-0.002650499,-0.00092092843,0.009706285,0.007701417,0.034845296,0.027096191,-0.004662873,0.039087884,0.0025043776,-0.06094183,0.09372163,-0.04672465,0.029106544,0.053320076,-0.019536093,0.014217476,0.021096373,-0.016679648,0.023299146,0.005877966,0.034780383,-0.016913591,-0.045542445,0.023726659,0.0062182136,-0.0081471605,0.03906915,0.0020726786,0.0037666867,0.024374275,0.019775638,0.038615927,0.043755822,0.020661619,-0.00046564126,-0.013166615,-0.04179731,-0.012602356,0.056590073,0.016740527,0.002540012,0.08926294,-0.093212396,0.06711342,-0.04795177,0.050224923,-0.005273763,-0.024681699,0.040525623,0.023145644,-0.03180005,0.011094978,-0.041197527,0.009456079,-0.045994367,-0.03718214,0.030768681,0.046335824,0.018090485,-0.009362231,-8.4238985e-07,-0.01443175,0.006518465,-0.051793948,-0.036113963,0.04614645,0.037477955,0.0063933283,0.10186323,-0.010720893,0.027032001,-0.048857715,0.021605171,0.03137434,-0.01349399,-0.022580888,0.029005043,0.008386184,0.049546402,0.03458839,-0.020492703,0.05161573,0.02812246,-0.062867336,0.0073072063,0.04031089,-0.0039762203,-0.0774419,0.025249207,0.029946197,0.023392001,0.010878984,-0.017213084,0.0037586666,-0.0034809513,0.02042388,-0.05012529,0.0019263283,0.021386482,-0.030003067,-0.005986421,0.0046321237,0.072925515,0.03786856,0.04345239,0.047496453,-0.010276083,-0.021317605,0.068780474,-0.023531329,-0.047011137,0.02816114,-0.005274679,-0.042175382,0.02183838,-0.011850264,0.024380827,-0.056249272,0.07666739,-0.0017944187,0.027511442,0.09259136,-0.04444967,0.059690624,-0.014186609,0.0040756576,-0.06329927,0.047462195,0.027072355,-0.045823574,-0.003245027,0.03480407,0.03639721,0.043887258,0.00295647,0.05308447,0.0019101526,0.0013841217,-0.03153037,-0.022231251,0.005730198,0.004061314,0.020140758,0.03214394,0.019694136,-0.009387084,0.008350546,-0.014747585,-0.050015233,-0.0010062262,-0.0032910043,0.0018514858,0.042581502,-0.04327396,0.05858841,-0.004049149,-0.016406974,0.0017132512,0.029391132,0.043109287,0.0077189496,-0.0879321,-0.0046772133,0.033820376,0.009313464,0.035244472,0.0041415873,-0.021093631,-0.028737804,-0.018877799,0.045191884,-0.019896625,0.006603357,-0.0017246295,0.037533518,0.031267457,-0.04223361,0.01467382,0.009379762,0.0011990003,-0.0046653994,-0.055771444,0.011749637,0.07591189,0.02624245,0.005323849,0.0024074325,0.013867549,0.019354166,-0.015771002,0.0807216,0.012889235,-0.036302134,0.015330935,0.011287588,-0.03178678,0.03438417,-0.0350177,0.021129195,-0.009454972,0.027505333,0.023066867,0.007597814,0.038807556,0.04120658,0.026206033,-0.0067935404,0.03670263,-0.005503843,0.04986091,-0.07521295,0.0017288039,-0.045022786,-0.037481453,0.011947471,-0.05726806,-0.08033618,-0.006007634,-0.0473722,-0.0049189907,0.017163463,0.013537203,-0.044254724,-0.0130999,-0.00880663,0.061408166,-0.0013673946,0.039160687,0.019540306,-0.0037009984,0.060013685,0.04102677,0.035205584,0.015346186,0.023917876,0.02030614,0.038672116,0.059326593,0.012243122,-0.005317187,0.048977178,0.020171221,0.014726581,-0.010696394,-0.05208045,-0.04359626,-0.0094167655,-0.018900657,-0.004570501,-0.010400426,0.01802956,-0.020366712,0.021083739,0.010971688,0.010317824,0.0070656026,0.057162505,0.013460802,0.00015286621,0.054926638,-0.012501767,0.048517756,0.024401113,0.06446848,0.0063871504,-0.038644165,0.025044594,0.0026357367,-0.025053713,-0.013392966,0.014825192,-0.008935066,-0.033615556,0.013554166,0.024186084,0.0072461315,-0.048640367,0.040734712,0.030727481,0.022170577,-0.097739704,-0.057538588,-0.022781618,-0.0460103,-0.011734789,0.0034436628,0.013055871,-0.028313892,0.006537313,-0.0022579199,0.05633307,-0.11285186,-0.01322747,-0.07563187,0.0128932865,-0.02693935,-0.0072567444,-0.007998359,-0.04661135,0.017436607,-0.0040100925,0.060272504,-0.011428652,0.0022961465,-0.035676785,-0.008733826,-0.022423033,-0.0008162788,-0.024861278,-0.006234418,0.009127299,-0.040696662,-0.015461515,0.054538455,-0.019481193,-0.003218394,0.042060878,-0.0015112394,-0.07619705,-0.027298454,0.00865661,0.04441546,0.004429625,0.041632302,0.021411207,-0.0010243341,-0.08182741,0.02626305,-0.048920378,-0.016384123,0.018518558,0.029538546,0.064598836,-0.048921518,-0.06295024,-0.018557532,0.015236204,0.016472213,0.0010221625,0.010794308,0.031793844,0.06095797,0.05002082,0.014251865,-0.08185451,-0.030669194,-0.030481685,-0.030624501,-0.03780317,0.04320296,0.017791387,-0.012289607,-0.026933517,-0.01003612,0.018122027,0.022706814,0.026344134,0.00912361,-0.03522766,-0.026070863,0.015033159,0.011545808,-0.019078685,-0.004435917,0.05834017,-0.0013992738,-0.041468363,-0.022620302,-0.07806669,-0.035086937,0.03230639,0.009595011,-0.013758246,-0.013440043,-0.034800764,-0.013774966,0.05319603,-0.0411891,-0.05353258,0.0022144536,0.00068842387,0.050026797,0.045492906,0.021527573,-0.018446434,-0.08269652,0.0054053306,-0.028554311,-0.033949275,0.039610602,-0.048724864,-0.033392813,-0.00017063753,0.0009172464,0.024104264,0.030472204,0.004615488,0.05031642,0.023765847,0.008831684,0.011986539,-0.09770305,-0.024334213,0.020038433,-0.051155683,0.0031052514,0.05669713,-0.0008844032,-0.022316065,-0.052530143,0.013969223,0.015405756,0.0150075015,-0.034787875,0.05000861,0.0044574966,0.042458106,-0.010787242,0.019276218,0.05072341,-0.0058001303,-0.024894679,0.010111521,0.02887758,0.023030892,0.012362465,0.00042374115,-0.066726916,-0.02429457,0.0520859,0.038561467,-0.04236385,0.030630682,0.009506669,-0.035256457,-0.038504183,-0.030170806,-0.040149283,0.03599709,0.0734255,-0.0062339758,-0.06122947,0.00983215,-0.022711838,0.04009686,0.008911293,0.02535713,0.024413051,0.018929541]	Keywords: multi-head attention, aggregation, linear transformation, re-parameterization\nKey Objects: Attention Head, Weight Matrix, Linear Transformation\nRefers to Images: None\nHypothetical Questions:\n- Why is the concatenation-and-project method equivalent to a summation of attention outputs?\n- How does dividing the weight matrix (W O) into blocks facilitate the aggregation process?\n- Could alternative aggregation methods, beyond concatenation and linear transformation, potentially improve multi-head attention?\n---\nSummary:\nVanilla multi-head attention combines the outputs of individual attention heads by concatenating them and applying a linear transformation, which is mathematically equivalent to a summation of re-parameterized attention outputs. This process involves dividing a weight matrix into blocks to achieve this aggregation.\nOriginal Text:\n4.6.3 Multi-head with Refined Aggregation . After each attention head computes its output representation, the vanilla multi-head attention [137] concatenates these representation and then apply a linear transformation to the concatenated representation to obtain the final output representation, as formulated in Eq. (2). Combining Eq. (1)(2) and (3), one can see that this concatenate-and-project formulation is equivalent to summation over H re-parameterized attention outputs. To this end, we first divide W O $\\_{R}$ D$\\_{m}$  D m into H blocks  \n$$W O = [ W _ { 0 } ^ { o }, W _ { 0 } ^ { o }, \\cdots ; W _ { O } ^ { o } ],$$  \nwhere each W O i is of dimension D$\\_{u}$  D$\\_{m}$ . It's thus easy to see that multi-head attention can be reformulated as  \n$$M u l t i h e A t t { n } ( Q, K, V ) = \\sum _ { i = 1 } ^ { H } \\text {Attention} ( Q W _ { i } ^ { o }, K W _ { i } ^ { o }, V W _ { i } ^ { o } W _ { o } ).$$\nContextualized Text:\nIn vanilla multi-head attention, after each attention head computes its output, these outputs are combined. Specifically, the outputs are concatenated and a linear transformation is applied to obtain the final output representation. This aggregation process is mathematically equivalent to summing the outputs of each attention head.	{"tags": ["architecture", "NLP", "transformer", "attention"], "doc_id": "105c28b4-afc6-413e-ae73-0d070559a875", "summary": "Vanilla multi-head attention combines the outputs of individual attention heads by concatenating them and applying a linear transformation, which is mathematically equivalent to a summation of re-parameterized attention outputs. This process involves dividing a weight matrix into blocks to achieve this aggregation.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["multi-head attention", "aggregation", "linear transformation", "re-parameterization"], "key_objects": ["Attention Head", "Weight Matrix", "Linear Transformation"], "contextual_text": "In vanilla multi-head attention, after each attention head computes its output, these outputs are combined. Specifically, the outputs are concatenated and a linear transformation is applied to obtain the final output representation. This aggregation process is mathematically equivalent to summing the outputs of each attention head.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["Why is the concatenation-and-project method equivalent to a summation of attention outputs?", "How does dividing the weight matrix (W O) into blocks facilitate the aggregation process?", "Could alternative aggregation methods, beyond concatenation and linear transformation, potentially improve multi-head attention?"]}
3d024558-828d-4909-b391-82fc938134b8	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0040559266,0.005852373,0.023031846,0.03872742,-0.02995388,0.07418251,-0.020254945,0.051364645,0.037991006,0.019377396,0.0008116734,-0.013083796,-0.017448766,-0.011387874,-0.013015033,0.018961713,0.022283895,0.055666372,-0.007823496,-0.036113575,0.045550555,0.03925979,0.041593976,-0.0007555425,0.011807495,0.01603814,0.034034427,0.039180506,-0.034721885,0.013889859,0.015704524,-0.05880789,0.054136112,0.009102064,0.06483155,-0.019773293,0.014557721,-0.07399553,0.002683846,0.0041070087,-0.006351653,0.04322229,-0.031671703,-0.020224323,-0.010479979,-0.06486569,-0.05161312,-0.040594783,0.027981894,-0.002326067,-0.010481663,0.007853712,-0.03916736,-0.008340584,-0.03219717,-0.005490172,0.0037218258,-0.041375767,0.00905407,-0.092883795,-0.0112581225,0.004458295,0.036664627,-0.0045945197,0.042845793,-0.019918835,0.014098754,-0.020654691,0.041965153,0.121526316,-0.008603269,-0.0026617548,0.051932428,-0.0674753,0.11720198,0.039570253,-0.017854152,-0.026307337,-0.0435128,-0.006323639,0.022594504,0.05277333,-0.050795946,-0.0427429,0.069575004,0.025948577,-0.017496577,0.0069858967,0.032192007,-0.060353607,-0.0019733233,-0.012149449,0.000536485,0.071964666,-0.033407424,-0.049788736,-0.008964251,-0.024644574,-0.010304693,-0.11038545,0.016887471,0.024041653,0.09558508,0.10373419,0.0080828415,-0.04058968,-0.00517662,-0.044052236,0.0012883258,0.027538218,-0.013075857,0.064733505,-0.06695407,0.016510826,0.013012041,0.0415647,-0.025525868,-0.0058768983,0.01570948,-0.011158451,0.05505308,-0.012998482,-0.026887268,0.025356865,0.04835157,0.044954073,0.07194666,-0.017212756,-0.033522453,-0.010119116,0.06761394,-0.004124415,-0.0076574623,-0.004438531,-0.027888538,-0.0040030894,0.08098112,-0.007427689,0.0017195938,0.017687635,-0.0254316,0.017448282,-0.022693535,0.04142251,0.02621029,0.059466317,-0.04333161,0.052343994,-0.0130574675,0.07783431,0.07114382,-0.028025068,0.030630687,0.008692521,-0.018803716,-0.03016005,-0.00089046214,-0.06485894,0.04470667,-0.015882837,-0.044035614,-0.012163619,0.023650624,0.07884017,0.050543744,0.11374196,0.023414508,-0.008484706,-0.008148107,0.0019768083,-0.008955906,-0.00918915,0.028273256,-0.034152508,-0.017804794,0.021502016,0.02436339,-0.028150432,0.02476926,0.008766576,0.013987073,0.039942857,0.0002548597,0.029719427,-0.012742813,0.03751178,-0.0069881915,0.020936552,0.016665772,-0.046343807,0.019992974,0.007605748,0.03265945,-0.0018524338,0.020771135,0.048859887,0.025748497,-0.08560148,0.0015011008,-0.012877331,-0.00021597085,-0.023320574,-0.011551936,-0.017908514,-0.007779771,-0.03491427,-0.002802454,-0.049347673,0.035459194,0.038693283,-0.026504666,-0.0047508418,-0.033773575,0.03872957,0.043021925,-0.003727179,0.023059936,0.012429463,0.019752137,-0.047618188,-0.03930066,-0.0058178413,-0.07856793,0.024110476,0.010236327,0.027861044,-0.010162629,0.034767255,0.015282634,-0.016897136,-0.011597423,0.01307896,-0.025750821,0.015575096,-0.029751476,-0.0011929575,0.003383602,0.041350886,-0.019478755,0.010485519,0.042784434,0.045452908,0.014292926,0.10515873,0.018677525,-0.029803481,0.02545613,0.028923886,0.014837316,-0.007979016,-0.054933716,0.055792756,0.010433773,-0.02352328,-0.03559191,-0.055337075,-0.01855926,0.0033519529,0.014299475,-0.0024089294,0.0070745517,-0.041373633,-0.00605608,0.009108753,0.038830496,0.02582274,0.024592234,0.03570829,-0.052616656,0.031676725,0.030600484,0.03972313,-0.00181741,0.0062875035,-0.02202721,0.050353497,0.01145117,-0.026901353,0.013743933,-0.006138497,-0.033335395,-0.025580138,0.05489147,-0.014437951,0.034270454,0.015730891,0.04284921,-0.030710524,-0.020385731,0.01700356,-0.013121715,0.0034013258,-0.0011619842,-0.010791477,0.003855179,-0.0071368273,0.001170033,-0.016490862,-0.01067794,0.027475927,0.09967422,-0.02056832,-0.00016641112,0.008014156,0.044002336,0.036691364,-0.050601143,0.032112867,0.0074140592,-0.0031490887,0.06190066,0.007018363,-0.024564456,0.0004569155,0.0023675265,0.09753369,0.07892673,0.0016069531,-0.0034840477,0.003263061,0.008486212,-0.029532582,0.020303167,0.027102599,0.024679897,0.01931366,0.044347014,0.0037269052,-0.07295129,0.06974621,-0.06678317,0.016696636,0.039523657,0.011346595,-0.0041826763,0.0127846,-0.023320295,0.0010097931,0.040914603,-0.01115702,-0.0026395456,-0.036632575,0.0545674,-0.009271519,-0.0380385,0.032604985,-0.01374316,0.0028779388,0.046459842,-0.008733356,0.009137606,0.047430288,0.021001428,-0.004259563,-0.009997655,-0.030408999,-0.021214163,0.025334448,0.0069013597,0.014610688,0.06778781,-0.1047212,0.046102718,-0.047324955,0.079366095,-0.00015167997,-0.026124403,0.042667452,0.014154597,-0.031496692,0.030818444,-0.027150469,0.027096868,-0.029873775,-0.037723657,0.030446561,0.056750454,0.011992403,-0.02396846,0.00272908,1.8315728e-05,0.009785914,-0.06449993,-0.021297012,0.07394808,0.02260046,0.007224089,0.07428906,-0.0058383155,0.017121771,-0.046478063,0.042647082,0.033464964,0.0025638037,-0.032335393,0.017975513,0.009595785,0.037187252,0.036145557,-0.02571495,0.06391718,0.025919333,-0.060107294,0.034632325,0.02588455,-0.022239111,-0.06704345,0.029085346,0.014030581,0.0052921437,0.018840505,-0.026486354,0.016327063,-0.018960366,0.036807112,-0.03000892,0.0021920828,0.007145646,-0.034894444,-0.021335999,-0.0018892037,0.087881766,0.023727616,0.03624975,0.04638155,-0.020474179,-0.01893206,0.077639826,-0.025013242,-0.027804928,0.030405086,-0.017168002,-0.013249162,0.019890059,-0.014702042,0.021609122,-0.047911983,0.031788584,-0.013468675,0.031531956,0.08412843,-0.050702456,0.023260212,0.007221667,-0.010726052,-0.05891415,0.04382018,0.03745308,-0.04767583,0.008596189,0.039829724,0.031505715,0.04271315,-0.0068183397,0.07614091,0.007760588,0.017205652,-0.021662341,-0.051984895,0.017085414,0.02155345,0.01759407,0.033810984,0.02417026,-0.008831948,0.011212282,-0.0065033976,-0.05788788,-0.008370492,0.0018651586,-0.009287271,0.030016063,-0.031222194,0.05373234,0.008330789,-0.0062371143,0.0019678883,0.059231933,0.074809,0.013023864,-0.081082314,-0.012675166,0.010644746,-0.008064154,0.023105934,0.002849117,-0.010021606,-0.032191332,-0.00980341,0.043948922,-0.022089342,0.018114943,-0.019289462,0.022669526,0.05366155,-0.06771977,0.0039401427,0.034702316,-0.012434646,-0.0241268,-0.039591417,0.022514416,0.065584294,0.031009447,0.010916587,0.0034305246,-0.018412627,-0.007313606,-0.0013457728,0.06963328,0.010894365,-0.02239727,-8.5589214e-05,0.011101442,-0.035853997,0.029072125,-0.04187584,0.034053415,-0.005081252,-0.009333001,0.02294517,0.023808982,0.04223939,0.029505026,0.026873859,-0.0007941273,0.040718302,-0.0076774675,0.044087444,-0.0840322,-0.00752151,-0.022619413,-0.009557864,-0.0009531653,-0.033068664,-0.075890884,0.0024721078,-0.04487098,-0.03088184,0.036889978,0.0076857526,-0.035622854,-0.011528964,0.017073842,0.0492256,-0.034610186,0.0507499,0.0005245768,0.027066614,0.08171863,0.027419252,0.031852372,0.024163017,0.030227778,0.011955268,0.005817187,0.033077735,-0.00062282867,-0.01394428,0.04560985,0.0076037543,0.018809116,-0.021924386,-0.070387,-0.05337693,-0.0045426562,-0.019379294,-0.023510633,-0.016601725,-0.019401245,0.003944177,0.018946119,0.014791573,0.019548362,-0.011990395,0.063056014,0.02063047,0.0045226333,0.052390456,-0.016834646,0.050027806,0.037011854,0.07912348,-0.0032457088,-0.014944203,0.025857797,-0.0059047383,-0.02123143,0.00642313,0.00095380464,-0.033691235,-0.012170386,0.020110136,0.013815635,-0.008897375,-0.07884488,0.035963178,-0.009291494,0.022548553,-0.0897188,-0.049694233,0.010796256,-0.05735032,-0.05404924,0.022559172,0.0019288642,-0.022725344,0.019226093,-0.011854488,0.057784118,-0.123337865,-0.016604438,-0.047047853,0.025091888,-0.02935327,-0.01999307,-0.009245504,-0.0139266895,0.0045142383,-0.007893212,0.07524509,-0.010558834,-0.0132466955,-0.018070757,0.000306442,-0.0015306976,-0.02949997,-0.03234521,-0.012395986,0.009458592,-0.03681292,-0.015583785,0.07454711,-0.04079283,-0.033833224,0.0538546,-0.0013582379,-0.08063723,-0.02332596,-0.024364637,0.042008545,0.00024090752,0.020419812,0.03883469,0.02688527,-0.045166686,0.025803175,-0.04282974,-0.0048891115,0.030551644,0.027393151,0.056360822,-0.030497596,-0.071062915,-0.03560444,0.019381665,0.02549442,0.0068103755,0.06746293,0.038969252,0.063069254,0.024102023,-0.013267807,-0.08852784,-0.0172547,-0.015515493,0.010725294,-0.026005294,0.041126005,0.0032292365,-0.014204101,0.0037130236,0.0064197746,0.0072078337,-0.009228333,0.00437724,-0.0112531725,-0.04284987,-0.075948544,-0.0057291524,0.013416966,0.009760005,-0.024241822,0.07066908,-0.009131147,-0.040058687,-0.036903657,-0.07353287,-0.014089728,0.02144094,-0.011309608,-0.005796433,-0.0160563,-0.03595836,-0.008374311,0.06524393,-0.01491431,-0.06370468,0.0009636109,0.0151313385,0.043730807,0.035294894,0.022326997,0.0012061569,-0.09140334,-0.011921368,-0.034064975,-0.05593359,0.025555372,-0.037164964,-0.021488814,-0.017454645,-0.0066169137,0.02576854,0.02021079,0.0002655359,0.059765957,0.05318086,0.029684884,0.019705975,-0.102284476,-0.019429479,-0.0071376483,-0.044590168,-0.0195372,0.06554999,0.0025289704,-0.014644169,-0.05713922,0.027391283,0.016918074,0.01972873,-0.053948935,0.04931997,-0.0005417108,0.029324949,-0.0002454029,0.034764636,0.046116866,-0.00944127,-0.025300238,-0.00508396,0.024149971,0.004174475,-0.010771629,-0.012390796,-0.031607743,0.0074490937,0.020407626,0.021942122,-0.028917437,0.017174287,-0.004377804,-0.042198934,-0.052364238,-0.049170215,-0.02729325,0.032970544,0.03831427,-0.008949913,-0.04601752,0.014431998,-0.012110276,0.043623775,0.026409123,6.649072e-05,0.024126528,0.006019004]	Keywords: multi-head attention, aggregation, expressiveness, summation\nKey Objects: multi-head attention, outputs\nRefers to Images: None\nHypothetical Questions:\n- What are the limitations of the summation approach for aggregating attention heads?\n- How might more complex aggregation methods improve the performance of multi-head attention?\n- What are some examples of alternative aggregation techniques that could be used?\n---\nSummary:\nThe standard multi-head attention mechanism aggregates outputs by summation, but some researchers argue this is a simplistic approach that doesn't fully utilize the potential expressiveness of the mechanism.\nOriginal Text:\n$$M u l t i h e A t t { n } ( Q, K, V ) = \\sum _ { i = 1 } ^ { H } \\text {Attention} ( Q W _ { i } ^ { o }, K W _ { i } ^ { o }, V W _ { i } ^ { o } W _ { o } ).$$  \nOne might argue that this simple aggregate-by-summation paradigm does not fully exploit the expressiveness of multi-head attention and that it is more desirable to use a more complex aggregation.\nContextualized Text:\nIn multi-head attention, after the attention heads compute their individual outputs, these outputs are typically combined using a simple summation approach. However, some researchers suggest that this method may not fully exploit the expressiveness inherent in the multi-head mechanism, leading them to explore more complex aggregation strategies.	{"tags": ["architecture", "NLP", "transformer", "attention"], "doc_id": "3d024558-828d-4909-b391-82fc938134b8", "summary": "The standard multi-head attention mechanism aggregates outputs by summation, but some researchers argue this is a simplistic approach that doesn't fully utilize the potential expressiveness of the mechanism.", "doc_type": "text", "entities": ["capsule networks"], "keywords": ["multi-head attention", "aggregation", "expressiveness", "summation"], "key_objects": ["multi-head attention", "outputs"], "contextual_text": "In multi-head attention, after the attention heads compute their individual outputs, these outputs are typically combined using a simple summation approach. However, some researchers suggest that this method may not fully exploit the expressiveness inherent in the multi-head mechanism, leading them to explore more complex aggregation strategies.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["What are the limitations of the summation approach for aggregating attention heads?", "How might more complex aggregation methods improve the performance of multi-head attention?", "What are some examples of alternative aggregation techniques that could be used?"]}
aa1e0322-3d75-4010-8a56-af98c83245cf	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0060975743,0.012167517,0.03404881,0.01282122,-0.0325489,0.07665905,0.0034538773,0.0548309,0.021677684,0.017877933,-0.0074745496,-0.006781106,-0.020750783,-0.0013706299,-0.039551232,-0.004380288,0.0009008296,0.08747647,0.005497958,-0.044672847,0.044725206,0.0019377472,0.04190194,-0.009708145,0.010075362,-0.009396091,0.036979277,0.006556895,-0.016458996,0.0052958266,0.00032084543,-0.05965364,0.038594026,0.015593007,0.04833363,-0.032405145,0.01972726,-0.066395946,0.001819524,0.026548095,-0.008754331,0.022665106,-0.054242566,-0.033459213,-0.006852992,-0.056588396,-0.043379996,-0.063517205,0.045451958,-0.009334181,-0.0049365405,0.024429219,-0.029290581,0.005800636,-0.017444136,-0.021537187,-0.00096562033,-0.047284726,0.020920217,-0.101459265,-0.019214032,0.016458778,0.022191757,-0.010138454,0.029521167,-0.00585364,3.5552432e-06,-0.020219775,0.03256564,0.1165191,-0.039003294,-0.009529956,0.032546695,-0.06738737,0.12665446,0.041156802,-0.02962621,-0.03807301,-0.024095189,0.013492032,0.022889217,0.053261746,-0.047467157,-0.039465163,0.081525505,0.008620253,-0.028275693,0.023230791,0.0542689,-0.04552488,0.00948555,-0.03339084,0.001896373,0.06925622,-0.03934704,-0.03173422,-0.011989287,-0.010784355,-0.036114432,-0.11836192,-0.008219519,0.0019574258,0.09620466,0.10855377,-0.0007827664,-0.052077647,0.012754859,-0.013091904,-0.0216542,0.021274354,-0.011134934,0.06738199,-0.062287632,0.02164757,-0.011894249,0.037552826,-0.046383593,0.0015941577,0.010880431,-0.01713044,0.061992094,0.004648194,0.0036221622,0.027166547,0.049881443,0.044281468,0.065299526,-0.04282244,-0.0076396437,-0.009891605,0.060354523,-0.00037963042,-0.030979995,-0.0075242845,-0.043355275,-0.029966684,0.07354605,-0.018827265,0.0055165254,0.023864934,-0.05386746,-0.0011706902,-0.043877557,0.028997283,0.032170966,0.039233476,-0.04016968,0.03039893,-0.0021630102,0.057739303,0.057230357,0.003491418,-0.0123474235,0.0078255115,-0.015971882,-0.0030912957,-0.005781372,-0.056234777,0.029777203,0.0016362205,-0.04745074,-0.038483102,0.019218788,0.09012243,0.04748711,0.115789205,-0.0020010383,-0.024973297,0.012230593,0.013024384,-0.01207058,-0.015335381,-0.004441496,-0.035973374,-0.029065983,0.029199682,0.03760625,-0.029082276,0.023483211,0.005628911,0.007754792,0.037896648,-0.0029798977,0.033306893,-0.02128593,0.030240312,0.014003295,0.0044451533,-0.0014205764,-0.07280711,0.023363935,-0.03150721,0.033495516,-0.01785081,0.016121073,0.038976986,0.04296177,-0.08272592,-0.0066155195,-0.034543414,-0.011379865,0.01165015,-0.02106614,0.0035070044,0.009589222,-0.017493175,-0.019817537,-0.03657637,-0.011627362,0.02660269,-0.010256617,0.009870331,-0.017597804,0.030020017,0.025451744,0.041621834,0.014491758,0.0142659005,0.02330474,-0.044965,-0.029554563,-0.0071399393,-0.06471073,0.050186984,-0.0018250104,0.029544374,-0.009031816,0.030466288,0.033939958,-0.012275412,0.0021152103,0.021445904,-0.034511562,0.0251007,-0.01628971,0.0016943448,0.009991349,0.036068864,-0.028488275,0.016974974,0.04130313,0.05299806,0.026986208,0.10024991,0.027185429,-0.034701042,0.022130352,0.013979968,0.018379265,-0.014015833,-0.03947361,0.0508612,0.027548963,-0.006302362,-0.009106988,-0.03937002,0.004885336,-0.011112559,0.033888973,-0.03550627,0.039894316,-0.008376582,0.0019219797,0.010148789,0.059774075,0.015305757,0.005660132,0.01560797,-0.026967686,0.02927938,0.019140327,0.010753894,-0.006306186,-0.008737654,-0.003195961,0.049412787,0.025325261,-0.0046363366,0.0016698138,-0.011977328,-0.014862738,-0.031130988,0.042052213,0.003615154,0.020375429,0.0034367002,0.03431626,-0.06217594,-0.026635097,0.01872683,-0.0044094645,-0.011391841,0.024397232,-0.018217413,0.0010881444,-0.0077401074,0.003736476,-0.017005742,-0.0019751224,-0.007357861,0.11895314,-0.021965196,0.0071026427,-0.011547278,0.023966523,0.023327133,-0.053674214,0.061366353,0.005690887,-0.016359624,0.070862226,-0.0066932356,-0.03483812,-0.007142559,0.011910104,0.10238592,0.07140224,-0.009943669,0.021975538,0.048848327,0.01511598,-0.032580122,-0.016923683,0.004007465,0.022507513,0.027968569,0.043947373,-0.021809353,-0.070436545,0.09932391,-0.06503476,0.004932544,0.0585234,-0.017456092,-0.014100132,0.011653504,-0.04001649,-0.0101787485,0.030275635,-0.003687448,0.00529806,-0.05720365,-0.002152018,-0.012848475,-0.039372817,0.052441817,-0.021723084,-0.0020318164,0.038857818,0.0016496368,-0.0033837035,0.060012273,0.035643864,-0.018351778,-0.0007513722,-0.028027121,-0.0053503904,0.016713183,0.015384501,0.015465768,0.086763345,-0.097814,0.052434746,-0.018238444,0.044843607,0.010516929,-0.008404752,0.0128422,0.04688909,-0.0030639123,0.018863305,-0.046917323,0.02807912,-0.048674133,-0.003986016,0.037280705,0.05171786,0.021387791,-0.030167472,0.010147066,0.01644235,-0.00013864267,-0.025861649,-0.0010825312,0.03160706,3.993992e-06,0.024933897,0.08951097,-0.016279979,0.043350197,-0.026009403,0.03853509,0.036697216,7.036017e-05,-0.040814746,0.015861169,-0.011578707,0.02344496,0.02280248,-0.023967078,0.06129121,0.024562204,-0.06053626,0.028388178,0.033445805,-0.006520079,-0.05371342,0.04495319,0.0024665832,0.018797358,0.020759936,-0.035373945,0.025419548,-0.021360932,0.052103918,-0.025064997,-0.02061239,0.0037931965,-0.031451255,-0.02311869,-0.019818747,0.042452484,0.0071443683,0.019946255,0.015698796,-0.014364685,-0.032969456,0.08936693,-0.016586358,-0.034278926,0.008383471,-0.010321383,-0.023649866,0.035220236,-0.03920166,0.00523009,-0.07378259,0.05149366,-0.03849303,0.014875224,0.07163915,-0.05350609,0.02652433,0.0022246202,0.007561844,-0.03161954,0.027615871,0.05824406,-0.04462668,0.012109049,0.022008143,0.014932544,0.042240087,-0.015670493,0.06534204,-0.006428029,0.00036408287,0.008214735,-0.014582774,0.005164466,0.016435064,0.038963493,0.026555581,0.019621598,0.0014502775,0.025001317,-0.030103294,-0.067248106,-0.019912787,0.0025141428,-0.014033623,0.049010407,-0.033874884,0.07395869,0.017038597,-0.005637638,0.008910733,0.06773541,0.08000499,0.018956473,-0.05493316,-0.009445712,0.009610608,0.004678634,0.006696939,-0.007389369,-0.0052146916,-0.010823608,-0.017200653,0.047834676,-0.020145616,0.005315423,-0.009418063,0.02433966,0.049734894,-0.04832294,0.029637033,-0.008600539,-0.01925958,-0.038611334,-0.053739313,0.038948946,0.04464761,0.04131681,0.009793947,0.007999251,-0.02067999,-0.017070724,0.013259288,0.0590294,-8.7218556e-05,-0.018567558,0.001452971,-0.0131165255,-0.00633262,0.048861507,-0.027601155,0.006641442,0.0026011108,0.023741007,0.018234864,0.012512282,0.050695274,0.007558788,0.012735284,0.0049990714,0.04300859,0.007155319,0.06384288,-0.066524506,0.024235716,-0.041695617,0.0003473199,0.0053681517,-0.04542465,-0.05863158,0.0013756319,-0.031460486,-0.012612501,0.021875761,-0.0043033455,-0.037501898,-0.009572989,-0.026562788,0.055265367,-0.031215545,0.04803983,0.015896536,0.033581413,0.08884201,0.037979715,0.02462099,0.020998493,3.757107e-05,0.018857066,-0.018778782,0.043923337,0.0014191676,-0.011775382,0.047781326,-0.007240487,-0.0025006256,-0.03675218,-0.058554117,-0.047076244,-0.026227187,-0.01982875,-0.0061476175,-0.0041416213,0.012578004,0.0054789837,0.009672888,0.035237577,0.017849958,-0.03169376,0.0554963,0.010076128,0.008075573,0.08201444,-0.0037309157,0.06518714,0.040051337,0.07704204,-0.010122854,-0.031922806,0.0070591494,-0.0026468951,0.006075643,0.041269284,-0.008096524,-0.06144916,-0.0287588,0.030350365,-0.0018981919,-0.037660476,-0.067116454,0.06440937,0.00300711,0.03883749,-0.08227722,-0.051803563,0.02112893,-0.03338634,-0.032870986,0.00094785105,0.030240964,-0.024595924,0.007866791,-0.0074686557,0.053859584,-0.12702043,-0.00487195,-0.036615327,0.03674646,-0.01081889,-0.016091865,-0.035114735,-0.008334828,0.00868647,-0.022894556,0.06374292,0.0027764265,-0.032435574,-0.0025309455,-0.025317093,0.0030987721,-0.0106879715,-0.017250016,-0.045226473,0.008750451,-0.032026462,-0.0006240111,0.07985226,-0.042656094,-0.042656396,0.048617046,-0.013339846,-0.07727462,-0.04480461,-0.031274993,0.03613795,-0.008539004,0.0025501284,0.044866294,0.0065764384,-0.0480181,-0.005085255,-0.055683162,-0.016604796,0.00029455003,0.029062556,0.018168006,-0.030247716,-0.04864595,-0.014498649,0.01608973,0.0011776221,0.02260313,0.08867056,0.045519713,0.06975385,0.04556591,-0.0164189,-0.0779364,-0.0053604213,-0.03132958,-0.022289725,-0.004030894,0.070629455,-0.0057218284,-0.02071856,0.008718808,-0.0017389291,-0.014867519,-0.0008042385,-0.0049202116,-0.01589694,-0.018643785,-0.080898486,0.0019265994,0.0118312035,-0.0011153847,-0.022720944,0.06832045,0.008812259,-0.02717874,-0.05719436,-0.055822507,-0.02713216,0.028746163,0.00014532327,-0.012499907,0.003234674,-0.036365643,0.00512233,0.073674776,0.030600628,-0.049198836,0.0154012125,0.010125004,0.034131948,0.026791602,0.03682903,-0.031106504,-0.10150524,0.02029871,-0.01195003,-0.035639007,0.016005583,-0.03292588,-0.041040882,-0.023641234,-0.01352528,0.02486467,0.028014803,-0.005233904,0.06248018,0.03531876,-0.0061334693,0.020757716,-0.07324921,-0.013332441,-0.010428363,-0.020192249,-0.04110035,0.05327958,0.04335926,-0.013362174,-0.051162455,0.021942336,0.00470485,0.018157022,-0.062544845,0.029791482,0.024039268,0.03244379,0.02468044,0.041403122,0.043488268,-0.013840225,-0.029136848,0.032481704,0.0011499195,0.012205601,-0.026835924,-0.036226362,-0.018487062,-0.020266503,0.013933802,0.013319413,-0.0047000237,0.029007832,0.01726734,-0.040286385,-0.061509516,-0.04236807,-0.030344887,0.047896706,0.017537076,0.0050457045,-0.045575146,-0.022001397,-0.01747146,0.03575597,0.014586051,-0.0047496385,0.022068283,0.0051113605]	Keywords: multi-head attention, aggregation, routing methods, capsule networks, dynamic routing, EM routing\nKey Objects: attention heads, routing mechanisms, output capsules\nRefers to Images: None\nHypothetical Questions:\n- Why might the simple summation method for aggregating attention heads be considered limiting?\n- How are capsule networks and routing methods being applied to improve multi-head attention?\n- What are the trade-offs associated with using routing methods in multi-head attention, and why is applying them only to lower layers often preferred?\n---\nSummary:\nSome researchers argue that the standard summation method for aggregating outputs from multiple attention heads in multi-head attention may not fully utilize the mechanism's expressiveness. They propose using routing methods, inspired by capsule networks, to aggregate information, with lower layer routing offering a balance between performance and computational cost.\nOriginal Text:\nOne might argue that this simple aggregate-by-summation paradigm does not fully exploit the expressiveness of multi-head attention and that it is more desirable to use a more complex aggregation.  \nGu and Feng [40], Li et al. [74] propose to use routing methods, originally proposed for capsule networks [112], to further aggregate information produced by different attention heads. The outputs of attention heads are first transformed into input capsules, then output capsules are obtained after the iterative routing process. The output capsules are then concatenated as a final output of multi-head attention. These two works both utilizes two routing mechanisms, namely dynamic routing [112] and EM routing [53]. One would notice that iterative routing introduces additional parameters and computational overhead. Li et al. [74] empirically show that applying the routing mechanism only to the lower layers can best balance the translation performance and computational efficiency.\nContextualized Text:\nWithin multi-head attention, the standard practice of summing the outputs from individual attention heads is sometimes viewed as a limiting factor in fully leveraging the mechanisms capabilities. To address this, researchers have explored alternative aggregation methods, such as routing mechanisms adapted from capsule networks. These routing methods transform the outputs of individual attention heads into capsules, which are then iteratively processed to produce a final aggregated representation, with the lower layers benefiting the most from this approach.	{"tags": ["architecture", "attention", "transformers", "deep-learning"], "doc_id": "aa1e0322-3d75-4010-8a56-af98c83245cf", "summary": "Some researchers argue that the standard summation method for aggregating outputs from multiple attention heads in multi-head attention may not fully utilize the mechanism's expressiveness. They propose using routing methods, inspired by capsule networks, to aggregate information, with lower layer routing offering a balance between performance and computational cost.", "doc_type": "text", "entities": ["capsule networks", "dynamic routing", "EM routing"], "keywords": ["multi-head attention", "aggregation", "routing methods", "capsule networks", "dynamic routing", "EM routing"], "key_objects": ["attention heads", "routing mechanisms", "output capsules"], "contextual_text": "Within multi-head attention, the standard practice of summing the outputs from individual attention heads is sometimes viewed as a limiting factor in fully leveraging the mechanisms capabilities. To address this, researchers have explored alternative aggregation methods, such as routing mechanisms adapted from capsule networks. These routing methods transform the outputs of individual attention heads into capsules, which are then iteratively processed to produce a final aggregated representation, with the lower layers benefiting the most from this approach.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["Why might the simple summation method for aggregating attention heads be considered limiting?", "How are capsule networks and routing methods being applied to improve multi-head attention?", "What are the trade-offs associated with using routing methods in multi-head attention, and why is applying them only to lower layers often preferred?"]}
00ddb65b-c179-4c65-9ce3-39684fa02a4f	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.029117474,-0.04330369,0.030185457,0.020647388,-0.052779444,0.0515147,0.014447378,0.041939758,0.018556364,-0.0153549425,0.020733774,-0.023476832,-0.012771712,-0.025803102,-0.0034356655,0.02048409,0.039805047,0.09190933,0.014174388,-0.021058463,0.038889363,0.044648122,0.029903125,0.026400203,-0.020646138,0.034175858,0.04848618,0.026527429,-0.034574393,0.013847999,0.037920095,-0.053580422,0.04338531,0.043695435,0.077213764,-0.034359716,-0.0049494077,-0.07914284,0.022801261,-0.001115993,0.005561535,0.018032396,0.01167083,-0.02527659,-0.011497108,-0.053099092,-0.0645632,-0.014739899,0.0058568814,-0.040500846,-0.025295416,0.034750152,-0.039240226,0.019338459,-0.012400815,0.010559971,0.06818903,-0.057529915,0.017232833,-0.13258858,-0.0016067922,0.0004843224,0.016157325,0.0020225276,0.03726833,-0.023262748,-0.014460986,-0.0011322787,0.041508947,0.14093257,-0.02515837,0.0029043034,0.031880345,-0.04298733,0.11391779,0.031860154,-0.025741592,-0.018870214,-0.037624348,-0.023766086,-0.016744182,0.08260766,-0.07876224,-0.033760164,0.053551562,0.013555802,-0.014934121,0.032670263,0.045729127,-0.04113654,-0.008286727,0.016248321,-0.038674027,0.089624815,-0.034969285,-0.040886078,0.0011987555,-0.006298305,-0.007740984,-0.07876869,0.019158505,0.005295822,0.07001876,0.098111704,-0.0012564712,-0.03334356,-0.013424167,-0.056514338,0.007271096,0.0009887158,-0.0061158496,0.04167097,-0.043993987,0.018134538,0.0057076677,-0.010629798,-0.025076434,0.00079359853,0.014933174,-0.034765925,0.023610689,0.0068123396,-0.0032335313,0.043164942,0.04637577,0.018325215,0.030314155,0.0100301,-0.047577877,0.013897609,0.084907845,0.003205232,-0.0056662075,-0.01798609,-0.03187855,-0.0019022075,0.07189663,-0.0154249165,-0.0046415604,0.025583737,-0.026298502,-0.041352637,-0.050071955,0.025078872,-0.015862435,0.032055657,-0.07624575,0.04228912,-0.036389247,0.058200315,0.054245822,-0.022488432,0.020615697,-0.019620994,0.0003235666,0.005030184,-0.003460927,-0.05148174,0.028418766,-0.024523582,-0.031753782,-0.06913418,-0.021433225,0.063154586,0.09201661,0.14747053,0.018988693,-0.013166539,0.00015749162,-0.025040084,-0.026911551,-0.011387913,0.013719573,-0.045334376,0.01600022,0.002598847,0.027974132,-0.0555018,0.022312826,-0.006368388,0.007581178,0.05540961,0.0016795155,0.0070623034,-0.01737417,0.035695337,-0.0008275313,0.052368324,-0.029909186,-0.008409597,0.001787048,0.017859839,-0.0019982588,-0.009048216,0.025013374,0.02079558,0.035519864,-0.07314689,0.014543141,-0.039705016,-0.015222959,0.011583211,-0.011158988,-0.042044926,-0.0019855618,-0.06566339,0.0045755766,-0.049191624,-0.00040066807,0.053711787,-0.045217555,-0.030337058,-0.023390448,0.02533434,-0.023409216,0.0066389632,-0.007467368,-0.0022800525,0.026035486,-0.03440101,-0.023547847,-0.010613381,-0.07644921,0.034408636,-0.02572481,0.061162014,0.0146693215,0.0011166077,0.01568619,-0.035263427,-0.013453898,0.02332474,-0.028516172,0.0095689865,-0.050175156,-0.03593124,0.018746642,0.030259712,-0.05085965,0.020303898,0.05842751,0.05779316,0.0113067115,0.10891128,0.03492431,-0.0163334,0.018304037,0.00731566,0.016832653,0.016684221,-0.081649564,0.05818355,0.018503288,-0.01988976,-0.021515992,-0.023809705,0.0031476452,0.0028716028,0.019845555,-0.04358357,0.029797731,-0.025358183,0.010816489,0.022901451,-0.0063832086,0.030908233,0.017010426,-0.007220331,-0.026380941,-0.0077122925,0.007810337,0.077016085,0.018022688,0.0024495884,-0.03319395,0.043078624,0.009340068,-0.00081635907,-0.0065309023,0.005463776,-0.032861065,-0.02881568,0.03437404,0.008496071,0.015038107,-0.01636315,0.064031884,-0.0153766405,-0.029481001,0.016996935,0.006298169,-0.04112882,0.03737784,-0.016722359,0.0045857383,-0.031398103,-0.0041327993,-0.014200903,-0.014895561,0.0137942135,0.1362532,-0.029624427,0.003047935,0.050421637,0.017577596,0.00710574,-0.068211526,0.03578231,0.022162104,-0.007346811,0.028041214,0.018234976,-0.012917234,-0.030402256,0.028652601,0.054507617,0.06282154,-0.008626588,-0.013527845,-0.009096073,0.03435504,0.014157703,0.00045537163,0.047185548,0.030448677,0.002271521,0.06604363,-0.003645639,-0.0524828,0.06910109,-0.03261607,0.020907288,0.07716876,-0.021013739,-0.0054530287,-0.011408169,-0.0023830093,0.0102200555,0.016901504,0.0021357138,-0.04379104,-0.02749514,0.030098004,0.012334029,-0.012486336,0.051757697,-0.0054080053,0.001119541,0.022941655,0.019166388,0.030931357,0.031220213,0.010473716,-0.0015438906,-0.014025634,-0.022941753,-0.04305398,0.031629357,-0.007736391,-0.012982955,0.074720226,-0.1197421,0.068279766,-0.057822622,0.04179817,0.0003669692,-0.06709248,0.05210981,0.052826922,-0.010926568,0.027333157,-0.05263431,0.036090862,-0.014780987,-0.024032658,0.033317946,0.03007146,0.011940911,-0.045644686,0.016319454,0.028589627,-0.033185996,-0.011171075,-0.036174126,0.023028526,0.012892086,0.017359124,0.06462711,-0.025563793,0.03772956,-0.04171614,0.0056248363,0.02161669,0.017557774,-0.03293993,-0.00021184256,-0.0143387895,-0.0049854256,-0.008271257,-0.012306981,0.02903185,0.01776195,-0.07315444,0.02446716,0.02655316,0.0059254616,-0.08315849,0.023764536,0.02039627,0.005280018,0.020605765,-0.04498249,0.0038446758,-0.008264903,0.028311519,-0.050956834,0.023096252,-0.006103864,-0.051380035,0.007834024,-0.026802666,0.05311983,0.018879205,0.014072228,0.034465645,-0.025034193,-0.046025973,0.06425782,-0.023544341,-0.04224268,0.017021291,-0.029743057,0.009915027,0.039202567,-0.02447121,0.049566712,-0.03954764,0.10356489,-0.0010890963,0.007140746,0.006989133,-0.027291596,0.014195085,-0.046217293,0.02764082,-0.04362893,0.033474166,0.016364966,-0.028828807,0.016515074,0.0453398,0.017621912,0.035317123,0.020630142,0.06319209,-0.0055292994,-0.0047022793,-0.028910613,-0.061793752,0.013647301,0.044453014,0.010778027,0.035967495,-0.0038293737,-0.0017691796,-0.010219894,0.007568544,-0.061580382,-0.025424264,-0.0002730754,0.024717923,0.051295023,0.017982572,0.041110296,-0.012223053,-0.044756737,-0.02019849,0.051369175,0.037124787,0.024992105,-0.09915279,0.003917006,0.003022719,0.018324204,0.020746846,-0.019416653,-0.016130881,-0.03127877,-0.015116485,0.022533016,-0.017993081,-0.004421787,0.008726331,0.039072413,0.03218292,-0.035284754,0.05788247,-0.009969584,0.010895132,0.00089075335,-0.026052836,0.012632626,0.05299196,0.044566967,-0.0023825,-0.024340251,-0.020904917,-0.02338112,0.0077729626,0.036807027,-0.016880458,-0.01908593,0.030706307,0.012167107,-0.04882485,0.01033497,-0.03516016,0.055461913,-0.0061459797,0.04366137,-0.02722433,0.0040849163,0.015052799,0.023341166,0.009041235,0.0022834146,0.029839959,0.02259763,0.03047622,-0.07288247,-0.01267278,-0.050363425,-0.0008788918,0.024569578,-0.065638684,-0.08284609,-0.020886702,-0.049757957,-0.0035888783,0.0017028651,-0.008083668,-0.0585232,0.015249943,-0.027805794,0.055929873,-0.03308448,0.0032227451,0.0030551464,-0.020352867,0.08456665,0.044211436,0.03808247,0.03468852,0.008984082,-0.01552482,0.03807362,0.0672243,0.00017253446,-0.00092764164,0.031645052,0.007550855,0.030734368,0.011823618,-0.053037982,-0.015105124,-0.020440804,0.00932017,-0.026212297,-0.013242174,-0.0011974012,-0.0010803308,-0.0034175487,0.013087991,-0.0129067,0.020190766,0.021474348,0.0027212407,-0.028716713,0.044667337,-0.008139727,0.015115488,0.010281951,0.035503466,-0.0056647346,-0.02071128,-0.0037125363,0.016888857,-0.012696351,-0.039910004,-0.005046244,0.01251587,-0.017317731,0.017848132,0.0068433248,-0.0039118095,-0.07650493,0.007214368,0.016656281,0.055270415,-0.04652008,-0.055529527,-0.019640513,-0.042872984,-0.04340111,-0.0119115235,0.03135543,0.002382769,0.016390223,0.00038735205,0.07597667,-0.10073333,0.016004613,-0.052712932,-0.012669382,-0.019836193,-0.024853932,0.0069946786,-0.023142526,0.032809112,-0.043180104,0.109156385,-0.008072342,-0.016644709,-0.0057454663,-0.008218619,0.020978574,-0.02454119,-0.023330435,0.0023121343,0.00084217073,-0.013505812,-0.019929899,0.08403803,-0.05068193,-0.012159211,0.012803483,0.004235243,-0.08447888,-0.019186465,-0.019639455,0.028171597,-0.050759956,0.040205743,0.020591145,-0.025296478,-0.026890382,0.009551532,-0.06745813,0.013106678,-0.00015417939,0.041818205,0.04842044,-0.016665101,-0.043406185,-0.03174574,0.038030304,0.020961072,0.023506502,0.047342032,0.019606382,0.072120875,0.017763052,-0.0077253636,-0.030735172,-0.029207854,-0.017669741,-0.012911738,-0.012243305,0.05447505,-0.0030048252,-0.005353988,0.02791024,0.026466997,-0.024308011,-0.01341079,0.019311195,-0.015760865,-0.07738961,-0.028766047,0.00327672,0.004911104,0.006622512,-0.005895012,0.09531091,-0.007975826,0.01529769,-0.04799842,-0.09488992,-0.042301897,0.021712808,-0.036645394,-0.023954695,0.028885312,-0.018409718,-0.0065565407,0.05805737,0.0372232,-0.043833055,0.009173534,-0.0025004186,0.04918446,0.026273843,0.016379612,-0.026797254,-0.06444142,0.021510798,-0.01105822,-0.03857082,0.032499377,-0.019995254,-0.043185376,-0.009730053,-0.011246578,-0.03433692,0.0018979426,0.009033033,0.011898179,0.03164205,-0.008759255,0.014775943,-0.07948483,-3.0781852e-05,-0.0047847433,-0.010164863,-0.027437916,0.063263156,0.021479316,-0.023974024,-0.030320913,0.003635811,0.013830133,0.0061143446,-0.0084318975,0.047297787,0.009049183,0.016201556,-0.016503163,0.00064571365,0.037138443,0.002882107,-0.016880663,0.030078312,0.0027394698,0.010401053,-0.02840228,-0.0037451582,-0.05373941,-0.040255763,0.044176538,0.041159492,-0.02259233,0.021439208,-0.015551255,-0.045711894,-0.008584908,-0.012082442,-0.033691783,0.04983954,0.014012851,0.0028222566,-0.052685317,-0.008188856,0.00053299614,0.049153417,-0.006178724,0.05178233,-0.034797758,0.0316899]	Keywords: multi-query attention, head size, attention mechanism, memory bandwidth\nKey Objects: attention heads, memory bandwidth, head size\nRefers to Images: None\nHypothetical Questions:\n- How does multi-query attention improve decoding speed?\n- Why is it important to consider the size of attention keys?\n- What are the trade-offs when modifying the multi-head attention mechanism?\n---\nSummary:\nSeveral modifications to the multi-head mechanism have been proposed to improve attention, including multi-query attention which reduces memory bandwidth and disentangling head size from the number of heads to better represent distributions.\nOriginal Text:\n4.6.4 Other Modifications. Several other modifications to the multi-head mechanism have been proposed to improve multi-head attention.  \nShazeer [117] propose multi-query attention, where key-value pairs are shared among attention heads (i.e., to use only one key projection and one value projection for all attention heads). The advantage of this method is that it reduces the memory bandwidth requirements for decoding and results in a model that is faster to decode, while incurring only minor quality degradation from the baseline.  \nBhojanapalli et al. [11] establish that small attention key size can affect its ability to represent arbitrary distribution. They thus propose to disentangle head size from the number of heads h , as opposed to the common practice that sets the head size to be D$\\_{m}$/h . It is observed empirically that setting attention head size to be input sequence length is beneficial.\nContextualized Text:\nTo further enhance multi-head attention, several modifications have been introduced. For example, Shazeer proposed multi-query attention, which reduces memory bandwidth for decoding by sharing key-value pairs across attention heads. Additionally, Bhojanapalli et al. addressed the impact of small attention key sizes by proposing to disentangle head size from the number of heads, a departure from standard practices.	{"tags": ["NLP", "deep-learning", "attention", "transformers"], "doc_id": "00ddb65b-c179-4c65-9ce3-39684fa02a4f", "summary": "Several modifications to the multi-head mechanism have been proposed to improve attention, including multi-query attention which reduces memory bandwidth and disentangling head size from the number of heads to better represent distributions.", "doc_type": "text", "entities": ["BERT", "Shazeer", "Bhojanapalli"], "keywords": ["multi-query attention", "head size", "attention mechanism", "memory bandwidth"], "key_objects": ["attention heads", "memory bandwidth", "head size"], "contextual_text": "To further enhance multi-head attention, several modifications have been introduced. For example, Shazeer proposed multi-query attention, which reduces memory bandwidth for decoding by sharing key-value pairs across attention heads. Additionally, Bhojanapalli et al. addressed the impact of small attention key sizes by proposing to disentangle head size from the number of heads, a departure from standard practices.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "4 ATTENTION", "h3": "4.6 Improved Multi-Head Mechanism"}, "hypothetical_questions": ["How does multi-query attention improve decoding speed?", "Why is it important to consider the size of attention keys?", "What are the trade-offs when modifying the multi-head attention mechanism?"]}
bf833412-831c-4d70-85ff-a38cdc256573	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.031980194,0.027580595,0.01615716,0.06216539,0.0037754364,0.08927532,0.029767666,0.0568326,-0.0016160749,-0.03780919,0.002165369,0.06657658,0.037028346,0.022702832,-0.023359675,0.03357897,0.016396003,0.06644917,0.01638126,-0.03245395,0.051026568,-0.014897564,0.040632043,-0.014185155,0.04405884,0.065845385,0.004971807,-0.004925082,-0.009362395,0.007602493,0.005930923,-0.021335335,0.056714617,-0.02008714,0.018983567,0.0018895234,0.034428723,-0.06365508,-0.0075757937,-0.009148908,-0.011406652,0.047382083,-0.08818659,0.025009563,-0.004259865,-0.03383144,-0.080605865,-0.02383403,0.025854308,0.002082129,0.0027664728,-0.0036478848,-0.03938035,-0.0106634,-0.043242887,0.016146526,-0.02696631,-0.021617124,0.024159713,-0.1012382,-0.047592692,0.026173558,-0.039574593,0.0014015894,-0.014536805,-0.004643208,0.03368448,0.054776475,0.041991252,0.12643589,-0.0070464727,-0.006914709,-0.0255533,-0.054207023,0.105063535,0.025136383,-0.06746718,-0.039112568,-0.04084567,-0.012822875,-0.017831078,0.036897626,0.005454534,-0.026799819,0.08330947,0.0337813,-0.029576708,0.007658873,0.09153321,-0.046811458,0.024317231,0.043552224,-0.016241688,0.041813485,0.009509663,-0.054865785,-0.029540177,-0.036768246,0.0052009183,-0.05610856,0.028824447,-0.045719612,0.092269935,0.07372143,-0.0028738098,-0.034335323,-0.040003773,-0.036468413,0.020791491,0.02557211,-0.07409276,0.005972765,-0.064173736,0.0041392604,-0.01947032,0.0053721364,-0.027990945,-0.016958931,-0.027938167,0.0038481266,-0.00060654443,-0.023071712,-0.022836545,0.02779668,0.047155563,0.011312285,-0.026376532,-0.0019622906,-0.055434015,-0.025457116,-0.004012543,-0.006772272,-0.009799618,0.017783677,0.012385638,-0.01981425,0.054342695,-0.00787206,0.006722141,0.053942874,0.019538995,-0.0613333,-0.014110651,0.015838545,-0.040567234,0.023949645,-0.004330711,0.027304288,-0.03198067,0.026205594,0.03342779,-0.014988149,0.058508947,-0.02454591,-0.013298596,-0.025044754,-0.004410495,-0.035529263,-0.007866788,-0.0367628,-0.06922132,-0.06196923,-0.046102002,0.06739222,-0.001203743,0.07765652,-0.0022755128,-0.004824071,-0.0025949092,0.055156436,-0.06298465,0.008256305,-0.026583027,-0.013338333,0.007287206,0.012041506,-0.009814828,0.0067277174,0.023452103,-0.008434536,-0.036300782,0.029472185,0.017750924,0.048822917,0.00984845,-0.0050342744,0.010889569,0.07501695,-0.01817068,-0.008824198,0.014916823,-0.018412648,-0.03318797,0.004420522,0.050696973,0.049613133,0.032035727,-0.008026335,-0.05439287,0.030651199,-0.0009673517,0.034686882,-0.0043388596,-0.01488656,0.049303137,-0.022827784,-0.034809776,0.0040346743,-0.040377785,0.054103475,-0.021546317,-0.040053513,-0.045036875,0.036514796,0.01987951,-0.0054756505,0.017117497,0.0548969,0.019852325,0.0026608233,-0.0024274916,0.045008123,-0.016924107,0.036933906,-0.032307338,0.005492509,-0.034668036,-0.04534054,0.008100706,-0.049683504,0.04284471,0.042183924,-0.059092946,0.04037132,-0.04068277,0.02324833,-0.00026686752,-0.020487938,-0.007849894,0.022285646,0.022908727,0.029304221,-0.010410002,0.06993103,0.01793811,-0.015663778,0.03181619,0.0468169,0.008065553,0.03235427,0.02486757,-0.017308747,0.050080597,-0.0051445607,0.012665527,-0.041379377,0.036573544,0.010509739,0.02717671,0.020210082,0.0290908,0.007432546,-0.04805522,0.0020457762,-0.018712712,-0.03035624,0.0064729126,0.04191294,-0.028048368,-0.018523693,0.023138015,0.03716197,0.07495408,-0.010672513,-0.005883819,0.04896642,0.00548603,-0.010583444,0.03567679,-0.032415427,-0.014971871,0.026016152,-0.013500358,-0.003244781,0.005103092,0.0034905938,-0.03586085,-0.06136912,0.0142848585,-0.02959533,-0.05514837,-0.024492525,0.010681616,0.0035370218,-0.046266817,-0.004328954,-0.0048612827,-0.025750307,-0.002535901,0.030894717,0.09732391,-0.033687763,0.0023820633,-0.018647175,0.028335322,-0.041446913,-0.042508163,0.0074231634,0.052134253,-0.0060807816,0.023440948,-0.030608324,-0.027091147,0.017339041,0.062338315,0.08750322,0.07339643,-0.0775887,0.043584283,0.03724328,0.03243239,0.008126337,-0.028778085,0.014697008,0.012243324,-0.0024156016,-0.015564618,-0.017667687,-0.03521877,0.061095424,-0.040114712,0.07196941,0.060773563,0.018547358,-0.0047661313,-0.009409001,0.016407322,0.0483119,0.0004755126,-0.0230257,0.00484378,-0.09003758,-0.009037472,0.00793159,-0.028121205,0.036291692,0.0028240767,0.0300454,0.001190045,-0.039199956,0.0250467,0.029356124,0.0074217627,0.023681644,0.008019759,-0.029346077,0.013393486,0.02473653,0.011531687,-0.044543706,0.04276563,-0.035537995,0.05133479,-0.018463623,0.0056168204,-0.020761147,-0.079319365,0.010437046,0.052702602,-0.02935666,0.010343351,-0.05011681,0.029489588,-0.035384167,0.009811168,0.025349176,0.048450332,0.024413595,-0.042283222,-0.01925833,-0.022995368,-0.008481516,0.011942849,-0.011341094,0.03146649,-0.004047392,-0.010507621,0.0932839,-0.009249152,0.06558062,-0.01874487,0.008206921,0.02015232,-0.005536172,-0.010480793,0.046257913,0.02455494,0.055841796,-0.032954555,-0.030721279,0.0038152323,0.029781656,-0.07276235,0.027275437,-0.024659242,-0.008132791,-0.053626448,0.023098126,0.049985543,-0.015016673,0.026040941,-0.047810063,-0.032415826,0.011477844,-0.01690013,-0.0067123934,-0.018233672,0.06496348,-0.052484818,0.009239297,-0.0031328406,0.03309688,0.0018780546,0.03898495,0.057781875,-0.011920744,-0.033288684,0.04235996,0.022699846,-0.032154594,0.01086101,-0.038305208,-0.008725753,0.05877083,0.007521889,0.01771998,-0.06596807,0.068655044,-0.0683516,0.013025793,0.072806366,-0.023195695,0.03025966,0.004908378,0.031807277,-0.047765907,0.03426446,0.037934616,0.009130951,-0.051929742,0.04126721,0.015057974,-0.024936559,-0.013632564,-0.005825746,-0.0015488905,0.028336333,-0.011257461,-0.028278796,-0.0054279906,-0.030554939,0.06275427,-0.0022083847,0.0048707137,-0.041822,-0.03383998,-0.022732979,-0.03146459,-0.024635406,-0.0013211621,-0.0037277516,-0.014696692,-0.052436028,0.042063072,-0.001602635,0.031533286,-0.02537598,0.05730537,0.038769376,-0.032803692,-0.04558009,-0.035928626,0.019168152,0.052435,0.026866803,0.031582218,-0.018659059,0.019774862,-0.03965383,0.014420154,0.010527433,0.033017773,0.05692528,0.01758164,0.02311383,-0.064744934,-0.006681368,0.030947829,-0.01106512,-0.017985351,-0.03547408,-0.05139409,0.011739523,0.029351477,-0.038739797,0.009898504,-0.025588838,0.016315565,-0.018585071,0.06547535,0.0017692694,-0.0004904481,-0.020887768,0.007830498,0.0010495313,0.042368855,-0.010135486,-0.02544097,0.012187962,0.05329819,-0.012197086,-0.014865658,0.024816189,-0.0061370903,0.01810975,0.018603083,0.046027012,0.029025488,0.041079827,0.008709627,0.019390643,-0.04448536,-0.059796795,-0.006352634,-0.055824295,-0.0388555,0.0038208838,-0.045719374,0.013398062,0.0394418,-0.0188214,-0.006383951,0.030841544,-0.05458682,0.11590187,-0.03708949,-0.0077798655,0.0076471413,-0.04134521,0.07235973,0.052276395,0.03255042,-0.039074495,-0.016113117,0.021597052,0.013902803,0.07040035,0.0018738337,-0.015728023,0.07065129,-0.03631649,0.0045706267,-0.04560558,-0.031452753,-0.0019416818,-0.06359218,-0.013992406,0.018963777,-0.06089024,0.04102589,-0.04299727,-0.034636226,0.006000025,0.035624202,-0.050687905,0.047320623,0.044970647,0.008946799,0.026351463,0.0049034837,0.06268551,0.0013134568,0.03196026,0.036392868,-0.0024408905,0.010541588,-0.039369915,0.016456876,0.0035302388,0.009049462,-0.024434298,-0.06988326,0.007820499,-0.0024209635,-0.0051120194,-0.011760544,0.011607376,0.049736716,0.015805427,-0.031436548,-0.065547936,-0.01911218,-0.016263256,-0.04144776,0.010318204,0.021791097,-0.00056986354,0.04335396,-0.08183672,0.05471755,-0.036477838,-0.03997277,-0.052569263,0.021275878,-0.009432353,-0.03289386,0.012111292,-0.04474332,0.0077605806,-0.022299256,-0.009321269,0.07393511,-0.017657144,-0.02700622,-0.013781591,-0.000998658,0.0029575224,0.007645877,0.014906837,0.014628281,-0.008592523,-0.020809414,0.063355036,-0.011181546,0.02147971,0.052981395,0.011041834,-0.06554989,-0.019023469,0.07031123,0.020469235,0.01331646,0.0038721769,-0.000268189,0.037360027,-0.020375164,-0.028262312,-0.037002373,-0.049857646,-0.013089589,-0.024173439,0.04128778,-0.021317134,-0.06532968,-0.003744154,0.012018533,-0.06193862,0.028394857,0.008965321,0.081411146,0.0026097798,0.04009159,-0.034350492,-0.0650012,-0.027462633,-0.018937057,-0.00026504014,-0.024985084,0.04781766,0.040387418,-0.007086967,-0.06896104,0.02235524,-0.018543491,0.016052434,0.032763768,-0.019670581,-0.043928318,0.0021064982,0.02189194,0.018546876,0.009543399,-0.024226896,0.10725279,0.04111487,-0.018654875,-0.018424332,-0.0075462386,-0.059669558,0.00021714637,-0.007846652,0.01561718,-0.029455794,0.014704532,-0.0022036508,0.055017248,-0.0082688425,-0.018745884,-0.042317454,0.06902073,0.019665426,0.06686931,-0.041404102,-0.01840844,-0.077069856,0.01317655,-0.01653366,-0.025169896,0.09274355,-0.007156469,-0.044753738,-0.0147372,0.022973338,-0.0147959385,0.002975366,-0.015793517,0.043627765,0.0186361,0.03456757,-0.015090849,-0.0511513,0.020059623,-0.0428888,-0.016375175,-0.005683233,0.019963585,0.014008098,-0.02383085,-0.015643483,-0.04321812,0.007899516,0.019036813,0.010573205,0.048940644,-0.039641805,0.00016968085,-0.05184628,0.013415436,0.103861324,-0.043126147,0.020829335,-0.01864085,-0.0090568,0.0002506669,0.019291976,-0.0314856,-0.033424757,-0.040189274,0.034898378,0.048144925,0.012220693,0.06692107,-0.015859792,-0.036551345,-0.015668623,-0.056571614,-0.03455282,0.045178533,0.022225525,0.0688677,-0.03790305,-0.027978929,-0.038913976,0.040822983,-0.022569736,-0.022626135,0.036065385,-0.0006249789]	Keywords: permutation equivariant, function, permutations, Transformer, self-attention\nKey Objects: function, permutations\nRefers to Images: None\nHypothetical Questions:\n- What is the significance of permutation equivariance in the context of neural networks?\n- Why are Convolution and Recurrence networks not permutation equivariant?\n- How does the permutation equivariance of Transformer components relate to the need for positional encodings?\n---\nSummary:\nA permutation equivariant function, as defined, satisfies the property that applying a permutation to the input results in a corresponding permutation of the output. This concept is relevant to understanding the behavior of networks like Convolution and Recurrence networks, which are not permutation equivariant, while self-attention modules and position-wise feed-forward layers in Transformers are.\nOriginal Text:\n## 5 OTHER MODULE-LEVEL MODIFICATIONS  \n### 5.1 Position Representations  \nDefinition 5.1 (permutation equivariant function). Let $\\_{n}$ be the set of all permutations of indices { 1 , 2 ,    , T } . A function f : X $^{T}$ Y T is said to be permutation equivariant if and only if for any   $\\_{T}$  \n$$f ( \\pi x ) = \\pi f ( x ).$$\nContextualized Text:\nIn the context of Transformer models, it's important to understand the concept of permutation equivariant functions. A permutation equivariant function, defined as f(x) = f(x), means that applying a permutation to the input results in a corresponding permutation of the output. This is relevant because networks like Convolution and Recurrence networks are not permutation equivariant, while self-attention modules and position-wise feed-forward layers in Transformers are.	{"tags": ["mathematics", "theory", "transformer"], "doc_id": "bf833412-831c-4d70-85ff-a38cdc256573", "summary": "A permutation equivariant function, as defined, satisfies the property that applying a permutation to the input results in a corresponding permutation of the output. This concept is relevant to understanding the behavior of networks like Convolution and Recurrence networks, which are not permutation equivariant, while self-attention modules and position-wise feed-forward layers in Transformers are.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["permutation equivariant", "function", "permutations", "Transformer", "self-attention"], "key_objects": ["function", "permutations"], "contextual_text": "In the context of Transformer models, it's important to understand the concept of permutation equivariant functions. A permutation equivariant function, defined as f(x) = f(x), means that applying a permutation to the input results in a corresponding permutation of the output. This is relevant because networks like Convolution and Recurrence networks are not permutation equivariant, while self-attention modules and position-wise feed-forward layers in Transformers are.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["What is the significance of permutation equivariance in the context of neural networks?", "Why are Convolution and Recurrence networks not permutation equivariant?", "How does the permutation equivariance of Transformer components relate to the need for positional encodings?"]}
30477ff3-f931-44b2-9bad-d122624fa3ac	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03498714,0.031226214,0.01838512,0.054199502,-0.022964079,0.06941239,0.030193506,0.060718928,0.021428341,-0.05035268,0.0024098724,0.0580996,0.029963924,-0.0072958465,-0.008123189,0.017245527,-0.016762324,0.07820574,0.01116572,-0.025332619,0.06520472,-0.018688671,0.028148172,-0.009221994,0.025761828,0.051566053,0.029562688,-0.02444658,0.016156854,0.030975487,0.0269941,-0.04410589,0.057018433,0.012691167,0.029880257,0.014519597,0.04923599,-0.06329269,-0.010445673,-0.0062684817,-0.018530102,0.04300658,-0.0839836,0.011269365,-0.012369874,-0.062393356,-0.0799956,-0.03179169,-0.00093038083,-0.0067457817,-0.026619183,-0.007453523,-0.027520236,-0.020671636,-0.017477432,0.01122834,-0.009659818,-0.025104113,0.020895563,-0.09660185,-0.012726443,0.025201956,-0.055675384,-0.011668786,0.0016285977,-0.013163198,0.015207766,0.07563874,0.019141769,0.12401744,-0.028522499,0.002481927,-0.0134641845,-0.07003037,0.13351007,0.0078971535,-0.06112884,-0.020473942,-0.060240947,-0.019593528,-0.018894631,0.07312616,-0.0065751406,-0.030126436,0.05235458,0.01231395,-0.024869045,-0.0053609237,0.09923359,-0.029877897,0.0027334657,0.0460183,0.028857952,0.046710566,-0.012423332,-0.07607684,-0.03253877,-0.04470058,-0.0041881725,-0.042729907,0.0099535305,-0.049351033,0.08491052,0.08941307,0.028635891,-0.031269636,-0.0201106,-0.047204737,0.030923838,0.012290902,-0.06500078,0.019382644,-0.06168431,0.011778662,-0.011863388,-0.0211285,-0.03798099,-0.021036737,0.0017536896,-0.00092342164,0.00083000713,-0.037189227,-0.040521786,0.044791956,0.054365285,0.0073261624,-0.01791372,-0.0018275549,-0.045109946,0.014146189,0.015254445,-0.008865253,-0.04565664,0.029466376,-0.022339353,0.018875835,0.05743512,0.03095048,-0.0043913266,0.039428487,0.030693209,-0.071670614,-0.029378016,0.029027004,-0.01891976,0.02339197,-0.021291185,0.013283303,-0.02174252,0.03464523,0.032733172,-0.018869402,0.057658188,-0.0017318929,0.009486574,-0.012366116,-0.005664414,-0.052140962,-0.0067962436,-0.034347996,-0.0665027,-0.056159325,-0.033909496,0.050034974,-8.422464e-05,0.09995403,0.005097147,-0.004941314,1.0733265e-05,0.05016043,-0.0466137,0.015693197,-0.025441358,-0.05554885,0.009789705,0.024562357,-0.004328638,-0.0151017,0.032908663,-0.0029531638,-0.011349107,0.044226404,0.01929235,0.05351641,-0.019652313,-0.0076015648,0.03389391,0.06698532,-0.017173288,-0.0076414323,0.01045392,-0.013930102,-0.026923062,-0.008166099,0.050833263,0.04065408,0.043491405,-0.021240918,-0.051554166,0.009585034,0.007536016,0.015635937,-0.0017763051,-0.020808248,0.061344013,-0.03843032,-0.009879707,0.0014917521,-0.025050001,0.044298965,-0.010329559,-0.028862966,-0.038450845,0.036882665,0.020592788,-0.015884763,0.010806988,0.04915735,0.028120259,0.019475535,-0.0059752744,0.039894152,0.0022671313,0.032219537,-0.043231733,0.012975207,-0.04911403,-0.022095209,0.0026278603,-0.055894125,0.037380032,0.019009136,-0.051126108,0.0510622,-0.014341539,0.035072625,-0.01482236,0.03400875,0.019258779,0.015065452,0.045468807,0.022059765,-0.03491658,0.07304335,0.023551954,-0.03208501,0.037223104,0.07017503,0.01305301,0.036545087,-0.0021518797,0.024474133,0.05786056,-0.0048205312,0.011781231,-0.03095706,0.022949813,0.032237794,0.013744224,-0.0017034817,0.010133846,-0.029156141,-0.02593209,-0.006649379,0.0012308635,-0.035533775,0.026359087,-0.01017615,-0.038454402,-0.019313728,0.041093145,0.02715243,0.049544483,-0.015289219,0.009201751,0.039164737,0.020526078,0.008540133,0.06494749,-0.018016797,-0.026764112,0.009798643,0.011824805,0.0031314963,-0.0030131016,-0.015566813,-0.022630885,-0.052473154,-0.028107442,-0.0064482247,-0.039692275,-0.04325281,0.032931928,-0.023912644,-0.039937675,-0.026754482,-0.00025913885,-0.003731658,-0.036229294,0.03244947,0.104216516,-0.014473275,0.0026693377,-0.019989967,0.023417909,-0.0011058912,-0.038062394,-0.0067808717,0.030653352,-0.014497861,0.038984135,-0.020444673,-0.026428204,-0.009309575,0.054034486,0.096711785,0.09566678,-0.06543396,0.0106480345,0.007184116,0.022483211,0.00477312,-0.016240247,0.007677776,0.038465362,-0.0033644056,0.019000636,-0.020167619,-0.07363722,0.038383663,-0.053522635,0.06618294,0.06439771,0.013263697,-0.0026939076,0.010186706,0.024335058,0.05897857,-0.012908629,-0.017556768,0.002183159,-0.07439485,-0.029856974,-0.0051895957,-0.017698806,0.049634047,0.016788514,0.015852937,0.0136002675,-0.005178753,0.019789837,0.02190513,0.0145213995,0.012881422,0.013751131,-0.0338396,0.021598134,0.041908517,0.004297058,-0.015872864,0.06764773,-0.056705404,0.057388414,-0.007818061,0.00060879026,-0.026549254,-0.07049166,0.016226387,0.03784013,-0.03199018,0.022270532,-0.04905038,0.033964667,-0.04513539,-0.002135482,0.011094905,0.03166668,0.034505133,-0.033423215,-0.022776527,-0.006172438,-0.017097916,0.0018459359,0.0032541822,-0.004348844,0.020744098,-0.013354248,0.077159405,-0.017756604,0.055641945,-0.04230345,0.023144592,0.024731465,0.0044196458,-0.011423871,0.024235604,-0.0008352024,0.039601922,-0.036817685,0.0018033168,0.029080017,0.03184775,-0.055058476,0.016887002,0.022137778,0.007156522,-0.033159103,0.00659637,0.05819255,-0.007926327,0.014731037,-0.035222787,-0.021739649,0.02748302,0.003982766,-0.0056712013,-0.022798698,0.046202615,-0.054757036,-0.012027395,-0.0055936933,0.022753952,0.012279942,0.035789512,0.07076836,0.005776838,-0.04241114,0.044493284,0.026562916,-0.029477255,0.010989113,-0.06974423,-0.027281506,0.048611768,0.025297351,0.01991887,-0.06762919,0.08739444,-0.050396144,0.02115503,0.09026858,-0.036063816,0.041045267,0.0044218856,0.02233274,-0.021563986,0.0012218795,0.027530288,-0.016668342,-0.03702863,0.031512167,0.0013936425,-0.013173914,-0.004576393,-0.01232226,0.009158502,-0.0013097354,-0.011646538,-0.05132206,0.0034358716,-0.023201225,0.042425834,0.00037344423,0.0082163885,-0.0010381896,-0.05043735,-0.027187498,-0.010567039,-0.029781424,-0.015251669,0.005903768,0.01515293,-0.037921924,0.04986631,0.017565005,0.021075249,-0.02681484,0.03628317,0.04230032,-0.0074441973,-0.050408512,-0.02339189,0.03326922,0.03668854,0.012243698,0.02035355,-0.010313936,0.018523717,-0.056157216,-0.011865377,-0.0023887223,0.04146284,0.041075062,0.020617388,0.021351617,-0.03552051,-0.0022586081,0.025269095,0.0057572722,-0.037709035,-0.04012833,-0.02589223,0.015035937,0.05369974,-0.055436324,0.014830561,-0.014088948,0.006853178,-0.0068226103,0.053722866,-0.0069088573,-0.006502176,-0.024327539,0.0040004714,-0.0059723835,0.022010311,-0.012842518,-0.0134607395,0.029131563,0.046568386,0.0074575664,0.0179315,0.019337546,0.017438645,-0.0036952775,0.0023773238,0.028952762,0.034753717,0.061830882,-0.036855996,0.04095769,-0.07595779,-0.007053314,0.0056781606,-0.06763797,-0.050653,-0.0037064846,-0.0180792,0.023068864,0.03635064,-0.025749853,-0.006080448,0.028419744,-0.057259973,0.08225074,-0.062216125,0.011695855,0.012125759,-0.028652558,0.07422158,0.022980299,0.005147052,-0.013098937,-0.0024266678,0.0066187517,0.0051477593,0.092056125,0.01143025,-0.005901734,0.064868785,-0.025971206,0.0020524769,-0.033222266,-0.039485656,-0.02433176,-0.057990216,-0.00657787,0.013537757,-0.042476438,0.038077075,-0.030421782,-0.02791114,-0.0338544,0.055243924,-0.05699956,0.034018353,0.053563245,0.016635962,0.039013844,0.006562657,0.054229714,0.019028112,0.0043815807,0.018036492,0.0056055253,0.0036437404,-0.009926899,0.014588017,0.014530245,0.006281864,-0.03371236,-0.07027398,0.015413083,0.007637134,-0.026089441,0.00038902124,0.0230591,0.035810485,0.037284017,-0.022186184,-0.06607593,-0.026718965,-0.04896403,-0.02990164,0.013758173,0.006434584,-0.019320315,0.054540597,-0.07828585,0.064086765,-0.11319452,-0.033551846,-0.039304227,0.02325944,0.0037452509,-0.0140366955,-0.0025549005,-0.041996807,-0.0018737366,-0.029226111,0.022091143,0.060849093,-0.03749957,-0.0060440367,-0.005857292,-0.0067001004,0.009591388,-0.0077828653,0.010018484,0.023064412,0.008646129,-0.03306621,0.105849475,-0.0028566064,0.016980264,0.05214148,-0.0023799178,-0.057266492,-0.022886183,0.07190554,0.006978682,0.007998435,0.011774003,0.006204866,0.024622224,-0.036051538,0.00071715406,-0.06887482,-0.03289882,-0.011239091,0.0017888978,0.037907016,-0.01800517,-0.04368799,-0.017804243,0.0068952907,-0.06467657,0.031933226,0.049797967,0.07751144,0.057373457,0.046872403,-0.050872028,-0.053824008,-0.032876156,-0.02240974,0.008257647,-0.04154497,0.04754373,0.02356508,-0.00838361,-0.03394564,0.03397862,-0.0048439465,-0.0155906025,0.015128406,-0.017093832,-0.028053507,-0.016099034,0.042821113,0.022581091,-0.012269382,-0.0069517973,0.08118062,0.016646625,-0.036134727,-0.0064123757,-0.011185484,-0.06306895,0.01488917,-0.021427222,0.025023464,-0.031796798,-0.003906729,-0.019814536,0.06397197,0.005113098,-0.041516628,-0.0432025,0.026480827,0.01076894,0.05140905,-0.015415049,-0.022219945,-0.11204889,0.01776828,-0.013608106,-0.05679253,0.086073585,0.007588728,-0.029816816,-0.048193663,0.009846225,-0.0051026335,0.008882124,-0.0046073324,0.01752533,0.028053645,0.0039467826,-0.02326371,-0.07023891,-0.0003320787,-0.025183713,-0.012517366,-0.0016471606,0.013292134,0.014896216,-0.009055152,-0.004675403,-0.036811963,0.017055081,0.0226372,-0.01364297,0.076062016,-0.028692605,0.04597795,-0.016315412,0.00052372797,0.09229461,-0.050635304,0.0087386845,-0.0080500385,0.013157188,-0.0103834495,0.032641415,-0.033199552,-0.046518993,-0.044133432,0.042970438,0.029749159,0.01279126,0.046742298,-0.015565112,-0.044382997,-0.032226723,-0.027883735,-0.014583921,0.054258388,0.042324126,0.043940056,-0.037472673,-0.0044998275,-0.03723994,0.04051951,0.01694474,-0.0045856666,0.027637394,0.013013613]	Keywords: permutation equivariant, convolution, recurrence networks, self-attention, positional information, transformer architecture\nKey Objects: permutation equivariant function, Convolution networks, Recurrence networks, self-attention modules, positional information\nRefers to Images: None\nHypothetical Questions:\n- What does it mean for a function to be 'permutation equivariant'?\n- Why is permutation equivariance a potential problem when modeling sequences of text?\n- What are some ways to inject positional information into Transformers to address this issue?\n---\nSummary:\nConvolution and Recurrence networks are not permutation equivariant, while self-attention modules and feed-forward layers in Transformers are. This can be problematic when modeling sequence data where order is important, necessitating the addition of positional information.\nOriginal Text:\n$$f ( \\pi x ) = \\pi f ( x ).$$  \nIt is easy to verify that Convolution and Recurrence networks are not permutation equivariant. However, both self-attention modules and position-wise feed-forward layers in Transformer are permutation equivariant, which could be a problem when it comes to modeling problems other than set-input problems where the structure of inputs is needed. For example, when modeling sequences of text, the ordering of words matters and it's thus crucial to properly encode the positions of words in Transformer architecture. Therefore, additional mechanisms are required to inject positional information into Transformers. A common design is to first represent positional information using vectors and then infuse the vectors to the model as an additional input.\nContextualized Text:\nIn Transformer architectures, while self-attention modules and feed-forward layers exhibit permutation equivariance, Convolution and Recurrence networks do not.  This presents a challenge when modeling sequential data where the order of elements is significant.  Therefore, mechanisms to encode positional information are required to inject positional information into Transformers.	{"tags": ["architecture", "NLP", "transformer", "permutation"], "doc_id": "30477ff3-f931-44b2-9bad-d122624fa3ac", "summary": "Convolution and Recurrence networks are not permutation equivariant, while self-attention modules and feed-forward layers in Transformers are. This can be problematic when modeling sequence data where order is important, necessitating the addition of positional information.", "doc_type": "text", "entities": ["Transformer", "Convolution", "Recurrence Networks"], "keywords": ["permutation equivariant", "convolution", "recurrence networks", "self-attention", "positional information", "transformer architecture"], "key_objects": ["permutation equivariant function", "Convolution networks", "Recurrence networks", "self-attention modules", "positional information"], "contextual_text": "In Transformer architectures, while self-attention modules and feed-forward layers exhibit permutation equivariance, Convolution and Recurrence networks do not.  This presents a challenge when modeling sequential data where the order of elements is significant.  Therefore, mechanisms to encode positional information are required to inject positional information into Transformers.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["What does it mean for a function to be 'permutation equivariant'?", "Why is permutation equivariance a potential problem when modeling sequences of text?", "What are some ways to inject positional information into Transformers to address this issue?"]}
e776da59-728f-440c-b5f4-ee313733a9a9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.032978043,-0.025235321,-0.016417535,0.06795451,-0.006182438,0.066071585,-0.02318022,0.08402638,0.009290661,-0.052742448,-0.052969746,0.016229134,0.027266882,0.032496165,-0.043958567,0.050491452,0.038055267,0.058329824,0.018148713,-0.0020769418,0.012598444,0.0070281466,0.03735962,0.016372358,0.024473337,0.025959961,0.026154943,-0.012162098,-0.016496409,0.03887333,0.019859172,-0.032930765,0.0057371026,0.021432443,-0.00946312,0.021931907,-0.0034574145,-0.03806478,-0.02681614,0.025757687,-0.04151703,0.078048766,-0.085367456,0.049915377,0.023210527,-0.04272974,-0.102315016,-0.037251484,0.019753201,0.009797109,0.007219752,-0.007580461,-0.059993144,0.0026003884,9.0346985e-05,-0.013624414,0.00035068966,-0.040300667,0.026724074,-0.062909305,0.012334149,0.0014586034,-0.021150911,0.016291834,0.007772534,-0.036910594,0.025998805,0.034471408,0.02127218,0.13796757,-0.027104858,0.031922456,0.0019661451,-0.08789519,0.101286665,0.057680532,-0.03804178,-0.041560113,-0.043650687,-0.021070667,-0.0063868575,0.05044338,-0.004692841,-0.03799057,0.07156967,-0.004844867,-0.035405673,-0.003883121,0.037948735,-0.07411742,-0.014666701,-0.012353419,0.03627731,0.03327735,-0.013931193,-0.022436311,0.016543131,-0.0010390028,0.028537927,-0.033935625,0.033884283,-0.018298145,0.053881984,0.090647735,0.005481694,-0.03159402,-0.012955397,-0.030057596,0.008661436,0.04089315,-0.03471044,0.046525806,-0.08931185,0.014367957,-0.04658331,0.007411383,-0.025293138,-0.016796555,-0.026994878,0.0063056336,0.02206109,-0.049303185,0.013085526,-9.023823e-05,0.03687589,0.05912181,-0.0019283233,0.012956043,-0.031642597,0.030552605,0.008103964,-0.009761554,-0.035180707,0.009406068,-0.053891562,0.04294881,0.053344663,-0.032250438,0.004616765,0.0010141323,0.046571814,-0.06067521,-0.012682737,0.024788909,-0.039151654,0.03146888,-0.040403698,0.026485613,-0.054176483,0.073462315,-0.0038995817,0.02554129,0.07963473,-0.0220655,-0.04441036,0.022676902,0.04893012,-0.009600984,0.035693083,-0.0153867435,-0.047111347,-0.03594049,-0.040638093,0.01706585,-0.022587357,0.066545375,0.01844776,0.001765157,0.024579665,0.024699895,-0.044904027,-0.0062448774,0.016062664,-0.011522158,-0.01942036,0.012860094,-0.012018363,0.004784968,0.014182791,-0.029159594,-0.0013704258,-0.01638815,0.030014891,0.046243217,-0.020282267,-0.00014752173,0.04741621,0.04014245,-0.010267184,0.027670395,0.036417488,-0.0071753757,-0.018700052,-0.012150312,-0.0039759846,0.032583743,0.05304132,-0.0078123715,-0.09023349,0.012615504,-0.012279387,0.019634785,-0.004150943,-0.025657432,0.04425664,-0.026044423,0.026721938,-0.029138526,-0.00024235457,0.032674484,0.010869194,-0.036808807,-0.049616016,-0.007375805,0.024887012,-0.0076517365,-0.003918617,0.03653184,0.002927284,-0.06499474,-0.022163996,0.040220685,-0.019824296,0.041508112,-0.029579282,0.015261994,-0.058327712,-0.027341992,0.004672759,-0.05505971,0.012459451,0.012006599,-0.03713261,0.0038561893,-0.03504407,0.026650188,-0.0058474736,-0.021888299,-0.0255548,0.04760972,0.05178166,-0.00069921097,-0.07611274,0.03896513,0.0058020474,0.0045849984,0.025527783,0.033620317,-0.006056009,0.06591555,-0.016254276,0.06413792,0.06465024,0.009280821,0.013157563,-0.040039193,0.016701922,0.009383372,0.011500379,-0.009888101,-0.0132197905,-0.010773078,-0.035726197,-0.0020727068,-0.02416087,-0.043694235,0.03443429,0.0148864975,-0.0148480525,-0.027644068,0.06276843,0.061158895,0.0399845,0.003228889,0.0047383965,0.03409209,-0.0010791821,-0.0071568117,0.038817428,-0.061479762,-0.043594006,-0.011900204,0.012266508,0.014501604,0.005927216,0.013253122,0.010664229,-0.015583614,-0.050385054,-0.017088126,-0.00758404,-0.05613297,0.021397687,0.019212374,-0.031212278,0.01577233,-0.00961357,-0.048956323,0.01232598,0.058540057,0.0954821,-0.0057157367,0.011823129,0.009517911,0.0020592846,-0.008472633,-0.021044133,0.05146624,0.05823212,-0.061668485,-0.0013063411,-0.039970465,-0.03271354,-0.025292251,0.050942704,0.057179444,0.083975695,-0.028956203,0.0039348225,0.012863859,-0.0017417475,0.010205489,0.004062607,0.00032056984,0.072665386,-0.02596109,0.02588182,0.012565287,-0.055213388,0.021840774,-0.055842046,0.08031459,0.06246824,0.026504233,0.0076997695,-0.0066915937,0.012469999,0.03352694,-0.006160535,-0.02036706,-0.04299942,-0.029573899,0.008396588,-0.011882877,0.010365981,0.03604925,0.02291589,0.0027849565,0.03930808,-0.03409381,0.017700225,0.015174806,0.024137532,-0.0013319757,0.006071553,0.0030440108,0.007548952,0.021102754,0.00016867869,-0.017692113,0.07573118,-0.05560719,0.028419182,-0.03609137,-0.007383135,-0.02379624,-0.055576313,-0.008649648,0.062018696,-0.033210307,0.0136389565,-0.038393732,0.023116998,-0.008597974,-0.010971231,0.004610635,0.07641118,0.056442313,-0.0040330435,-0.03170422,-0.01539758,0.017642178,-0.045147948,-0.051282637,0.010491909,0.017605197,0.004055361,0.11072159,0.009003435,0.033578433,-0.04959785,0.006036991,0.005602177,-0.0148867015,0.00543569,0.0084355455,0.045306396,0.04405405,-0.005151545,-0.011479878,0.019155212,0.0167681,-0.05457282,0.0032907773,0.016108489,0.03893104,-0.03694379,0.023617817,0.05957509,0.019452319,0.031913053,-0.05413334,-0.07227366,-0.0045099086,-0.015930668,0.00059794745,-0.030650033,0.06915186,-0.03982264,-0.02184652,-0.03148729,-0.006251956,0.041078143,0.07673642,0.0126297055,-0.014361543,-0.061624944,0.004658645,0.002544655,-0.038870733,0.042396545,-0.026752187,0.032704424,0.03365714,0.051814444,0.0059792893,-0.04326717,0.0741002,-0.041738596,0.020134151,0.082373105,-0.025780091,0.011251633,0.0031952413,0.006719204,0.0065516233,0.011357619,0.027573537,0.010685368,-0.018894939,0.016040355,0.009221522,0.0016679381,-0.020333853,0.014762342,0.013029984,0.028412823,-0.00046017443,-0.059011634,-0.020572552,-0.027480332,0.0068110917,-0.023891259,0.0028006106,-0.029438177,-0.033023316,-0.0216414,-0.0841461,-0.004859503,0.040130515,0.005824738,0.006267384,-0.07017098,0.073683605,0.0046497765,0.027945703,0.009272366,0.014902905,-0.012618316,0.017166005,-0.073523976,-0.020129083,0.0382114,0.042429842,-0.025454015,-0.0010216498,-0.031452373,-0.019688291,-0.06503783,0.0098781325,0.017134609,0.010411313,0.03573483,0.026114669,0.0012726143,-0.051364485,-0.012204654,-0.013426837,0.01386778,-0.047497623,-0.0059232805,-0.046435747,-0.0022723437,0.057422303,-0.028132213,-0.00832012,-0.003189857,0.027744858,-0.047645442,0.06453518,-0.01978213,0.026401464,-0.0009569113,-0.007380681,-0.020904113,0.037894845,-0.026040295,-0.001401293,0.03180553,0.01637049,0.006653784,-0.006505037,0.035568323,0.008899847,-0.007986106,0.060085095,0.032806065,-0.00047130097,0.02657996,-0.03118801,0.046099972,-0.034653734,-0.005416844,0.0038180405,-0.047415182,-0.04932711,-0.005629531,-0.07029638,0.024991857,-0.0037382194,-0.04636226,-0.050860893,0.016145563,-0.05735146,0.10173264,-0.09769722,0.009827904,0.01720657,0.0010658109,0.07962671,0.010447116,0.025738625,-0.041700393,-0.050265476,0.007105579,-0.02367158,0.0529438,0.008715059,-0.004952345,0.070954725,-0.0339676,-0.06493746,-0.019065417,-0.022321593,0.0068003354,-0.04298012,-0.026045935,-0.0064398358,-0.028470892,0.003567629,-0.026147762,-0.0048612906,-0.0092788,0.049928103,-0.030701416,0.07055653,-0.0014054343,0.03281441,0.010725821,0.010201871,0.056873076,-0.010161655,0.06388279,0.03911504,-0.011829594,0.015566373,-0.011827688,0.035722207,0.017515583,0.066367514,-0.010544467,-0.057007905,0.0066066766,0.013996584,0.03754234,-0.037487872,0.033974342,0.023715826,0.0022186695,-0.016122527,-0.086218424,-0.032302372,-0.041266665,-0.038950957,-0.02056491,0.018806782,-0.019787956,0.033796076,-0.058023073,0.08759366,-0.09312507,0.0014107211,-0.055387888,0.0640588,-0.017504388,-0.03295019,-0.023688875,-0.023930026,0.0066579664,-0.046882845,0.01608212,0.04259387,0.005518553,0.007982586,-0.007068247,-0.017300798,0.02401026,-0.025672687,0.01593868,-0.011231682,-0.01589352,-0.006593779,0.069525465,0.008394143,0.004181259,0.021050183,-0.007436424,-0.077548094,0.0047463104,0.099971294,0.054432288,-0.0069555556,0.031310122,-0.019909225,0.03407231,-0.043117378,-0.040841028,-0.010821397,-0.01908741,0.03186944,-0.045021378,0.05112082,-0.02201615,-0.059401378,-0.02493116,-0.0064433836,-0.06413182,0.044236362,0.043784425,0.058488965,0.051649813,0.07652311,-0.03914676,-0.05930591,-0.030803025,-0.020777412,0.013259261,-0.033879597,0.07872275,0.016101347,0.013475214,0.017147271,0.0005655366,-0.008736541,0.020586336,-0.020210339,-0.022621926,-0.0075115715,0.038587477,0.011722149,0.041539352,0.046233077,-0.0031324343,0.056327466,0.033857696,-0.032876115,-0.011141507,-0.020448389,0.028911335,0.00521318,0.026232759,-0.006093256,-0.006203641,0.01709962,0.033414774,-0.014343169,-0.009355779,-0.044979192,-0.061048824,0.04495068,0.039647296,0.055072885,0.022912761,-0.02494764,-0.07374475,0.037007295,-0.0028217738,0.0040816306,0.06596461,0.0027241285,-0.038317475,-0.011841376,-0.0015254999,-0.015298959,0.047584083,-0.004268738,0.012269299,0.037934866,-0.006610207,-0.042730726,-0.049597356,-0.015266108,-0.018919721,-0.038338613,-0.022399474,0.049650002,-0.023245739,0.009656991,0.010932929,-0.019482242,-0.03063396,-0.01705168,-0.031822227,0.018722707,-0.029478256,0.025201024,-0.0071921395,0.004802053,0.07990851,-0.02699311,-0.024013862,0.0026383281,0.061151233,0.017965145,0.0026495857,0.008300986,-0.062026825,-0.03627271,-0.0009784098,0.008855121,-0.0055993386,0.015370311,0.02274666,-0.036436703,-0.05241956,-0.044362687,0.0058295275,0.017463543,0.015211852,0.026761454,-0.03648445,0.015949639,-0.03301936,0.06544819,0.011963508,-0.024039296,0.01775162,0.024421839]	Keywords: positional encodings, sinusoidal functions, absolute position representations, token embeddings, Transformer\nKey Objects: Positional Encodings, Token Embeddings, Frequency\nRefers to Images: None\nHypothetical Questions:\n- Why were sinusoidal functions chosen for positional encoding?\n- What is the purpose of having different frequencies for different dimensions of the positional encoding?\n- How does adding positional encodings to token embeddings affect the model's understanding of sequence order?\n---\nSummary:\nIn the original Transformer model, absolute sinusoidal position encodings are used to represent positional information. These encodings are vectors where each element is a sinusoidal function of the position index, with frequencies defined for each dimension, and are added to token embeddings before being fed to the Transformer.\nOriginal Text:\n5.1.1 Absolute Position Representations. In vanilla Transformer [137], positional information is encoded as absolute sinusoidal position encodings. For each position index t , the encoding is a vector p$\\_{t}$ = PE ( t )  R $^{D$\\_{m}$}$, of which every element is a sinusoidal (sin/cos) function of the index with pre-defined frequency.  \n$$P E ( t ) _ { i } = \\begin{cases} \\sin ( \\omega _ { i } t ) & \\text {if i is even,} \\\\ \\cos ( \\omega _ { i } t ) & \\text {if i is odd,} \\end{cases}$$  \nwhere $\\_{i}$ is the hand-crafted frequency for each dimension. The position encoding of each position in the sequence is then added to the token embeddings and fed to Transformer.\nContextualized Text:\nThe original Transformer model encodes positional information using absolute sinusoidal position encodings.  For each position index, a vector encoding is generated using sinusoidal (sine/cosine) functions based on a pre-defined frequency. This vector, denoted as p, is then added to the token embeddings and fed to the Transformer.	{"tags": ["architecture", "NLP", "transformer", "positional encoding"], "doc_id": "e776da59-728f-440c-b5f4-ee313733a9a9", "summary": "In the original Transformer model, absolute sinusoidal position encodings are used to represent positional information. These encodings are vectors where each element is a sinusoidal function of the position index, with frequencies defined for each dimension, and are added to token embeddings before being fed to the Transformer.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["positional encodings", "sinusoidal functions", "absolute position representations", "token embeddings", "Transformer"], "key_objects": ["Positional Encodings", "Token Embeddings", "Frequency"], "contextual_text": "The original Transformer model encodes positional information using absolute sinusoidal position encodings.  For each position index, a vector encoding is generated using sinusoidal (sine/cosine) functions based on a pre-defined frequency. This vector, denoted as p, is then added to the token embeddings and fed to the Transformer.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["Why were sinusoidal functions chosen for positional encoding?", "What is the purpose of having different frequencies for different dimensions of the positional encoding?", "How does adding positional encodings to token embeddings affect the model's understanding of sequence order?"]}
6570a177-3b21-4481-90e4-ee048780dfa9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.012073851,-0.012625075,-0.0059766574,0.052223913,-0.0077484297,0.05285352,-0.0024852636,0.08684815,0.022272157,-0.043891426,-0.04916171,0.027569838,0.0008204246,-0.0020451546,-0.051880617,0.06054427,0.06369811,0.053314812,0.029383095,0.023606744,0.043822303,0.007867814,0.035227243,-0.0058957837,0.010954937,0.030784737,0.04246028,-0.017170941,-0.014407369,0.01735855,0.0105180945,-0.02069793,0.020589687,0.044219345,-0.014908292,0.0377758,-0.0058963904,-0.046140634,-0.01758385,0.0229957,-0.029397067,0.06310675,-0.104681276,0.016624877,0.0048266808,-0.061187673,-0.1290308,-0.028909968,0.016394624,-0.018321281,-0.02046194,-0.012080559,-0.056711122,-0.00897243,0.009151078,-0.006578651,-0.027146213,-0.018755702,0.016624982,-0.08325674,-0.0030363705,0.014467735,-0.02587293,-0.02330847,-0.004621182,-0.052611418,0.028543735,0.047050256,0.015212931,0.10877298,-0.027676228,0.00746884,0.019499307,-0.08260606,0.1048798,0.08017458,-0.049471296,-0.03870059,-0.04665482,-0.009896524,-0.021404535,0.0550011,-0.017557276,-0.042988062,0.07057954,-0.0033236418,-0.035269186,-0.050173957,0.030758316,-0.06212837,-0.023176802,0.004213649,0.024605118,0.025674652,-0.0046473993,-0.024589492,-0.020327399,-0.0060552103,0.055902563,-0.02008774,0.041264277,-0.025852326,0.039780788,0.08074879,0.010547858,-0.010476909,-0.004794186,-0.03196678,0.008378144,0.025546052,-0.01905858,0.012219157,-0.05111771,0.027160779,-0.034560144,-0.00080289226,-0.018148607,-0.012611479,-0.0070979996,0.0048732315,0.0145654585,-0.04761208,0.0036702151,0.0044512083,0.052795473,0.036496397,0.012461967,0.0063931905,-0.016330902,0.03842289,0.011576645,-0.021805704,-0.029305773,0.026597679,-0.022820452,0.035514887,0.06277179,-0.023749407,-0.0050807265,-0.018160984,0.027105981,-0.021638766,-0.006480708,0.03706489,-0.058961585,0.059655476,-0.040011257,0.029448697,-0.05186094,0.05454154,-0.011623488,0.0056260284,0.04911272,-0.0116117885,-0.05138312,0.00895507,0.0174905,-0.019162487,0.025452882,0.011761957,-0.036465857,-0.031670596,-0.043648496,0.032562688,-0.016239207,0.06461275,0.021348676,0.013186841,0.02789994,0.013646528,-0.037629593,-0.0074015562,-0.009501003,-0.00013635754,-0.035438705,0.019016707,0.003839048,0.03188171,0.0088704005,-0.010406223,-0.0014899734,-0.023432616,0.025231732,0.055414826,-0.01389167,0.0013457182,0.014073159,0.053784665,-0.013746825,0.021055885,0.031007763,-0.01853216,-0.021827659,-0.0025482632,0.0031981766,0.04929535,0.05185997,-0.020416776,-0.09807757,0.0074535217,-0.0043761246,-0.0039731646,0.014087037,-0.01774948,0.05421639,-0.004320907,0.03553153,-0.036599733,-0.015791995,0.049090363,0.0071837096,-0.061007153,-0.03539367,0.0053355596,0.037444912,-0.030404557,0.010019783,0.054427687,-0.00051972707,-0.051488236,-0.013112639,0.02264462,-0.018842498,0.019893523,-0.035512652,0.014983365,-0.043590587,-0.0061343396,0.021995129,-0.04243206,0.022874704,-0.02012215,-0.04100234,0.014038409,-0.013991461,0.04078085,-0.007070601,-0.036352053,-0.013639759,0.057319324,0.06591044,0.01320914,-0.074788354,0.054899797,-0.011391216,-0.0030114185,0.02892773,0.015997333,-0.0024490557,0.078054465,-0.0022138837,0.041683305,0.06414782,0.020839773,0.022037072,-0.06214112,0.013358624,0.004575139,-0.006296653,-0.004702143,-0.023412997,0.015551634,-0.032659095,-0.0085024545,-0.0021112459,-0.03940551,0.029306285,0.02412507,0.004776805,-0.048269525,0.056470457,0.049293827,0.018804176,-0.008361412,0.0004722859,0.038730886,0.0017507481,-0.021621753,0.051504686,-0.058566686,-0.031007042,-0.031107793,0.01817152,0.010620117,0.020599756,-0.01174102,0.0043937443,0.008201409,-0.083648756,-0.017643709,-0.02407495,-0.06961216,0.029106446,-0.0019296411,-0.063540086,0.037780825,-0.0055743037,-0.026996449,-0.015161768,0.04157744,0.09742819,-0.013908987,0.016450249,-0.015330464,-0.00046438686,0.012678118,-0.044269208,0.010821346,0.058247074,-0.042299666,-0.010612132,-0.035349276,-0.043150637,-0.014184379,0.025593206,0.062727645,0.054409135,-0.012501974,-0.030920863,0.033670615,0.0016229426,0.0027824948,0.012400902,0.01177226,0.060217492,-0.033141695,0.021181809,0.015764121,-0.071912006,0.03227187,-0.07524601,0.08468246,0.04239797,0.037085623,-0.005702784,-0.014294786,0.01742463,0.05627666,-0.0076876413,-0.016967185,-0.020191899,-0.038382053,0.0031793013,-0.0073124096,0.026996806,0.026381137,-0.014876922,0.009917287,0.038871918,-0.035818726,7.2159455e-05,0.009324647,0.026627913,0.012638491,-0.00760228,-0.006354486,0.0008719918,-0.0011931311,0.015739294,-0.023100268,0.07663774,-0.05150048,0.059424978,-0.063768424,-0.0040851533,-0.037940726,-0.0554255,-0.01108848,0.047040775,-0.042262122,-0.0029294752,-0.026142087,0.04726031,-0.036212943,0.0037196777,-0.008137774,0.0709457,0.03930622,-0.026552018,-0.028145973,-0.0020098952,0.020413354,-0.054736644,-0.06709772,0.018828953,0.0032647408,-0.009444059,0.08836018,-0.0050774654,0.046953153,-0.036260094,-0.00484076,0.00080440147,-0.013661176,-0.004620675,-0.0047558765,0.01378949,0.038362257,-0.0109199425,-0.019185252,0.0047716033,0.013561902,-0.047611117,0.02290336,0.01975936,0.054482494,-0.058193754,0.006506371,0.046613425,0.020778518,0.036549255,-0.04656112,-0.05551152,0.0036663401,-0.036723677,0.01470665,-0.041901045,0.0765402,-0.027081355,-0.021972083,-0.044188295,0.014244844,0.021558534,0.06491136,0.038045086,0.00828682,-0.0411168,-0.00037663523,-0.012649476,-0.03867957,0.049575742,-0.0417813,0.0025084692,0.041289844,0.046880472,0.026773067,-0.054471675,0.06310727,-0.049538527,0.028874872,0.071006075,-0.0363871,0.018232994,0.020733654,-0.0056749387,0.0064608906,0.00858706,0.031513214,-0.013468757,-0.024660021,-0.00415503,0.018981732,-0.0059997425,-0.032268047,-0.0018673831,0.00044227846,0.03928113,0.0016415077,-0.04371707,-0.015836874,-0.027330678,0.013175432,0.0034727403,0.00521825,-0.009016254,-0.025515134,-0.024221526,-0.06250128,-0.022057083,0.032736585,-0.016674308,0.024240013,-0.07213251,0.04977016,-0.003953583,0.019822037,0.011926653,0.025506195,-0.020383243,-0.0088873245,-0.053244088,-0.04310007,0.0150720095,0.039425354,-0.040551122,0.003094331,-0.032226708,-0.029146636,-0.07040435,0.0071069715,0.030419428,0.025866285,0.040756915,0.041525226,0.031378508,-0.044405505,-0.018248914,0.018828843,0.00082163437,-0.045937844,-0.01580042,-0.043919977,0.039467912,0.06187746,-0.025806386,-0.0042713904,-0.016929077,0.01225317,-0.050153367,0.07153084,-0.0008816718,0.008785374,-0.0011596864,-0.00093868346,-0.03268439,0.032161333,-0.04365758,-0.016482754,0.005553324,0.0314472,-0.005011132,0.024645325,0.021864176,-0.003971564,0.005705659,0.04730494,0.030179167,0.0028545363,0.030215811,-0.033166084,0.039941,-0.03581834,-0.02499809,0.012334852,-0.041672237,-0.029423833,-0.006293726,-0.060048144,0.034570966,0.03519842,-0.020610722,-0.006218505,0.048820775,-0.043720443,0.09581239,-0.08820044,0.022349464,0.028735954,0.022265065,0.06837438,0.020842748,0.01911399,-0.05199675,-0.06747858,-0.023348495,0.0041625793,0.045214273,0.02692044,-0.024564857,0.06824512,-0.024253484,-0.06395268,-0.01841659,-0.030782811,0.0075789047,-0.039283693,-0.025885995,0.0066777472,-0.0028583058,0.026573628,-0.046893682,0.0010222212,-0.025805777,0.05558394,-0.0411518,0.06714224,0.022065595,0.038998976,0.03022186,0.01999051,0.036274124,0.014190873,0.03649414,0.035349127,-0.015111584,0.029428143,0.018352523,0.0064588026,0.0071094595,0.074222766,-0.03452363,-0.022882774,0.018694712,0.009187489,0.029844703,-0.019032834,-0.0011848153,0.038035933,0.019160366,0.005235921,-0.09077143,-0.051810145,-0.03612568,-0.051914573,0.005658682,0.015208047,0.005593321,0.031091332,-0.0429496,0.07701482,-0.076641016,-0.0073008025,-0.053719863,0.08665023,-0.005778864,-0.038023952,-0.017377125,-0.02677379,0.00935771,-0.056285493,0.028374687,0.071456194,0.008601827,0.020462556,0.012471452,0.0014220811,0.017839393,-0.017540311,0.0023510375,-0.0134520335,-0.017930353,-0.023028819,0.06278193,0.016821323,0.010667836,0.04005952,-0.0019088801,-0.071138166,0.0015711387,0.0750619,0.04976813,0.05353707,0.06618293,-0.022255203,0.03738064,-0.043685116,-0.038223013,-0.0033672936,-0.0012115989,-0.0015620143,-0.010803024,0.052329656,-0.01856778,-0.06396747,-0.022459535,-0.016877417,-0.06823336,0.01256847,0.043892536,0.06185566,0.062232,0.06641325,-0.021378882,-0.07824766,-0.0048864274,-0.02794369,0.021721806,-0.049009133,0.063110135,0.020680366,0.0044059046,0.02158782,-0.026147999,0.013191454,0.025440155,-0.030161472,-0.0102689,-0.021141937,0.0142527195,-0.026386913,0.022643343,0.009776254,-0.0054421043,0.07567042,0.054128956,-0.058391612,-0.021638466,-0.047200218,0.010085413,-0.007279948,0.0045251455,0.012016722,0.011398144,0.008708123,0.015874457,0.0043303403,0.0043800077,-0.037685033,-0.049057297,0.051627867,0.01772762,0.051372495,0.001909414,-0.012066467,-0.0885601,0.037251182,-0.00491491,0.0004910713,0.049039554,0.0030456316,-0.023254529,-0.005637193,0.008757669,-0.020082776,0.036889143,-0.008478772,-0.0014064084,0.058583397,0.025352336,-0.030154549,-0.04859325,-0.010352488,-0.022123141,-0.04295078,-0.010151156,0.058311626,-0.053439505,-0.006887823,0.03148434,0.0037611185,-0.024738895,-0.018386811,0.005796708,0.058638643,-0.042353924,0.032625332,0.017156161,-0.012636887,0.09427154,-0.0048372475,0.007498008,0.0012741241,0.043168757,0.0023632625,-0.018816337,-0.005915033,-0.06562635,-0.060812864,0.023123687,0.019986665,0.0028475232,0.026539452,0.012721271,-0.040972736,-0.063851975,-0.04597927,0.010935839,0.010331633,0.004812395,0.0063509406,-0.013038241,0.034972582,-0.013362491,0.04409521,0.033241622,-0.024845727,0.03905154,0.021350922]	Keywords: position encodings, sinusoidal functions, learned embeddings, sequence length, inductive\nKey Objects: position encodings, absolute position representations, token embeddings\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of adding position encodings to token embeddings?\n- Why are learned positional embeddings limited in sequence length?\n- How do sinusoidal position encodings provide information about sequence order?\n---\nSummary:\nVanilla Transformers utilize absolute sinusoidal position encodings, which are vectors whose elements are sinusoidal functions of the position index. Alternatively, learned positional embeddings can be used, but these are limited by the maximum sequence length seen during training.\nOriginal Text:\nwhere $\\_{i}$ is the hand-crafted frequency for each dimension. The position encoding of each position in the sequence is then added to the token embeddings and fed to Transformer.  \nAnother way of representing absolute positions is to learn a set of positional embeddings for each position [28, 37]. Compared to hand-crafted position representation, learned embeddings are more flexible in that position representation can adapt to tasks through back-propagation. But the number of embeddings is limited up to a maximum sequence length determined before training, which makes this approach no longer inductive , i.e., not able to handle sequences longer than sequences seen in the training time [20, 85].\nContextualized Text:\nIn vanilla Transformers, absolute positional information is encoded. A common method is to use absolute sinusoidal position encodings, which are vectors constructed from sinusoidal (sin/cos) functions of the position index. These encodings are added to the token embeddings before being fed to the Transformer.	{"tags": ["architecture", "NLP", "transformer", "positional encoding"], "doc_id": "6570a177-3b21-4481-90e4-ee048780dfa9", "summary": "Vanilla Transformers utilize absolute sinusoidal position encodings, which are vectors whose elements are sinusoidal functions of the position index. Alternatively, learned positional embeddings can be used, but these are limited by the maximum sequence length seen during training.", "doc_type": "text", "entities": ["Transformer", "Vanilla Transformer"], "keywords": ["position encodings", "sinusoidal functions", "learned embeddings", "sequence length", "inductive"], "key_objects": ["position encodings", "absolute position representations", "token embeddings"], "contextual_text": "In vanilla Transformers, absolute positional information is encoded. A common method is to use absolute sinusoidal position encodings, which are vectors constructed from sinusoidal (sin/cos) functions of the position index. These encodings are added to the token embeddings before being fed to the Transformer.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["What is the purpose of adding position encodings to token embeddings?", "Why are learned positional embeddings limited in sequence length?", "How do sinusoidal position encodings provide information about sequence order?"]}
28a97303-e5d4-4d9a-ad0c-4d2920b029de	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.029140165,0.014549849,0.053297758,0.033070914,0.04084278,0.06583631,0.005162525,0.047291033,0.043369602,-0.030501364,-0.029181493,0.01967698,0.003242642,0.000107333006,-0.0011306246,0.007374143,0.0037453356,0.08359083,0.01877057,-0.03530049,0.052707314,-0.007170385,-0.01602217,-0.040912062,0.02855736,0.0076265107,0.031066533,-0.046364445,-0.0068111564,0.01654402,0.03196053,0.0024345096,0.007590752,0.035034027,0.010747376,0.027448589,0.04932399,-0.034657713,-0.029354533,0.012401278,-0.04081269,0.081149235,-0.06609871,0.0117151765,-0.013894046,-0.022421995,-0.085711926,-0.05014351,0.03186117,-0.00242862,-0.04424669,-0.01269654,-0.0008642041,-0.015206567,0.018815756,-0.027975772,-0.023984317,-0.015166703,0.030098889,-0.07121754,-0.025298137,0.014643604,-0.055584244,0.0052862144,0.023649529,-0.06820774,-0.02337411,-0.016925436,0.01317043,0.16216592,-0.023845565,0.030657187,0.010599452,-0.048374012,0.09360395,0.022602051,-0.029630667,-0.025337778,-0.035939995,-0.026948355,-0.01189066,0.047688033,-0.049742617,-0.059324384,0.09895235,0.029046683,-0.040376026,0.007196019,0.056101717,-0.018868888,-0.025127318,0.0048549753,0.027113039,0.061819866,-0.02926379,-0.07410547,0.021513099,-0.025830586,0.033914026,-0.036446027,-0.015510903,-0.02168979,0.10739952,0.0687145,0.03094016,-0.026643557,-0.006515357,-0.068165764,0.013840491,0.04374373,-0.02940891,0.03289042,-0.0008094668,0.0059912386,-0.014435747,0.014775359,-0.056911245,0.027291957,0.032806557,0.00087392307,0.00069055,-0.04197803,0.0074211736,0.022823868,0.06594433,0.0611516,0.021187924,0.022603385,-0.011446944,-0.0032440624,0.0117224585,-0.023325393,-0.0036079746,-0.015150938,-0.007323312,-0.025186555,0.053786915,0.013605159,0.020000169,0.058738373,0.030812675,-0.0706543,-0.02655128,0.051385906,0.0031358397,0.03229494,-0.02522229,0.021301247,-0.015455534,0.061703168,0.034789093,0.020487199,0.025867118,0.025171306,-0.0039299997,-0.017335378,-0.01082907,-0.06254138,-0.01867609,-0.02134351,-0.026571218,-0.04502645,-0.04275691,0.06417737,0.012672992,0.09534768,0.003726267,0.013767616,0.051225554,-0.00012836908,-0.044213526,-0.015346619,0.030569606,-0.05572571,0.008520886,0.013092227,-0.014500497,-0.02456077,-0.022287445,-0.0070826784,0.010446871,0.016025297,0.008915521,0.027326742,-0.047313347,0.030372659,0.022835968,0.08861308,-0.026691677,-0.014205645,0.05940111,0.0065534753,-0.024447702,0.011860622,0.047998805,0.015036445,0.05897315,-0.06430004,-0.08070154,0.010570869,0.0037103614,-0.015604224,0.010940401,-0.01798169,0.020786788,-0.013402987,0.026730187,-0.037263032,-0.041659795,0.02389495,0.008164138,-0.04608881,-0.025846489,0.07686395,0.039212715,-0.029416898,0.011869448,0.00033036142,0.015747216,-0.03368262,-0.07273551,0.007532698,-0.011709536,-0.006353297,-0.050954662,0.0075367023,-0.009264364,0.026811607,0.017649317,-0.041104965,0.028336417,0.032001887,-0.040156554,0.034099728,-0.012197956,0.033321578,-0.013148638,0.01643426,0.013269531,0.03315114,0.042635955,0.01731598,-0.03816118,0.028228706,0.0005303098,0.0052028964,0.017481238,0.049225003,0.01599103,0.021433905,0.027255999,0.054593913,0.053798918,-0.021119058,0.019785836,-0.05473483,-0.008687385,0.040398877,0.038286686,0.0021617033,-0.016733222,0.013873637,0.0077155298,0.019594904,0.030599812,0.0120546315,0.02704123,0.022014186,0.025754757,-0.020135706,0.03444422,0.041108113,-0.01740024,-0.019729966,0.012929254,0.025292903,-0.0073901797,-0.0052752784,0.04504824,-0.030978695,-0.015376217,0.00059867935,0.052271597,0.00231926,0.026748478,-0.02513285,0.008451493,0.008173062,-0.06050173,-0.006225143,-0.009960683,-0.051074766,0.031343088,-0.019024532,0.025299368,0.0450179,-0.030191585,-0.049810544,-0.027412813,0.039049383,0.06929101,-0.041817546,0.0123194335,0.040439777,0.033341635,0.027687948,-0.030748887,0.009461902,0.016132824,0.011261122,0.0125428485,0.0041016173,-0.023858296,0.0025650107,0.011119079,0.0791947,0.0631566,-0.03794822,-0.0021181758,0.0035192815,0.028540786,-0.032305405,-0.00788726,0.0363993,0.031263653,-0.05495474,0.023202522,-0.0358658,-0.085632116,0.026062405,-0.098469004,0.067093454,0.058958784,-0.010645944,0.030217322,-0.0010180061,0.024895404,0.059267752,-0.021572737,0.040340167,0.00917754,-0.06380702,-0.039093696,-0.028570587,0.017082615,-0.011910786,0.02068258,0.017117854,0.036748394,-0.034476053,0.0049746856,0.055738974,0.031895697,0.0090809865,0.011977487,-0.02456722,-0.00052623503,-0.0027859483,0.00782293,-0.004681653,0.027553882,-0.01857868,0.042030152,0.011373476,-0.017913193,-0.0062309075,-0.06321747,-0.0046691108,0.060902882,-0.014277201,-0.024158617,-0.03673268,0.037284233,-0.0070784595,-0.01950061,0.0079353,0.044133008,0.01307083,-0.04052435,-0.0056063,-0.02375534,0.042627122,-0.05328491,-0.034727994,-0.015810931,0.0319941,0.009831741,0.019703478,-0.010814007,0.023160083,-0.057849508,-0.0044138827,0.042834118,-0.016792214,0.0061721522,0.01773356,0.023400525,0.072362624,-0.041475285,-0.052169785,0.028177064,0.020906765,-0.048389748,0.052971095,0.038769446,0.027753241,-0.049467467,0.010284606,0.05545402,0.074057035,0.009789919,-0.02967451,-0.050181672,0.05126288,-0.01950382,0.021170631,-0.026656732,0.03915343,-0.051007032,0.04145725,-0.03386962,0.052210912,0.04518498,0.061304502,0.041206606,-0.028556304,0.0014239643,0.016859746,-0.034273203,-0.02947634,-0.005586141,-0.09541679,-0.022503475,0.06834087,0.034990862,0.04163544,-0.0794862,0.057993136,-0.07516586,0.04778,0.05809674,0.0020787062,0.0045415373,-0.013098256,0.0015421519,-0.025668584,0.019143136,0.014394339,-0.020190319,-0.019717583,0.00011808502,-0.0053701284,0.03192723,0.024432035,0.041375417,-0.019779142,-0.023788955,0.0015734731,-0.032078095,-0.014548436,-0.00975186,0.04093935,0.009362976,0.017896414,0.031659316,-0.0041077267,-0.019956086,-0.009624701,-0.0035895773,0.011091201,0.012143746,0.006401465,-0.03235268,0.034279138,-0.021760968,-0.027352367,0.003665306,0.025453512,0.022680223,0.031364612,-0.06957609,-0.028870894,0.01182406,0.06471549,-0.028742677,-0.012881941,-0.011885933,-0.047606453,-0.068788916,0.02156607,-0.0033421526,0.025603298,0.011562186,0.04139648,-0.00043047275,-0.02864489,-0.034236696,0.013345377,0.019159678,-0.03045991,-0.04053279,-0.017318178,0.053007115,0.037883826,0.014528943,-0.020465465,0.02622429,0.021873185,-0.032931555,0.032939527,0.0051972447,0.013152202,-0.03856214,-0.010639993,-0.021105215,0.009883015,-0.013767099,-0.029393492,0.037943937,0.06852699,-0.023841996,-0.0060468493,0.020697473,-0.01910578,0.005942604,-0.014923898,0.026853161,0.053604264,0.0815941,-0.06062827,0.037542034,-0.0027065019,-0.04086523,0.0093934005,-0.044852357,-0.0123066865,0.04092601,-0.050127853,0.0092572495,0.001967726,0.01980904,-0.015750343,-0.010706925,0.010126112,0.059585486,-0.079977855,-0.006478188,-0.0015388329,-0.0054217963,0.07762577,-0.004060523,-0.0041462015,-0.03019326,-0.030627875,-0.043644782,0.042647436,0.04732006,0.016145717,0.011058953,0.073751435,0.001788756,0.018443393,-0.024344273,-0.053162284,-0.040764946,-0.018170495,-0.04009168,0.016278498,-0.019494543,0.04138506,0.01845507,0.0026014387,0.016580345,0.009668432,-0.027268479,0.07462087,0.034076024,-0.0010715085,0.030498382,-0.024680488,0.0027396877,0.0358356,-0.003670625,0.024557525,0.019825866,0.016884306,0.031442843,-0.045598496,0.007767109,0.018335212,-0.0047984053,-0.00788216,0.012317453,0.039549887,-0.01376111,-0.0002149961,0.008794833,0.022071548,-0.010527831,-0.017814921,-0.088895544,-0.028561886,-0.05133199,-0.05493023,0.009354561,-0.034631725,0.008364146,0.04216912,-0.04685636,0.07916354,-0.11262472,0.008250924,-0.06257835,0.013058717,0.015790282,0.004024449,-0.00491735,-0.01162856,-0.004671295,-0.07074902,0.06903126,0.05744918,-0.037932467,0.020525275,0.039054334,-0.011259846,-0.034378782,0.013207692,-0.02756531,-0.0028331953,-0.006754655,-0.0059777563,0.059310067,0.003947579,-0.00038641327,0.021437379,-0.0350534,-0.08001137,0.006685783,0.034733843,0.043515276,0.042388465,-0.027388474,0.030956885,0.06955256,-0.060979493,-0.005951454,-0.03332199,-0.011327058,-0.0020366341,-0.0029333488,0.06961258,0.0131056,-0.04913321,0.058409482,0.00022014265,-0.05679354,0.016827883,0.011202432,0.044199493,0.05592731,0.024626167,-0.028628403,-0.11709112,-0.034898072,-0.03573816,0.0046322565,-0.04793522,0.051631927,0.0006022459,0.005099809,-0.028194482,-0.019297397,-0.010505168,0.0005182767,0.03042457,0.0056486735,-0.03105021,-0.036180995,0.021143353,0.024939615,-0.018996736,0.0011551115,0.04741665,0.049467083,-0.05506449,0.0026699947,-0.0594145,-0.016743692,0.0095739085,-0.017065728,0.03570118,0.017294722,-0.016483795,0.0063122725,0.038725525,0.0012916145,-0.0030940515,-0.009510829,2.9343633e-05,0.022228325,0.040989675,-0.025837034,0.0017856533,-0.12572059,0.040463526,-0.0130015435,-0.05952876,0.058622997,0.033449244,-0.011969629,-0.019622236,0.01636012,0.00358325,-0.010581373,0.02537486,0.019985937,-0.0046240566,0.035966437,-0.026555056,-0.057605438,0.008437632,-0.016458774,-0.0036232185,-0.031460606,0.025463916,-0.0017372179,-0.013069303,0.014653648,0.021233996,0.0073520876,-0.009990227,0.0022281155,0.0173779,-0.048685692,0.020530732,0.0145667195,0.020233085,0.06061052,0.01229729,0.060237776,0.006916782,0.016335648,0.022717072,-0.030426273,-0.023792574,-0.027259808,-0.08422869,0.054593205,0.032686405,-0.020452285,0.045716755,-0.030519165,-0.007403703,-0.031163346,-0.0013664217,0.00015982709,0.06618464,0.037672296,0.0054405094,-0.042925306,0.024826126,-0.049422737,0.080190666,0.024737703,-0.037115775,0.021353656,0.0025843016]	Keywords: machine translation, natural language processing, computer vision, audio processing, flexible architecture\nKey Objects: Transformer architecture, machine translation\nRefers to Images: None\nHypothetical Questions:\n- Why was the Transformer architecture initially created?\n- What features of the Transformer make it adaptable to various tasks?\n- Besides NLP, what are some of the areas where Transformers have found application?\n---\nSummary:\nOriginally designed for machine translation, the Transformer architecture has gained widespread adoption across diverse fields like computer vision and audio processing due to its flexible design.\nOriginal Text:\n## 8 APPLICATIONS OF TRANSFORMER  \nTransformer was originally designed for machine translation but has been widely adopted in various fields besides NLP, including CV and audio processing, due to its flexible architecture.  \n- (1) Natural Language Processing. Transformer and its variants have been extensively explored and applied in NLP tasks, e.g., machine translation [35, 91, 104, 123, 137], language modeling [24, 103, 111, 122] and named entity recognition [80, 154]. Massive effort has been dedicated to pre-training Transformer models on large-scale text corpora, which we believe is one of the major reasons of Transformer's wide application in NLP.\nContextualized Text:\nThe Transformer architecture, initially created for machine translation, has demonstrated remarkable adaptability, leading to its adoption in fields beyond natural language processing (NLP), such as computer vision (CV) and audio processing. This versatility stems from the Transformer's flexible design.	{"tags": ["architecture", "NLP", "CV", "audio"], "doc_id": "28a97303-e5d4-4d9a-ad0c-4d2920b029de", "summary": "Originally designed for machine translation, the Transformer architecture has gained widespread adoption across diverse fields like computer vision and audio processing due to its flexible design.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["machine translation", "natural language processing", "computer vision", "audio processing", "flexible architecture"], "key_objects": ["Transformer architecture", "machine translation"], "contextual_text": "The Transformer architecture, initially created for machine translation, has demonstrated remarkable adaptability, leading to its adoption in fields beyond natural language processing (NLP), such as computer vision (CV) and audio processing. This versatility stems from the Transformer's flexible design.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "8 APPLICATIONS OF TRANSFORMER"}, "hypothetical_questions": ["Why was the Transformer architecture initially created?", "What features of the Transformer make it adaptable to various tasks?", "Besides NLP, what are some of the areas where Transformers have found application?"]}
2cf160dd-50ea-4653-a229-a1ed1b928f83	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02686062,0.009195321,-0.014840555,0.06461544,-0.03937714,0.04459653,-0.009789951,0.062624834,-0.010306111,-0.03460956,-0.013302318,0.014782575,-0.0077458676,-0.0048339176,-0.039526355,0.023020318,0.050868403,0.038014095,0.044263627,-0.0032730335,0.025072716,0.0016796545,-0.013168926,0.018100925,0.045106582,0.03894899,0.04311595,-0.03874368,0.009624503,0.012424233,0.022732124,-0.0028236492,-0.00034438653,-0.0045741335,0.007089524,0.022733763,0.044664804,-0.048126142,-0.00814166,-0.038046166,-0.03396258,0.06772967,-0.0810001,0.060116567,0.019178985,-0.049285665,-0.063785225,-0.029097648,-0.0058693625,-0.0125237135,0.019259471,0.021344883,-0.0516205,0.014624113,-0.020053435,0.016176706,0.051141504,-0.02399397,0.0239591,-0.057893544,-0.014535328,0.038245704,0.0006311425,0.0070189456,0.04701104,-0.037059654,0.013237522,0.007878007,0.02544747,0.122808084,-0.0138140535,0.025228528,-0.008527592,-0.076867424,0.08966062,0.08121119,-0.044916727,-0.0040488592,-0.023965072,0.047309432,-0.0039498257,0.05855663,0.025788194,-0.01619549,0.09032784,0.0054116165,-0.049201343,-0.023921773,0.043451734,-0.033357766,-0.01787596,-0.03292737,0.010792667,0.040871523,-0.052688815,-0.035039313,-0.036045562,-0.020483432,0.014302672,-0.04616252,0.017430717,0.0034704055,0.049158778,0.08948819,0.004440897,0.0028055497,0.027185151,-0.017134903,-0.013731535,0.0046671596,-0.03168994,0.068864375,-0.058621656,0.033165652,-0.07259562,-0.0047304197,-0.020777056,-0.022366697,0.0011770313,-0.009210165,0.059646063,-0.004283423,0.032070972,0.0016415749,0.07166109,0.06278414,0.028111234,0.012835317,-0.009036441,-0.0043106973,0.030028855,-0.020246614,-0.030561808,0.043111194,-0.021570029,0.0011634711,0.051506657,-0.020030385,-0.020436635,-0.03988521,0.00902999,-0.049497016,-0.010716477,0.008005959,-0.0169076,0.031395264,-0.05880521,0.023056164,-0.04754733,0.061711013,0.017467925,0.018628743,0.0487433,-0.014291254,-0.04505988,0.006092882,0.0079144,0.026597371,0.021682667,-0.013159487,-0.009385623,-0.02697524,-0.022842824,0.059351403,-0.0053016823,0.093553245,0.011015581,-0.006709176,-0.0017303623,0.017122094,-0.03767492,-0.0027293824,0.0521037,-0.013120942,0.004605682,0.0046880567,0.013444896,-0.0046644756,-0.01820271,-0.009428611,-0.015116063,0.004090032,0.012976465,0.04860577,-0.025177086,0.014049451,0.025966786,0.030440107,-0.026166216,0.009760682,0.0050417194,-0.0143306395,-0.024035642,-0.004037258,-0.020433681,0.060387887,0.04754454,-0.043161325,-0.05346545,-0.00082364277,0.012337429,0.021982111,0.03446239,-0.020024495,0.02491095,-0.026560832,0.003110734,-0.046336032,-0.032797627,0.049792886,-0.007717873,-0.03248212,-0.032074094,0.009873966,0.017418353,0.0073575433,-0.007504681,0.0131196175,0.018422434,-0.04810097,-0.030190581,0.016944287,-0.031043371,0.061919145,-0.0075068437,0.03290844,-0.02881812,-0.023875458,0.04682067,-0.02816414,0.01252246,-0.0015309482,-0.028895434,0.048910912,-0.0059518837,0.037510015,0.028794704,-0.0031896608,-0.027542952,0.058324717,0.058681324,-0.011148571,-0.07563623,0.05042795,0.0030640783,-0.0009026566,0.016579725,0.015630508,-0.00072027463,0.037020493,-0.04106441,0.061171863,0.051601414,0.021284144,0.019022392,-0.023093013,0.02047657,0.01154622,0.026888587,-0.017023807,0.031205682,0.0011388278,-0.019181626,0.010918797,0.0044335923,-0.043761246,0.019913096,-0.0043550786,0.011331396,-0.03403187,0.033849,0.0631935,0.021597818,-0.015000256,-0.060570445,0.05883598,0.0018416883,0.014760471,0.069152795,-0.065767325,-0.033895288,0.013559387,-0.006433289,-0.00214106,0.02172304,-0.004412156,0.027000021,0.027738659,-0.025486832,-0.0025868716,-0.008649144,-0.048557248,0.039046,0.021519573,-0.05118578,0.0303536,-0.010946189,-0.07140768,-0.019623829,0.03248011,0.11423223,0.008620881,0.018802758,-0.009644291,0.0073123937,-0.0022180805,-0.042204052,0.05909746,0.05554967,0.0020268362,0.0047759237,-0.05168393,-0.08335996,-0.019109113,0.031569246,0.018684756,0.06901793,-0.024701066,-0.0023390884,0.021207858,0.016434358,0.0077713793,-0.0024082162,0.042795062,0.054922793,-0.04707851,0.035410147,0.01842453,-0.07651722,0.051340625,-0.042140193,0.06616224,0.046246633,0.027286192,0.00037134075,0.002000847,-0.019585008,0.032591537,0.036871415,-0.026631324,-0.013367726,-0.045508232,0.058984343,-0.004387602,-0.01710702,0.037604377,-0.009208302,-0.00011130065,0.02936928,0.0039076363,-0.056749597,0.011720568,0.026915718,-0.021331621,0.0062638936,0.015643205,-0.019106766,-0.015333582,-0.00047059022,-0.050090037,0.07808209,-0.08196414,0.063915364,-0.0329111,-0.0018598555,-0.035517868,-0.045299813,-0.024620349,0.05661171,-0.014973905,0.024331313,-0.014175118,0.053812843,-0.057401426,0.013052857,0.0072139734,0.045877382,0.019208264,-0.014972255,-0.040659357,-0.041935943,0.03353954,-0.041583337,-0.031786643,0.032342285,0.024001202,0.022581905,0.12661988,-0.023384765,0.006281496,-0.040828016,-0.016233705,0.017930247,-0.016857833,-0.029552644,0.01018034,-0.0079838745,0.023804072,0.029815506,-0.028912487,0.009137811,0.004265569,-0.060739294,0.007310029,0.008213318,0.018179873,-0.078850076,0.0028926625,0.055559814,0.008110569,0.05259085,-0.046069052,-0.05448055,0.0039525456,-0.044656612,-0.011042081,-0.026421463,0.04645163,-0.016095024,0.0018621867,-0.055839293,0.03345391,0.02190625,0.06794811,0.026168369,-0.015429521,-0.056673206,-0.01157339,-0.014129821,-0.05315468,0.035215482,-0.0219003,0.04991159,0.01880042,0.051586494,0.0076228655,-0.016511424,0.043458734,-0.037986398,0.041286916,0.05946583,-0.035128824,0.043541905,0.034618,0.026112013,-0.049477626,-0.004107863,0.01680382,-0.0050794054,-0.045460816,0.020642256,0.009416346,-0.006421797,-0.009067156,0.040702377,0.0078012436,0.024309035,-0.01013704,-0.033551313,0.015441977,-0.01368882,0.0024035955,-0.024053318,0.045920487,-0.02449802,-0.03727966,-0.041779794,-0.060700435,-0.008015084,-0.0023000273,-0.006256,0.015369784,-0.059184622,0.061188,-0.0066524013,-0.006231594,0.03319633,0.010099108,-0.0038052069,0.0030609472,-0.063553624,-0.010204222,-0.0015595941,0.054607566,0.0041714283,-0.0059970343,-0.04282693,-0.03181137,-0.032400932,0.027547173,0.0182457,0.019629205,0.028137716,0.021970278,0.011218649,-0.025212822,0.027315028,-0.026522677,-0.0208491,-0.053197466,-0.032152668,-0.037049595,0.01813402,0.069507614,0.016524436,0.009307326,-0.0182416,0.013521494,-0.040041372,0.053903986,-0.0042000776,0.012185954,0.037339572,-0.018810095,-0.022117572,0.022253921,-0.028104989,0.0061676432,0.019760843,0.003845647,0.031685725,0.010722444,0.010057276,0.025998803,0.004197157,0.035001647,0.0368133,-0.024323538,0.036182716,-0.020310633,0.030743062,-0.0629592,-0.015235189,0.005110733,-0.056592014,-0.063862756,-0.025104892,-0.057172835,-0.008081114,-0.0004647009,0.0034370706,-0.021964056,-0.009661188,-0.038497265,0.059233427,-0.09127295,0.0008825381,0.015936757,0.0021990607,0.09051212,0.038771242,0.017833248,-0.028746182,-0.01836523,-0.020679003,-0.008104579,0.024066642,0.06583835,0.03169504,0.07672421,-0.02489865,-0.049850207,-0.030447837,-0.015710378,-0.013363076,-0.056401808,-0.04295993,-0.03297886,-0.01576373,0.015324702,-0.029488835,-0.012167498,-0.017189467,0.01781976,-0.062405854,0.0941094,0.010593579,0.042428564,-0.022379164,0.013690237,0.045815006,0.021734744,0.029654264,0.04456342,-0.023385858,-0.0007650292,0.0045510274,0.01593814,0.018805733,0.057765953,-0.03753808,-0.058103785,0.011664346,0.0078297835,0.029574124,-0.02616836,0.060487334,-0.004602391,0.013057977,-0.00072135095,-0.07759282,-0.012660812,-0.016386764,-0.03783456,0.021161623,0.010850611,-0.012112561,-0.012507955,-0.05650845,0.08135168,-0.052677244,-0.01362696,-0.03687106,0.065911844,-0.03258449,-0.00835238,-0.009460544,-0.037156343,-0.0067536235,-0.085588366,0.015378464,0.036352046,-0.041310146,0.026712148,-0.0039204033,-0.022954546,-0.0105901025,-0.0025818346,0.0076052253,-0.014571416,-0.04086724,-0.017300272,0.085513026,-0.029760415,0.017501386,-0.006320401,0.029015154,-0.07825437,-0.036792926,0.0885521,0.018248599,0.004977007,0.019040164,-0.03854606,0.019299187,-0.048554894,-0.041511044,-0.047546826,0.020736592,0.018236933,-0.03239003,0.040201336,-0.0075526806,-0.037320986,-0.049720127,0.0016619337,-0.05556494,0.042381372,0.017604858,0.049621493,0.056295387,0.06731334,-0.03691729,-0.09138396,-0.003966635,0.03810528,-0.020152122,-0.025483385,0.044115707,-0.012283838,-0.03348532,0.059136305,-0.00884328,0.011040169,0.0124410605,-0.043915603,-0.03762756,-0.025822751,0.010351554,-0.057744328,0.030935146,0.0324459,-0.01994463,0.090310164,0.056429412,-0.031888273,-0.0544797,-0.0187445,-0.016353823,-0.012906703,0.018985003,0.0028378593,0.03214558,0.026464578,-0.008313854,0.0072250804,0.025794925,-0.068053305,-0.07820216,0.040372495,0.03342387,0.04417471,0.025785688,-0.047524177,-0.063531205,0.026828926,-0.020896284,0.026793193,0.0048179305,-0.015331342,-0.033910204,0.023562754,-0.031857505,-0.0045138747,0.035433445,-0.020204438,-0.017011655,0.052769013,-0.0219651,-0.04178355,-0.051995628,-0.023996813,-0.03163077,-0.019500835,-0.0066793985,0.016375778,-0.011736506,0.013124884,-0.004807396,-0.00879042,-0.014245327,0.0025476264,-0.034370895,0.032666273,-0.043262202,0.020354124,-0.007989687,-0.0037295108,0.08759809,-0.038347773,-0.017358648,-0.009149896,0.0447889,0.021675933,-0.062432233,-0.007066743,-0.095918715,-0.040868472,-0.004161458,0.053712837,-0.013971991,0.026162008,0.030892642,-0.04930691,-0.034743607,-0.03795642,-0.015985081,0.046762973,0.020679649,-0.0035302716,-0.008711231,-0.0023931412,-0.011681246,0.07828958,0.021603383,-0.020987194,0.026467565,0.012036105]	Keywords: positional representations, sinusoidal encoding, Neural ODE, inductiveness, FLOATER\nKey Objects: positional representations, sinusoidal encoding, Neural ODE\nRefers to Images: None\nHypothetical Questions:\n- What are the advantages of learning sinusoidal frequencies compared to using fixed frequencies?\n- How does Neural ODE contribute to the efficiency and flexibility of positional representations?\n- Why is inductiveness a desirable property in positional encoding approaches?\n---\nSummary:\nWang et al. introduced a method to learn sinusoidal positional frequencies from data, enhancing flexibility compared to hand-crafted approaches. FLOATER utilized a continuous dynamical system with Neural ODE for end-to-end training, offering a parameter-efficient and inductive approach.\nOriginal Text:\nWang et al. [139] propose to use sinusoidal position representation, but with each frequency $\\_{i}$ (in Eq. (28)) learned from data. This approach retains inductiveness but is more flexible than hand-crafted sinusoidal encoding. FLOATER [85] frames positional representation as a continuous dynamical system and adopts Neural ODE to enable end-to-end training with backpropagation.  \nThis method is inductive and flexible while being parameter efficient compared to a fully learnable approach.  \nThe Vanilla approach to incorporating absolute position representations is to add position encodings/embeddings to token embeddings. However, as the input signals propagate through the layers, the positional information might get lost in the upper layers. Later works find it beneficial to add position representations to inputs to each Transformer layer [2, 26, 45, 85].\nContextualized Text:\nTo improve upon hand-crafted methods for encoding positional information in Transformers, Wang et al. proposed learning the sinusoidal frequencies used in position encodings from data.  Additionally, FLOATER frames positional representation as a continuous dynamical system utilizing Neural ODE for efficient, end-to-end training. This method maintains the property of inductiveness while being more flexible than fully learnable approaches.	{"tags": ["NLP", "transformer", "architecture", "positional encoding"], "doc_id": "2cf160dd-50ea-4653-a229-a1ed1b928f83", "summary": "Wang et al. introduced a method to learn sinusoidal positional frequencies from data, enhancing flexibility compared to hand-crafted approaches. FLOATER utilized a continuous dynamical system with Neural ODE for end-to-end training, offering a parameter-efficient and inductive approach.", "doc_type": "text", "entities": ["Wang et al.", "FLOATER"], "keywords": ["positional representations", "sinusoidal encoding", "Neural ODE", "inductiveness", "FLOATER"], "key_objects": ["positional representations", "sinusoidal encoding", "Neural ODE"], "contextual_text": "To improve upon hand-crafted methods for encoding positional information in Transformers, Wang et al. proposed learning the sinusoidal frequencies used in position encodings from data.  Additionally, FLOATER frames positional representation as a continuous dynamical system utilizing Neural ODE for efficient, end-to-end training. This method maintains the property of inductiveness while being more flexible than fully learnable approaches.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["What are the advantages of learning sinusoidal frequencies compared to using fixed frequencies?", "How does Neural ODE contribute to the efficiency and flexibility of positional representations?", "Why is inductiveness a desirable property in positional encoding approaches?"]}
85a1979b-3043-4464-a8a9-7f4f54c3efc2	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03470637,-0.014272921,0.004466425,0.030408463,-0.0026632273,0.08172415,-0.009617665,0.07098249,-0.0014784202,-0.026777362,-0.017341135,-0.025865216,0.029836796,0.009978419,-0.037126202,0.021446139,0.028485112,0.10740284,0.04205988,0.003671904,0.065451786,0.028484276,0.0043307804,-0.03849594,0.010915671,0.050190877,0.034611486,-0.0041448427,-0.0143458275,0.025169048,0.05700556,-0.038955323,0.021737777,0.03259636,0.029625552,-0.0326765,-0.013676071,-0.05013075,-0.008376179,-0.012651834,-0.046563268,0.058215566,-0.074700125,0.046229426,0.013694861,-0.06386959,-0.10340354,-0.020927096,0.044718593,-0.007902673,0.0021533482,0.015266152,-0.027650455,-0.025013462,-0.0044586104,0.041538943,-0.0020432652,-0.028835695,0.022245625,-0.08346125,-0.0026169838,-0.010089686,-0.046262875,0.012711728,0.020374754,-0.045720823,0.0006723484,0.04305963,0.026474051,0.12194895,-0.044430226,0.011616158,-0.0028227728,-0.059201274,0.10199113,0.0031525765,-0.029675527,-0.02915899,-0.06628633,0.0010863122,-0.041131042,0.09151312,-0.01920899,-0.039637387,0.08188795,0.029173475,-0.055532813,-0.008102755,0.013580589,-0.05395333,-0.004758893,-0.0021932162,-0.0064548803,0.042902082,-0.005379368,-0.03777799,0.014674108,0.0029926542,-0.0051637525,-0.058608774,0.024885245,0.000785728,0.09786054,0.07759748,0.0014785018,-0.008808722,-0.02936858,-0.055834204,0.00756387,0.0021121374,-0.014704321,0.019400327,-0.07259992,0.017050333,-0.024909945,0.0093181245,-0.023188356,-0.008278984,0.0074836286,0.0054104477,0.018114015,-0.017242678,-0.02294534,0.010980717,0.027710816,0.07856576,0.003263701,-0.025771245,-0.0021649934,-0.007053764,-0.008958251,0.024782827,-0.036683284,0.021412577,-0.028468924,0.0059424923,0.062903754,-0.013941876,0.0012454486,0.05182884,-0.0030929404,-0.05848515,-0.03721482,0.06328364,-0.041510787,0.025354732,-0.03938779,0.006839371,-0.060633767,0.041742448,-0.003741984,-0.015532457,0.056957364,-0.011355556,-0.028964676,-0.0022626657,0.026814258,-0.04614251,0.04614558,-0.03343676,-0.032949295,-0.07100344,-0.054268155,0.036084324,0.025652418,0.062163707,0.005602375,-0.013145725,0.03613875,0.003353604,-0.017411353,0.012153767,0.009738703,-0.0042760554,0.021994054,0.026519842,-0.018319288,-0.022113333,0.020280344,-0.03491065,-0.025727475,0.013981578,0.004495605,0.030314287,-0.05920962,-0.0038916355,0.0004452996,0.059409566,-0.00017417832,0.011539464,-0.0034864459,0.0016875494,0.0016076283,-0.0064830403,-0.02628305,0.051360156,0.05073919,-0.06313349,-0.0076599186,-0.017088866,-0.032936756,0.011633634,-0.00062584924,-0.008354532,0.025359951,-0.02616501,0.012920418,-0.0018905846,0.009232802,-0.0004419564,-0.028187431,0.005971922,-0.027899155,0.0014154564,0.028571513,0.029133536,-0.020192157,0.03361252,0.029847074,-0.04367492,-0.0123649165,0.028046908,-0.05802799,0.031389877,-0.057226498,0.007193333,-0.025192525,0.0075422875,-0.0064862147,-0.019614676,0.03214532,0.034360778,-0.008740876,0.030177964,-0.005066835,0.031008318,0.0065685054,-0.046362076,-0.016883831,0.04898675,0.034921605,0.051569015,-0.06359057,0.07973109,0.003337322,0.026875492,-0.000112785856,0.030309249,-0.001209329,0.056910194,-0.008536953,0.030865174,0.10022797,0.02153204,0.0015829279,-0.03727281,0.0068359217,-0.029629378,0.0328369,-0.019925233,0.019139005,0.0014663291,0.004562308,0.034428425,0.005703612,-0.05678897,0.012551446,0.04771488,-0.013993195,-0.034253832,0.08083691,0.06741033,-0.002124733,-0.011119469,-0.05921535,0.013445186,0.0029226965,0.011552419,0.02830977,-0.020922305,-0.057876162,0.026151419,0.036548205,0.0011578014,0.028950384,-0.000667066,0.005805924,0.00395972,-0.068106726,-0.041153334,-0.029583102,-0.05630161,0.06009482,-0.005859038,-0.013119791,-0.005215434,0.001811763,-0.009590681,-0.005289109,0.04509452,0.1024934,-0.021146975,-0.0013560549,0.024416497,0.024147535,0.017440204,-0.042908657,0.034475334,0.03929016,-0.03760739,0.0015360567,0.0038362567,-0.05561536,-0.03064097,0.022899896,0.06468858,0.077400245,-0.020936353,-0.006619563,-0.0029200346,0.0021323042,0.018962387,-0.012387526,0.026370926,0.043671034,-0.028167143,0.03295257,0.0016260385,-0.058264654,0.021088736,-0.078058325,0.045232166,0.058315095,0.021272345,-0.00821168,0.008221718,0.016613508,0.05358809,0.0056530787,0.0037758485,-0.04030067,-0.049594942,0.024231737,0.00053784356,-0.025153471,0.036977865,-0.023949854,-0.0040530167,0.024928652,-0.015146333,0.022070846,-0.009042549,0.02790657,-0.042996336,-0.0032540178,-0.018555678,0.0034632965,0.0015841086,0.0047598523,-0.04690039,0.07590363,-0.0687901,0.05446774,-0.043449655,0.00045152375,-0.044691157,-0.044521112,-0.014288863,0.053760912,-0.056230884,0.009345008,-0.02336948,0.061237372,-0.008435469,-0.05622716,0.020788867,0.037040193,0.038401496,-0.03485596,-0.03581261,-0.0120825935,0.012196903,-0.040547416,-0.06363504,0.015877198,0.007360436,0.026623616,0.09298055,0.015951796,0.051969398,-0.052106325,-0.00029291757,0.04742131,-0.0011609306,0.0127454195,0.040786695,-0.009890691,-0.002065453,0.008261929,-0.020349128,0.04101365,0.025194399,-0.036161844,0.043443494,0.033293597,0.030680614,-0.08316058,0.037739113,0.0659964,0.015455578,-0.001585911,-0.047598705,-0.07279025,0.019615216,-0.012765167,-0.007906764,-0.018456833,0.035127208,-0.051584587,-0.03309772,-0.09295128,0.023926992,0.032707907,0.047739275,0.011271539,0.029574756,-0.05023794,0.016121035,-0.0108148465,-0.061939653,0.038104862,-0.020019647,0.009977424,0.05478603,0.017932575,-0.005886621,-0.035221033,0.06860497,-0.0637106,0.008542209,0.027765239,-0.04049943,-0.00023812936,-0.0034823467,0.009113954,-0.028182173,0.018663632,0.024650017,-0.013449172,-0.02917082,0.006853613,0.045277815,0.007355806,-0.02612204,0.001833888,-0.0014350248,0.0056802863,-0.010449832,-0.049577106,-0.034236684,-0.017277135,0.030711498,0.0042403624,-0.007977156,0.00041533302,-0.069213636,-0.0085139545,-0.06852694,-0.0036860243,0.04946555,-0.022634018,-0.0076045515,-0.035963975,0.049184196,0.013589313,-0.01915896,-0.0040027443,0.038679842,0.03956148,0.007991613,-0.05676963,-0.057260085,0.016077818,0.04164484,-0.035493203,-0.017055413,-0.016455404,0.0010602757,-0.078383714,0.0430068,-0.006646631,0.041562296,-0.008371969,0.005555079,-0.0072710677,-0.04147932,-0.014713115,0.022021154,0.019203303,-0.024983311,-0.005614803,-0.051463183,0.017022273,0.053460397,0.014019155,-0.0039406265,-0.03821131,-0.01852151,-0.05681407,0.027872358,-0.009048716,-0.03478158,-0.015957085,0.002708044,-0.0386294,-0.0027260908,-0.046331156,0.011345555,0.009332563,0.067146644,-0.041405994,-0.0019279158,0.021817306,-0.026127838,0.021358274,0.03085875,0.02327282,0.02083619,0.042205513,-0.023828035,0.036306616,-0.023279501,-0.02470382,0.0038843749,-0.061434794,-0.010141529,0.005540725,-0.06218858,-0.009009846,0.035819564,-0.023852255,-0.011009305,0.007911352,-0.07255596,0.09658161,-0.07641975,0.010920482,-0.007425472,0.025094427,0.07227746,0.02715765,0.04765854,-0.036948796,-0.06380963,-0.034498394,0.025944695,0.050106917,0.018272482,0.005883346,0.0815186,-0.039456777,-0.041487675,-0.007930126,0.0056395927,-0.0038246259,-0.053079296,-0.048896205,0.025268998,0.0008877703,0.019946989,0.012952081,-0.02328534,-0.047128793,0.033897687,-0.05007111,0.030335689,-0.020902716,0.022229934,0.0064569316,-0.031075317,0.02350744,0.005122675,0.022171473,0.03900163,0.025713082,-0.013983159,-0.009078668,0.045447182,0.007287382,0.019347597,-0.019174471,-0.05160248,0.002718205,0.057828907,0.03160745,-0.009184529,0.02622951,0.025123583,0.02905656,-0.008075859,-0.05989766,-0.0036081572,-0.037853323,-0.056961272,-0.010553409,-0.001169207,0.005160879,0.017531822,-0.053641684,0.077451855,-0.061097488,0.011515219,-0.05639556,0.030004514,-0.023730898,-0.024571994,0.0035678924,-0.02531396,0.009293798,-0.042653855,0.020285202,0.10125505,-0.043553002,-0.02094796,-0.033449665,0.006711596,-0.005835438,-0.03543327,0.031422824,-0.0097453715,-0.010876393,-0.01890858,0.07825788,-0.008158117,0.026868595,0.03871176,-0.028900255,-0.064342394,-0.051225737,0.0940459,0.044508033,0.006500581,0.012489083,0.018528366,0.022800598,-0.042986885,-0.018429562,-0.025179664,-0.0073802974,0.01401365,-0.018496878,0.043223344,0.010364917,-0.048931453,-0.007144619,0.023047687,-0.05653352,0.016705217,0.07214457,0.0126398,0.044895526,0.060922105,-0.056776512,-0.055411045,-0.034323573,-0.050830845,0.013244636,-0.0049278513,0.07657539,-0.0071590105,-0.009541326,-0.008974767,0.043605108,-0.013840668,0.030530911,0.019485869,-0.0031551484,-0.009319888,-0.011259534,-0.03732333,0.0156110255,0.0077812294,-0.02695862,0.08230318,0.024509232,-0.009974084,-0.014968651,-0.048026565,-0.019605491,0.017094739,0.024309415,0.008841061,0.032872085,0.004737035,-0.011865624,0.019397086,0.017755352,-0.04163041,-0.028540347,0.059704125,-0.010632077,0.064443186,-0.0067611295,-0.013280774,-0.07216284,0.026552495,0.013112408,-0.024594823,0.023955882,0.02678707,-0.029164515,-0.018549373,-0.005436062,-0.020595964,0.009698154,-0.008344818,0.01759537,0.05267692,0.022288563,-0.035757214,-0.057643767,-0.031835884,-0.008293068,-0.0075382283,-0.012444648,0.029959248,0.010225383,0.0053147804,0.0060962387,0.011446138,-0.04673171,-0.029282255,-0.012936432,0.03597892,-0.0039454727,0.055181943,-0.04147476,0.015808336,0.09895424,-0.053503808,-0.0016761352,0.007367437,0.031820763,-0.031533446,-0.03255566,0.014091195,-0.03388702,-0.044156406,0.0077247364,0.044403546,0.009891037,0.06187644,-0.0047800015,-0.04181945,-0.06644355,-0.022657037,0.0147939315,0.007711551,0.033947363,0.022853838,-0.013634214,0.032138962,-0.012426157,0.044940934,0.008643766,0.024419488,-0.012044123,0.02313611]	Keywords: relative position representations, self-attention, positional relationships\nKey Objects: relative position embeddings, attention mechanism, keys\nRefers to Images: None\nHypothetical Questions:\n- Why would modeling relationships between tokens be more beneficial than modeling their absolute positions?\n- How does adding a learnable relative position embedding to the attention mechanism keys help the model?\n- What is the significance of clipping (max/min) relative position embeddings?\n---\nSummary:\nSome Transformer models utilize relative position representations, which focus on modeling relationships between tokens rather than their absolute positions, to improve performance.\nOriginal Text:\n5.1.2 Relative Position Representations. Another line of works focuses on representing positional relationships between tokens instead of positions of individual tokens. The intuition is that in self-attention, pairwise positional relationships between input elements (direction and distance) could be more beneficial than positions of elements. Methods following this principles are called relative positional representation. Shaw et al. [116] propose to add a learnable relative position embedding to keys of attention mechanism  \n$$k _ { j } ^ { \\prime } = k _ { j } + r _ { j }, \\, \\, f o r \\, \\, i = 1, \\cdots, n,$$  \n$$r _ { i j } = R _ { i j } ( i - j ),$$  \n$$c l i p ( x ) = \\max ( - K, \\min ( x, K ) ),$$\nContextualized Text:\nTo enhance the ability of Transformers to understand the structure of inputs, some research focuses on relative position representations. This approach focuses on modeling pairwise positional relationships between tokens, suggesting that these relationships (direction and distance) can be more beneficial than absolute positions. Shaw et al. [116] propose to add a learnable relative position embedding to the keys of the attention mechanism to achieve this.	{"tags": ["architecture", "NLP", "transformer"], "doc_id": "85a1979b-3043-4464-a8a9-7f4f54c3efc2", "summary": "Some Transformer models utilize relative position representations, which focus on modeling relationships between tokens rather than their absolute positions, to improve performance.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["relative position representations", "self-attention", "positional relationships"], "key_objects": ["relative position embeddings", "attention mechanism", "keys"], "contextual_text": "To enhance the ability of Transformers to understand the structure of inputs, some research focuses on relative position representations. This approach focuses on modeling pairwise positional relationships between tokens, suggesting that these relationships (direction and distance) can be more beneficial than absolute positions. Shaw et al. [116] propose to add a learnable relative position embedding to the keys of the attention mechanism to achieve this.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["Why would modeling relationships between tokens be more beneficial than modeling their absolute positions?", "How does adding a learnable relative position embedding to the attention mechanism keys help the model?", "What is the significance of clipping (max/min) relative position embeddings?"]}
f2658249-5e01-4879-abde-2ce05b458f96	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.021976266,-0.009556368,-0.009911806,0.038226444,0.00074706745,0.10266588,-0.017952804,0.06972096,-0.013858292,-0.029441744,-0.051840022,-0.034559112,0.043782465,0.021629067,-0.046280783,0.033493623,0.029342208,0.09153056,0.014056612,0.013559614,0.057954293,0.029981421,-0.005602883,-0.0117510855,0.009732689,0.0370964,0.021003343,-0.0037500975,-0.012621029,0.00906349,-0.0071608783,-0.017942064,-0.008033095,0.02213636,0.050788704,-0.03550019,-0.02026048,-0.060508855,-0.029517224,-0.0026821427,-0.017561402,0.031856295,-0.043312125,0.04462309,0.022551507,-0.03534758,-0.088946305,-0.02015125,0.039400704,0.0068936236,0.011207908,0.023284802,-0.035644967,-0.027288212,-0.0141027,0.043722432,0.00983549,-0.032849934,0.016520029,-0.07603217,0.018658828,0.019571397,-0.037734244,-0.007228076,0.011072769,-0.06513456,-0.00950825,0.03582784,0.029377032,0.13220085,-0.07234353,0.009206756,0.013820866,-0.07941499,0.09847157,0.04242741,-0.05174418,-0.04476897,-0.051027097,0.013272936,-0.010820118,0.069384284,0.00060946465,-0.016242635,0.10072078,0.0034511294,-0.051782895,-0.027529951,0.030197253,-0.063602366,-0.02041233,-0.022872338,0.0161992,0.044503894,0.0017429307,-0.016552823,-0.0068165665,-0.018357255,0.04336879,-0.046085916,0.0058408207,0.019412177,0.075763546,0.068651155,0.00053394114,0.033099312,-0.039444413,-0.017956685,-0.0011373295,0.028330067,-0.0075402423,0.04308062,-0.11745362,0.045182317,-0.0406392,-0.018895283,-0.040953975,0.007963575,-0.013255364,0.00072465994,0.024563132,-0.03374752,-0.02047323,-0.011062838,0.045427673,0.0512386,0.02180734,-0.035838045,0.0041473117,0.03390837,0.016023688,-0.0024204003,-0.054504603,0.014211712,-0.025695315,0.006400351,0.066321835,-0.03371175,-0.031710103,0.040831305,-0.002502511,-0.07067917,-0.03820637,0.036617372,-0.014337136,0.056477368,-0.044386268,0.0029560656,-0.062468126,0.062489294,-0.006685785,0.007994754,0.052764688,-0.01258288,-0.030372746,-0.010162785,0.022477172,-0.03832864,0.037515227,-0.032496717,-0.02451768,-0.04071841,-0.050702248,0.0443367,0.022187117,0.0890424,-0.04880817,-0.00899157,0.027392896,0.029575935,-0.025120078,-0.0139156245,0.0057973233,-0.036015015,0.017270131,0.018865574,0.006202904,-0.015792595,0.014169537,-0.03094986,-0.01424185,0.006879906,0.018331842,0.027662832,-0.050197575,0.025560793,-0.0063100047,0.057051405,-0.0012481592,0.0056714797,-0.0037582177,0.0082924,-0.014046838,0.0073101595,-0.03859736,0.045173675,0.03664924,-0.039204866,-0.02563817,-0.024347588,-0.021202639,0.023205321,-0.016303945,-0.028755732,0.043393716,-0.026496448,0.021676045,-0.008055285,0.0019710548,0.027860003,-0.018928738,-0.007482053,-0.042161103,-0.025283264,0.041358307,0.011416476,-0.038067102,0.005093133,0.028940732,-0.04287485,-0.018103942,0.010280314,-0.047552735,0.02671061,-0.042566147,0.03837254,-0.028594082,0.03647175,0.014340436,-0.005454508,0.02586661,0.01653622,0.00067086,0.03236865,0.00088408426,0.01794836,0.008099856,-0.031628184,-0.010749657,0.044965148,0.066227324,0.038211606,-0.08319718,0.0814919,-0.012954941,0.033312827,-0.0014838745,0.026275594,0.006947854,0.061387032,-0.007469158,0.020435631,0.057816766,-0.021466635,-0.019474093,-0.02843957,-0.014082184,-0.031287987,0.016738994,0.013513902,0.024255801,0.011432043,-0.0047235624,0.02264198,0.013471606,-0.03839665,0.021204924,0.02202761,-0.0064192163,-0.02385966,0.033533506,0.08735991,0.013625272,0.011773248,-0.020677742,0.0074017844,0.011384007,0.012346601,0.026117017,-0.010236247,-0.049548447,0.0012116703,0.02558054,0.0031039508,0.010037893,-0.014493449,-0.000723067,-0.024578497,-0.028970715,-0.016933475,-0.03509924,-0.06443671,0.070816725,0.0091407215,0.0006983639,-0.009636781,-0.017423961,-0.03963739,-0.009872053,0.08070258,0.12981221,-0.035961982,0.036913216,0.045093164,0.017075215,0.017913131,-0.055175044,0.02813679,0.07741365,-0.024328372,0.013375848,0.016365327,-0.07709456,-0.013995132,0.056658022,0.057696756,0.088574074,-0.015144534,-0.0061815167,0.018653594,-0.005595103,0.035120893,0.0028727676,0.01873818,0.048639305,-0.017544739,0.0015980168,-0.009235675,-0.05550075,0.037868794,-0.044100918,0.035325535,0.056020986,0.027032364,-0.013290363,0.04068409,0.028891565,0.060062144,0.012355568,0.007660879,-0.030656049,-0.034680296,0.053941965,0.0030619795,-0.029225502,0.05610812,-0.019979494,-0.011505626,0.03306137,-0.027396638,0.014094702,-0.0053715482,0.012409204,-0.018683165,-0.005523913,-0.00014560987,-0.013758742,-0.0072671943,-0.005364138,-0.04425736,0.058076404,-0.053035937,0.06822593,-0.04603504,0.027703777,-0.0537492,-0.049124043,-0.03134986,0.022748144,-0.05487554,0.00837203,0.0037243436,0.05570951,-0.008484067,-0.012533458,0.0424445,0.07065744,0.029356241,-0.03827356,-0.039044358,-0.014302764,0.013625732,-0.0100306505,-0.08599099,0.017351788,-0.009748289,0.0054679103,0.094889656,0.0051254295,0.054240808,-0.047097154,-0.0051928703,0.028611843,0.0017337417,0.009310359,0.0145404525,-0.0067483303,0.0102849305,0.0037165007,0.0079226,0.047276374,0.01237108,-0.06627108,0.011896306,0.005364596,0.027852874,-0.091876395,0.02717704,0.0511946,0.026284583,0.017373653,-0.057338107,-0.04940437,0.018939927,-0.019730164,-0.001988924,-0.012055487,0.02429427,-0.039057516,-0.015667971,-0.057438243,0.0174254,0.042969797,0.041633893,0.021254836,0.016518854,-0.04741814,0.01455671,-0.04404865,-0.0720857,0.027501648,-0.043437738,0.013679387,0.048461434,0.01688184,-0.013476959,-0.014165139,0.05283337,-0.04476288,0.034513567,0.044952758,-0.022417763,0.010020554,-0.018422721,-0.009246965,-0.022956006,0.0011666729,0.019067524,0.01576087,-0.040404428,0.037618145,0.022599036,-0.008239187,-0.00093752146,0.057568394,0.02460725,-0.003701644,-0.039917372,-0.0473935,-0.0036219803,-0.0039482177,0.035293695,0.00813896,-0.0049246014,-0.0069169346,-0.05676547,0.005315641,-0.09189902,-0.0066694384,0.01945994,-0.019076766,-0.019655187,-0.040084433,0.054929145,-0.016230058,-0.022377215,-0.0072450857,0.026724372,0.032995697,-0.021614235,-0.030696914,-0.049224246,0.021795448,0.034639418,-0.024278348,-0.00516304,-0.051343698,-0.01812922,-0.055889104,0.032996096,0.0017720046,0.048218243,-0.00933923,0.01631609,0.013925614,-0.03425897,-0.0009027526,0.015362584,0.029183634,-0.04583819,-0.009678254,-0.051606797,0.037974834,0.06036704,-0.010501523,0.019601118,-0.03867596,-0.022011522,-0.022570945,0.042280722,0.009038569,-0.029011657,-0.01033056,0.010104577,-0.017413594,0.0042519816,-0.046151273,0.0041415435,-0.007077627,0.026625419,-0.023748996,-0.039943963,0.013923756,0.0024427911,0.009611348,0.035742134,0.022733603,0.0039266716,0.0044306256,-0.006048865,0.0032358167,-0.02717651,-0.036791064,0.013659566,-0.047386136,-0.043831095,-0.015276878,-0.08042689,0.01948926,0.003962837,-0.0019417966,-0.010976888,0.0034569083,-0.028894868,0.08921508,-0.05253513,-0.013239402,0.0072289505,0.00021903601,0.08693793,0.0003726173,0.019995458,-0.026632173,-0.057588037,-0.016170917,0.01972587,0.010857745,0.040837023,-0.0025961495,0.07500237,-0.036452327,-0.060965117,-0.017011506,-0.03936357,0.002422708,-0.05274441,-0.043060027,-0.009613927,0.0032837517,0.0046350984,-0.0061000856,-0.026617102,-0.022740973,0.031826723,-0.053410817,0.030772243,-0.0059247883,0.061940793,-0.0021159037,-0.0013961787,0.03990356,0.03201922,0.050178528,0.03726164,0.0045602233,-0.009893216,0.0073823896,0.030814268,-0.0075318916,0.017001074,-0.010289677,-0.061023105,0.012283405,0.03959467,0.029273111,-0.0128480345,0.054877896,0.015719721,-0.007826998,-0.027501503,-0.033609726,0.009285869,-0.006114817,-0.048867118,-0.0017631025,-0.0069111423,0.009066085,0.032711547,-0.05356611,0.058865994,-0.056080073,0.006393034,-0.07058516,0.03046983,-0.044219647,-0.023516756,-0.004722574,-0.039980426,0.008263894,-0.04368807,0.00983099,0.082548626,-0.025905687,-0.012072277,-0.02914771,0.009173303,-0.00059751916,-0.026985392,0.01999887,-0.013610431,-0.03285534,-0.031112727,0.08145076,0.013630082,0.035988346,0.033308443,-0.015306939,-0.08641476,-0.059132814,0.09157687,0.048647176,-0.00021267906,0.039875913,0.014851967,0.023213161,-0.029406087,-0.025685297,-0.03635319,0.015070314,0.03310342,-0.01374181,0.019091425,0.017766068,-0.053600635,-0.018539723,-0.008348547,-0.084883384,0.03540961,0.05174666,0.009290187,0.05346958,0.06321089,-0.04362526,-0.0796306,-0.04076405,-0.024822608,-0.00013441879,0.008020489,0.057509705,-0.008100807,-0.028456813,-0.0059879585,0.039689515,-0.009067277,0.051856533,0.016700583,-0.0059037874,0.023454644,0.00544661,-0.065422334,0.020826006,0.018990347,-0.019456277,0.074973665,0.0024363173,-0.019642748,-0.0010103475,-0.05034434,0.006518779,0.02536003,0.012355478,0.024239914,0.028804608,0.0013716884,-0.0028539007,-0.004043434,0.025822086,-0.055097297,-0.05387324,0.021794992,0.008071853,0.052578997,-0.015432545,0.0029578425,-0.09210111,0.016511936,-0.007452167,-0.017594066,0.034511715,0.01295573,-0.0456644,-0.03704392,0.00856518,-0.012216984,0.03782413,0.0041514323,0.011497856,0.056713406,0.028553504,-0.025901888,-0.062045597,-0.033145204,-0.0012047192,-0.027618114,-0.008687014,0.046193574,0.021865359,0.012402736,0.020124707,-0.0016291018,-0.02209537,-0.010934027,-0.019663667,0.027767617,0.0064864,0.036399525,-0.038148075,0.028253663,0.11337315,-0.050103404,-0.016689165,0.02910304,0.036325686,0.00046312992,-0.035378657,0.0013794588,-0.04018132,-0.014480563,0.0018587597,0.05736627,0.023883102,0.072236136,-0.0046886303,-0.029430676,-0.0639251,-0.009440108,-0.0048502455,0.023398405,0.035801254,0.030068578,-0.030893756,0.037210733,-0.025724955,0.05512002,2.9240387e-05,0.002928719,-0.0049920995,-0.0076713366]	Keywords: relative position embeddings, attention mechanism, token relationships\nKey Objects: Relative Position Embeddings, Keys, Attention Mechanism\nRefers to Images: None\nHypothetical Questions:\n- Why would representing relationships between tokens be more beneficial than representing individual token positions?\n- How does the parameter 'K' influence the effectiveness of relative position representations?\n- What are some advantages of using relative position representations over absolute position encodings?\n---\nSummary:\nRelative position representations introduce learnable embeddings to keys in the attention mechanism, calculated based on the difference between token positions, to capture relationships between tokens.\nOriginal Text:\n$$k _ { j } ^ { \\prime } = k _ { j } + r _ { j }, \\, \\, f o r \\, \\, i = 1, \\cdots, n,$$  \n$$r _ { i j } = R _ { i j } ( i - j ),$$  \n$$c l i p ( x ) = \\max ( - K, \\min ( x, K ) ),$$  \nwhere r$\\_{ij}$  R D k is the relative position embedding for relation between position i and j and K is the largest offset that determines the number of embeddingg. Typically K is set to a length that can accommodate most input sequences. As a special case, InDIGO [39] sets K to 3 for their specially designed framework for non-autoregressive generation. As an incremental effort, Music Transformer [56] further introduce a mechanism to reduce the intermediate memory requirements for this approach. Similar to this approach, T5 Raffel et al. [104] adopt a simplified form of relative position embeddings where each embedding is only a learnable scalar that is added to the corresponding score used for computing the attention weights.\nContextualized Text:\nTo represent positional relationships between tokens instead of individual token positions, relative position representations add learnable embeddings to the keys of the attention mechanism.  These embeddings, denoted as 'r_ij', are calculated based on the difference between the positions of tokens 'i' and 'j', and a parameter 'K' limits the maximum offset used for these embeddings. This approach is utilized in models like InDIGO, Music Transformer, and T5 to improve the model's understanding of token relationships.	{"tags": ["NLP", "transformers", "attention", "positional encoding"], "doc_id": "f2658249-5e01-4879-abde-2ce05b458f96", "summary": "Relative position representations introduce learnable embeddings to keys in the attention mechanism, calculated based on the difference between token positions, to capture relationships between tokens.", "doc_type": "text", "entities": ["InDIGO", "Music Transformer", "T5"], "keywords": ["relative position embeddings", "attention mechanism", "token relationships"], "key_objects": ["Relative Position Embeddings", "Keys", "Attention Mechanism"], "contextual_text": "To represent positional relationships between tokens instead of individual token positions, relative position representations add learnable embeddings to the keys of the attention mechanism.  These embeddings, denoted as 'r_ij', are calculated based on the difference between the positions of tokens 'i' and 'j', and a parameter 'K' limits the maximum offset used for these embeddings. This approach is utilized in models like InDIGO, Music Transformer, and T5 to improve the model's understanding of token relationships.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["Why would representing relationships between tokens be more beneficial than representing individual token positions?", "How does the parameter 'K' influence the effectiveness of relative position representations?", "What are some advantages of using relative position representations over absolute position encodings?"]}
4c3895cf-fe6e-4aa4-a270-c825c441b077	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.027369387,0.022633133,-0.0016026318,0.075018495,-0.002888829,0.078703865,0.018329356,0.07006942,-0.006051476,-0.024839697,-0.012485927,0.0042379634,0.0013638922,0.024944574,-0.038459916,0.036752768,0.0111563895,0.08293771,-0.0062290765,-0.012034719,0.035488088,0.0009898288,0.0033033867,-0.015541927,0.015020617,0.04392369,0.027312463,-0.0068580527,-0.011679124,0.0046048164,0.01993037,-0.05962326,0.018868491,-0.010797411,0.007930817,0.033364944,0.017458154,-0.042283565,-0.00018802511,-0.0020288145,-0.024539484,0.07060517,-0.07699178,0.019793697,0.014303003,-0.06493158,-0.106835335,-0.018482305,0.03133577,-0.021164564,-0.011505566,-0.0015761236,-0.041208867,-1.1498651e-05,-0.0178727,0.012136372,0.030257372,-0.028649835,0.015155267,-0.07164382,-0.008370898,0.0049396567,-0.03132195,0.0008981412,0.008423898,-0.022430463,0.01632334,0.009682441,0.030914998,0.1268805,-0.031088307,0.02149983,0.0029453272,-0.068407625,0.098385535,0.051036593,-0.0435729,-0.011877958,-0.06624609,-0.005170613,-0.003263457,0.08280587,-0.004554414,-0.016068373,0.106017366,-0.010461763,-0.043675292,0.0012842313,0.06339138,-0.030699847,0.02409783,0.00614317,0.032420617,0.03560624,-0.018093247,-0.089110084,-0.01987864,-0.039162517,0.03608827,-0.056217603,0.027336689,0.004437218,0.08455828,0.09080447,-0.0061851954,-0.015523574,-0.012291665,0.00030470462,-0.010349893,0.029425284,-0.025057796,0.047594693,-0.08843558,0.022661963,-0.02741576,0.0031412633,-0.030535232,-0.0063400413,0.0005172136,0.0042555104,0.025663773,-0.040273517,-0.008175006,0.0097938,0.056651596,0.06670355,0.041194756,-0.018188579,-0.008620185,0.031535003,0.03954058,0.011769753,-0.01035279,0.011763872,0.0014778306,0.036054645,0.045355223,-0.006262105,-0.012369997,0.014088078,0.0066104755,-0.047397185,-0.015373582,0.034821,-0.032693308,0.023631226,-0.050574448,0.023148928,-0.04138975,0.06200599,0.008687209,0.008399635,0.036879465,-0.0027135634,-0.028974956,-0.004833402,0.034159943,-0.021555759,-0.018660845,-0.018236049,-0.037255283,-0.030898312,-0.04128295,0.05367654,-0.003955015,0.08552882,-0.036289953,0.0025955231,0.044173148,0.019233467,-0.046481594,-0.016894901,0.028550155,-0.018156836,-0.020704646,-0.00617528,-0.0016362962,-0.042813867,-0.011779122,-0.037714854,-0.0184022,0.03501717,-0.0059399977,0.052617047,-0.048183955,0.032662496,0.042706307,0.049432717,-0.03711458,0.0026355474,0.015732255,-0.0031137103,0.0006625221,0.006669563,-0.0061631883,0.043569922,0.05593368,-0.08118272,-0.04173704,-0.03306404,-0.009580336,0.020984326,0.0066411407,-0.011111386,0.016918,-0.030658297,0.013847994,-0.02002032,-0.002305998,0.046417397,0.016956741,-0.042306658,-0.03840947,0.004140509,0.021244965,0.03465973,-0.02219783,0.013180295,0.030364487,-0.06503727,-0.06220327,0.03353978,-0.025546608,0.051971577,-0.02289401,0.04738353,-0.021760868,0.021668827,0.012011783,-0.008372499,0.042844042,0.024607457,-0.013651392,0.028632082,-0.028112503,0.024674678,0.021334706,-0.0006301537,-0.04183581,0.05435632,0.071713105,0.01381947,-0.0793362,0.06686288,0.020573853,-0.005216692,0.034000184,0.05910095,0.0105852885,0.036882266,-0.0060393387,0.058357332,0.069369905,0.018940898,-0.0074391523,-0.06740755,0.012050115,0.00087622233,0.04285105,0.012920788,0.017201109,-0.019203737,-0.02544207,-0.02798297,-0.021254072,-0.032965448,0.042598866,0.03796411,-0.02667418,-0.019631967,0.046756182,0.04383028,0.0042433194,0.00558172,-0.040387705,0.027342154,0.02659758,-0.010590244,0.04478851,-0.042888563,-0.03873651,-0.022910368,0.013492397,0.009807715,0.033284146,-0.01704279,0.029879613,-0.010487178,-0.015273472,-0.010541758,0.0026392024,-0.059271567,0.0021140284,0.030247478,-0.056832246,0.007096168,-0.00857676,-0.07468581,-0.012237503,0.028671313,0.105669275,-0.0018742593,0.018931067,-0.0012236498,0.0029369227,0.012895993,-0.04827334,0.07503223,0.053142495,-0.03017573,0.027638163,-0.038796805,-0.032392934,-0.0036327648,0.04249291,0.074586704,0.054204438,-0.028733741,0.0006804716,0.012978819,-0.024036707,0.02758767,0.011074335,0.005870932,0.063673265,-0.027491082,0.043160852,0.017321113,-0.075592875,0.048918042,-0.050844554,0.065103434,0.055728514,0.041553415,0.003727369,0.012858302,0.0011517914,0.03250896,0.018755801,0.0036314363,-0.039964482,-0.019226594,0.033765152,-0.009638012,-0.008974632,0.03556876,0.021531912,0.025460571,0.024797885,0.0006386831,0.0060364436,0.017596055,0.026321592,-0.019048905,0.020909343,-0.0060784565,-0.0051620756,-0.003006487,0.00015686237,-0.0015131034,0.07250546,-0.0685186,0.042500336,-0.043285567,-0.0006522195,-0.01502185,-0.077837594,-0.014135377,0.059246015,-0.037111416,-0.0021703285,-0.026943086,0.043743826,-0.019024651,-0.035262648,0.03720735,0.058997806,0.022208113,-0.03348921,-0.020959606,-0.019289942,-0.013808498,-0.044248227,-0.055278335,0.034398783,0.015839646,-0.014295374,0.110395566,0.024172477,0.047265325,-0.03100427,0.017248148,0.0016858064,-0.027472839,-0.006996851,0.018747084,0.00901982,0.044614874,-0.009681432,-0.024545649,0.00722534,0.04232104,-0.063048154,0.0066202227,0.03130711,0.033839777,-0.07692478,0.055900052,0.032610193,0.022345742,0.027240124,-0.04285774,-0.053574137,0.01166576,0.013621647,-0.015362371,-0.032468677,0.056947753,-0.032807127,-0.009947686,-0.024858527,0.02472743,0.014242011,0.054270726,0.0077433297,-0.026733268,-0.05428,0.01949568,-0.024157414,-0.057557426,0.034900937,-0.03497517,0.030543193,0.04307648,0.04172923,0.014534685,-0.028148415,0.07763801,-0.073638394,0.030783009,0.06880225,-0.0070078443,0.035190072,-0.016717292,0.03010104,-0.0391821,0.020192241,0.036637753,-0.0263217,-0.04009761,0.015799772,-0.016824797,0.009167449,-0.014186052,0.040913206,0.03299136,0.011466711,-0.018712709,-0.054305412,0.0060609025,0.0052096415,0.04813433,0.007775955,-0.0049353265,-0.01214229,-0.05099413,0.0002482024,-0.041354522,-0.04581442,0.011355611,-0.019348856,-0.010998725,-0.038035676,0.04527926,-0.013169425,0.001076306,0.02024538,0.024764301,0.025209418,-0.014417514,-0.041751564,-0.013460708,-0.009104547,0.05747488,0.013590037,0.02887265,-0.034151167,-0.040837258,-0.0023983459,0.046873398,-0.014305922,0.050721377,0.040840227,0.011489169,0.020953814,-0.04612391,0.010878742,0.010363052,0.0039891778,-0.06476922,-0.026929393,-0.019265687,0.005652444,0.03704792,-0.0014442416,0.0019564051,-0.01577417,0.0257258,-0.044042263,0.032727275,0.015752783,-0.011753999,0.010506891,-0.004635136,-0.022524824,0.019878991,-0.03460359,-0.0028653727,-0.0139863,0.04029886,-0.013349575,-0.020626092,0.045673564,0.005536929,0.028905086,0.021666205,0.03357789,0.011673402,0.033234064,-0.033600587,0.01591514,-0.03931485,-0.045610227,0.026547797,-0.050718073,-0.0681727,-0.03718007,-0.059816904,0.019906916,-0.02515981,-0.024596283,-0.031270735,0.024301272,-0.046418633,0.10324241,-0.04321535,0.0012365745,-0.017864255,0.01493386,0.0806734,0.019423163,0.027237784,-0.034766898,0.0006881655,-0.0027939396,0.013173867,0.053972807,0.056855872,0.020760756,0.095456876,-0.018617308,-0.038389966,-0.03859299,-0.0086201895,-0.03230726,-0.03793665,-0.03367253,0.020681964,-0.008853801,0.032623183,-0.014768417,0.015375637,-0.032930065,0.02808898,-0.03782558,0.073225096,0.038352057,0.0070711398,0.013358099,-0.0062498725,0.08801057,-0.0015232168,0.048064943,0.03228427,-0.0044900742,0.03776275,-0.02054255,-0.023207417,0.034557123,-0.023094416,-0.020408714,-0.03040418,0.0014407355,0.00997148,0.026760876,-0.030430606,0.021567455,0.014435997,-0.010522618,-0.016604677,-0.069271505,-0.03429362,-0.029858716,-0.041081112,0.011687563,0.035780624,-0.013215122,0.05391064,-0.044964004,0.07130812,-0.10194775,-0.00046381986,-0.010375693,0.054277763,-0.02981378,-0.023207292,-0.0042689415,-0.012617659,-0.00833201,-0.013103955,0.052617315,0.0148155335,-0.039515838,0.01820935,-0.0047292747,-0.017190147,0.009258484,-0.02538608,0.006849553,-0.0043913783,-0.034347665,-0.017629564,0.07349795,-0.0035072928,0.008490581,0.044334292,0.012063864,-0.088499814,-0.041422784,0.06942596,0.044658996,-0.022644982,0.03601086,-0.018995216,0.025364658,-0.04280568,-0.012047336,-0.028698055,0.004271641,-0.00034636154,-0.032376923,0.028501155,-0.025837747,-0.050918385,-0.021414446,0.015297489,-0.054210495,0.038336184,0.0035910744,0.059048723,0.05522564,0.06477183,-0.040504836,-0.12014438,-0.03981723,-0.028658496,0.013742051,-0.02892402,0.060002677,-0.00087227544,0.010788501,0.0010809691,0.002905445,0.02597325,0.038757853,-0.025674583,-0.022216491,-0.002325594,0.0001077007,-0.022158762,0.052447606,0.017675767,-0.052752938,0.057565197,0.0475017,-0.0008860616,-0.027484706,-0.03370808,-0.021814398,0.0028816238,0.013620764,0.023769097,-0.011032644,-0.001471783,0.015912138,0.035178795,-0.0037896058,-0.065928586,-0.06524715,0.021242414,0.025773702,0.027037952,0.01930481,-0.002608828,-0.09099141,0.05583396,-0.011563293,-0.03964365,0.05689656,0.009870938,-0.036645867,-0.010809505,-0.016426513,0.016657164,0.04155893,0.0081261275,-0.008083276,0.02538436,0.00038994386,0.0010904416,-0.06235985,-0.0010900748,-0.023825902,-0.02608337,-0.013733617,0.037657242,0.007895543,0.0015860954,0.0140457,-0.013533777,-0.024924502,-0.0035765804,-0.033909764,0.066361316,-0.015978137,0.05970007,0.0034607989,0.024360145,0.0705051,-0.029332038,-0.045892723,-0.0021339993,0.02512476,0.024025623,-0.014587715,-0.0009812716,-0.043254793,-0.043160126,-0.019441338,0.015971022,-0.0058534485,0.039105766,-0.011374544,-0.03216041,-0.036683466,-0.036258053,-0.0041696513,0.01244028,0.014700745,0.006955406,-0.039911956,0.049950883,-0.02661891,0.07785128,0.004473282,-0.0226181,0.011878092,0.02956461]	Keywords: attention scores, sinusoidal encoding, positional relationships, Transformer-XL, DeBERTa\nKey Objects: attention scores, sinusoidal encoding, positional relationships\nRefers to Images: None\nHypothetical Questions:\n- How does fusing content and positional information benefit the Transformer model?\n- What is the significance of using a sinusoidal encoding matrix in this approach?\n- In what ways does DeBERTa build upon the techniques used in Transformer-XL?\n---\nSummary:\nTransformer-XL fuses content and positional information by redesigning the attention score computation using a sinusoidal encoding matrix (R) and learnable parameters, allowing for a more integrated representation of positional relationships.\nOriginal Text:\nTransformer-XL [24] use a sinusoidal encoding to represent positional relationships but fuses contents and position information by redesign the computation of attention scores 9  \n$$A _ { i j } = q _ { k } ^ { \\dagger } + q _ { i } ( R _ { i j } - W _ { i } R ^ { k } ) ^ { T } + u _ { i } ^ { 1 } + u _ { j } ^ { 2 } + u _ { k } ^ { 2 } ( R _ { i j } - W _ { i } K _ { r, R } ) ^ { T },$$  \nwhere W K R, R  R D m  D$\\_{k}$, u 1 , u 2  R D$\\_{k}$ are learnable parameters and R is a sinusoidal encoding matrix similar to position encoding in vanilla Transformer. Then softmax function is applied to scores A to provide attention weights. Note that the learnable sinusoidal encoding[139] is also a drop-in replacement to hand-crafted R.  \nDeBERTa [50] utilizes position embeddings like Shaw et al. [116] and applies the embeddings to the model in a disentangled style similar to Transformer-XL [24]\nContextualized Text:\nTo represent positional relationships, Transformer-XL reworks the attention score computation. This involves using a sinusoidal encoding matrix (R) and learnable parameters (W, u) that combine content and position information.  The approach, which uses a sinusoidal encoding matrix (R) and learnable parameters, is also adopted by DeBERTa, which utilizes position embeddings in a style similar to Transformer-XL.	{"tags": ["architecture", "transformer", "positional encoding"], "doc_id": "4c3895cf-fe6e-4aa4-a270-c825c441b077", "summary": "Transformer-XL fuses content and positional information by redesigning the attention score computation using a sinusoidal encoding matrix (R) and learnable parameters, allowing for a more integrated representation of positional relationships.", "doc_type": "text", "entities": ["Transformer-XL", "DeBERTa"], "keywords": ["attention scores", "sinusoidal encoding", "positional relationships", "Transformer-XL", "DeBERTa"], "key_objects": ["attention scores", "sinusoidal encoding", "positional relationships"], "contextual_text": "To represent positional relationships, Transformer-XL reworks the attention score computation. This involves using a sinusoidal encoding matrix (R) and learnable parameters (W, u) that combine content and position information.  The approach, which uses a sinusoidal encoding matrix (R) and learnable parameters, is also adopted by DeBERTa, which utilizes position embeddings in a style similar to Transformer-XL.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["How does fusing content and positional information benefit the Transformer model?", "What is the significance of using a sinusoidal encoding matrix in this approach?", "In what ways does DeBERTa build upon the techniques used in Transformer-XL?"]}
2ee30529-b245-4f80-95fd-c03e8e37ddd2	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.031329617,0.008189558,0.020907428,0.05034252,-0.011320513,0.09634039,0.020842586,0.059678905,0.0047747884,-0.035228737,-0.010118853,0.021903861,-0.011450402,0.029567152,-0.0128728645,0.015560406,0.018679986,0.079196125,0.015746199,-0.013087661,0.053797852,-0.0012798046,-0.01714205,-0.003998947,0.014824033,0.05957012,0.015462959,-0.0633167,-0.0046114735,-0.01827725,0.038716856,-0.07914507,0.0037241166,0.020347789,0.071912386,0.016556635,-0.016502576,-0.030253155,0.0017903476,-0.03499725,-0.05354545,0.068906,-0.07862214,0.029324733,-0.009513162,-0.04996925,-0.11770078,0.0018356347,0.0062020333,-0.03402628,0.014736347,0.022074023,-0.017654018,-0.007104801,-0.035896286,0.014935063,0.043154825,-0.028161936,0.019721894,-0.07004094,-0.0037062569,-0.0034019605,-0.036429618,-0.0077531124,0.033832103,-0.01712826,0.016347371,0.029784909,0.052248206,0.12238091,-0.048844174,0.010142177,0.018093094,-0.060990654,0.09676765,0.052938573,-0.024200728,0.012318347,-0.04378227,-0.004906979,-0.0027634413,0.10019269,-0.027298687,-0.0131918555,0.085504696,-0.034511015,-0.030030735,-0.0014025037,0.021828014,-0.018972589,-0.0062206527,0.014779402,0.0052693943,0.032358237,0.0061220126,-0.0593305,-0.032250542,-0.027433524,0.033230152,-0.06407083,0.008663963,-0.03732077,0.084097505,0.08332789,0.0066838968,-0.016293108,-0.023204025,-0.022893626,-0.031658348,0.002529724,-0.010447072,0.027765531,-0.07462033,0.034148846,-0.011471573,-0.02025337,-0.043624785,-0.045593165,0.0025535405,0.0055994955,0.014377576,0.020103373,-0.016566722,0.014310496,0.041501835,0.044057418,0.005575746,-0.03352136,-0.005506624,0.024669029,0.011244776,0.007848144,0.0039911554,0.015723297,-0.0049378816,-9.945249e-05,0.059012026,0.01837122,-0.01211334,0.028653508,-0.015927242,-0.017708983,-0.016945805,0.06055964,-0.051097833,0.020774737,-0.036717225,0.006431653,-0.03672889,0.038260147,-0.01380432,0.0032878846,0.046299424,0.008173668,-0.0066520236,-0.022729209,0.037650093,-0.0061253523,-0.01156911,-0.035017323,-0.050218202,-0.047099218,-0.04456255,0.04631096,0.012678773,0.0747726,0.0095429,0.0022724778,0.04124355,0.012653025,-0.032305285,0.008131435,0.00685533,-0.029875366,-0.0040935394,-0.008835394,0.006773623,-0.034752663,-0.011885055,-0.012775103,-0.04021529,0.054182395,0.008531209,0.04397423,-0.04557137,0.011200486,0.018950688,0.043973584,-0.02858789,-0.0017032867,0.015376948,0.01078415,-0.0017964115,0.00921528,0.02062675,0.06140737,0.048834156,-0.052852523,-0.06596488,-0.02840586,-0.005546157,0.02610846,-0.0058812764,-0.015796755,0.019643547,-0.040711407,0.008364113,-0.0074048396,0.00018040917,0.04012437,-0.028515963,-0.00069545174,-0.033748154,0.017440703,0.0111363875,0.014068714,-0.03657259,-0.008996827,0.037135363,-0.039992742,-0.0465099,0.03156182,-0.027937276,0.022377359,-0.04066792,0.0050276117,-0.009919125,-0.016112894,0.012281762,-0.017412992,0.0147543475,0.06540342,-0.012662752,0.018487943,-0.03193617,0.017867366,-0.02348686,-0.0062895557,-0.054768298,0.059529398,0.06765611,0.017782168,-0.07449432,0.062035806,0.026879052,0.020645738,0.0060085924,0.06114722,0.0004948103,0.043381788,-0.013382237,0.050589398,0.1060838,0.007770361,-0.025920741,-0.041538864,0.014946389,-0.012445453,0.027329417,0.036192074,0.029719306,-0.0077869627,0.0064617763,-0.018047454,0.00058896776,-0.04780519,0.034325343,0.041570976,-0.041199494,-0.011048003,0.043024104,0.045774482,0.016310921,-0.022138283,-0.043879505,0.0123589905,-0.0104549825,0.01722607,0.047866136,-0.04384051,-0.04643758,-0.001081004,0.032186534,-0.0025044733,0.028249208,-0.008149211,0.042558447,0.051939804,-0.01432206,-0.0371905,-0.011844423,-0.03823041,0.043767236,0.021670168,-0.028222444,0.00842673,0.0049692835,-0.044806562,-0.030090574,0.04983166,0.077813864,-0.016106132,0.047324568,0.01921128,0.008641771,0.007812228,-0.040766574,0.042085614,0.019959645,-0.036996827,0.005678938,-0.020528194,-0.0529151,-0.0074774874,0.0023332436,0.06428442,0.084152795,-0.025917491,-0.004075675,0.02240824,0.008232835,-0.019548556,0.006922507,0.030890862,0.053075,-0.04125508,0.0063710837,0.0005561807,-0.079495996,0.030423064,-0.06553222,0.02327367,0.02844918,0.02783047,0.029050868,0.015824052,-0.011962425,0.040922083,0.02618532,0.022995597,-0.036507033,-0.024627319,0.018723091,-0.00959698,-0.018280009,0.040726524,0.0021538022,-0.0034707696,0.043213416,-0.025790602,-0.019143669,0.01450194,0.0063213366,0.020348407,0.0486385,-0.039402973,-0.02904981,0.0014644241,-0.002932826,0.01618971,0.06788382,-0.06797333,0.04044559,-0.016331894,0.02136393,-0.050902773,-0.056343686,-0.024056459,0.05919362,-0.04237635,-0.03850225,-0.024238393,0.055374883,-0.027478006,-0.03236068,0.022726623,0.023843382,0.022885367,-0.068163976,-0.008272481,0.00531904,-0.016570073,-0.016811356,-0.030335119,0.0055851014,0.008484241,0.017043907,0.11496541,0.022527708,0.041302066,-0.03857193,0.038294155,0.004945919,-0.0016077023,-0.019555233,0.010360735,-0.005831535,0.033989716,0.020827936,-0.0145476,0.014816188,0.029673604,-0.050048728,0.02975414,0.04274962,0.016483756,-0.082641,0.07694696,0.028687082,0.033635333,0.03238124,-0.05448446,-0.07354315,0.0070122327,0.026493046,-0.018184379,-0.0055258614,0.008204652,-0.040390648,-0.0055253645,-0.022190938,0.03595147,0.028471494,0.028902406,0.038764413,0.00046958175,-0.053976364,0.017537134,-0.024017619,-0.062428325,0.055193733,-0.07041456,0.037313633,0.07443211,-0.0055210656,-0.0064801495,-0.035993792,0.060120054,-0.026489748,0.023535037,0.046905935,-0.026431762,-0.0068581575,0.00429455,0.018713672,-0.017884733,-0.013073197,0.008667338,-0.011054554,-0.025745487,-0.004713447,0.0059274957,0.0075898296,0.033251714,0.031661425,0.010258533,0.032211233,-0.01902683,-0.029109977,0.040991623,-0.0050602416,0.0601272,0.019894745,-0.012213148,-0.018896034,-0.04438869,0.010393081,-0.031575467,-0.0356788,-0.011306418,-0.005472649,-0.0035361375,-0.007373959,0.04503381,-0.021669772,0.005687077,-0.017836325,0.039280057,0.037448984,-0.008410267,-0.0167148,-0.042196505,0.016535468,0.045650125,0.04090778,0.037077874,-0.031304494,-0.025026977,-0.015446461,0.049826995,-0.0077041998,0.07098953,0.025868554,0.020768551,0.0042151003,-0.056538682,-0.005138403,0.024842834,-0.0077692573,-0.0528211,-0.016523574,-0.035323247,0.03234837,0.043349132,0.025567852,0.002002467,-0.027633876,-0.0026933157,-0.038788415,0.01175803,0.04196048,-0.017739853,0.015156385,0.00066254835,0.0015351633,0.04829907,-0.049502898,0.0021441353,-0.022734705,0.005250856,-0.0048213108,0.0036099376,0.033783097,0.030848024,0.053594954,-0.013482838,0.019001074,0.013932928,0.015425912,-0.02287227,-0.0032310304,-0.058501124,-0.044801395,0.015414105,-0.038306765,-0.06003175,-0.0019518519,-0.056448337,0.013946504,0.021533795,-0.03234278,-0.023253879,0.012074266,-0.045743585,0.09363342,-0.0704492,-0.006310536,0.00040115297,0.02195452,0.0886521,-0.0005344824,0.06525752,-0.030080056,-0.027064338,-0.026943691,0.01869909,0.032894798,0.06953971,0.023419566,0.11187939,-0.0013101201,-0.048021067,-0.032690406,-0.019904204,-0.020520013,-0.06094271,-0.041847125,0.025768105,0.0014376072,0.03866402,-0.015824936,0.0001600445,-0.02514764,0.04140805,-0.07604778,0.06448939,0.02428461,0.004314414,0.0009896861,-0.020053864,0.0540793,0.020968085,0.023201887,0.04267284,-0.024575815,-0.0005979405,0.033352908,-0.033543162,0.0140122175,-0.011233294,0.000605417,-0.03564518,0.006654967,0.010871525,0.025535908,0.0033794807,0.053843915,0.026530687,0.0069384584,0.012218592,-0.04590615,-0.024637295,-0.043412954,-0.039698835,0.0032495218,0.027953684,0.002475148,0.031711314,-0.029936206,0.046702333,-0.10132726,0.009693647,-0.033263136,0.05203094,-0.038605977,-0.01488467,-0.0038968485,0.0061849076,-0.0052692895,-0.038021296,0.030565675,0.02460339,-0.06413332,0.010884017,-0.016169213,-0.01287578,-0.007836101,-0.033888854,-0.006360311,0.020524653,-0.03817791,-0.03402301,0.10006552,-0.022824956,0.012075161,0.0442396,0.013282328,-0.11125775,-0.055235848,0.051286176,0.03166941,0.0075876764,0.053830694,0.0023814386,-0.0084014805,-0.043315124,-0.008495441,-0.025617316,0.0020615496,-0.02355759,-0.047740348,0.024472676,-0.0018557567,-0.06588796,-0.040272664,0.018811429,-0.058836374,0.011416733,0.059070818,0.050585076,0.022142874,0.06909183,-0.044172205,-0.12004888,-0.043566853,-0.0019056341,0.01639639,-0.026236923,0.054499906,-0.01354616,0.018332275,0.04055829,0.02949263,0.0033075835,0.032727603,0.0041271034,-0.026603486,-0.03139137,0.021201266,-0.032157492,0.027339809,-0.004433546,-0.04415465,0.083926804,0.04175561,-0.010217888,0.009053829,-0.034717172,0.03024513,0.03901004,0.012678541,0.04130806,-0.027439103,0.0073019313,-0.023201216,-0.0077831205,0.028175827,-0.046796184,-0.04534344,0.021498192,-0.011824816,0.049045432,0.008884421,0.012122366,-0.096723534,0.04995567,-0.019556766,-0.052534178,0.02246086,0.006291734,-0.020200144,0.0038075023,-0.042056978,-0.003675706,0.0004583857,0.018565625,-0.031822708,0.028561182,-0.0055415793,0.024574751,-0.044481058,0.012997855,-0.03324146,-0.0451147,-0.0005400443,0.025013156,0.005303198,-0.0119031565,0.020884434,-0.013704964,-0.01723843,-0.030777214,-0.008851793,0.04982884,-0.026273133,0.049622428,0.021828895,-0.00043443832,0.08555349,-0.010561825,-0.045565926,-0.010361009,0.005793563,0.028095916,-0.023426564,0.009180743,-0.074601434,-0.070488855,0.0100067295,0.011922882,-0.035223346,0.036790952,-0.029372057,-0.046445534,-0.04893012,-0.07087845,-0.014319526,0.03851123,0.014686036,0.018172204,-0.023585523,0.036265,-0.018898897,0.047436755,0.021338914,-0.0052938163,0.0035246771,0.018324018]	Keywords: DeBERTa, position embeddings, relative position, disentangled style, Transformer-XL\nKey Objects: position embeddings, attention mechanism\nRefers to Images: None\nHypothetical Questions:\n- How does DeBERTas disentangled style of applying position embeddings differ from previous methods?\n- What advantages does DeBERTa gain from incorporating the design principles of Transformer-XL?\n- What role do the learnable parameters W and W, play in DeBERTas attention mechanism?\n---\nSummary:\nDeBERTa utilizes learned position embeddings, similar to Shaw et al., and applies them in a disentangled style, drawing parallels with Transformer-XL.\nOriginal Text:\nDeBERTa [50] utilizes position embeddings like Shaw et al. [116] and applies the embeddings to the model in a disentangled style similar to Transformer-XL [24]  \n$$A _ { i j } = q _ { k } ^ { \\dagger } J + q _ { i } ( r _ { i j } W _ { k, R } ^ { K } ) ^ { T } + k _ { j } ( r _ { i j } W _ { Q, R } ^ { Q } ) ^ { T },$$  \nwhere W K R, W Q, R  R D m  D$\\_{k}$ are learnable parameters and r$\\_{ij}$ is the learnable relative positional embedding as in Eq. (30). The first term is interpreted as a content-to-content attention, and the latter two terms are interpreted as (relative) content-to-position and position-to-content attention, respectively.  \n5.1.3 Other Representations. Some research studies have explored using hybrid positional representations that contains both absolute and relative positional information. Transformer with Untied Position Encoding (TUPE) [63] re-designs the computation of attention scores as a combination  \n$^{9}$the scaling factor is omitted without loss of generality.\nContextualized Text:\nDeBERTa, building upon approaches like Shaw et al., incorporates learned position embeddings and applies them in a unique, disentangled style. This method mirrors the design of Transformer-XL, which also uses a similar approach to handle positional information within the model's architecture.	{"tags": ["architecture", "NLP", "transformer", "positional encoding"], "doc_id": "2ee30529-b245-4f80-95fd-c03e8e37ddd2", "summary": "DeBERTa utilizes learned position embeddings, similar to Shaw et al., and applies them in a disentangled style, drawing parallels with Transformer-XL.", "doc_type": "text", "entities": ["DeBERTa", "Transformer-XL"], "keywords": ["DeBERTa", "position embeddings", "relative position", "disentangled style", "Transformer-XL"], "key_objects": ["position embeddings", "attention mechanism"], "contextual_text": "DeBERTa, building upon approaches like Shaw et al., incorporates learned position embeddings and applies them in a unique, disentangled style. This method mirrors the design of Transformer-XL, which also uses a similar approach to handle positional information within the model's architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["How does DeBERTas disentangled style of applying position embeddings differ from previous methods?", "What advantages does DeBERTa gain from incorporating the design principles of Transformer-XL?", "What role do the learnable parameters W and W, play in DeBERTas attention mechanism?"]}
692a39da-5068-4015-90cb-013e820b6365	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01664739,0.0067575886,-0.013714216,0.052944344,-0.0038841453,0.089850806,0.0010237705,0.08158714,0.015428921,-0.038180217,-0.015574513,-0.004317339,0.011241334,0.004096427,-0.022920258,0.013088885,0.030010553,0.09157955,-0.003860613,0.011413759,0.061428104,0.03196789,-0.014227819,-0.0029282556,0.038717672,0.07196648,0.009124097,-0.032227803,-0.0135574015,-0.0065626763,0.0107897455,-0.045249406,0.011455077,0.020460404,0.059149634,0.016702415,-0.009399425,-0.06776047,-0.026359543,-0.01876198,-0.061964374,0.053986046,-0.081701905,0.024264751,0.02407116,-0.065282345,-0.10432987,-0.007253717,0.03038359,0.0029314128,0.0040184096,0.0074839224,-0.034112394,-0.0056100204,-0.032918632,-0.010071573,0.015435631,-0.03587235,0.041080274,-0.080304295,0.01987903,0.04368636,-0.0029138098,0.014530827,0.02525069,-0.04263062,-0.013767632,0.026001256,0.020247253,0.11050207,-0.08048459,0.018530939,-0.008652793,-0.096479416,0.107100494,0.066095866,-0.016837794,0.032626227,-0.071534716,-0.016109701,-0.00016354767,0.06943035,-0.0067723654,0.023902547,0.09102708,-0.0046800766,-0.028157242,-0.008104429,0.025003204,-0.052471995,-0.008690602,-0.03405248,0.018369658,0.041790284,-0.020338994,-0.02680373,-0.014467662,0.0012442321,0.06529662,-0.06149868,-0.0018770118,0.010885098,0.09791081,0.08289365,0.007364558,-0.027698606,-0.030395214,-0.02859308,0.016684847,0.0041466947,0.024531923,0.048117295,-0.068396606,0.04426373,-0.033260826,0.0022380205,-0.01901479,-0.03524842,0.0065093976,-0.03595522,0.027526138,0.0015599139,-0.011383219,0.012603988,0.049951613,0.029136041,0.031689264,-0.023129934,0.0018045403,0.028357534,0.023573214,0.0050600003,-0.03755645,0.021248652,-0.008996582,0.0124834115,0.051771373,-0.021311644,-0.017387932,-0.009749565,-0.005636251,-0.055030715,-0.02172829,0.034944933,-0.042072464,0.039161146,-0.061362278,0.0062320842,-0.009596764,0.029949233,-0.0030797876,0.008394156,0.043764893,-0.030473595,-0.036001552,-0.05314611,0.032194037,-0.018772792,0.010079088,-0.02498949,-0.055942915,-0.06200403,-0.026231201,0.04575228,-0.0032467104,0.09746157,-0.017091967,-0.010358977,0.0067718257,0.05725019,-0.013902248,0.010976816,-0.017586432,-0.032276057,-0.020604646,0.028764334,-0.007030498,-0.058459856,-0.01412125,-0.0071899653,-0.016310276,0.011829458,-0.008479676,0.036013205,-0.05609147,0.029687958,0.01259897,0.040205017,0.013067324,-0.012103204,-0.012208233,0.0162312,0.0039346986,0.008396898,-0.022610998,0.048037395,0.048700303,-0.02302472,-0.03641685,-0.03839453,0.0037913397,0.044395383,-0.013440934,-0.031974565,0.030182084,-0.04511331,0.015442232,-0.0051549124,-0.0017238427,0.029136201,-0.036009476,-0.024988275,-0.012137826,-0.029198917,0.029755376,0.0032552578,-0.037383117,0.023274945,0.02676417,-0.03223516,-0.020435669,0.047754962,-0.029772146,0.068020195,-0.034249976,0.045446213,-0.022241678,0.0037018498,-0.007721878,-0.006323435,0.0212957,0.035605058,-0.007442506,0.058204837,-0.036747474,0.0050214906,0.007732539,-0.0071751755,-0.041995183,0.03532573,0.05147869,0.027384734,-0.06680018,0.0810355,-0.014383853,0.027184725,0.03026534,0.040368002,0.026900293,0.061358895,-0.0071722316,0.04551953,0.06937558,0.011431517,0.009228401,-0.041042954,-0.020720089,0.010117672,0.037061386,-0.007326389,0.030286072,0.010952634,-0.011480433,0.020007024,0.0147246495,-0.034917727,0.034985177,0.021379549,-0.026500622,-0.016284805,0.04332464,0.060793538,-0.0068479483,0.009845456,-0.057860233,0.03302008,-0.0105506005,0.01800248,0.062305458,-0.05003288,-0.03046751,0.03810821,0.010691101,-0.0026829366,0.04091636,-0.0017328718,0.036688577,0.026494607,-0.024794716,-0.03130957,-0.0276769,-0.01965766,0.05260204,0.0020169022,-0.0041447924,-0.007427132,0.00067505706,-0.0381527,-0.0062868665,0.038181614,0.09578958,-0.0025194199,0.023818927,0.02434016,0.045113057,0.03094581,-0.055280298,0.042285323,0.030760193,-0.03070042,-0.0018977969,-0.028810672,-0.035772007,-0.024753543,0.028392658,0.045497406,0.09558621,0.0153531665,-0.008399776,0.00705717,0.029593118,0.014625139,-0.00052251125,0.022616861,0.05840914,-0.018100703,0.049310833,-0.0022677623,-0.07827188,0.01878033,-0.041124426,0.0569243,0.042159703,0.026494395,-0.0015223698,-0.007561723,-0.021779573,0.04493019,0.022549558,0.011893433,-0.047988556,-0.017692596,0.05136409,-0.0049933386,-0.0020637237,0.05448082,-0.02388788,-0.038397912,0.043326635,0.01260362,-0.008698896,0.026662124,-0.0013324698,0.018220333,0.0030791594,0.0036461693,-0.029864945,-0.010976297,0.011140777,-0.01566037,0.046166275,-0.06884552,0.057957944,-0.02751861,0.033472046,-0.033265494,-0.053320557,-0.026939042,0.06847519,-0.031470384,0.016882706,-0.0019068351,0.050563656,-0.039576676,-0.0042265467,0.026594814,0.05952765,0.039368518,-0.056559462,-0.029723108,-0.014729912,0.00091225916,-0.03217318,-0.036321405,0.026023844,0.032907106,-0.0029278372,0.08845953,0.029956121,0.055960935,-0.041432295,0.00023168421,-0.034525204,-0.022658749,0.020340338,0.018711876,-0.009089696,0.0021673322,0.0014145386,-0.000833672,0.06294408,0.005311565,-0.053668343,-0.009574021,-0.005311789,0.019975001,-0.06685919,0.05295127,0.042648192,0.01904422,0.023235105,-0.035087638,-0.039676696,-0.035378687,0.012911044,-0.021013767,-0.021007707,0.01763843,-0.043471247,-0.02028621,-0.036672406,0.03977597,0.04452542,0.028677505,0.056669656,0.0072733485,-0.08930928,0.018014507,-0.030190868,-0.05496609,0.020058308,-0.033100452,0.025676813,0.06022676,0.016969929,0.021345148,-0.013028754,0.06817967,-0.021083578,0.018794395,0.03154618,-0.020094562,-0.013737107,-0.00879769,0.011748924,-0.0074126087,0.011651005,0.005981859,-0.04179769,-0.018364567,0.008546938,0.017491078,0.009351136,0.0125665525,0.036259018,0.042078014,0.04067248,-0.02182893,-0.051567104,-0.01775651,0.027148888,0.023317777,0.0043992614,-0.01870366,-0.00063513767,-0.06407238,-0.04801228,-0.08200402,-0.022022078,0.004740357,-0.0067093046,-0.0069295648,-0.05105804,0.039821573,0.020997696,-0.0077254577,0.008179344,0.019201564,0.028335057,-0.020366384,-0.015362879,-0.0388723,0.01940536,0.028554743,-0.01498216,0.006147626,-0.018256797,-0.013955767,-0.041773926,0.030487001,-0.0023044995,0.053230807,0.03263996,0.0010862387,-0.0116107445,-0.03180931,-0.02130772,0.02209144,0.012080103,-0.012899523,-0.029323714,-0.029788403,0.018821668,0.069934085,0.009005061,0.0238184,-0.03141173,0.018654948,-0.0574382,0.031909555,0.03538528,-0.023321465,-0.0050638127,0.020938283,-0.00041111614,0.057994258,0.0039282925,0.015497727,0.00818883,0.03338202,0.004149851,-0.0041939393,0.0119066555,0.02630565,0.03034183,0.056383345,-0.013942941,0.026270293,0.014091352,-0.011833199,-0.034523033,-0.064877845,-0.020164307,0.0123742325,-0.048453663,-0.06556197,-0.02483333,-0.08198813,0.013291434,0.0021741178,-0.011232297,-0.0015889258,0.026756555,-0.061356347,0.06037135,-0.0833234,0.015279659,0.012354443,0.0012064206,0.072012916,0.011219184,0.00816524,0.0034658888,-0.05637572,-0.018396875,0.0125339115,0.014405501,0.050926812,0.031469963,0.08584012,-0.025205825,-0.07072227,-0.03423497,0.016155543,-0.026959626,-0.04816674,-0.039042793,-0.02192176,-0.021042697,0.056047145,-0.024954068,-0.012186555,-0.031092945,0.014753999,-0.03239333,0.040968057,0.04832215,0.041487508,-0.0041025872,-0.031179603,0.061838653,0.014406834,0.053079315,0.040231947,-0.01771738,-0.0033222607,-0.045610048,0.018641666,0.010536223,0.041392848,-0.023586044,-0.057725433,0.006754089,0.007834026,0.017337155,-0.002976431,0.035776112,-0.005966534,0.0032571547,-0.04738967,-0.044122912,-0.00052213395,-0.032090236,-0.03044591,0.02721941,0.024504256,0.013998365,0.013620522,-0.03248985,0.04246376,-0.109737255,0.0021745455,-0.081974626,0.039301965,-0.036068358,-0.03498738,0.0016400394,-0.020786393,0.003336885,-0.011997457,0.015247394,0.06274266,-0.025037104,-0.0075569856,0.015905099,-0.01958071,-0.00010876505,-0.027220694,0.02617351,-0.0072749984,-0.03654657,-0.016763508,0.08827049,-0.0103657115,0.03496473,0.01771329,-0.03531675,-0.08270535,-0.058110725,0.08390132,0.005739326,0.007876661,0.015882114,-0.030469326,0.01758213,-0.041349947,-0.023357091,-0.008575165,0.0055311956,0.003817208,0.005352112,0.06146606,0.0015758513,-0.048842754,-0.041537922,-0.014461007,-0.040187728,0.044415604,0.06568588,0.039391477,0.049554147,0.0721243,-0.019884683,-0.08795692,-0.027449997,-0.015124436,0.013977264,-0.022846272,0.06024958,-0.021842925,-0.020561354,-0.0073113684,0.042263772,0.01250183,0.059948448,0.010006851,-0.035839807,0.015226296,-0.015292177,-0.018919848,0.031317633,0.012275941,-0.015077145,0.09751098,0.027872171,-0.0064605772,-0.001553373,-0.007188644,0.0023652688,0.025063016,0.009742174,0.027142966,0.02766346,-0.0070388494,-0.02461781,0.022493884,0.03589981,-0.043647338,-0.037470076,0.035309944,0.010534535,0.036139842,-0.023522232,-0.025938943,-0.092170544,-0.004804964,-0.0057440978,-0.01685513,0.013202565,0.0036436399,-0.03595066,-0.010404993,-0.030529931,-0.043277744,0.022248833,-0.011473196,-0.020837866,0.042082176,-0.022796994,-0.022503115,-0.09274495,-0.003612909,-0.020445796,-0.020759277,-0.011030229,0.030762855,0.010660138,0.033097357,-0.010461311,0.015058531,-0.024138441,-0.024423713,-0.04007394,0.042378128,0.024879929,0.04848994,-0.023407608,0.025965156,0.08406924,-0.051020727,-0.049017,0.03960928,0.024947818,0.018412914,-0.01893994,-0.004074327,-0.07858388,-0.017256381,0.03467941,0.046141043,-0.008233137,0.059556603,-0.017512219,-0.043201156,-0.0702103,-0.037623208,-0.030450832,0.025501419,0.024400644,0.0001227845,-0.042362798,0.0020337892,-0.014612296,0.010910518,0.043531053,0.0088698305,-0.0031834655,0.02620342]	Keywords: positional relationships, attention scores, position embeddings, relative position\nKey Objects: position embeddings, attention scores, relative position\nRefers to Images: None\nHypothetical Questions:\n- How does combining these three terms - content-to-content, position-to-position, and relative position - provide a more comprehensive representation of positional information?\n- What are the advantages of using learnable parameters and position embeddings in this approach?\n- How does this hybrid positional representation compare to approaches that use only absolute or relative position encodings?\n---\nSummary:\nOne approach combines content-to-content, absolute position-to-position, and relative positional relationship terms in attention scores using learnable parameters and position embeddings, allowing for a unified representation of positional information.\nOriginal Text:\n$^{9}$the scaling factor is omitted without loss of generality.  \nof a content-to-content term, an absolute position-to-position term and a bias term representing relative positional relationships  \n$$A _ { i j } = q _ { i } k _ { j } ^ { T } + \\left ( p _ { i } W _ { Q, P } \\right ) \\left ( p _ { j } W _ { K, P } \\right ) ^ { \\top } + b _ { j - i },$$  \nwhere W $^{K,P}$, W $^{Q,P}$  R D m  K are learnable parameters, p$\\_{i}$ , p$\\_{j}$ are the position embeddings for positions i , j , and b$\\_{j}$$\\_{-}$$\\_{i}$ is a learnable scalar relative position embedding.  \nOne can also design a single set of positional representations that express both absolute and relative information. Reformer [124] uses Rotary Position Embedding (RoPE) to represent the position of a token by multiplying the affine-transformed embedding of the t -th input x$\\_{t}$ by a rotatory matrix R$\\_{e}$$\\_{,}$$\\_{t}$  \n$$q _ { t } = x _ { 1 } W ^ { Q } R _ { e, t } \\ \\ k _ { t } = x _ { 1 } W ^ { K } R _ { e, t }$$\nContextualized Text:\nTo represent positional information, a hybrid approach combines a content-to-content attention term, an absolute position-to-position term, and a bias term representing relative positional relationships. This is achieved using learnable parameters (W<sup>K,P</sup>, W<sup>Q,P</sup>) and position embeddings (p<sub>i</sub>, p<sub>j</sub>), along with a learnable scalar relative position embedding (b<sub>j-i</sub>).	{"tags": ["transformers", "positional encoding", "architecture"], "doc_id": "692a39da-5068-4015-90cb-013e820b6365", "summary": "One approach combines content-to-content, absolute position-to-position, and relative positional relationship terms in attention scores using learnable parameters and position embeddings, allowing for a unified representation of positional information.", "doc_type": "text", "entities": ["Reformer"], "keywords": ["positional relationships", "attention scores", "position embeddings", "relative position"], "key_objects": ["position embeddings", "attention scores", "relative position"], "contextual_text": "To represent positional information, a hybrid approach combines a content-to-content attention term, an absolute position-to-position term, and a bias term representing relative positional relationships. This is achieved using learnable parameters (W<sup>K,P</sup>, W<sup>Q,P</sup>) and position embeddings (p<sub>i</sub>, p<sub>j</sub>), along with a learnable scalar relative position embedding (b<sub>j-i</sub>).", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["How does combining these three terms - content-to-content, position-to-position, and relative position - provide a more comprehensive representation of positional information?", "What are the advantages of using learnable parameters and position embeddings in this approach?", "How does this hybrid positional representation compare to approaches that use only absolute or relative position encodings?"]}
08e1c734-555f-4e8f-b403-62b54a2dd6d4	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.023039741,-0.013261221,0.014983457,0.05416766,0.013564939,0.12362558,0.010628675,0.048437547,0.026195604,-0.02376703,-0.019555401,0.00020297997,-0.01622316,0.026364036,-0.04485883,0.03240834,0.0003878244,0.08940173,-0.0033496404,-0.029974964,0.02219096,0.020710573,0.03214568,-0.00457882,-0.014349284,0.060782578,0.023330197,-0.010595249,-0.007085007,0.009860737,-0.0019116037,-0.041415717,0.021223603,0.028038116,0.041589007,0.00071886875,0.010354439,-0.042483736,-0.035743374,-0.018500412,-0.045809463,0.06858452,-0.08841059,0.03820426,-0.008593007,-0.055371247,-0.08471701,-0.05611172,0.01212809,0.01753225,0.008720789,0.022568582,-0.038635973,-0.018171702,-0.042808704,0.029134553,-0.0014721986,-0.055890143,0.027168911,-0.0897276,0.022095324,-0.0017973233,-0.029777022,-0.008835576,-0.006226158,-0.028343609,0.0010591084,0.011428228,0.00818103,0.1267428,-0.054093562,-0.009844299,0.0036801503,-0.084205836,0.09314214,0.0730026,0.014212541,0.011278012,-0.0427577,-0.019765865,0.050496127,0.042395063,0.0032082861,-0.033458218,0.10818464,-0.0018596929,-0.017826838,0.0050631347,0.05558591,-0.066926226,0.018099124,-0.009487656,0.0040096054,0.07898534,-0.011259323,-0.039557993,-0.041063767,-0.023699716,0.029568559,-0.06503302,-0.01103272,0.011009522,0.07151484,0.08468319,-0.0038444893,-0.02061962,-0.03059739,-0.010126355,0.0049036937,0.01801391,-0.019498346,0.04162341,-0.06501849,0.013013967,-0.027497338,-0.011363029,-0.015772913,0.0156165585,-0.009328329,0.0052341307,0.033430062,-0.011504063,0.016728522,0.013684804,0.04284789,0.055460587,0.040064637,-0.040028844,-0.024984352,0.019829912,0.016855525,0.03809454,-0.044574652,-0.013892771,-0.010379478,-0.0008772136,0.049173076,-0.025911544,-0.0070355246,0.040027086,-0.00093078444,-0.038860966,-0.023819758,0.017675545,-0.033008974,0.02801126,-0.043153405,-0.018799864,-0.005555421,0.043325115,0.031000094,0.028192347,0.03809216,-0.013161124,-0.053138096,-0.019484285,0.049661536,-0.02427023,-0.005065308,-0.02941144,-0.043954257,-0.030213656,-0.034428563,0.040653896,-0.010134014,0.086369,-0.03130381,-0.0019845634,0.035199482,0.043078046,-0.030914139,-0.01580115,-0.005424512,-0.008852798,0.027658798,0.034021508,-0.017016077,-0.05785631,-0.01428465,-0.032356888,-0.0064268066,0.034780707,0.02099411,0.023837537,-0.063100874,0.0029971045,-0.0063440986,0.05793756,-0.028588725,-0.022901887,0.020208322,-0.005626962,-0.005034315,0.011637087,-0.01548772,0.05863188,0.05296322,-0.054647367,-0.062060688,-0.009164541,0.011197618,0.03552147,0.0053952355,-0.022355562,0.08283058,-0.051263448,0.021898085,0.0064370856,-0.015016007,0.012025028,0.010305479,-0.016343875,-0.041862197,0.00760686,0.0010752722,-0.01314961,-0.011624347,-0.006101707,0.017375072,-0.045106772,-0.043541145,-0.00024787924,-0.08138493,0.015915414,-0.036915693,0.027997272,-0.019967256,0.0051925615,0.01207382,0.010203277,0.053031452,0.0316956,-0.006594433,0.028365435,-0.020939281,-0.02146199,0.016909456,-0.0082607465,-0.053449187,0.028830016,0.07419521,0.04175505,-0.051955625,0.07612395,0.015744235,0.05346956,0.021856906,0.027192697,0.014100974,0.037248768,-0.060279973,0.021241087,0.06958014,0.0061838133,-0.0031659633,-0.06419299,-0.024987938,0.025292993,0.0126235355,0.01568945,0.029043952,0.0025646442,0.0024956998,-0.0027204603,-0.00502412,0.022690209,0.03808482,0.021791894,-0.019855184,-0.032000814,0.051365517,0.07968908,0.012379855,-0.018856632,-0.026751112,0.015504919,0.025081532,-0.00590244,0.0361197,-0.012628504,-0.0081800455,-0.011896992,0.046183486,0.01059381,0.0067668553,-0.0055547245,0.039901495,-0.004041812,-0.006872444,-0.025027897,-0.0313783,-0.02911495,0.017421287,-0.0035266334,-0.023584744,-0.0030207396,-0.017202694,-0.07583321,-0.036596965,0.013288135,0.14035697,-0.03916592,0.03576088,-0.011142516,0.012455634,0.024293808,-0.054273296,0.028739797,0.026980627,0.0031703273,0.019938914,-0.00079120963,-0.012462788,0.012019572,0.06434418,0.078588545,0.07753178,-0.00448933,-0.0012729265,-0.0025632274,0.030738488,0.04311537,0.007553359,0.01970039,0.025376583,-0.023787856,0.028424885,-0.006036953,-0.06597126,0.045262266,-0.03308367,0.04122677,0.06647528,-0.0048422567,-0.011082645,0.019101145,0.010504536,0.056185845,0.017506365,-0.011992857,-0.009511409,-0.032274447,0.017459827,-0.030073868,-0.014711061,0.055701118,-0.00822655,-0.021473164,0.02624667,-0.034897774,-0.003409324,0.023292167,0.025009548,-0.0011183051,0.018615687,-0.005177694,-0.013231666,0.0045590056,-0.014016965,-0.011606561,0.051085405,-0.07316817,0.039880265,-0.039751887,0.026252259,-0.033285413,-0.056118693,0.005278757,0.027124835,0.0056961225,-0.004712047,-0.025099365,0.053430643,0.013799883,-0.036865268,0.024594598,0.06076787,0.022071965,-0.017863626,-0.019225935,-0.045808934,-0.0037001257,-0.052794836,-0.048757184,0.004411029,0.01939437,-0.014652421,0.09711944,0.027240707,0.03639457,-0.059862673,0.0115448795,0.018615512,-0.013446907,-0.0073694023,0.021456666,-0.007850263,0.04589854,0.015757643,-0.012460982,0.044470485,0.0132265985,-0.058960795,0.0005521599,0.014794044,0.043324497,-0.07974317,0.015045718,0.061918728,0.026864542,0.017111344,-0.028804014,-0.015407112,-0.021599926,0.018425882,-0.02996959,-0.023012584,0.032492466,-0.03910126,0.0071497033,-0.03519326,0.0005405867,0.010910326,0.02154114,0.032245792,0.007602669,-0.043414645,0.02011541,-0.020325044,-0.070550755,0.057262655,-0.031229006,0.0076838313,0.06281714,0.017701384,-0.014680905,-0.03812699,0.08813277,-0.02462791,0.028826272,0.056035742,-0.027852451,0.041978016,0.015903112,0.037151504,0.010519686,0.043787137,0.0042399056,-0.020877086,-0.02558054,0.007248225,0.010393471,-0.0053141224,0.013653572,0.04253341,0.008097264,-0.012017546,0.001721851,-0.024195088,-0.020790363,0.007981181,0.0453687,-0.0019207094,0.002144346,-0.03298014,-0.020279583,-0.033436827,-0.02803734,-0.015933875,-0.0019145783,0.010019644,0.0007167632,-0.0359658,0.060503427,-0.016239366,-0.06219087,0.018338222,0.035560578,0.023968326,-0.015576716,-0.026143592,-0.028065633,-0.008191345,0.027364267,-0.0021803807,0.063867204,-0.034876905,-0.039237414,-0.034316983,-0.014244398,0.011119213,0.035694886,0.0069246446,0.049700756,0.03976902,-0.033053726,0.024596034,0.0028913037,0.039840907,-0.03636036,-0.043304887,-0.0076607326,0.045328803,0.063968755,-0.005197729,0.014220279,-0.005272481,0.019923404,-0.014443062,0.04837784,0.00746551,-0.023125596,-0.02208716,-0.0013070407,-0.021168876,0.0005400511,-0.056298465,0.024575286,-0.023835214,0.033876203,-0.006837031,-0.059057754,0.012517875,0.034691464,0.025471667,0.024314716,0.043819834,0.011177235,0.043203507,-0.022113413,0.050880935,-0.030180542,-0.032971337,0.023956714,-0.011262958,-0.056417637,-0.024574483,-0.072046086,0.018788142,-0.012078972,-0.0052565844,-0.041759595,0.0096132755,-0.08287725,0.105442576,-0.021374036,0.006204494,0.019093782,-0.014723491,0.10299701,0.0025188953,0.0051535177,-0.027592104,-0.07069701,-0.012533206,0.03697493,0.06705293,0.028671348,0.0027221611,0.08862122,0.004893767,-0.019716116,-0.038498275,-0.028432123,-0.0017911951,-0.04532338,-0.055543188,-0.0116401175,-0.028394261,0.020430416,-0.009655565,-0.015956448,-0.043091957,0.015043791,-0.022594012,0.043779697,0.018826306,0.027226647,-0.0039731413,-0.022374699,0.026415648,-0.004869931,0.05212512,0.0401083,-0.016615327,-0.00076914346,-0.020319132,0.02155771,-0.0195635,0.019101461,0.014421208,-0.06846186,-0.017402485,0.018894725,0.0153137855,-0.01654161,0.07140045,0.02520816,-0.045414675,0.0019909206,-0.025816388,-0.00934672,-0.015092397,-0.04695917,-0.022746952,0.002357117,-0.01354512,0.038611718,-0.058972813,0.05770837,-0.07406639,-0.02257111,-0.06996919,0.026066365,-0.022671837,-0.03902293,-0.030194238,-0.043824986,-0.022724248,0.018868338,0.030595025,0.038672432,-0.03165306,-0.021927696,-0.021762546,0.009941178,-0.011766098,-0.022244874,0.010107253,-0.009782306,-0.029336926,-0.014450621,0.083183035,0.034378834,0.016323255,0.017893149,-0.003032152,-0.088618465,-0.0420276,0.10375212,0.045053814,-0.010071253,0.029438993,-0.03259624,0.017052105,-0.03579227,-0.075627625,-0.0005704155,-0.007427659,0.0076968134,-0.00045393722,-0.0007681223,-0.009202921,-0.057250917,-0.01914186,0.00813993,-0.08966229,0.04987722,0.048135683,0.060330864,0.04219388,0.08231512,-0.056580033,-0.08017632,-0.03240512,-0.040849194,-0.0037377751,-0.04472343,0.03771473,0.008217892,-0.014659301,0.010277799,-0.00030419428,0.015653523,-0.0051217265,-0.03673402,-0.027397888,-0.014540595,-0.0027703391,-0.0025323052,0.036542706,0.003214554,-0.0067842924,0.05900691,0.04803622,-0.042423025,-0.013244019,-0.052890405,0.0028165877,0.00970979,0.0003406106,0.026582686,0.0037727796,0.011207813,0.04737317,0.016777867,0.0050629936,-0.060350202,-0.06406224,0.0119719785,0.017304717,0.012265506,-0.021714246,-0.00039632237,-0.07848738,0.028479302,0.012818277,-0.027267454,-0.003982774,0.02652104,-0.045011263,-0.011051288,-0.039835528,-0.024042858,0.03413849,0.0053076902,-0.013434235,0.024872296,0.016004939,0.0036706862,-0.08928179,-0.027998917,-0.004800265,-0.033957094,0.032146588,0.058770865,-0.025401445,-0.00761287,0.02277914,0.020718131,-0.011443637,0.013977408,-0.012055802,0.05435142,-0.0172822,0.031150546,-0.048000433,0.03361708,0.07355081,-0.019289566,-0.017351782,0.019095307,0.0007001985,0.0026861185,-0.026648978,-0.007834509,-0.04541339,-0.045134164,0.007437558,0.03567248,0.015607637,0.014682205,-0.0012492398,-0.028212015,-0.07708738,-0.047911327,-0.025964096,0.039059136,0.05036068,-0.0062157274,-0.03625564,0.049511354,-0.030314632,0.07810432,0.0010267575,0.0022065004,-0.013700341,0.0068443865]	Keywords: Rotary Position Embedding, RoPE, translation invariance, positional encoding\nKey Objects: Rotary Position Embedding, Rotatory Matrix, Attention Scores\nRefers to Images: None\nHypothetical Questions:\n- What does it mean for a representation to be 'translation invariant'?\n- How does the rotatory matrix encode positional information?\n- What are the advantages of using RoPE compared to traditional positional encodings?\n---\nSummary:\nReformer utilizes Rotary Position Embedding (RoPE) to represent token positions by multiplying embeddings with rotatory matrices, resulting in a translation-invariant representation where attention scores depend on relative position offsets.\nOriginal Text:\n$$q _ { t } = x _ { 1 } W ^ { Q } R _ { e, t } \\ \\ k _ { t } = x _ { 1 } W ^ { K } R _ { e, t }$$  \n$$R _ { e, t } = \\bigoplus _ { j = 1 } ^ { D _ { k / 2 } } \\mathbf M ( t, \\theta _ { j } ),$$  \nwhere D denotes direct sum of matrices. Each M ( t, $\\_{j}$ ) is a 2-D clockwise rotatory matrix of angle t  $\\_{j}$  \n$$M ( t, \\theta _ { j } ) = \\left [ \\cos ( t \\cdot \\theta _ { j } ) \\ \\sin ( t \\cdot \\theta _ { j } ) \\right ] \\quad \\quad \\quad ( 3 7 )$$  \nThe key advantage of this formulation is that the induced representation is translation invariant, i.e., the attention score of ( q$\\_{k}$, j$\\_{k}$ ) is only related to their relative position offset  \n$$q _ { k, j } ^ { T } = \\left ( x _ { i } W ^ { Q } \\right ) R _ { e, j - i } ( x _ { j } W ^ { K } ) ^ { \\top } \\,.$$\nContextualized Text:\nReformer employs Rotary Position Embedding (RoPE) to encode token positions. This method multiplies token embeddings with rotatory matrices (R,), resulting in a representation that is translation invariant.  This means the attention score depends only on the relative position offset between tokens, not their absolute positions. This is achieved through the use of rotatory matrices and is a key component of the Reformer architecture.	{"tags": ["architecture", "transformer", "positional encoding", "representation"], "doc_id": "08e1c734-555f-4e8f-b403-62b54a2dd6d4", "summary": "Reformer utilizes Rotary Position Embedding (RoPE) to represent token positions by multiplying embeddings with rotatory matrices, resulting in a translation-invariant representation where attention scores depend on relative position offsets.", "doc_type": "text", "entities": ["Reformer"], "keywords": ["Rotary Position Embedding", "RoPE", "translation invariance", "positional encoding"], "key_objects": ["Rotary Position Embedding", "Rotatory Matrix", "Attention Scores"], "contextual_text": "Reformer employs Rotary Position Embedding (RoPE) to encode token positions. This method multiplies token embeddings with rotatory matrices (R,), resulting in a representation that is translation invariant.  This means the attention score depends only on the relative position offset between tokens, not their absolute positions. This is achieved through the use of rotatory matrices and is a key component of the Reformer architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["What does it mean for a representation to be 'translation invariant'?", "How does the rotatory matrix encode positional information?", "What are the advantages of using RoPE compared to traditional positional encodings?"]}
ff9f65af-e55f-46ce-9220-b3c023c37cab	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.023717085,-0.01933422,0.0076622036,0.050696477,8.726279e-05,0.09399855,0.01396538,0.055648938,-0.015051673,-0.029520612,-0.027606824,0.0017544721,-0.0016057575,0.025023196,-0.028982228,0.03426815,0.041994765,0.086439766,0.001212833,-0.022215646,0.024083983,0.028828701,0.0559509,-0.02669083,0.016373891,0.048794087,0.0215616,-0.005536313,-0.006751308,0.022023201,0.019312,-0.048814483,-0.009054451,0.024617989,0.04150276,-0.037192494,0.01123467,-0.07436159,-0.036752064,0.0072122496,-0.011494133,0.058229297,-0.107691534,0.01931586,0.013388712,-0.091718785,-0.10590122,-0.039633796,0.020233696,0.029408218,0.00859058,0.018311724,-0.019871458,-0.02458763,-0.014725831,0.029455582,0.0016758136,-0.040279955,0.034917437,-0.10115618,0.027218208,0.005995506,-0.02825351,-0.004151763,0.00048649876,-0.012057164,0.009559128,0.023831105,0.018204508,0.1231274,-0.06731249,-0.012093114,-0.0015664756,-0.07255657,0.08411245,0.05432189,-0.0047998047,-0.01813568,-0.05330811,0.01708027,0.031186383,0.031224461,0.03022115,-0.03152365,0.08480345,-0.011497476,-0.031136937,0.0084521845,0.05280153,-0.05510522,0.007657509,-0.0116844475,0.023371756,0.07437751,-0.028250124,-0.05557462,-0.03965044,-0.011558023,0.033618093,-0.06459861,-0.0025062526,-0.016496474,0.05419257,0.095827766,-0.0022785834,-0.035988893,-0.0010262225,-0.022514151,0.021367608,0.0146290185,-0.02294136,0.03725175,-0.0337196,0.011785348,-0.03416495,0.020412946,-0.021736948,0.02609113,-0.008760987,0.00017729813,0.04985718,-0.031894762,-0.00017870494,0.03865552,0.04728387,0.060125757,0.04152247,-0.033192348,-0.0329161,0.035336055,0.005079754,0.04992623,-0.07038719,-0.015217901,-0.029582933,0.018315574,0.047087897,-0.047250625,-0.014645476,0.04178596,-0.006557188,-0.044853345,-0.023497602,0.026581129,-0.031458598,0.008554816,-0.057382833,0.012929119,-0.012425158,0.053724013,0.016527724,0.012599877,0.02533512,0.00014883112,-0.032635137,-0.004949172,0.032369904,-0.027772747,0.006028262,-0.009947328,-0.029998552,-0.047285236,-0.027820973,0.031790476,-0.00670243,0.079033546,-0.013870662,-0.012835046,0.035571862,0.022787448,-0.019186802,-0.011195868,0.0017000358,0.0065484974,0.010031405,0.040383957,0.004172814,-0.058589518,0.013827624,-0.036784794,-0.03639162,0.023962827,0.006975965,0.024432702,-0.059536558,-0.0052955286,0.020147095,0.030540744,-0.036191735,-0.017811876,0.0033947693,0.006562276,-0.0045528556,0.006227263,-0.027604055,0.043669045,0.04376182,-0.049454592,-0.0508339,-0.011448422,0.0073762066,0.022445878,0.0058067273,-0.004263093,0.06868775,-0.062173773,0.031017393,-0.006571101,-0.020130193,0.010182026,0.0073710415,-0.016040746,-0.033779606,-1.1380848e-05,-0.014729203,-0.0110535,-0.0034333025,-0.02419856,0.025181508,-0.017051404,-0.040933814,-0.005207042,-0.036306184,0.020930152,-0.04465408,0.05804137,-0.009757201,0.012840272,0.00536334,0.020275874,0.025840111,0.04468267,-0.023651175,0.041897986,-0.0466571,-0.010629329,0.021010706,-0.029626267,-0.04787209,0.01771002,0.06000025,0.072272964,-0.035868086,0.063685164,0.010568297,0.022336533,0.02659759,0.0028269582,0.030865708,0.03931163,-0.051916655,0.042539008,0.083864816,0.008031225,0.017646722,-0.04711667,-0.027076615,0.017167684,0.02656362,-0.010789409,0.021605035,-0.0027342252,-0.0024941766,0.012632705,0.0012629447,-0.030343803,0.029987209,0.030993678,-0.015904048,-0.017259676,0.058517627,0.083716445,0.017757924,-0.016145457,-0.006129721,0.031890787,0.019967072,0.012005791,0.010698216,-0.04154579,-0.029787563,-0.013056849,0.027470157,0.003973353,0.016178416,-0.007839026,0.02055586,-0.0011154143,0.0009917027,-0.018110402,-0.039205555,-0.03735364,0.037636545,-0.006249748,-0.037398174,-0.0064498554,-0.020790488,-0.06537106,-0.03804831,0.052142747,0.13470219,-0.009691409,0.036397923,0.012449024,0.01420934,0.03363762,-0.030576952,0.048966736,0.035495356,-0.0071777366,-0.0059902254,-0.033062648,-0.03367298,-0.018949918,0.020556467,0.08144544,0.096773736,0.0189044,0.0062275263,0.01937635,0.050998498,0.053428788,0.0006204259,0.04689912,0.03383891,-0.028064247,0.03844994,-0.015201935,-0.06271411,0.055337675,-0.038701776,0.03468018,0.06753607,0.0054942505,0.0057467297,0.007833419,0.001835085,0.03891393,0.0048293592,-0.003147805,-0.026844254,-0.033786092,0.007200874,-0.019793564,-0.006463082,0.018429067,-0.006177236,-0.021640344,0.032836672,-0.034515794,0.010927187,0.02009536,0.012143679,0.0016408948,0.012045684,0.008325508,-0.0064175995,-0.009977093,-0.027840355,-0.008758534,0.07556547,-0.07386394,0.04468815,-0.025612399,-0.0034788123,-0.02773067,-0.04497278,-0.0148622105,0.0385035,-0.02816214,0.0024198121,-0.02130677,0.07635503,0.009464029,-0.0036681679,0.028916402,0.060178068,0.030029561,-0.018728867,-0.010307093,-0.02212,-0.011005914,-0.04407959,-0.05178885,0.0077982475,0.02439922,-0.020660715,0.09630612,0.010001158,0.04667662,-0.05309933,0.0017668613,-0.021589149,-0.015248607,-0.0040070424,0.005967162,0.019079264,0.038548883,0.024312543,-0.0041039083,0.045442913,0.0035024018,-0.057640865,-0.0055548865,0.026213339,0.05454855,-0.060090378,0.02726061,0.059789926,0.02100146,0.0006578397,-0.03964492,-0.020500075,-0.019707328,0.02899618,-0.012133231,0.001994788,0.037493523,-0.049569003,-0.0057428814,-0.04074055,0.012208317,0.04009703,0.028190563,0.056045774,0.010516542,-0.07395006,0.025878109,-0.009376716,-0.059637707,0.028309472,-0.022386178,0.0057661217,0.07098188,-0.0029852756,-0.004164123,-0.044055246,0.073477544,-0.04029515,0.030049035,0.05122271,-0.058968678,0.04960838,0.02080708,0.030575918,0.01138296,0.034225926,0.028669529,-0.008547247,-0.018086823,0.019282779,0.010048695,-0.010943297,0.015432096,0.042832516,-0.0083155865,0.022845497,-0.010852876,-0.023139207,-0.029906513,0.0041614473,0.061192818,0.0037226358,0.007830752,-0.0034958557,-0.032817706,-0.0374223,-0.054176398,-0.0097453585,-0.0067136423,0.0024148629,0.0046990695,-0.04993914,0.03574387,-0.017283088,-0.02263597,0.07083141,0.017863898,-0.0017647517,0.024996953,-0.023857016,-0.0424231,-0.008516494,0.018877316,-0.0071380106,0.021738982,-0.0010578358,-0.032088548,-0.046270102,0.010776484,0.02472032,0.01805068,0.018014176,0.038737763,0.033464245,-0.03933095,0.017310563,0.024665324,0.05893513,-0.054244068,-0.035054967,-0.049335495,0.00030805197,0.07799352,-0.0071533937,-0.0120121185,-0.025683748,0.026813472,-0.021860646,0.07781842,0.018169764,-0.029008314,-0.011815157,-0.0041028536,-0.0023257206,0.027264059,-0.024768623,0.02414925,-0.013931149,0.028174821,0.011798315,-0.02455336,0.01925481,0.0168102,-0.014950392,0.04095557,0.048716094,-0.00375412,0.025372952,-0.027490353,0.03088547,-0.022392767,-0.003418439,0.010944325,-0.026605459,-0.06346528,-0.0411435,-0.06641073,0.026378786,0.012229017,-0.018508509,-0.024967022,0.044292267,-0.09656075,0.089459166,-0.06341188,0.04125382,0.004055828,-0.016594592,0.08700918,0.029332139,0.008030609,-0.029987752,-0.066937424,-0.021043187,0.011465015,0.06030835,0.019851854,0.011022261,0.079253495,0.0028725122,-0.05212518,-0.043086857,-0.01571478,-0.003255446,-0.037500657,-0.039506547,0.022237355,0.006383448,0.044504605,-0.0079075,-0.009933134,-0.025319144,0.032190837,-0.036955196,0.05298344,0.0038322657,0.02452761,0.0069349892,-0.02434652,0.037229557,-0.0022499163,0.034407277,0.013681833,-0.0056977854,-0.0036421458,-0.038418874,0.008477768,0.0035238655,0.029233534,-0.024588788,-0.05021498,0.001626259,0.019420046,0.01697551,-0.018227974,0.06429077,0.021748735,-0.017417831,0.01533292,-0.05295936,-0.026231378,-0.026227554,-0.06440519,-0.007900117,0.0019371398,-0.014281036,0.045611225,-0.04041787,0.070565805,-0.066318326,-0.0075004366,-0.042679068,0.034835283,0.015646629,-0.03917776,-0.044643536,-0.057293333,0.0018436234,-0.008618044,0.024818394,0.052989934,-0.017423248,0.00727186,-0.010264582,0.020485066,0.030024027,-0.010845538,0.020784412,-0.012423223,-0.01924713,-0.015268866,0.08608931,-0.0010912742,0.021323744,0.004979782,-0.00018695714,-0.063485704,-0.0326889,0.072176516,0.013333948,0.004457228,0.016154798,-0.034085814,0.022518847,-0.040335514,-0.048521873,-0.026146663,-0.019844832,0.019848423,0.016155425,-0.0055565382,-0.008581408,-0.06119801,-0.027650166,-0.011516149,-0.09331034,0.019433396,0.06166027,0.072414845,0.07037662,0.088800676,-0.026619457,-0.061653554,-0.022849688,-0.03485379,-0.007050538,-0.049155213,0.034774523,-0.013980243,-0.014311621,0.019125616,0.018450378,0.02238264,-0.0045853998,-0.05336728,-0.011594889,-0.020266242,0.01988107,-0.024026262,0.02332833,0.013122076,-0.0077040414,0.057667375,0.04200508,-0.027446235,-0.018162334,-0.033383667,-0.019737922,0.008700146,-0.031257782,0.0040682377,0.016715502,0.013190713,0.032629505,0.025704378,0.013924118,-0.06657273,-0.0546277,0.025783416,0.007191255,0.052660223,-0.015483823,-0.007840989,-0.10270696,0.017222606,-0.012109606,-0.029248146,0.020175112,0.0058088046,-0.046757873,-7.584404e-05,-0.019838622,-0.040331606,0.034702417,-0.017408466,0.0062198336,0.04501896,0.020476332,-0.026875906,-0.0876234,-0.02318933,-0.0076200934,-0.036147516,0.014950379,0.051883124,0.007310348,0.007945554,0.000350305,-0.023380343,-0.045373075,0.011864863,-0.020063771,0.0390212,-0.013619445,0.042916376,-0.011320868,0.013550257,0.127439,-0.020109026,0.016819255,0.025707707,0.017086355,0.010768076,-0.011780216,0.019087788,-0.04881698,-0.050482154,0.029808713,0.059378132,0.0054679704,0.03261581,0.021534601,-0.02482493,-0.046365496,-0.03853568,-0.02897518,0.05592903,0.052747898,0.010875339,-0.037183788,0.041646674,-0.013796792,0.06245785,0.0303474,0.012950941,-0.012427752,0.029767297]	Keywords: rotary position embedding, relative position, linearized attention, translation invariance\nKey Objects: Token Positions, Rotatory Matrix, Relative Positional Relations, Embedding Matrix Multiplication\nRefers to Images: None\nHypothetical Questions:\n- How does RoPEs translation invariance benefit transformer performance?\n- Why is it advantageous for RoPE to be compatible with linearized attention?\n- In what ways does RoPE's approach to positional encoding differ from traditional absolute positional encodings?\n---\nSummary:\nRotary Position Embedding (RoPE) utilizes a rotatory matrix to represent token positions, allowing for the capture of relative positional relations while being compatible with linearized attention and exhibiting translation invariance.\nOriginal Text:\n$$q _ { k, j } ^ { T } = \\left ( x _ { i } W ^ { Q } \\right ) R _ { e, j - i } ( x _ { j } W ^ { K } ) ^ { \\top } \\,.$$  \nIn practice, the embedding matrix multiplication can be implemented by two element-wise multiplication for lower memory footprint. The RoPE uses the form of absolute embedding but can capture relative positional relations. This approach is compatible with linearized attention in Sec. 4.2.  \n- 5.1.4 Position Representations without Explicit Encoding. Instead of explicitly introducing additional positional encodings, Wang et al. [140] propose to encode positional information in word embeddings, by generalizing embedding to continuous (complex-valued) functions over positions. R-Transformer [144] model locality of sequential data with a local RNN. Specifically, inputs to each block of R-Transformer are first fed to a local RNN and then to multi-Head self-attention module. The RNN structure introduces ordering information and captures local dependencies as a complement to self-attention.\nContextualized Text:\nRotary Position Embedding (RoPE) is a technique used in transformers. It uses a rotatory matrix to represent token positions, enabling the capture of relative positional relations. A key advantage is that its compatible with linearized attention and exhibits translation invariance, meaning attention scores are only related to the relative position offset between tokens. This allows the positional representation to function as an absolute embedding while still capturing relative information.	{"tags": ["NLP", "transformers", "positional encoding", "architecture"], "doc_id": "ff9f65af-e55f-46ce-9220-b3c023c37cab", "summary": "Rotary Position Embedding (RoPE) utilizes a rotatory matrix to represent token positions, allowing for the capture of relative positional relations while being compatible with linearized attention and exhibiting translation invariance.", "doc_type": "text", "entities": ["RoPE", "R-Transformer", "RNN", "ViT"], "keywords": ["rotary position embedding", "relative position", "linearized attention", "translation invariance"], "key_objects": ["Token Positions", "Rotatory Matrix", "Relative Positional Relations", "Embedding Matrix Multiplication"], "contextual_text": "Rotary Position Embedding (RoPE) is a technique used in transformers. It uses a rotatory matrix to represent token positions, enabling the capture of relative positional relations. A key advantage is that its compatible with linearized attention and exhibits translation invariance, meaning attention scores are only related to the relative position offset between tokens. This allows the positional representation to function as an absolute embedding while still capturing relative information.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["How does RoPEs translation invariance benefit transformer performance?", "Why is it advantageous for RoPE to be compatible with linearized attention?", "In what ways does RoPE's approach to positional encoding differ from traditional absolute positional encodings?"]}
6a0e338a-0cf4-4c02-bd9d-88b6785375b7	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.019367442,-0.016828261,0.033507936,0.035277266,-0.00414597,0.048177905,-0.04189612,0.10855728,0.0286244,-0.03744609,-0.006812932,0.04642662,0.0011691827,0.016266797,-0.030903166,0.050686147,0.006576972,0.06904769,0.036115207,-0.01674978,0.03339675,-0.010082855,0.008883696,-0.017058255,0.052358426,0.063839614,0.031587534,-0.027589906,0.0019008723,0.011798824,0.040651273,-0.021442961,0.028318645,0.022207305,0.013466554,0.022079838,-0.0013707584,-0.030773213,-0.0030109964,-0.03074046,-0.0015428702,0.08969735,-0.07312754,0.00047022314,0.03686634,-0.021526124,-0.060813624,-0.06816984,0.019415492,0.013087608,0.019144738,-0.020563366,-0.025061576,-0.036393818,0.008869192,0.0010821279,0.003970534,-0.037484705,0.014261141,-0.08626854,-0.0005395437,0.01318953,-0.039750014,0.050558876,0.014126524,-0.04857334,-0.000616731,0.045606013,0.008202264,0.13141067,-0.039938696,0.0017149871,0.01390748,-0.06741118,0.09607003,0.055198733,-0.05339955,-0.05406232,-0.032020863,-0.03310189,0.019673709,0.03298684,-0.019255605,-0.037906673,0.045785155,0.013445945,-0.027101438,0.002978299,0.045375466,-0.01214217,0.010527652,0.035160366,0.002078762,0.055064697,-0.0021452429,-0.06639447,0.016209621,-0.006833624,0.019836016,-0.0053225057,0.035211474,-0.044102453,0.08325904,0.08303973,0.040320873,-0.0028385904,-0.035548694,-0.05703557,0.0014467265,0.010983831,-0.056768373,0.011066227,-0.0648048,0.036646787,-0.057261825,-0.011110974,-0.034774367,-0.0042294334,0.023675237,-0.010809589,0.0024953887,-0.05111663,-0.02220076,-0.0007636714,0.058061983,0.041153386,-0.0072397185,0.015405971,-0.017473213,0.025731517,0.0189473,-0.0049481383,-0.03371075,0.013561132,0.01639426,0.012733921,0.06680695,-0.0074417307,0.0042750644,0.018875975,-0.008512927,-0.024894454,-0.032924857,0.041310564,-0.03742947,0.024434011,-0.026087519,0.031439293,-0.02930877,0.0111374315,0.0021255484,0.046651255,0.04778023,-0.014984043,-0.0120916935,0.006651594,0.0119509995,-0.013069519,-0.008440134,-0.074217364,-0.05948107,-0.03443471,-0.07587328,0.03968376,-0.0030713317,0.08515673,0.0045343773,0.04032519,-0.003646085,-0.018292006,-0.05873918,0.002542096,-0.007402298,-0.047348537,0.02204365,0.017309217,0.00081414095,0.0024404894,-0.001998679,-0.04621616,-0.013816544,0.008790614,0.007566423,0.020102235,-0.04364313,-0.010527045,0.02938575,0.042188782,-0.01973886,0.02834423,0.008859394,0.008739154,0.01430932,-0.014980438,0.022477193,0.031133667,0.032569487,-0.06361346,-0.043136515,0.020906735,-0.028407268,0.01767018,-0.041891076,-0.030693311,0.07332791,-0.021660678,-0.006674862,-0.032802638,-0.048243303,0.0064609908,0.0015656187,-0.04404345,-0.029153189,-0.0012451654,0.03724684,-0.0075118053,0.008896393,0.029810887,0.0031228883,-0.027193727,-0.035484083,0.00927643,-0.037620515,0.031324055,-0.068445556,0.033027604,-0.027380757,0.013826781,0.007641229,-0.027377069,-0.003943661,-0.0017166479,-0.007867234,0.01949007,-0.026307262,0.06784141,0.002867106,0.011456111,0.014387575,0.020687029,0.096485056,0.007834712,-0.08173344,0.04888623,0.0027584855,-0.0038824447,-0.003474208,0.037123255,0.020014584,0.043306954,-0.02459514,0.04304772,0.069538325,0.006750226,-0.008326351,-0.013452417,-0.018162481,0.034908317,0.027964842,-0.011032385,0.010296397,0.0070368615,-0.041767314,-0.011170774,-0.008717893,-0.05139447,0.008192712,0.0065104915,-0.019703649,-0.01893705,0.019445723,0.03573965,0.019147271,-0.036023308,-0.04535526,0.016974967,0.013957514,-0.00309295,0.03845874,-0.012063863,-0.014877438,-0.0053217565,0.020786704,-0.0087939715,0.034427933,0.02434118,0.0021715728,-0.006194796,0.0020823886,-0.031011846,-0.03693715,-0.034238823,-0.00320145,-0.026833775,-0.02431249,-0.0077435775,-0.0023677598,-0.03157331,-0.0026788292,0.031468455,0.07607292,-0.03041999,0.011665849,0.0083714435,0.045530833,-0.024729377,-0.0240366,0.024263233,0.031183498,-0.0064913183,0.029274737,-0.061091326,-0.025307685,-0.00757296,0.040276382,0.061088104,0.07543878,-0.010994689,0.009903365,0.01775675,0.04931926,0.011983085,0.017886149,0.058128525,0.04621721,-0.011167813,0.050130147,0.044588223,-0.088028535,0.037095487,-0.046736035,0.06364904,0.049243204,0.017025013,0.0032076158,0.0074947565,0.020911168,0.028463058,0.02079368,-0.009792846,-0.055720937,-0.045147553,-0.039952386,0.011932436,0.017495174,0.015172439,0.006849379,-0.004152587,0.02775503,-0.024132434,-0.005206988,0.04787452,0.0095215915,0.019323736,0.015255547,0.036066525,-0.018188728,0.043446567,-0.00013423074,-0.01824588,0.06585037,-0.06743537,0.044216268,-0.034703452,0.024307832,-0.030916937,-0.029822268,-0.028203195,0.016079005,-0.03790745,0.026470516,-0.011238214,0.047790665,-0.0090942085,-0.010205276,0.024425521,0.041646387,0.054257147,-0.039394703,-0.0036614174,-0.012953962,-0.02085476,-0.007732362,-0.024134276,0.01917767,-0.003573344,-0.0260442,0.108217835,0.025328169,0.030128881,-0.030890163,0.026367351,-0.023157107,-0.01193335,-0.0047211614,0.019869015,0.01936603,0.03404817,0.004720891,0.0062093874,0.0627528,-0.006340357,-0.043882236,-0.0033075938,0.022989893,0.032516904,-0.075631,0.022308812,0.06805998,-0.00074496603,0.04987503,-0.04148948,-0.040161893,-0.035182875,0.014537314,0.023912255,-0.016691051,0.027429197,-0.03881589,-0.03078995,-0.040378913,0.008404858,0.03480462,0.061374467,0.039536357,0.021483656,-0.020431036,0.004239185,0.03440242,-0.046677068,0.03251876,-0.03711938,0.0027841202,0.054942604,0.02684359,0.005418477,-0.06510052,0.10048366,-0.08982557,-0.015827958,0.0653493,-0.03156736,0.006385431,0.0011331233,-0.017098168,0.029049102,0.0019197902,0.026070183,-0.025713075,0.01348885,0.040291026,-0.034039564,-0.033721786,0.0068059126,-0.01636984,-0.008437949,-0.0060120802,-0.0035175115,-0.06002513,-0.0007963147,0.0079543935,0.0062547885,-0.010588688,-0.0016397773,0.04006298,-0.047365498,-0.02664451,-0.037110303,-0.023705428,0.010711974,0.025890198,0.015036744,-0.108012736,0.03550494,-0.0019416745,0.019527042,-0.0263758,0.01523962,0.001532967,-0.012217516,-0.05058793,-0.020100005,0.03418146,0.03153418,-0.028116254,-0.01911113,-0.016068706,-0.015533293,-0.023399659,-0.0029359306,-0.005737176,0.029577332,0.0035800396,0.013905513,-0.0038739468,-0.064480886,0.0001244786,0.03791263,0.049409874,-0.024881061,-0.0003244471,-0.08254881,0.040679608,0.0554544,-0.033187743,-0.0067465883,0.0097010005,0.051466968,-0.029503472,0.05357359,-0.010249918,0.0033029986,0.016485525,0.0071634557,0.03995306,0.03259367,-0.021247502,0.011747916,0.007332423,0.01628339,0.017349005,0.011350985,0.027633864,0.015048348,0.033803664,-0.011593556,0.022405574,0.010653292,0.04120809,-0.07626455,0.03681497,-0.01880797,0.0018404371,-0.007257815,-0.051303655,-0.005715711,-0.01650611,-0.052059043,0.029180324,-0.008166226,-0.007751443,-0.040392384,0.022104742,-0.045563743,0.07016678,-0.09834451,0.03377788,0.010785571,-0.039826706,0.06369899,0.019326309,0.009752955,-0.009938331,-0.021910189,-0.024805974,0.023638722,0.051760413,0.024401935,-0.0013854137,0.10030965,-0.04626607,-0.072960176,-0.032894548,-0.024561092,-0.016855527,-0.016015448,-0.0019602503,0.02998347,-0.042965807,0.05078257,-0.04294145,-0.01996512,-0.057393163,0.065652646,-0.06877583,0.070465975,0.03879159,0.023760777,-0.019050281,-0.010780128,0.035094414,-0.002881375,0.038308423,0.01745325,0.0036803428,0.03641292,-0.010844712,0.0064460677,0.039537724,0.025345303,-0.03523816,-0.054923043,0.0016331668,0.0069588386,0.011580209,-0.014026451,0.010878107,0.0061528324,0.02212779,-0.013429083,-0.08869933,-0.017099531,-0.06801076,-0.03927407,0.0226445,-0.02846837,-0.0037738585,0.038863137,-0.027420616,0.060854726,-0.08267559,-0.005253232,-0.043069992,0.025412899,0.012296624,-0.040866155,-0.018738123,0.0032329138,0.0018029872,-0.041597124,0.029388271,0.045545775,0.002034436,0.015920546,0.0013175574,-0.02463073,0.046135336,0.022929372,0.026257385,0.0058316053,0.028118927,-0.018074995,0.07688787,-0.032873753,-0.01040878,0.014268956,0.008019153,-0.08000871,-0.016528543,0.06827424,0.032922518,0.01832801,-0.014616604,-0.014311852,0.065626785,-0.017931523,-0.004101825,-0.05068873,0.0014022522,-0.008938784,0.00097414205,0.039352987,-0.025155038,-0.07603673,-0.048767228,0.01492863,-0.065287754,0.0165518,0.027912928,0.107174754,0.05395365,0.08625649,-0.0015809909,-0.09004847,-0.038305406,-0.03870435,0.0152064115,-0.061164703,0.047231037,-0.014690952,-0.019090269,0.0021060198,0.030021816,0.017143399,0.051275473,-0.0063876575,-0.03988335,-0.002032657,-0.0034244154,0.009491739,0.053932887,0.02726245,-0.015721101,0.06393207,0.02849897,-0.036135335,-0.05303675,-0.020355636,-0.013530895,0.06890189,0.026155235,0.031896327,-0.010751816,0.0076235826,-0.0042709284,0.027863692,0.04273183,-0.078833915,-0.037566274,0.017125081,-0.0018770479,0.047457006,-0.031885065,0.017092904,-0.077255316,0.008639306,-0.015266438,-0.057119384,0.0558977,-0.0007244195,-0.01739642,-0.010761148,0.0018013522,0.0014069703,-0.01916891,0.024041273,-0.017734317,0.041862074,0.013315664,-0.059143707,-0.049657445,-0.020957572,-0.02129794,-0.03194947,-0.012207984,0.025093174,-0.012274943,-0.0029700086,-0.03101819,-0.017442971,-0.06379901,0.018451743,-0.034956854,0.045850594,-0.020166567,0.05947294,0.015437749,0.032012463,0.09101881,-0.028542444,-0.0021024242,-0.0040870225,0.0069669806,-0.0109870685,-0.02445528,-0.023760892,-0.06849709,-0.048765197,0.010348796,0.028625479,0.03962007,0.02979626,-0.030929115,-0.026995348,-0.048912738,-0.059797995,0.019801496,0.047213092,0.022649884,0.045857824,-0.05318364,0.0145184565,-0.02744381,0.0651584,-0.02702234,0.014067875,-0.013023889,0.026551452]	Keywords: conditional positional encoding, ViT, convolution, positional information, zero-padding\nKey Objects: positional information, convolutional networks, conditional position encodings\nRefers to Images: None\nHypothetical Questions:\n- How does zero-padding in convolutional networks help encode positional information?\n- What is the benefit of using conditional positional encodings versus traditional positional encodings?\n- Why is positional encoding important for transformers and how does CPE address this need?\n---\nSummary:\nConditional positional encoding (CPE) for Vision Transformer (ViT) uses 2D convolutions with zero-padding to generate conditional position encodings at each layer, leveraging the ability of convolutional networks to implicitly encode positional information.\nOriginal Text:\nConditional positional encoding (CPE) [20] generate conditional position encodings at each layer for ViT with a 2-D convolution with zero-paddings. The intuition behind this approach is that convolution networks can implicitly encode absolute positional information with zeropaddings [60].  \n- 5.1.5 Position Representation on Transformer Decoders. It is worth noticing that masked selfattention is not permutation equivariant [133]. Thus a model that exploits only the decoder of Transformer has the potential of sensing positional information without incorporating explicit positional representation. This is confirmed by some empirical results on language modeling tasks [59, 113], where the authors find that removing position encodings even improves performance.\nContextualized Text:\nIn the Vision Transformer (ViT) architecture, a technique called conditional positional encoding (CPE) generates position encodings at each layer using 2D convolutions with zero-padding. This approach exploits the property of convolutional networks, which can implicitly encode positional information by utilizing zero-padding.	{"tags": ["NLP", "deep-learning", "architecture", "ViT", "positional encoding"], "doc_id": "6a0e338a-0cf4-4c02-bd9d-88b6785375b7", "summary": "Conditional positional encoding (CPE) for Vision Transformer (ViT) uses 2D convolutions with zero-padding to generate conditional position encodings at each layer, leveraging the ability of convolutional networks to implicitly encode positional information.", "doc_type": "text", "entities": ["Vision Transformer", "ViT", "convolutional networks"], "keywords": ["conditional positional encoding", "ViT", "convolution", "positional information", "zero-padding"], "key_objects": ["positional information", "convolutional networks", "conditional position encodings"], "contextual_text": "In the Vision Transformer (ViT) architecture, a technique called conditional positional encoding (CPE) generates position encodings at each layer using 2D convolutions with zero-padding. This approach exploits the property of convolutional networks, which can implicitly encode positional information by utilizing zero-padding.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.1 Position Representations"}, "hypothetical_questions": ["How does zero-padding in convolutional networks help encode positional information?", "What is the benefit of using conditional positional encodings versus traditional positional encodings?", "Why is positional encoding important for transformers and how does CPE address this need?"]}
36e24e29-6359-479b-9b3d-acce2fb6a1ca	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.029353688,0.034331966,0.035127997,-0.0018516546,-0.010491578,0.041580707,0.015805319,0.009382214,0.004096286,-0.02682867,-0.016103303,0.0011820031,0.033608817,0.0054443493,-0.056054287,0.048197396,0.008655026,0.108507924,0.017727602,-0.05472422,0.052891232,-0.0001556346,0.016913954,-0.010909037,0.00609893,0.048485726,0.032260444,-0.012781032,0.024986226,0.0075596524,-0.009936505,-0.02761061,0.046435513,-0.0016215217,-0.00872516,-0.0059563606,0.0142531665,-0.012188948,-0.0053760735,-0.053023487,-0.037789814,0.06147438,-0.083811596,0.012289149,0.016268998,-0.029178513,-0.08605841,-0.02184725,0.0060350443,-0.034197643,-0.02431994,0.060062855,-0.043019306,-0.013281226,-0.019181058,-0.019898249,-0.010601251,-0.026604276,-0.0034811199,-0.052145705,-0.030475674,0.011996047,-0.044136494,0.009101725,0.030036766,-0.011624522,0.05734683,0.061893657,0.050744705,0.10500336,-0.03540408,0.021976922,-0.01248047,0.013631887,0.100177996,0.040073488,-0.010527731,-0.03892536,-0.060623102,-0.049976368,-0.011745783,0.05078762,-0.004650758,-0.050629955,0.088789634,0.016383782,-0.04833306,-0.0628751,0.06141396,-0.030650659,-0.04509565,-0.0021611198,0.02047408,0.07369192,0.015221132,-0.03969337,0.0013514726,-0.045901317,-0.012601986,-0.03700503,0.03769207,0.00852689,0.081786744,0.07320386,0.041876994,0.008288417,-0.057022844,-0.016094703,0.016886963,0.040962897,-0.053649005,0.030288827,-0.06935392,0.043641355,-0.03668434,0.03711656,-0.019212103,0.01972849,0.04638402,-0.007698789,0.042869937,0.011200535,0.024611145,0.029986918,0.034244422,0.042618997,0.016619554,-0.06979452,-0.011413161,0.029744323,0.006097778,-0.022854747,-0.03554143,0.002241889,-0.028481796,-0.03321262,0.056641106,0.010808815,0.002429017,0.033024743,0.016139206,-0.04475288,-0.054111782,-0.042046946,-0.060420886,-0.004684643,-0.06762505,0.009903723,-0.017341042,0.031058483,0.018651608,-0.04471934,0.028140265,0.004355148,-0.051487993,-0.014825069,-0.036831267,-0.033535656,-0.00433844,-0.015706228,-0.049052645,-0.05286918,-0.015478805,0.06896941,0.05373836,0.060963713,-0.006333896,0.015050458,0.02846313,-0.031014333,-0.04343726,-0.013594817,-0.012522078,-0.02208269,0.039337073,-0.018986009,-0.01705283,-0.051358145,-0.013449243,0.05092283,0.002435026,0.0097010415,-0.007834224,-0.009461666,-0.042837676,0.00017526723,0.0030758274,0.06687605,-0.043284323,0.0011373931,0.03242185,0.006039174,-0.006002658,0.0046788934,0.017999616,0.0108016515,0.03980068,-0.035536047,-0.038768128,0.015473549,0.017556267,-0.0034183115,0.02478105,-0.04032873,0.07890217,-0.013143307,0.0015526826,0.014595264,-0.050043713,0.054333538,-0.03150033,0.004217993,-0.003049477,-0.0066782827,0.042273045,0.0031678388,-0.045624983,-0.001995608,0.048198488,-0.035134885,-0.057571676,-0.021559382,0.018332299,0.054375563,-0.043523077,0.02013169,-0.043677576,0.026530964,0.0014693551,0.033424072,0.01971572,0.044824287,-0.07780436,0.04002553,0.023870483,0.014477746,0.008199322,-0.018913839,0.033553664,0.018469661,0.06196357,0.03188896,-0.013877307,0.075440295,-0.00035051254,-0.018463079,0.040216252,0.056443702,0.006167649,0.034028064,-0.049066514,0.057152785,0.062465455,0.031613994,-0.03175048,0.018807523,-0.008520352,0.06763192,0.02061532,-0.019974278,-0.0011643366,-0.0103520285,0.017715666,0.021807134,0.007073917,-0.005000085,-0.01089618,0.013419996,-0.05063237,-0.031127453,0.023459887,0.013288509,0.04203958,-0.041801274,-0.013576214,0.052429825,-0.012290073,-0.02653309,0.026263101,-0.0073194946,0.004877713,0.03831426,-0.02282269,-0.0039419197,0.031826936,0.025014915,-0.07671075,-0.007311599,0.0059988117,-0.031410664,-0.033402435,-0.037955984,0.015947172,0.010845299,-0.029202426,0.01916025,0.0056644585,-0.0030938347,-0.0193991,0.03881327,0.114346266,-0.018383909,0.0028298944,0.0036303275,0.016717548,0.0070301825,-0.07887932,0.02426561,0.03888294,0.006014626,0.025899779,-0.029548619,-0.040536497,0.014406214,0.057705812,0.10072214,0.06673787,-0.06806748,0.02651507,0.022695402,0.015007373,0.019785238,-0.020300958,0.08472446,0.004251117,-0.014583043,-0.039011285,-0.026837174,-0.04528551,0.053804792,-0.05503726,0.016811691,0.054102294,-0.015684279,0.02501429,0.022918545,0.00064496516,0.027032182,0.033149153,0.005931501,-0.012218786,-0.07053874,-0.023154512,0.0057101822,-0.0013385182,-0.026400138,-0.0021110096,-0.015568314,0.07125939,0.05280619,0.013323086,0.047895335,0.03981882,-0.0064773373,0.035964057,-0.014437104,0.030582694,0.03887287,0.016874079,0.0059896926,0.07935701,-0.008440242,0.012250521,-0.019412166,-0.04630825,0.012431302,-0.039242413,-0.00079496414,0.012316267,-0.026960786,-0.012707795,-0.07534452,0.04270999,-0.008288388,0.010626913,0.04333797,0.044613253,0.030209472,-0.0009680418,-0.015869584,-0.014232983,0.037607152,-0.03431491,0.005487335,0.032187037,0.01310371,-0.011532104,0.06030772,-0.03847905,0.03798944,-0.03064115,-0.007212218,0.04171571,-0.049766663,-0.06347374,-0.006700806,-0.02297752,0.034280133,-0.0015197566,-0.023049464,0.080989555,0.05391822,-0.03664498,0.02156731,0.047379903,0.04268939,-0.04417558,0.034498814,-0.007444617,0.021864494,0.07049008,-0.022780254,-0.06705591,0.01819149,-0.027956828,-0.024669787,0.024836835,0.023097975,-0.042372774,-0.014812203,-0.02348216,0.0012107865,-0.019054886,0.052237906,0.020249277,0.027472837,0.0020263498,0.012161613,-0.023715006,-0.0016333064,0.01567795,-0.01373812,-0.0363678,0.05761185,-0.017018666,0.020530831,-0.057072766,0.08158428,-0.045620285,0.006288979,0.029048387,-0.01888518,0.0715369,0.011765227,0.0059318747,-0.03142391,0.024511876,-0.0062227184,-0.01145773,0.017011292,-0.0055373893,-0.013829343,0.02463886,0.006102652,0.024222141,-0.029788757,0.013614384,0.008095484,0.021305844,0.0009132515,-0.018884731,-0.031021312,0.006805296,-0.0014832088,0.011930519,0.015582948,-0.0018702822,-0.028459812,-0.038342457,-0.009101369,0.010098544,-0.025764372,-0.03658914,0.039619856,0.011001632,0.013314859,0.012831484,-0.012147833,0.03417941,-0.030761098,-0.073300995,-0.020665271,0.025001653,0.073247254,-0.0669647,0.052353963,-0.015623366,-0.03524962,-0.02983847,0.01401331,-0.0066955206,0.07708467,0.0066395486,0.042702973,0.004643699,-0.012987186,-0.014509598,-0.009867686,-0.01568864,-0.031114906,-0.015587751,-0.022992121,0.094245866,0.009691963,0.0037057025,0.037181243,0.025540914,0.028694574,-0.027539697,0.059611868,0.06212375,-0.036437653,-9.801682e-05,-0.018191205,-0.012924428,0.045145743,0.0121125085,-0.0040892474,0.040161066,0.013511933,-0.038631327,0.06287868,0.008921458,0.01895046,-0.03542176,0.033029545,0.010401181,0.013500029,-0.011801226,-0.04882715,0.026800575,-0.028407633,0.0116698975,0.017241672,-0.07841502,-0.023538817,0.03167522,-0.016515773,0.0033502986,-0.001830203,-0.0019064739,-0.02347152,-0.002875752,-0.003186972,0.072727405,-0.07929659,0.04524823,0.051810082,-0.0006462523,0.058389507,0.0076615117,-0.009945195,-0.019545393,0.0116946185,-0.025471348,-0.0019406327,0.054033134,-0.0031241034,-0.051676285,0.003891027,-0.03839445,-0.023037616,-0.01056166,-0.04181506,-0.034715045,-0.044710226,-0.019532803,-0.0074487347,-0.044159822,0.04780236,-0.024148215,-0.018317377,-0.014637202,0.042189673,-0.019216469,0.031982407,0.06306016,-0.02090502,-0.034518633,0.00024823486,0.029841753,-0.024047237,0.013277189,0.019034334,0.0073973187,0.019692985,0.050127316,-0.0035976407,0.0336889,-0.006559454,-0.022986272,-0.055390555,0.02526633,0.018998474,-0.03788096,0.04690117,0.05447128,0.028020665,0.015336311,-0.034447838,-0.08905926,-0.020182967,-0.0448011,-0.047236264,0.023748742,0.020086432,0.030544955,0.054636564,-0.07053237,0.08794205,-0.09973913,0.0016958746,-0.025757136,0.0096417675,-0.019796936,-0.053485185,-0.021691598,0.00021816666,-0.010113936,-0.014030297,0.008735095,0.027044877,0.00012605642,-0.014268066,-0.022826787,0.009474296,-0.0018274299,0.03856054,-0.03970331,-0.021706242,0.056181666,-0.02298874,0.068944156,-0.027435288,-0.037019312,-0.00032036222,0.0066505033,-0.07001267,-0.037605185,0.020606497,-0.018966418,0.037469603,-0.0050976737,0.031674255,0.05594559,-0.046178438,-0.042164616,-0.069080964,-0.032529086,-0.0045418493,-0.012821314,0.030987998,0.05155956,-0.06329733,-0.025373647,0.026729532,-0.009703022,-0.00022356154,-0.005800488,0.05143966,0.024065271,0.04135029,-0.012316703,-0.04463157,-0.017948944,-0.047825567,0.051607884,0.011377433,0.06693336,0.027029071,0.008438727,-0.01724113,-0.006143696,-0.035491083,-0.016986996,-0.0021531126,-0.08136868,0.020463012,0.020113051,0.01386037,-0.0025068505,-0.009530374,-0.026727946,0.056137733,0.016136386,-0.06215586,-0.014084466,-0.014047432,0.0033967444,0.00087404315,0.04077375,-0.00973891,0.020861965,0.028102322,-0.0016799245,0.075290136,0.029754916,-0.064779766,-0.020415498,0.03112729,-0.01800901,0.016377823,-0.019760277,-0.023237513,-0.09259853,0.03301755,-0.008930227,-0.021631286,0.025011756,0.006733337,-0.01436207,-0.018006327,0.016514713,-0.0076573524,-0.014839624,-0.0013479074,0.053665496,0.00089133723,0.0024474957,-0.0055947453,-0.008424799,-0.052591395,-0.055603813,0.003921179,0.0040810043,0.047821958,-0.025485583,0.05564927,-0.04392717,0.0112386905,0.0059547694,0.037156675,-0.02007166,0.08083598,-0.011874422,0.046950404,0.01074487,0.017147103,0.05726981,0.07018626,0.01966962,-0.009446178,0.051858153,0.009999035,-0.003115508,-0.013557392,-0.007222717,-0.010601021,0.02930026,0.022159403,0.032152493,-0.012136669,0.020122757,-0.06049964,-0.055940576,-0.0022503424,0.01078689,0.04863744,0.05414435,0.025639778,-0.078332044,0.022781579,-0.02875999,0.06679097,-0.04396267,-0.004003103,0.05380639,-0.012284371]	Keywords: Layer Normalization, LN, residual connection, deep networks, training stabilization\nKey Objects: Layer Normalization, residual blocks\nRefers to Images: ./images/a-survey-to-transformers/image_12.png\nHypothetical Questions:\n- Why is Layer Normalization used in conjunction with residual connections?\n- What specific training problems does Layer Normalization aim to solve?\n- How does Layer Normalization contribute to the overall stability of deep networks?\n---\nSummary:\nLayer Normalization (LN), often used with residual connections, is a mechanism to stabilize the training of deep networks by addressing issues like ill-posed gradients and model degeneration.\nOriginal Text:\n### 5.2 Layer Normalization  \nLayer Normalization (LN), along with residual connection, is considered as a mechanism to stabilizing training of deep networks (e.g., alleviating ill-posed gradients and model degeneration). There are some studies that are dedicated to analyzing and improving LN module.  \nFig. 11. Comparison of Transformer Encoder with pre-LN and post-LN.  \n  \n5.2.1 Placement of Layer Normalization. In vanilla Transformer, the LN layer lies between the residual blocks, called post-LN [141]. Later Transformer implementations [67, 136] place the LN layer inside the residual connection before the attention or FFN, with an additional LN after the final layer to control the magnitude of final outputs, which is referred to as pre-LN $^{10}$. The pre-LN has been adopted by numerous following research studies and implementations, e.g., [6, 17, 141]. The difference between pre-LN and post-LN is shown in Fig. 11.\nContextualized Text:\nWithin the Transformer architecture, Layer Normalization (LN) is frequently employed alongside residual connections to improve training stability by mitigating problems such as ill-posed gradients and model degeneration. LN addresses these challenges by playing a key role in stabilization.	{"tags": ["normalization", "architecture", "training", "deep-learning"], "doc_id": "36e24e29-6359-479b-9b3d-acce2fb6a1ca", "summary": "Layer Normalization (LN), often used with residual connections, is a mechanism to stabilize the training of deep networks by addressing issues like ill-posed gradients and model degeneration.", "doc_type": "text", "entities": ["Transformer", "TensorFlow"], "keywords": ["Layer Normalization", "LN", "residual connection", "deep networks", "training stabilization"], "key_objects": ["Layer Normalization", "residual blocks"], "contextual_text": "Within the Transformer architecture, Layer Normalization (LN) is frequently employed alongside residual connections to improve training stability by mitigating problems such as ill-posed gradients and model degeneration. LN addresses these challenges by playing a key role in stabilization.", "mentioned_images": ["./images/a-survey-to-transformers/image_12.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.2 Layer Normalization"}, "hypothetical_questions": ["Why is Layer Normalization used in conjunction with residual connections?", "What specific training problems does Layer Normalization aim to solve?", "How does Layer Normalization contribute to the overall stability of deep networks?"]}
3e60e82d-bf7f-4118-92f6-a1b80d8e2777	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.046136536,0.021723837,0.0012186843,0.01242687,-0.021418752,0.035543084,-0.015868403,0.0015113178,0.044836227,-0.03477349,0.058252867,0.0022279536,0.008870764,-0.03014483,-0.00691871,0.015153117,0.023945551,0.04790114,-0.008088777,-0.060006626,0.0015417394,0.04411057,0.012344504,0.020326149,0.013069563,0.049036976,0.039890777,-0.039399236,0.009165213,0.023232887,-0.003301601,-0.040073246,0.009057162,-0.030438317,0.0427255,-0.040928576,0.020450981,-0.018262386,-0.021282973,-0.066974334,-0.023563577,0.09886934,-0.066011176,-0.022810314,-0.011123898,-0.04969489,-0.066721514,-0.04025861,0.009562465,-0.052151345,-0.025540413,0.036155153,-0.015428164,-0.031611957,-0.060713027,-0.023881678,-0.0069970055,-0.024979802,0.019618424,-0.08622525,-0.024566853,0.003157017,0.0056190426,0.014100384,0.01471283,-0.028072307,0.06897707,0.033469044,0.033162285,0.11807017,-0.023013039,0.02480794,-0.026752459,-0.008518444,0.09496898,0.050950736,-0.021821843,0.0015958758,-0.057425197,-0.007871021,0.012179061,0.056057736,-0.027596824,-0.056832004,0.060719248,0.0028861235,-0.014911032,0.0019668115,0.050523754,-0.02775304,-0.013092985,-0.016714895,0.03656102,0.04527742,-0.024621604,-0.022592358,-0.00138593,-0.02713461,-0.024592673,-0.07750606,0.033399086,0.015173852,0.054732047,0.09241546,0.042228423,-0.042825982,-0.017429514,-0.021623872,-0.027253535,0.021274442,-0.04260465,0.027500972,-0.03672768,0.024561014,0.043592166,0.025953393,-0.013801108,0.02248555,0.02288043,-0.01949803,0.05768097,-0.009434597,-0.011816702,0.034110237,0.034648903,0.037590146,0.0040336386,-0.04923917,-0.015489231,0.020243352,0.070278816,0.026846876,-0.050734226,0.030655922,-0.03144784,-0.000855428,0.058921494,-0.026593504,0.008958621,0.027254606,0.02805464,-0.010744412,-0.059174106,-0.008916923,-0.03994761,-0.008807674,-0.07814986,0.03163008,-0.035990175,0.02548451,0.014977155,0.012073951,0.01485744,0.0075852345,-0.044506785,0.016460588,-0.009092109,-0.014653089,0.03914273,-0.03793045,-0.038129386,-0.08825449,0.023026766,0.045627948,0.024719678,0.0767053,0.0054535936,0.002478287,0.0076394784,0.008953265,-0.04378698,0.009679581,0.0032421597,-0.015123294,0.045568816,0.00037620167,0.01433631,-0.03865308,-0.0017441482,-0.019101582,-0.0021957366,0.01836563,-0.015615113,0.02859771,-0.06880027,0.03472479,0.01327528,0.042121883,0.034076955,-0.047352884,0.023614528,-0.029376993,-0.020155476,-0.021893209,0.008935226,0.061514247,0.032253835,-0.040280398,-0.0432256,-0.0035746684,0.05761492,0.031865284,-0.00735409,-0.017847091,0.016469577,-0.02401678,0.0014429252,0.002132899,-0.009185105,0.02519738,0.0136469165,0.008877514,0.0020808748,0.017739883,0.01588451,0.0067864475,-0.020785851,-0.00030544182,0.048235416,-0.059645437,-0.024253815,-0.036588892,-0.0020175716,0.027942901,-0.013278407,0.0014215598,-0.06783398,0.01096882,0.00804156,0.0040586954,-0.0016317532,0.04749007,-0.07093521,0.023115702,-0.03370495,0.0122915115,0.03884572,0.009843551,-0.043156058,0.0035954935,0.0232009,0.026043488,-0.0030438134,0.06525885,0.0027157739,2.131846e-05,0.028313493,0.040199038,-0.019185353,0.054037064,-0.021625634,0.087038256,0.020333514,0.016494062,-0.029714128,-0.050478373,0.005179343,0.056289043,0.038841076,-0.032454956,-0.007888119,-0.03564449,0.009640942,0.036764305,0.012270158,-0.0061544105,0.031094063,0.018632343,-0.106982864,-0.017528005,-0.019933486,0.016408615,0.02016113,0.01890156,-0.034219533,0.014314983,0.0026198027,-0.014361012,-0.00041192272,-0.005710821,-0.011038774,0.014473908,-0.03437185,-0.020454543,0.036242623,0.036421098,-0.016583772,-0.028217226,-0.05463598,-0.0037542603,-0.03179706,-0.084758155,-0.0006791343,0.0011107693,-0.0023890217,0.022612594,0.011560987,-0.029372122,0.02961104,0.008217485,0.11703791,-0.020340791,0.0018698011,-0.019349674,0.008183597,0.04411957,-0.0481365,0.0098970495,0.020904996,0.010398443,0.018557487,-0.035916075,0.00032201497,0.053619802,0.042992774,0.09792193,0.07069032,-0.027006604,0.026401658,0.018265942,0.06748854,0.00814036,-0.0007125536,0.08806311,0.042978875,-0.056544866,0.0040625217,-0.045555413,-0.057054535,0.061324332,-0.031698566,0.059567064,0.029488984,-0.03521443,0.032144118,-0.04165915,-0.012044964,0.024894632,0.040941846,0.04161488,0.008449762,-0.07726215,0.026213499,0.012659122,-0.040877018,-0.009385127,0.0031560538,-0.013682156,0.07014457,0.050039455,-0.01916245,0.004901678,0.03386145,0.022086235,0.021625312,-0.055000227,0.0038591535,0.032683827,-0.023350304,-0.020097896,0.06986654,-0.049082614,0.06552439,-0.03674753,-0.016588552,-0.006049907,-0.042033445,0.029158484,0.0042937207,-0.04819452,-0.022422502,-0.03794138,0.05487766,0.012726946,-0.03108407,0.046497997,-0.013990584,0.020796657,0.018891364,-0.027653817,-0.027770666,-0.009157895,-0.05469425,0.008906457,0.019074308,-0.0018885537,-0.014673684,0.106555186,-0.02350946,-0.015143323,-0.019393293,0.0017107091,0.0175887,-0.015732715,-0.05315662,-0.017972458,0.05005686,0.06576256,0.031615004,0.01536569,0.06143893,0.05588292,-0.050764326,-0.021503184,0.074078746,-0.016468797,-0.040518463,0.054098383,0.01355853,0.004759419,0.047219727,-0.06223218,0.0024520117,-0.028219411,-0.01668892,-0.04860471,-0.019176712,0.016075378,-0.013829076,-0.016272647,-0.0016524283,0.0038287607,0.017780628,0.026378809,-0.0140708955,-0.00443729,-0.029194864,0.027807495,0.024570005,-0.024955535,-0.0031119508,-0.0076171304,0.0014568742,0.0064540296,-0.007516832,-0.010397538,-0.06123175,0.057869807,-0.042327598,-0.03310148,0.024964277,-0.008080331,0.05461268,-0.02138069,0.025105339,-0.035017475,0.01189333,-0.0083558345,-0.03658804,-0.00310138,0.017725773,-0.0017816552,0.035313506,0.028976493,0.022575442,-0.012467348,0.022421766,-0.013868892,-0.042591084,-0.018133122,0.0034765557,0.0018088992,-0.018441005,0.03045704,0.006012948,0.014294625,0.021803271,-0.04869439,-0.015362124,-0.020701585,-0.029770609,0.014126093,-0.027150195,0.05647228,-0.03411343,-0.010805142,-0.0175368,0.012022639,0.04912888,0.014314141,-0.044368718,-0.058883466,0.07025038,0.036188327,-0.032076873,0.014764787,-0.04808679,-0.0017627953,-0.00013115154,0.022705032,0.013159266,0.05631096,-0.0033704697,0.038831312,-0.029891005,-0.039162997,-0.02216921,0.015840014,0.014450418,-0.028371062,0.017743003,-0.015920985,0.081957646,0.031149825,-0.00069715426,0.054901596,-0.0027761825,0.03064296,-0.037142303,0.037961863,0.04160453,-0.043652713,0.012108023,0.014426451,-0.07253206,0.016564665,-0.023821434,0.0089154355,0.010277174,-0.009466549,-0.0039435537,0.08139445,0.04733715,0.044041384,-0.014491579,0.020301273,-0.0069467328,0.0024704763,0.047160182,-0.09271185,0.03684711,-0.10080062,-0.040628396,-0.00042247472,-0.03883442,-0.059724804,0.008067751,-0.003508301,0.021949662,0.036267094,-0.022838667,-0.057380818,0.0053347484,-0.022776915,0.068998896,-0.04806619,0.05078878,0.03798045,0.0042705066,0.046906915,0.03093745,0.00751706,-0.00030038346,0.044148263,0.043506633,0.054661427,0.07773161,0.011146961,-0.035402846,0.0103715835,0.000116590694,-0.032341152,-0.03364675,-0.03132915,-0.01177548,-0.009441337,0.004610239,-0.057753853,-0.067547165,0.030445112,0.0031716956,-0.027967693,0.043740954,0.03630727,-0.0013039241,0.035767213,0.060625553,0.02519487,-0.009427526,0.003972535,0.016086949,0.06352716,0.026364923,0.023453902,0.0093160765,-0.016941419,0.012385965,0.0087259,-0.010385586,0.016648479,-0.019710215,-0.040396202,0.00020770573,0.008997607,0.0043937233,0.029616911,0.026682096,0.056278978,0.0064146076,-0.021521684,-0.08243616,-0.019778302,-0.0073010335,-0.006795678,0.030992145,0.036092386,-0.02964206,-0.0028430605,-0.028466357,0.10920785,-0.110704474,-0.048791174,-0.05662573,0.065915205,-0.020690093,-0.0578286,-0.024942458,-0.010662633,-0.0025231394,-0.016233288,0.03268365,0.024987033,-0.024586182,-0.00937722,-0.0120972255,0.036066707,-0.0014934881,0.025079694,-0.06967546,0.014850279,0.005447438,-0.031411733,0.07692022,-0.05348225,-0.011320463,0.01672273,-0.009641759,-0.010949562,-0.04656825,0.03799104,-0.03842196,0.035914168,-0.034773357,0.017484538,0.064565085,-0.08317981,-0.025535142,-0.051592566,-0.0025681572,-0.012239305,0.05716049,0.04673011,0.012264763,-0.044621855,-0.028474497,0.020295506,-0.027622044,0.02840768,-0.00029573427,-0.00625905,0.022792917,0.06406225,-0.04459687,-0.039403133,-0.040605728,-0.05845012,0.031150114,-0.021449596,0.056369945,-0.0022957267,-0.0077390424,-0.025205892,-0.0034649167,-0.0017882374,0.020931536,0.042513803,0.016872434,-0.029197747,0.009410947,0.022579538,0.02704705,-0.013143578,0.010353304,0.034904316,0.029933132,-0.049112324,-0.036203336,-0.033633247,-0.009433022,0.01758262,0.017958967,-0.03517312,0.011506427,0.016290465,-0.040000845,0.05418325,0.0015682394,-0.07283824,-0.05439123,0.02417802,0.011410414,0.068771265,0.00078267755,-0.016950708,-0.0990335,0.020754168,-0.017535882,-0.017529998,-0.0035833113,-0.015801856,0.0039575933,-0.0041221906,0.015336477,-0.03233246,0.043171898,-0.010143425,0.01304817,-0.013528173,-0.028138826,-0.0015067135,-0.038313236,-0.01571639,-0.024075096,0.0013492866,0.032593295,0.034760308,-0.013199292,0.026147574,-0.04890012,0.014724408,0.03349976,0.039331704,-0.008538244,0.02234819,-0.053446718,4.9059323e-05,-0.049271047,0.052387934,0.015740262,0.022230078,-0.025684562,-0.032928597,0.07938518,-0.005004248,0.05995527,0.0014227006,0.011116693,-0.02331085,0.039384447,0.014884454,0.052733652,-0.010435299,0.022904495,-0.06998706,-0.025270207,-0.028636301,0.019843832,0.040481612,0.036535017,-0.0007222375,-0.087193675,-0.011801258,-0.03944561,0.047033064,-0.00037153668,0.0017728439,-0.010462902,-0.006165083]	Image title: Comparison of LayerNorm Placement in Self-Attention\nTags: self-attention, layer-normalization, neural-network, architecture, comparison\nKey objects: Self-Attention, Feed-Forward Network, LayerNorm, Input, Output\n---\nSummary:\nThis diagram illustrates two different architectures for incorporating Layer Normalization (LayerNorm) within a self-attention mechanism. Both diagrams show a self-attention block, but differ in the placement of the LayerNorm. (a) shows LayerNorm *after* the self-attention and feed-forward network, while (b) shows LayerNorm *before* both.\nFull description:\nThe diagrams compare two architectures for implementing self-attention. Both diagrams take an input and pass it through a self-attention mechanism.  In architecture (a), labeled 'post-LN', the output of the self-attention layer is then processed by a feed-forward network (FFN). Following the FFN, the result goes through a LayerNorm (LN) before being outputted. In architecture (b), labeled 'pre-LN', LayerNorm is applied *before* both the self-attention layer and the feed-forward network, then the result is outputted.\nText found in image:\n- (a) post-LN\n- (b) pre-LN	{"tags": ["self-attention", "layer-normalization", "neural-network", "architecture", "comparison"], "title": "Comparison of LayerNorm Placement in Self-Attention", "doc_id": "3e60e82d-bf7f-4118-92f6-a1b80d8e2777", "source": "./images/a-survey-to-transformers/image_12.png", "summary": "This diagram illustrates two different architectures for incorporating Layer Normalization (LayerNorm) within a self-attention mechanism. Both diagrams show a self-attention block, but differ in the placement of the LayerNorm. (a) shows LayerNorm *after* the self-attention and feed-forward network, while (b) shows LayerNorm *before* both.", "doc_type": "image", "key_objects": ["Self-Attention", "Feed-Forward Network", "LayerNorm", "Input", "Output"], "parent_doc_id": "36e24e29-6359-479b-9b3d-acce2fb6a1ca", "text_in_image": ["(a) post-LN", "(b) pre-LN"], "contextual_description": "The diagrams compare two architectures for implementing self-attention. Both diagrams take an input and pass it through a self-attention mechanism.  In architecture (a), labeled 'post-LN', the output of the self-attention layer is then processed by a feed-forward network (FFN). Following the FFN, the result goes through a LayerNorm (LN) before being outputted. In architecture (b), labeled 'pre-LN', LayerNorm is applied *before* both the self-attention layer and the feed-forward network, then the result is outputted."}
3eed7a8f-ab2c-44f5-8397-20e7e4c05a69	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.022992106,0.04131306,0.019892137,-0.022267247,0.005536947,0.036878068,-0.018625509,0.02922729,-0.022674194,-0.018044531,-0.044485696,0.015953805,0.011950397,-0.0066583487,-0.031294163,0.04185357,0.030826187,0.103013895,-0.0051291566,0.0019803697,0.050750732,-0.020591795,0.0018238442,-0.018449958,0.005607855,0.03180994,0.06505494,0.024276651,0.025236893,0.003487822,0.005646304,-0.049796496,-0.00840247,-0.001453124,0.013519573,0.024936948,-0.022021987,-0.013631025,0.015933642,-0.07072084,-0.077511184,0.05389933,-0.08075212,0.010956374,-0.004057244,-0.06222769,-0.09725993,-0.028808666,-0.003028545,-0.03036662,-0.077014506,0.017632494,-0.057812262,-0.020289231,0.007115127,-0.006829357,-0.02577209,-0.0114971595,-0.02269926,-0.03941814,-0.02161142,0.012476686,-0.05681931,-0.05433468,0.065621145,-0.031187223,0.029355498,0.01285226,0.040763292,0.1316676,-0.047833998,0.01958096,-0.023298526,-0.050490547,0.116837814,0.00599667,-0.028692503,-0.057955958,-0.028783819,-0.030515213,-0.051727947,0.08064885,-0.033945445,-0.040812813,0.050856616,-0.0019962648,-0.08745523,-0.05587842,0.0794041,-0.06367082,-0.035353996,0.011438335,0.01832648,0.046027254,-0.0077297683,-0.11498516,0.015365447,-0.053006332,-0.005130109,-0.053939562,0.018352373,-0.012464111,0.055950917,0.094846494,0.03738846,0.009500522,-0.023694351,-0.036465626,0.016232315,0.033557978,-0.026936915,0.024231685,-0.0023022613,0.036326453,-0.03472656,-0.0005616592,-0.057124518,-0.013587239,0.016138244,-0.043789506,0.026213424,-0.021738488,-0.04246924,0.05404465,0.053159494,0.05872037,0.03423123,-0.0127722565,-0.01953506,0.0071932324,0.024617774,-0.026997212,0.025204804,-0.003538016,-0.013156204,-0.011359153,0.06328375,0.002750057,0.002343564,0.01536646,0.014300509,-0.012596369,-0.044938643,0.014013482,-0.04688861,0.02461218,-0.043231092,0.020126974,-0.0007077504,0.06749371,0.028965887,-0.023054935,0.036752917,-0.007847172,-0.039277643,-0.025516454,-0.010843879,-0.055138398,-0.02019951,-0.0131754065,-0.029748507,-0.028569775,-0.016847203,0.03758597,0.04399718,0.07582999,-0.009195273,0.04639714,0.021879178,-0.018048422,-0.009101595,-0.025777144,0.010756454,-0.033211634,-0.00049492234,-0.025193293,-0.029364217,-0.04042369,-0.020555953,0.028640853,-0.019753048,0.01693379,-0.052867796,-0.007999499,-0.025203977,-0.013381898,0.022531502,0.037449628,-0.09121047,0.0013659324,0.03877843,0.014070271,-0.009757415,0.008763668,0.029155983,0.022372454,0.044173677,-0.042924654,-0.015137403,-0.00819619,0.037495133,0.02808772,-0.0187541,-0.015928047,0.046477925,-0.026553813,0.021243528,0.0066873278,-0.023530269,0.03762751,-0.0009939414,-0.014056113,0.034821875,0.040429972,0.04344385,0.016879044,-0.005406178,0.008041855,0.04049978,-0.0012252377,-0.052145585,0.045576748,0.008624856,0.039690703,-0.030264512,-0.016106188,-0.0417883,0.007231609,-0.005796158,-0.00454161,0.053100437,0.042221196,-0.04528835,0.0708293,-0.021062013,0.022857387,-0.037203737,-0.018632172,0.019752052,0.05780751,0.06809743,-0.004488196,-0.04294361,0.0720564,-0.013935117,-0.03004999,0.040872917,0.040384088,0.00039601146,0.025343843,0.016859535,0.02387536,0.060912315,0.02790747,0.016264977,-0.0111955125,0.02270156,0.055633795,0.021253124,-0.028959053,-0.008966366,0.012195188,0.008467549,0.021102332,0.0018857286,-0.02162947,-0.016204726,0.026321031,-0.02796253,-0.02447777,0.043583773,0.0071597085,0.003757011,-0.053409472,-0.030290935,0.073727876,-0.016406337,-0.016461588,0.063808806,-0.049597714,-0.0120197255,-0.029564435,0.018330436,0.008803347,0.021914678,-0.028621377,-0.050315235,0.009409009,-0.05132786,-0.031091541,-0.013653897,-0.048010662,0.0590833,0.025517792,-0.046899028,0.04789944,0.010064353,0.004252397,-0.012552749,0.020735363,0.062287044,-0.022855155,0.006912465,-0.018389165,-0.019399436,0.032938782,-0.070900664,0.024579627,0.07543889,-0.0052862037,0.05663452,-0.011295657,-0.04902086,-0.041428566,0.03697006,0.05772251,0.03570826,-0.05604338,0.011062764,0.0017829186,0.023747895,-0.030179467,-0.042963173,0.043639984,0.025052762,-0.03231013,-0.009596801,-0.022544367,-0.0957215,0.06848305,-0.03773019,0.03666799,0.047377575,0.020910012,0.003929545,-0.010199333,-0.008107875,0.013559995,-0.0052334187,0.035884432,0.01512899,-0.12638138,-0.03728378,0.017890295,0.01621235,0.0004955617,0.010834595,0.027545175,-0.0077070273,0.03198036,0.00056117924,0.037068482,0.03399572,0.019185923,0.01233092,-0.00094844523,-0.0009948431,-0.018347314,0.013264416,-0.020711277,0.07198215,-0.0026872256,0.0126674455,-0.004017138,0.022914631,0.014169245,-0.07187064,-0.03797158,0.0053140325,-0.039436057,0.00365512,-0.027473463,0.07018525,-0.041938923,0.009483928,0.012278911,0.035938732,0.027037801,-0.011557947,-0.05727155,0.012050923,0.029330747,-0.0422681,-0.016946385,0.019665964,0.0016087868,0.012129242,0.07399075,-0.004354913,0.08666953,-0.025353694,0.024213716,0.027049163,-0.029651571,-0.02908516,0.009248152,0.009207895,0.05215136,-0.012658648,-0.040038705,0.009734801,0.024355497,-0.040906288,0.015628794,0.0049516275,0.04606915,-0.073642455,-0.0067161536,0.045748208,-0.01442382,0.009667747,-0.030625362,-0.1043518,0.023844428,-0.041212108,-0.0137526,-0.013543685,0.023957353,-0.039278444,0.0034709503,-0.045652073,0.03927937,0.026247425,0.04478705,0.000348537,0.0009048248,0.008500577,0.01679834,0.004911352,0.026295967,0.0039243847,-0.035374623,-0.044132397,0.07323787,0.015630074,0.006398356,-0.06135066,0.04491615,-0.0550183,0.033427507,0.043478943,-0.025928969,0.051086735,0.02562577,0.018177308,-0.009904058,-0.012871703,0.010185531,0.0010471302,-0.00617454,0.02515269,0.037034526,0.0009952047,0.025517834,-0.020938389,-0.021164823,0.041481033,0.0019144415,-0.008599096,-0.013805295,-0.032114863,0.014468035,0.017831836,0.022857063,0.021376837,-0.034684435,-0.017150424,-0.026664482,-0.023027495,0.020263169,-0.003979232,0.025141772,-0.042483784,0.040950757,0.022314038,0.015687585,-0.0068460186,0.0132936565,0.02906223,-0.01675504,-0.07214796,0.022208253,0.012370123,0.040168323,-0.004973292,0.006388605,-0.03461192,-0.042287476,-0.030680794,0.02625294,-0.0068906117,0.047486167,0.052515578,0.026686557,0.017294416,-0.0022966124,-0.009519944,-0.0049949884,-0.040800583,-0.032955896,-0.035484817,-0.02325677,0.028981043,0.03914396,-0.005008912,0.023763591,-0.03449999,0.056179933,-0.03632365,0.046161655,-0.0046450146,0.008765286,0.014628547,-0.042349335,-0.031960387,0.016610688,-0.004284938,-0.022389563,0.06674265,0.02973035,0.007303796,0.04543116,0.023748966,-0.022809573,0.02245777,0.015303955,0.01892103,0.025266612,0.043562073,-0.066763565,0.013442106,-0.019414835,-0.056837812,0.006742887,-0.07599911,-0.028286641,0.06382519,-0.019831555,-0.02090762,-0.008793131,-0.034265235,-0.01339542,0.035852905,-0.027752275,0.06899298,-0.06517826,0.025479235,0.01199976,0.009314994,0.09053671,0.011216945,-0.054809157,-0.00975156,-0.024184687,-0.0129607795,0.018816024,0.057162255,0.022737553,-0.02216652,0.079293124,-0.012888005,0.014266007,0.012510731,-0.03133455,0.018118838,-0.02213344,-0.004177766,-0.042849354,-0.035783574,0.02632371,-0.006514829,0.0023595924,-0.03298449,0.017722724,-0.037719794,0.06379884,0.046962764,-0.016185457,0.0135374125,0.049640544,0.027813926,0.008710412,0.013406231,0.0017554034,0.024359219,0.023783773,0.027697371,-0.018883733,-0.014335093,-0.016688343,-0.033896364,-0.04607623,0.035693508,-0.031702615,-0.07303218,0.0048468695,0.036396686,-0.0010325729,0.048737034,-0.052755754,-0.064470075,-0.0556445,-0.03033661,-0.071696565,0.044337325,0.0016750309,0.034551125,0.014984384,-0.023931963,0.09748393,-0.06786766,-0.015474244,-0.0358178,0.022942966,-0.05259216,-0.0332774,0.017261881,0.0028289375,-0.029689206,-0.020841438,0.029879391,0.088758856,-0.013705284,0.00507895,-0.02460788,-0.0061114416,0.014107579,-0.021345679,0.00024221269,0.0051620365,0.010543987,-0.030686256,0.0911885,-0.025672905,-0.046464782,0.021771777,-0.015716325,-0.07795048,-0.005621157,0.012220508,0.0059018782,0.038017318,0.026458368,-0.01739032,0.03910696,-0.048361495,0.00038239502,-0.053815454,0.011502108,-0.027529564,0.0113587435,0.05466731,0.015395759,-0.05892834,-0.003794248,-0.04257328,-0.035693847,0.011094587,0.019229269,0.040905673,0.05212557,0.0010306357,-0.03027539,-0.05294514,-0.03223674,-0.022554718,0.029292187,-0.021209642,0.034919694,0.016235592,0.030080859,-0.037915476,0.009544301,-0.0003674497,-0.008971869,0.0011825631,-0.033544865,-0.03787642,-0.034036506,-0.008810006,0.015576297,-0.020566193,-0.043202035,0.08228935,0.04315746,-0.0596265,-0.018588407,-0.028566027,-0.044760738,0.0032117344,0.00032208924,-0.021932118,-0.004363547,-0.01969428,-0.029926358,0.06625934,0.023865996,0.0030250151,-0.030968558,0.00051885174,0.010396735,0.022738976,0.03603615,-0.022335622,-0.11485392,0.03998521,-0.0021593787,-0.03916729,0.00054023456,0.0122729875,0.0062950295,-0.029331876,-0.008252079,-0.02852898,-0.03122117,-0.033290107,0.040061448,-0.020791732,0.0036247931,0.0079544345,-0.061411366,-0.0035940465,-0.004971615,-0.016020944,-0.018150676,0.04162679,-0.0030124872,0.010949282,0.015262144,-0.0007678458,-0.009088826,0.0030903595,-0.014471534,0.060414847,-0.028221527,0.049579497,0.009510931,0.017236788,0.071388885,-0.014025779,0.024242554,-0.02849651,0.05560548,0.012099662,-0.02327532,-0.003773357,-0.029307365,-0.08241097,0.0128994845,-0.0066153025,0.0052944776,0.020208873,0.016563311,-0.053602397,-0.033784796,0.024110403,0.03099974,0.009047235,0.040652085,0.0025698629,-0.056668784,-0.007747658,-0.048212394,0.025343707,-0.039680056,-0.049724344,0.020668356,-0.030639637]	Keywords: gradients, warm-up, post-LN, pre-LN, Transformer\nKey Objects: gradients, learning rate warm-up\nRefers to Images: None\nHypothetical Questions:\n- Why are large gradients a problem during the initial stages of training?\n- What is a learning rate warm-up, and why is it needed for post-LN Transformers?\n- What is the fundamental difference in gradient behavior between pre-LN and post-LN Transformers?\n---\nSummary:\nXiong et al. investigated Transformer gradients and found large gradients near the output layer in post-LN Transformers, which can cause unstable training without a learning rate warm-up. This issue is not present in pre-LN Transformers, allowing for the safe removal of the warm-up stage.\nOriginal Text:\nXiong et al. [151] theoretically investigate the gradients of Transformers and find that the gradients near the output layer are large at initialization in post-LN Transformers, which could be the reason why post-LN Transformers without learning rate warm-up [137] 11 leads to unstable training, whereas pre-LN Transformers do not suffer from the same problem. They thus deduce and empirically verify that warm-up stage can be safely removed for pre-LN Transformers.\nContextualized Text:\nIn studying Transformer models, Xiong et al. found that post-LN Transformers, when not using a learning rate warm-up, experience unstable training due to large gradients near the output layer. This issue is not observed in pre-LN Transformers, allowing for the safe removal of the learning rate warm-up stage.	{"tags": ["architecture", "NLP", "transformer", "training"], "doc_id": "3eed7a8f-ab2c-44f5-8397-20e7e4c05a69", "summary": "Xiong et al. investigated Transformer gradients and found large gradients near the output layer in post-LN Transformers, which can cause unstable training without a learning rate warm-up. This issue is not present in pre-LN Transformers, allowing for the safe removal of the warm-up stage.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["gradients", "warm-up", "post-LN", "pre-LN", "Transformer"], "key_objects": ["gradients", "learning rate warm-up"], "contextual_text": "In studying Transformer models, Xiong et al. found that post-LN Transformers, when not using a learning rate warm-up, experience unstable training due to large gradients near the output layer. This issue is not observed in pre-LN Transformers, allowing for the safe removal of the learning rate warm-up stage.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.2 Layer Normalization"}, "hypothetical_questions": ["Why are large gradients a problem during the initial stages of training?", "What is a learning rate warm-up, and why is it needed for post-LN Transformers?", "What is the fundamental difference in gradient behavior between pre-LN and post-LN Transformers?"]}
5313702d-06b5-4188-b07a-ae77b007b271	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.011028745,0.032070696,0.018370174,0.009459893,-0.013892113,0.08503442,0.011761309,0.0334421,0.01161209,-0.02978621,-0.033750564,0.03025159,0.008463131,0.0069195,-0.042294513,0.003140557,0.017077276,0.100049645,-0.0004403606,0.0067494083,0.07446059,0.0077812066,-0.034043185,-0.009300191,-0.025203982,0.04527008,0.062658705,0.00024147135,-0.0051679234,-0.01876062,-0.04831523,-0.003164415,0.01105852,0.0007417092,0.008123192,-0.0031216631,-0.012192556,-0.02383836,-0.017866071,-0.08382967,-0.06290138,0.066278584,-0.08303969,0.020498719,0.005330246,-0.048088867,-0.12574264,-0.049865857,0.0059083286,-0.013813889,-0.04745609,0.0024184124,-0.038537465,0.029724667,-0.014910197,-0.0069538974,0.004400631,-0.034813248,-0.017177414,-0.044755574,-0.019089734,0.040891517,-0.028644247,-0.029358998,0.0147904465,-0.027333803,0.026541473,0.023633372,0.01901795,0.11210941,-0.027234789,0.02978397,-0.012700212,-0.045771398,0.1271335,0.012776764,-0.01228492,-0.03487041,-0.03829163,-0.018558074,-0.034865588,0.089813,0.009168498,-0.08330923,0.075849734,0.0015206961,-0.08542509,-0.02491086,0.08056938,-0.03687056,-0.05799464,-0.0038686118,0.02413627,0.061174016,-0.006295138,-0.08591541,-0.023038257,-0.05994615,-0.01433886,-0.032762926,-0.0030528677,-0.0073881247,0.082020074,0.0944323,0.06293612,0.0097177075,-0.031674847,-0.036883492,0.026513726,0.0092275,-0.032842,0.029241074,-0.038933013,0.01879259,-0.08439292,0.008780082,-0.02142801,0.026499283,0.009577736,-0.029518805,-0.01492994,-0.006185943,0.0017716455,0.04117225,0.060723357,0.065784,0.023819694,-0.030835103,-0.029840823,0.0011618728,0.040818803,-0.005438352,-0.018245427,-0.0025696112,0.013928266,-0.023069289,0.06079718,0.003812038,-0.007657073,0.026832238,-0.0019031238,-0.023104971,-0.059907123,-0.008806737,-0.056323994,0.052300353,-0.032462254,0.008741886,-0.01509317,0.07512053,0.002791158,-0.029375106,0.021291366,0.000773083,-0.04671767,-0.018575236,0.00026575397,-0.03892675,-0.010671147,-0.017412683,-0.041634995,-0.060025025,-0.015715633,0.08381413,0.03536297,0.07786065,0.009652031,0.026756508,0.03557799,-0.01287462,0.0068855635,-0.022790695,-0.01368546,-0.039958507,0.00028844617,-0.03452391,-0.024718018,-0.0028718314,-0.007084379,0.00572024,-0.010020216,0.020964189,-0.037472595,0.011420264,-0.041309662,-0.04014578,0.016189886,0.060163572,-0.05572365,-0.0057898727,0.024019822,-0.003883201,-0.011948137,0.02393061,0.044476535,0.021609938,0.038502548,-0.07125337,-0.035456885,-0.027045606,0.0058277077,0.034986284,0.025173178,-0.012425169,0.10181187,-0.010761595,-0.0026144104,0.027048511,-0.038355976,0.028179001,0.0113302255,-0.024078202,-0.0018571932,0.025949735,0.014989124,0.010465641,0.0026969344,0.039902125,0.05856366,7.0342685e-05,-0.06687356,-0.004586554,0.016773844,0.03830135,-0.046274316,0.009624813,-0.0336798,0.017911468,0.014117048,0.010917248,0.04191771,0.028008865,-0.039133847,0.05569163,-0.014036911,0.019633466,-0.02645608,-0.014749924,0.0017163983,0.06512413,0.09880941,0.010966848,-0.02124569,0.08960384,-0.003397554,-0.033173125,0.0012457579,0.0477068,0.01568564,0.040172637,0.0008383416,0.033420097,0.018699763,0.017174391,-0.0027710504,-0.035642933,0.0024736447,0.033686712,0.023655696,-0.006691692,-0.008535209,0.03024478,0.02939952,0.019308416,-0.01808046,0.0048478046,-0.0058156955,0.028505404,-0.0055626216,-0.013317488,0.05201907,0.0196815,0.040144946,-0.024992567,-0.041273538,0.046249658,-0.006735559,-0.059388313,0.051586486,-0.035607923,0.010261882,-0.012622229,0.031865448,0.0075830165,-0.013446441,-0.0044048387,-0.052967984,-0.003086656,-0.029849993,0.01464806,-0.028721705,-0.05102749,0.038685206,0.012853314,-0.04884293,0.048876666,0.0112534445,-0.019617734,-0.023677453,0.05235827,0.09368714,-0.015618841,-0.0045539965,-0.008752785,-0.012780976,0.0401154,-0.091746315,0.016799122,0.044603113,0.010858463,0.02072802,-0.031032026,-0.02451596,-0.030247794,0.044719215,0.051284663,0.033077877,-0.07149196,0.01730037,0.0020729343,0.0061355606,-0.008541325,0.0037662685,0.05385098,0.02656403,-0.024893172,-0.023524951,-0.013930834,-0.089372836,0.07864736,-0.064301305,0.041636705,0.06065022,-0.02149773,0.020255975,-0.02298535,0.03930861,0.027914941,-0.0115970895,0.033715297,-0.01564319,-0.076513305,-0.030937579,0.0244259,-0.023324778,0.0420529,-0.02939355,0.016685495,0.03297402,0.05394584,-0.014391729,0.052856434,0.0423599,0.012952692,0.04183569,0.019365678,0.0024105136,0.034692567,0.011912523,0.004208561,0.067239486,-0.012297748,-0.0100593325,-0.012376012,-0.0012998455,0.008753828,-0.050214037,-0.017213764,0.037398167,-0.026385512,0.012593178,-0.02970968,0.07033976,-0.031183768,-0.004057414,0.02600949,0.042056877,0.004617727,-0.029934749,-0.0458984,0.014833205,0.027142568,-0.022546038,-0.036926635,0.019573897,-0.0018047782,0.032465156,0.10528479,-0.008419334,0.056357827,-0.009237777,0.04123919,0.010365601,-0.054170776,-0.056041848,0.020508623,0.007808348,0.026693754,-0.016277684,-0.01628855,0.020088397,0.035396855,-0.04942953,0.04202992,-0.012168605,0.045538973,-0.04582202,0.012486349,0.036172625,-0.004262388,0.030495336,-0.034569122,-0.08687013,0.003433962,0.0014445707,-0.00017858595,-0.0060225525,0.007730241,-0.03002909,-0.005844,-0.029297212,0.03115374,-0.011447907,0.04848192,0.0024834974,-0.013954316,-0.0018571152,0.012668284,-0.027491847,-0.008846795,-0.00087126845,-0.0058647636,-0.023651386,0.057767127,0.015395181,0.018360851,-0.035236377,0.04933579,-0.048187677,0.04126234,0.03374979,-0.031057967,0.031350855,0.016581938,0.0011226782,-0.0142178275,-0.0016165901,-0.012691533,-0.008828654,-0.0019728455,0.0032448475,0.020617232,0.033393137,-0.0049960255,0.029143956,-0.012312864,0.049948465,-0.008450852,-0.007035905,-0.03882244,-0.0505942,0.02470559,0.019378386,-0.0025939248,0.027117733,-0.03522907,-0.03907215,-0.038697813,-0.04313454,0.002793414,-0.015997253,0.017447013,-0.049095176,0.035691477,0.01775671,0.014521625,0.018581392,0.027914058,-0.013305272,-0.022637114,-0.05535686,-0.007060744,-0.0060249753,0.05098225,-0.0045362264,0.014173428,-0.01903337,-0.03648346,-0.06406795,0.009840516,-0.009380727,0.027644215,0.035651032,0.0011823463,0.012492709,0.008517741,-0.017639233,0.011054998,-0.014473659,-0.020315781,-0.034056664,-0.015107719,0.056175455,0.03402884,-0.015947314,0.039208148,-0.008304553,0.035126787,-0.043516178,0.031171935,0.002024849,-0.0063120984,0.03790365,-0.035862528,-0.0020607694,0.029377097,-0.016907666,-0.009288838,0.05846415,0.02546853,-0.014448375,0.024056174,0.019427398,-0.024657927,0.03395007,0.002892017,0.012155929,0.004832047,0.012050868,-0.052885395,-0.008981978,0.007050474,-0.029443646,0.014213758,-0.079163745,-0.03169788,0.03415473,-0.027779296,-0.009072905,-0.0045355177,-0.038413584,-0.016709635,0.015268194,-0.023078794,0.08116475,-0.052931737,0.042563707,0.05217808,0.032788336,0.10209299,0.019397506,-0.026982572,-0.020295978,-0.019609362,-0.049437094,0.006896794,0.047945827,0.0024591626,-0.020874724,0.06224365,-0.034488082,0.0041123745,-0.030335974,-0.0282974,-0.0010884373,-0.023538787,-0.032930776,-0.022198563,0.0053423955,0.01702363,0.0010407777,-0.0106199235,-0.028409319,0.029596034,-0.033698034,0.028948916,0.05585472,-0.028457396,0.009559349,0.03516497,0.020890115,0.020685138,0.0021666682,-0.022132415,0.01800275,0.011539083,0.02771951,-0.003113244,0.018002216,-0.0108998455,-0.036246423,-0.041383326,0.02446862,0.026889188,-0.049983058,0.0040108045,0.040455732,0.021581573,0.030212743,-0.063939594,-0.04786926,-0.08385929,-0.045005642,-0.06860522,0.050368726,0.016588742,0.038409557,0.018727338,-0.035371255,0.047157533,-0.09092031,-0.0075555574,-0.0244092,0.024641005,-0.04600916,-0.025680875,0.022906112,-0.0133769745,0.019361045,-0.035818223,0.03255271,0.07159744,0.0027990483,0.016269308,-0.048774224,-0.0024238161,-0.011187373,-0.0015748879,-0.015954526,0.0045703617,0.028860414,-0.032843407,0.06955111,-0.004914496,-0.035878614,0.013770857,-0.009368138,-0.07789887,-0.00393456,0.013392322,0.034435496,0.0379874,0.033334363,0.0123329805,0.027177105,-0.017835744,-0.025729042,-0.045696925,-0.02303871,0.0027182736,0.0108781755,0.04668581,0.014532555,-0.045742106,0.0041664536,-0.029611157,-0.012822727,0.016644266,0.026908047,0.07078107,0.04792934,0.04481934,-0.0034110888,-0.09299502,-0.0255373,0.0016632229,0.05004176,-0.027284043,0.015060018,-0.0063535445,0.012276584,0.007053182,0.026223117,-0.00031140732,-0.019800879,-0.033230193,-0.031562172,-0.019964663,-0.030749144,-0.0031947452,0.004686192,-0.02015074,-0.034438,0.08177681,0.03366273,-0.061920606,-0.032934215,-0.045149595,-0.030085482,-0.0037664264,0.011511849,0.014156755,0.027169773,0.014019609,-0.017094573,0.064445175,0.018098412,-0.035916258,-0.03413831,0.006784526,0.056447614,0.028209979,0.027584301,-0.037211724,-0.09464306,0.03581205,0.013767915,-0.038109638,0.015840676,0.02260381,-0.01568141,-0.017447392,0.015467901,-0.024472384,-0.02461772,-0.04199955,0.049169477,-0.0010652624,-0.016989497,0.011319214,-0.029260477,-0.03161707,-0.016526885,-0.011655305,-0.044663005,0.047363985,-0.020250961,0.011038907,-0.0074265655,0.02088893,-0.01635183,-0.022519503,0.0024981794,0.05467524,-0.04042693,0.034578234,-0.017840847,0.0052627055,0.07824869,0.028863901,0.03266646,-0.022136912,0.08907007,-0.0142916795,-0.024487682,0.0035625203,-0.045416016,-0.09943235,0.011985883,0.010463551,0.030600155,0.0012106952,-0.029244307,-0.06151951,-0.055596393,-0.004315407,0.0077985236,0.06813877,0.042731203,0.03868853,-0.05204571,-0.016365508,-0.062231433,0.05405946,-0.019240802,-0.013314046,0.037194103,-0.005564555]	Keywords: post-LN, pre-LN, gradient imbalance, residual branch, output shift\nKey Objects: post-LN Transformers, residual branch, output shift\nRefers to Images: None\nHypothetical Questions:\n- Why do post-LN Transformers sometimes exhibit unstable training?\n- What is the 'amplification effect' mentioned in the text?\n- How do the introduced parameters help to control the output shift in post-LN Transformers?\n---\nSummary:\nDespite often leading to unstable training and divergence, post-Layer Normalization (post-LN) Transformers typically outperform pre-LN variants after achieving convergence.\nOriginal Text:\nAlthough Post-LN often results in unstable training and divergence, it usually outperforms preLN variants after convergence [83]. Similar to Xiong et al. [151], Liu et al. [83] conduct theoretical and empirical analysis and find that post-LN encoders do not suffer from gradient imbalance. They thus conjecture that the gradient issue is not the direct cause of unstable post-LN Transformer training and further identify the amplification effect in post-LN Transformers - at initialization, the heavier dependency on residual branch leads to a larger output shift in post-LN Transformers, thus resulting in unstable training. In light of this finding, they introduce additional parameters to post-LN Transformers to control residual dependencies of Post-LN. These parameters are initialized according to activation variations of sample data so that the output shift of post-LN Transformers is  \n$^{10}$To the best of our knowledge, this approach is adopted since v1.1.7 in the Tensor2Tensor implementation [136].\nContextualized Text:\nAlthough post-LN Transformers frequently exhibit unstable training and divergence, they generally achieve better performance than pre-LN variants once training converges. Researchers found that post-LN Transformers do not suffer from gradient imbalance and that the heavier dependency on the residual branch leads to a larger output shift, resulting in unstable training.	{"tags": ["normalization", "training", "architecture", "transformer"], "doc_id": "5313702d-06b5-4188-b07a-ae77b007b271", "summary": "Despite often leading to unstable training and divergence, post-Layer Normalization (post-LN) Transformers typically outperform pre-LN variants after achieving convergence.", "doc_type": "text", "entities": ["Tensor2Tensor"], "keywords": ["post-LN", "pre-LN", "gradient imbalance", "residual branch", "output shift"], "key_objects": ["post-LN Transformers", "residual branch", "output shift"], "contextual_text": "Although post-LN Transformers frequently exhibit unstable training and divergence, they generally achieve better performance than pre-LN variants once training converges. Researchers found that post-LN Transformers do not suffer from gradient imbalance and that the heavier dependency on the residual branch leads to a larger output shift, resulting in unstable training.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.2 Layer Normalization"}, "hypothetical_questions": ["Why do post-LN Transformers sometimes exhibit unstable training?", "What is the 'amplification effect' mentioned in the text?", "How do the introduced parameters help to control the output shift in post-LN Transformers?"]}
29d9c611-e000-4654-8140-f4e1da93160b	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.039028335,0.07819432,0.005953567,0.011048976,-0.050461177,0.07433042,0.0037485985,0.006069092,0.022157438,-0.027606975,-0.030379143,0.009705879,0.013710892,0.031602774,-0.052580055,-0.012510378,0.01277408,0.09122149,0.009672938,0.024822947,0.06794278,0.026377669,-0.009645695,-0.0014725231,0.007474065,0.0879178,0.07605959,-0.015181806,0.0015434506,-0.01360895,-0.023557387,-0.01138534,0.009068198,0.0066160704,0.0025112089,0.0032086247,-0.011185094,-0.015037148,-0.021363763,-0.10474752,-0.051651753,0.049900237,-0.0638316,-0.00815798,0.030624509,-0.040920462,-0.08702885,-0.04434467,0.012968179,-0.0495661,-0.05249889,0.041055236,-0.045096338,0.030455172,-0.013538534,-0.010212991,0.0106237205,-0.021639738,-0.019992188,-0.04693571,-0.016262798,0.030594291,-0.06610887,-0.03180194,0.009524963,-0.017931152,0.02999508,0.020482969,0.009316137,0.102366656,-0.02912658,0.02114974,-0.023821255,-0.035592448,0.13624297,0.06300964,-0.017954364,-0.04517506,-1.1512459e-05,-0.001357415,-0.005040926,0.028090369,-0.021623546,-0.02610694,0.0793672,-0.016356157,-0.08647738,-0.052800726,0.051596504,-0.009988874,-0.023473421,-0.042786784,-0.024808595,0.033953037,0.029754957,-0.011047622,-0.0016269728,-0.04109745,-0.013875364,-0.011296619,-0.014927241,-0.035901636,0.06090295,0.08709595,0.041385945,0.009114514,-0.056981068,-0.035414018,0.014075155,0.004768357,0.0014465763,0.039443824,-0.04655044,-0.022589374,-0.08275359,-0.049145393,-0.038540058,0.025963204,0.00048551633,-0.055648163,0.040109336,0.013528869,0.01628022,0.025893355,0.020032572,0.06785249,0.047380872,-0.019677466,-0.011677865,-0.017419133,0.025702324,-0.0051751183,-0.014966421,0.038056176,0.00094037387,-0.006211156,0.04969295,0.043746024,0.0051456466,0.008883544,-0.03404213,-0.008703604,-0.05612364,0.0018502625,-0.033087533,0.028555678,-0.050444238,0.019237913,-0.029668147,0.07541733,0.023945795,-0.011951223,0.008249088,0.008499739,-0.020869987,-0.0457228,-0.021915276,-0.0030640955,-0.014511213,-0.0463362,-0.023960108,-0.05608985,-0.0056233294,0.08041438,0.04747682,0.061492078,0.014353089,0.03490131,-0.0022123898,0.021724647,-0.012797993,-0.027749302,-0.0019397798,-0.048428506,-0.012733756,-0.029556056,-0.0019944548,-0.043405667,-0.021595193,0.01604783,-0.029420376,0.019486923,-0.05891568,-0.009845057,-0.040031616,-0.016850818,0.007871113,0.040006775,-0.03295614,-0.04651071,0.029186683,0.010128568,0.0134732975,0.049855143,0.04703006,0.0028981236,0.06639492,-0.03438931,-0.012377504,-0.051114663,-7.882753e-05,0.037136808,-0.0041123726,0.005266538,0.077262,-0.010346348,-0.013623588,0.033503354,-0.067912675,0.028261399,0.008641395,-0.015064695,-0.02620513,0.029358495,-0.013037246,0.0025734596,-0.031069677,0.030936053,0.058888085,0.0024378747,-0.038666885,-0.0014475234,0.02146364,0.05133658,-0.029494831,0.016487502,-0.00040797392,0.028402,0.06667165,0.053686827,0.0020568413,0.0353726,-0.058350015,0.057902128,-0.008560821,0.008109483,0.0062504564,-0.010224756,0.030585865,0.042405672,0.058434572,0.034136113,-0.021756567,0.09651538,0.019637072,-0.014442442,0.018181937,0.04394139,0.0231789,0.025694983,-0.03134525,-0.0053464635,-0.010096508,-0.00011857682,0.027331183,-0.01042206,0.0017539302,0.049047105,0.0076163798,-0.020193294,0.036306754,0.027443629,0.002801984,0.012912134,0.020382538,0.018940745,0.015335728,0.009459621,-0.0024315102,0.011081392,0.0348936,0.041421905,0.05325508,-0.023721045,-0.010660655,0.02780446,-0.029023416,-0.011660963,0.077439554,0.0034974136,0.0024266734,0.017698895,0.022644714,0.03160205,-0.009843969,-0.0075430567,-0.06248103,0.00580062,-0.005751867,0.05198563,-0.009916616,-0.030660048,0.014987222,0.03430383,-0.030488245,0.0059408383,0.00284926,-0.020285178,-0.06198927,0.044556353,0.111400366,0.0021275599,0.022513192,0.017312923,-0.021979708,0.043150533,-0.05268014,0.031248884,-0.020755777,0.025800617,0.016574476,-0.032834858,-0.022695063,0.008636539,0.039755058,0.048559558,0.03466661,-0.06625354,0.011591134,0.006770858,0.010890742,0.0012294583,-0.009917544,0.080958225,-0.019736836,0.00023790305,-0.0039791143,-0.06862657,-0.10781391,0.059365455,-0.066589996,0.042998612,0.069273494,-0.020058878,0.020315848,-0.051980235,0.037798777,0.03538667,0.023791192,0.049480837,-0.0068541635,-0.09029301,-0.009145613,0.007394258,-0.03405428,0.03837482,-0.022647446,-0.032936238,0.036299504,0.07872356,-0.0022158227,0.022438487,0.017011382,0.029023277,0.067881174,0.0059972745,-0.01810683,0.0143853305,-0.011967639,0.009953659,0.05066908,-0.021445667,0.035380818,-0.023970643,0.0077823186,0.03731436,-0.057747066,0.018279329,-0.018486058,-0.015445566,-0.0008066519,0.0134866405,0.056090526,-0.052031364,0.02287773,0.050883263,0.04620829,0.008803644,-0.030683596,-0.0822285,0.046580005,0.02060339,-0.00072352646,-0.019373642,0.07229638,-0.0016387808,0.035219714,0.09358513,-0.05860923,0.015039996,-0.0035564983,0.057096478,0.0052204104,-0.047945317,-0.030191135,0.024176024,-0.032664653,-0.004104067,-0.009839041,-0.03219974,0.024167359,0.008206104,-0.034438495,0.056672033,-0.0350548,0.019507432,-0.06381336,0.0020748104,0.06810763,0.006420872,0.04999297,-0.011204356,-0.07508676,-0.04369062,-0.0022060617,0.027151534,-0.0015375767,-0.012278586,-0.015592378,-0.029850395,-0.037169177,0.051646747,0.0028676442,0.009656289,0.005052894,-0.02382835,0.003369,0.013718058,-0.029310668,-0.0014700489,-0.014681338,-0.011989445,-0.03261945,0.057322104,0.019211037,-0.00080594036,-0.0387235,0.059373457,-0.013523092,0.02160036,-0.04292325,-0.030454712,0.028209042,-0.002666999,0.0047072778,-0.052937996,0.014967726,-0.029386822,-0.008312245,0.012898946,0.023784138,0.035100237,0.007093437,-0.0001425137,0.05823881,0.014751925,0.0009762158,-0.011336962,-0.0011942707,0.012803705,-0.052234028,-0.017442007,0.008851369,-0.022831228,-0.009299043,-0.05310694,-0.01424359,-0.051047534,-0.055173025,-0.002208833,-0.016526597,0.003393805,-0.058432385,0.008335255,0.0380372,0.030007368,0.0060652927,0.03157776,-0.007565007,-0.023960926,-0.07274571,0.0017820192,0.01052571,0.03417861,-0.022664396,0.018255105,-0.0117838625,-0.014222321,-0.02481721,0.011116346,-0.0053062197,0.04148958,0.00508565,-0.026217148,0.020909073,-0.006269852,0.043109525,0.00012421561,-0.017694192,-0.020109413,0.0015154788,0.0047930535,0.077962615,0.015067469,-0.026069414,0.084281795,0.0032952663,0.011310742,-0.05518938,0.046151023,0.0124469735,-0.02951064,0.055304226,-0.020851158,-0.027663384,0.026537843,-0.020316498,0.015887443,0.04198765,-0.02950919,-0.021330902,-0.005595146,-0.0046614152,-0.017547995,-0.002441913,0.01886808,-0.013190349,0.024670755,-0.033460207,-0.07405728,-0.021003516,-0.017881714,0.0149573395,0.04450554,-0.045131367,-0.038673677,0.038853686,-0.015222087,0.0034101931,-0.026265373,-0.024604177,-0.009412639,0.0075167697,-0.0253745,0.07005692,-0.070954055,0.026772816,0.052504905,0.03471743,0.07723974,0.0426015,-0.01857813,0.023661584,0.0301917,-0.049838997,0.033338685,0.04506942,-0.0065413527,0.008801167,0.0016883201,-0.03324351,-0.030445548,0.0118091535,-0.022023883,-0.01990279,-0.031835157,-0.049384285,-0.039220598,-0.015518824,0.012963297,-0.022108005,0.0070948834,-0.024826115,0.004955163,-0.027840992,0.050050423,0.051176917,-0.020752188,-0.0006344664,0.042686064,0.017018456,-0.033557117,0.047199547,0.0097555155,0.014597991,0.025186947,0.022415718,0.009034782,-0.0057416083,0.020791868,-0.057709496,0.0026458676,0.031407863,-0.007952536,-0.08007558,0.0043598777,0.016732583,-0.022959366,0.046000436,-0.06547908,-0.013546877,-0.049145583,-0.038792305,-0.03301858,0.09090094,0.018301927,0.04081049,0.019492473,-0.04115217,0.018917212,-0.078730814,0.0050618122,0.0024146072,-0.0063235853,-0.060977284,-0.029626999,0.002467972,-0.032040313,0.004940015,-0.040625382,0.05087821,0.04197752,-0.048955537,0.013073108,-0.04827288,-0.03660674,-0.0032831132,0.020976646,0.0063136397,0.0028589417,0.011570116,-0.0047744624,0.0740977,-0.009555879,-0.019831233,0.04021935,0.0076296693,-0.04889431,-0.009621435,0.0023876114,0.016525747,-0.0008361638,0.025310516,0.014208441,-0.0023168167,0.0050860406,-0.030013468,-0.08367431,-0.022965642,-0.034162562,0.0009376876,0.038025312,0.019849803,-0.027708437,-0.038773295,-0.015419684,0.03125701,0.03945783,0.02535659,0.015934857,0.048823107,0.08053264,0.019673102,-0.078796275,-0.037743468,-0.042258002,0.036474306,0.024044167,0.011917679,-0.006175103,0.014181557,0.028280359,-0.0004671025,-0.019340068,-0.029685717,-0.024262914,-0.07714438,-0.012723151,-0.016594727,-0.042564802,0.02682598,-0.02264844,-0.0256279,0.08467336,0.022848526,-0.012851208,-0.030491449,-0.018836133,-0.003865208,-0.01943261,0.046807855,0.0027456244,0.006409989,0.01954556,-0.03621589,0.04416441,0.040409893,-0.08433196,0.011516404,-0.008540873,0.0054585505,0.022338655,0.039535884,-0.038449947,-0.03537736,0.056434646,0.01839674,-0.029722143,-0.0010860119,0.0074217566,-0.04303235,0.011099292,0.016640311,-0.03791572,-0.0031054753,-0.026787926,0.03941791,0.01903668,-0.025964662,0.023944197,-0.055084396,-0.041465405,0.008821992,0.0023079782,-0.020135308,0.032953788,0.01026995,0.057685513,-0.07068543,0.039013173,0.0198324,-0.016774107,0.025873397,0.06921963,0.016980665,0.03553769,-0.025161209,0.0115362005,0.085097864,0.02559998,0.0010720708,-0.0061110347,0.030611163,-0.035270136,-0.03907126,-0.004856255,-0.03079452,-0.076213755,-0.0072922325,0.014056662,0.050151113,-0.006552412,-0.058887053,-0.051799264,-0.038242493,0.010859145,-0.021942148,0.009705166,0.054118644,0.04138416,-0.012866704,-0.055212114,-0.035950676,0.044067472,-0.02139858,0.0126907695,0.01772523,-0.025416143]	Keywords: post-LN, pre-LN, residual dependencies, convergence\nKey Objects: post-LN Transformers, pre-LN Transformers, residual dependencies\nRefers to Images: None\nHypothetical Questions:\n- Why do post-LN Transformers sometimes lead to unstable training?\n- How do the introduced parameters help to control residual dependencies in post-LN Transformers?\n- What is the significance of using Tensor2Tensor in the implementation of this approach?\n---\nSummary:\nLiu et al. introduced parameters to control residual dependencies in post-LN Transformers to prevent output shift, ensuring convergence and achieving better performance than pre-LN Transformers.\nOriginal Text:\n$^{10}$To the best of our knowledge, this approach is adopted since v1.1.7 in the Tensor2Tensor implementation [136].  \n$^{11}$Learning rate warm-up refers to starting optimization with an extremely small learning rate and then gradually increasing it to a pre-defined maximum value in a certain number of iterations.  \nnot amplified. This approach ensures and boosts convergence of post-LN Transformers and reaches better performance than pre-LN Transformers.\nContextualized Text:\nTo address instability issues and improve convergence in post-LN Transformers, Liu et al. introduced additional parameters designed to control residual dependencies. This approach prevents the amplification of output shift, leading to a more stable training process and ultimately enabling the post-LN Transformers to achieve better performance than pre-LN variants. This technique was adopted since version 1.1.7 in the Tensor2Tensor implementation.	{"tags": ["normalization", "training", "optimization"], "doc_id": "29d9c611-e000-4654-8140-f4e1da93160b", "summary": "Liu et al. introduced parameters to control residual dependencies in post-LN Transformers to prevent output shift, ensuring convergence and achieving better performance than pre-LN Transformers.", "doc_type": "text", "entities": ["Tensor2Tensor"], "keywords": ["post-LN", "pre-LN", "residual dependencies", "convergence"], "key_objects": ["post-LN Transformers", "pre-LN Transformers", "residual dependencies"], "contextual_text": "To address instability issues and improve convergence in post-LN Transformers, Liu et al. introduced additional parameters designed to control residual dependencies. This approach prevents the amplification of output shift, leading to a more stable training process and ultimately enabling the post-LN Transformers to achieve better performance than pre-LN variants. This technique was adopted since version 1.1.7 in the Tensor2Tensor implementation.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.2 Layer Normalization"}, "hypothetical_questions": ["Why do post-LN Transformers sometimes lead to unstable training?", "How do the introduced parameters help to control residual dependencies in post-LN Transformers?", "What is the significance of using Tensor2Tensor in the implementation of this approach?"]}
2373992f-cf4a-4662-b6a6-1656c2f2038d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.020745644,0.067609176,0.0074986597,0.0013213705,0.003342573,0.05428072,0.02045551,0.04031017,0.018817961,-0.014182229,-0.006121642,-0.00059731916,0.0010457331,-0.003237287,-0.048433263,0.0011469098,0.008851714,0.09528906,-0.004533401,-0.03146956,0.043582935,0.018346865,0.026460998,-0.024242196,0.025794007,0.05970414,0.03809553,0.02345386,0.036252446,-0.015775912,-0.014056504,-0.052193858,0.016654588,0.038748126,-0.004931138,-0.00062386016,-0.011482767,-0.03090938,-0.009598052,-0.0813316,-0.036060028,0.068069026,-0.06319724,-0.02406292,0.00991885,-0.02090062,-0.102296926,-0.023979561,0.011089856,-0.031139273,-0.06756181,0.062295504,-0.03724434,-0.0032052386,-0.040415823,0.010505751,0.008095503,-0.03982923,-0.0017499666,-0.062873684,-0.027492281,0.023089094,-0.03697889,0.002774137,0.073163785,-0.027758567,0.037091922,0.05177614,0.03795791,0.10806661,-0.056553774,0.04433772,-0.010414315,-0.038460445,0.09837716,0.041965283,-0.039105114,-0.025443844,-0.036119886,-0.061444126,0.00022361992,0.038334157,-0.01983253,-0.043441415,0.0708127,0.014087612,-0.07214087,-0.040993266,0.05677216,-0.034003265,-0.03937584,-0.018670423,0.02394123,0.05352692,0.020242184,-0.04598765,-0.00210255,-0.04613001,0.004017163,-0.033703696,-0.000936718,-0.0061962106,0.075165644,0.10745463,0.043935753,-0.03786009,-0.05443222,-0.003166192,-0.0025021152,0.042254984,-0.0024406868,0.058831923,-0.06986683,-0.004104279,-0.014990476,0.0085206,-0.009704731,0.0028414098,0.02953089,-0.026592439,0.038416754,-0.018184738,-0.015095947,0.047661178,0.041654695,0.030993655,0.038718354,-0.04048304,-0.03760254,0.014871564,0.053507157,-0.010534336,-0.040408958,9.24033e-05,-0.017852899,-0.036799982,0.063654,0.003559926,0.0039164904,0.010791868,-0.00014650937,-0.016251715,-0.060165163,-0.017361378,-0.05179683,-0.005339732,-0.059156463,0.031190343,-0.022129832,0.04036136,0.026965035,-0.018791601,0.027039228,0.024521343,-0.053938594,-0.011857621,-0.005295259,-0.03820709,0.021797009,-0.00431174,-0.030776776,-0.042766646,-0.02307822,0.06693614,0.025143974,0.09779877,0.027299345,0.026461229,0.035285715,0.010691631,-0.026227077,-0.0062770206,-0.020971717,0.014082676,-0.02157768,-0.044362355,-0.0006423193,-0.06316465,-0.007041291,0.04282911,-0.01010102,0.00786847,-0.058331918,-0.028646097,-0.07218694,5.4057393e-05,0.0258282,0.03046365,-0.02373094,-0.029173655,0.026327176,-0.0022440865,-0.0056854947,0.0012526677,0.024712747,0.020695806,0.04363132,-0.051454306,-0.020170456,-0.015561985,0.039208867,0.017083893,0.010735499,-0.061250176,0.06689269,-0.03970901,0.017160604,0.014549608,-0.0077679427,0.025142446,0.019700835,-0.001261404,0.0026162781,0.053734463,0.012140856,-0.00957512,-0.021978725,-0.01190934,0.061069086,-0.03199727,-0.035682153,0.0056583853,-0.009068517,0.069092155,-0.0310251,0.014169621,-0.019268984,0.016164338,0.048776448,0.024101287,0.02411067,0.031196931,-0.08775274,0.03648946,-0.031916153,0.031096995,-0.0033311609,-0.011438423,0.007423097,0.022914909,0.08095286,0.030237647,-0.0030964273,0.08925977,-0.016527822,-0.008055534,0.03807658,0.035303205,0.0005630289,0.03460898,-0.016712194,0.043311063,0.021970717,0.017640067,-0.011762621,-0.026331112,0.04828896,0.069207184,0.052632734,-0.0092891455,0.001549117,-0.0069907974,0.002243842,0.024344314,0.008599542,-0.0025446315,0.0036582039,-0.003888533,-0.04141443,-0.019645723,0.023133585,0.01731942,0.051839165,-0.033438254,-0.03349136,0.034682263,-0.0124607645,-0.03687157,0.015892066,-0.020699633,-0.0113505805,0.019360991,-0.02543093,0.0035552843,0.04396091,-0.0050608055,-0.06263483,0.008489084,-0.011790084,0.033323158,-0.011497065,-0.05523206,0.028688747,0.036832657,-0.017818065,0.044903327,0.01328205,-0.00075860723,-0.017242383,0.046621673,0.1095991,-0.03865107,0.022020197,0.0096495105,0.038746256,0.0016526175,-0.064580545,-0.0010770167,0.025889803,0.010120848,0.024557533,-0.00073128555,-0.028800188,0.0055685523,0.027922997,0.07119212,0.07045095,-0.04024617,0.02449839,0.0013304824,0.041748427,0.0024903468,-0.006045911,0.091083504,0.027042665,-0.023818059,0.011126957,-0.008763665,-0.07633016,0.07142665,-0.05984636,0.04237422,0.052778937,-0.008666813,0.010411578,0.008716056,0.017186573,0.042851545,0.0023266508,0.0024413336,-0.0026542123,-0.10144791,-0.030793596,0.001792245,-0.024791894,0.007480525,-0.029816678,-0.032820098,0.053747844,0.06534766,0.0016659554,0.027168404,0.035301004,0.009569856,0.064007364,0.009319342,-0.0067649554,7.9451675e-05,0.010404447,-0.023988301,0.041403383,-0.01759957,0.021077003,-0.01850429,0.014965035,0.026007771,-0.035370737,-0.001198257,0.0011073715,-0.022764396,-0.031495757,-0.026313191,0.04976412,-0.04592033,-0.004609363,0.03842179,0.06684307,0.04222897,0.013490198,-0.031875126,0.0008964888,0.025031416,-0.027490623,0.0070278975,0.040617898,0.03130685,0.017139677,0.089659266,-0.01153756,0.010712184,-0.014634537,0.02100015,0.0336302,-0.036585003,-0.055841517,-0.0011051045,0.014256744,0.06829926,-0.003210751,-0.017830832,0.047638338,0.026428087,-0.0426758,0.054996427,0.04233738,0.030456094,-0.06004455,0.024194643,0.0042193676,-0.002504343,0.005848936,-0.022854503,-0.08319803,0.007696148,-0.014311109,0.0116444,-0.020689588,0.025152268,-0.026133852,-0.035101607,-0.03128252,0.03408981,0.020739872,0.030697523,0.019230979,0.022828963,-0.023054251,0.026308931,-0.013053185,0.01514304,-0.009715923,-0.029989999,-0.06270224,0.064882286,-0.00104638,0.026199348,-0.0704719,0.06525287,-0.062442277,0.01773033,0.009755145,-0.034647483,0.043812346,0.005514072,0.0033302326,-0.035786398,-0.0049009807,-0.041714784,-0.026855042,0.0058153076,-0.008188109,0.0013143948,0.004590065,-0.011173663,0.04242247,-0.017395174,0.069011025,-0.013383593,0.0049027973,0.025567906,-0.02313242,0.019064113,0.004822052,0.010689102,-0.023718948,-0.008452613,-0.005924726,-0.02309985,-0.030438466,-0.0013912711,0.020967586,-0.020519488,-0.049622383,0.048696358,0.003608484,0.016223624,0.025952607,0.041245874,0.0071244575,-0.019583328,-0.06427229,-0.0023943644,0.043909494,0.058368698,-0.010292469,0.020817306,-0.035795957,-0.04152298,-0.03337189,-0.0018734421,-0.028082294,0.032446932,0.049011007,0.024613217,-0.016024686,-0.03308812,-0.00030812388,-0.009352872,0.021349335,-0.01575545,-0.010681841,0.00771148,0.03421787,0.0425902,0.041307516,0.04720771,-0.00027382548,0.049316872,-0.04428315,0.075753324,0.00786338,-0.048433922,0.02399282,-0.036863152,-0.032781504,0.046101574,-0.02172476,0.020640632,0.028308392,0.005321506,-0.0045672636,0.015405169,0.028651165,0.018833622,-0.0034885295,0.033849142,0.026519824,0.06449284,0.010217896,-0.04508088,0.039339326,-0.05214537,-0.037659187,0.013497501,-0.06962039,-0.009243876,0.053223163,-0.055866115,0.0032561594,0.009778115,-0.0344575,-0.029439347,0.0069844895,-0.024636809,0.08943504,-0.08206285,0.059011742,0.04439297,0.01670655,0.10635737,0.06001944,-0.009478004,-0.014354046,0.007307595,-0.051453855,0.012079061,0.05116295,-0.0055485507,-0.013723716,0.052804668,-0.046393808,-0.026816238,0.0031597193,-0.033388127,0.0018469051,-0.06584058,0.004928445,-0.00085453386,-0.039519068,0.029506616,0.011163874,-0.0055254903,0.011268233,0.048183125,-0.012915451,0.04504987,0.034347273,-0.0013007385,0.00842721,0.049373083,0.029322289,-0.018669926,0.027488494,0.017011093,-0.015289118,-0.003965801,0.0096612,-0.0061091697,0.011692753,0.01410679,-0.048605066,-0.034258716,0.054740313,-0.019448608,-0.050849248,0.014218784,0.032952465,-0.00031608102,0.009598198,-0.02891288,-0.055842206,-0.044878084,-0.04308985,-0.06199583,0.04903728,0.035779532,0.0055781975,0.017092388,-0.04621593,0.08148082,-0.10927228,-0.016698493,-0.043895356,0.017718544,-0.00020094494,-0.03595297,0.010454429,-0.008812726,0.007721305,-0.024912892,-0.009665404,0.05087193,-0.03192135,-0.0061566136,-0.04946472,-0.033486173,0.016979087,0.02961097,-0.030504461,-0.0072245398,0.036632597,-0.025462296,0.08113092,-0.027851056,-0.011142673,0.010000934,0.029116994,-0.060896605,-0.06099615,0.015676117,-0.01875787,0.03541313,0.0255869,0.020070469,-0.0007844905,-0.023238432,-0.03436712,-0.09096783,-0.03208619,0.004755218,0.0028645531,0.059400734,0.03082695,-0.041688416,-0.030097311,-0.027010977,-0.013425039,0.03258338,-0.0017620675,0.0770482,0.029810416,0.03985265,-0.015280839,-0.07184934,-0.00048721206,-0.019749386,0.05963379,-0.011790044,0.044487517,0.046680473,0.012192311,-0.0014223108,-0.026252143,0.005736902,0.016904525,-0.018761672,-0.060908463,-0.007555639,0.001853022,0.0023759855,0.020905253,-0.034749616,-0.013991602,0.07836948,0.025402999,-0.06261918,-0.049452372,-0.022879936,-0.0017479915,-0.014475298,0.009036675,0.022325605,0.025092509,-0.011573304,-0.017550433,0.0777522,0.004518215,-0.027374785,-0.01213765,0.030784413,0.009294433,0.023063337,0.011314611,-0.007798926,-0.06287859,0.010672341,0.00209144,-0.025565825,0.01052666,-0.0038194086,-0.015620707,-0.003810263,-0.0010700872,-0.03368527,-0.008680131,-0.0018233288,0.06846271,-0.011602457,-0.0028653988,0.034743633,-0.057026763,-0.030251523,-0.0020841302,0.023725055,0.0043404223,0.017419443,0.010918733,0.03311969,-0.042700786,0.03997318,0.025540506,-0.0036862958,-0.0063507985,0.06681993,-0.009717136,0.024418741,-0.024665432,-0.008739178,0.061198357,0.017766856,0.014578759,-0.02458929,0.06393938,0.035320263,-0.029212592,-0.032107774,-0.023478126,-0.067183666,0.06494485,0.035362106,0.027976515,-7.925396e-05,-0.020262742,-0.06511445,-0.047349922,0.005060566,7.0617e-05,0.002041464,0.04659533,0.00026398656,-0.052776463,-0.016786823,-0.03246863,0.048375424,-0.020875536,-0.0023909172,0.022632562,-0.02136784]	Keywords: layer normalization, post-LN, pre-LN, AdaNorm, normalization, gradients, convergence\nKey Objects: Layer Normalization, gradients, parameters, AdaNorm\nRefers to Images: None\nHypothetical Questions:\n- Why do post-LN Transformers often require specialized parameters to control residual dependencies?\n- What is the underlying reason AdaNorm avoids learnable parameters?\n- How does the approach using AdaNorm contribute to improved gradient behavior in Transformers?\n---\nSummary:\nTo improve the stability and performance of post-Layer Normalization (post-LN) Transformers, researchers introduced parameters to control residual dependencies, resulting in faster convergence and better performance compared to pre-LN Transformers. Subsequent research explored alternatives to Layer Normalization (LN), such as AdaNorm, which avoids learnable parameters to mitigate overfitting and focuses on the role of gradient rescaling.\nOriginal Text:\nnot amplified. This approach ensures and boosts convergence of post-LN Transformers and reaches better performance than pre-LN Transformers.  \n5.2.2 Substitutes of Layer Normalization. Xu et al. [153] empirically observe that the learnable parameters in the LN module do not work in most experiments, and even increase the risk of overfitting. They further conclude from controlled experiments that the forward normalization is not the reason why LN works for Transformer. From analysis and experiments, it is concluded that the derivatives of the mean and variance re-center and re-scale the gradients and play a significant role in LN. They thus propose AdaNorm , a normalization technique without learnable parameters  \n$$z = C ( 1 - k y ) \\, \\sigma, \\\\ \\mathbf x = \\frac { \\mathbf x - \\mu } { \\sigma },$$  \nwhere C , k are hyperparameters and GLYPH denotes element-wise multiplication. GLYPH and GLYPH are the mean and standard deviation of input x , respectively.\nContextualized Text:\nTo address instabilities in post-LN Transformers, researchers introduced parameters to control residual dependencies. This approach ensured faster convergence and ultimately yielded better performance than pre-LN Transformers. To further refine the process, alternatives to Layer Normalization (LN) like AdaNorm were explored, which avoids learnable parameters to avoid overfitting and focuses on the significant role of gradient rescaling.	{"tags": ["normalization", "transformer", "deep-learning", "optimization"], "doc_id": "2373992f-cf4a-4662-b6a6-1656c2f2038d", "summary": "To improve the stability and performance of post-Layer Normalization (post-LN) Transformers, researchers introduced parameters to control residual dependencies, resulting in faster convergence and better performance compared to pre-LN Transformers. Subsequent research explored alternatives to Layer Normalization (LN), such as AdaNorm, which avoids learnable parameters to mitigate overfitting and focuses on the role of gradient rescaling.", "doc_type": "text", "entities": ["AdaNorm", "Transformer"], "keywords": ["layer normalization", "post-LN", "pre-LN", "AdaNorm", "normalization", "gradients", "convergence"], "key_objects": ["Layer Normalization", "gradients", "parameters", "AdaNorm"], "contextual_text": "To address instabilities in post-LN Transformers, researchers introduced parameters to control residual dependencies. This approach ensured faster convergence and ultimately yielded better performance than pre-LN Transformers. To further refine the process, alternatives to Layer Normalization (LN) like AdaNorm were explored, which avoids learnable parameters to avoid overfitting and focuses on the significant role of gradient rescaling.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.2 Layer Normalization"}, "hypothetical_questions": ["Why do post-LN Transformers often require specialized parameters to control residual dependencies?", "What is the underlying reason AdaNorm avoids learnable parameters?", "How does the approach using AdaNorm contribute to improved gradient behavior in Transformers?"]}
2d84208d-dcdf-4724-b652-ed43c013af59	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01855055,0.03062721,-0.008376741,0.016100816,-0.0011406991,0.053809382,0.06387406,0.025348874,0.013005682,-0.02659803,0.021960054,-0.0005144384,0.008111531,-0.01822565,-0.02259882,-0.031404987,0.010494364,0.084939554,0.008232584,-0.036041334,0.019291023,0.008677173,0.021400088,-0.027245728,0.026549378,0.04999123,0.02734051,0.0063046813,0.03222187,-0.0036817077,-0.0066478373,-0.047401596,0.027961291,0.027747227,0.029852867,-0.017277738,-0.010122021,-0.045075882,-0.020296285,-0.032871358,-0.025979927,0.05109671,-0.036911383,-0.023953754,0.016405843,-0.03707848,-0.073149726,-0.04269212,0.0075871353,-0.0033126518,-0.028360713,0.055673968,-0.030708743,0.011348779,-0.02448598,0.0021757444,0.019919215,-0.016759383,-0.0023227339,-0.07106987,-0.009364864,0.03167598,0.00023191342,0.020170858,0.06924444,-0.021286277,0.014027336,0.030292686,0.05279339,0.09881082,-0.05499549,0.03314225,-0.0068349726,-0.048813462,0.100309804,0.050484136,-0.039288707,-0.018421674,-0.057577886,-0.048464216,0.03440255,0.025057202,-0.042864025,-0.05130506,0.07188997,0.0018676228,-0.035155572,0.0077558556,0.060132507,-0.046899326,-0.019673035,-0.033497863,0.0050158678,0.052138478,0.010827265,-0.01623508,-0.010654738,-0.055598013,0.013970405,-0.020867411,-0.0008301306,0.0035168084,0.09653779,0.11833589,0.028260322,-0.055288043,-0.03769382,0.027162386,-0.0050001116,0.051823262,0.030065887,0.034009825,-0.051971804,0.010504151,-0.00839675,-0.02584361,-0.025419898,0.011895255,0.028827153,-0.012886272,0.030124793,0.004610857,-0.0072555556,0.040133614,0.03963489,0.033550896,0.015361996,-0.01775701,-0.06223811,0.027101478,0.08925698,-0.018535348,-0.01600466,0.02986286,-0.027381884,-0.07367521,0.04358361,-0.01693514,-0.006286329,0.015103927,-0.002823973,-0.041003466,-0.07027217,-0.007276542,-0.030740017,-0.008573492,-0.08108606,0.030193718,-0.02041348,0.04463176,0.03206863,-0.010203551,0.021965224,-0.015874108,-0.040519666,0.037033506,-0.03699816,-0.030934064,-0.0053216415,-0.033129748,-0.05747122,-0.056585446,-0.0052921106,0.054134473,0.016910022,0.1114227,0.016608382,0.010380952,0.002203586,0.052238915,-0.05917123,0.0009182442,0.0027972881,-0.0039030141,-0.014592452,-0.0035530834,0.030153897,-0.057813507,0.0017425747,0.038248952,-0.009039745,0.0074424436,-0.022408674,-0.014980686,-0.059843984,-0.001846812,0.030521909,0.04264256,-0.008750234,-0.010985012,0.006862662,0.0055366647,-0.0039462117,-0.031077504,0.01968092,0.03674559,0.04048997,-0.049882706,-0.031171495,-0.022401467,0.051543117,0.043542583,0.005024592,-0.02652821,0.04431131,-0.050897878,0.015674302,-0.02603073,-0.009008701,0.04719343,0.020546172,0.023167754,-0.008974946,0.045886863,0.0041384585,-0.026105227,-0.036498845,0.010897957,0.042774774,-0.02484428,-0.02845246,0.001146686,0.010468231,0.06678189,-0.036481332,0.023118278,-0.013934423,0.0078547755,0.016369319,0.011871336,0.011636478,0.03501999,-0.06657538,0.024991345,-0.052226774,0.04374176,0.014305658,0.022756046,-0.001702789,0.026560567,0.07065484,0.04466065,-0.00018771237,0.07860587,0.02233598,0.027208993,0.04362931,0.043179724,-0.01728204,0.033525415,-0.021378588,0.031907383,0.032894883,-0.0011524205,-0.03835423,0.0010337622,0.01608247,0.043358,0.060261287,0.0058233757,-0.0032158417,-0.019086594,0.012504095,0.0170561,0.026513288,0.0148178,0.015789531,-0.034985878,-0.019494502,-0.022756433,0.008109035,0.049180977,0.046602637,-0.039155208,-0.0149270585,0.031948328,-0.0063612303,-0.002927632,0.008577504,-0.04798204,-0.0020725164,-0.009253616,-0.03695566,0.0058953115,0.028666507,0.0131624695,-0.028248949,-0.0071002385,0.015038189,0.02384103,-0.0055263545,-0.03915005,0.056352533,0.02450307,-0.017115826,-0.014947511,0.0034839315,-0.023215381,-0.017619807,0.044107404,0.10001822,-0.03583832,0.028966118,0.032177288,0.03467599,0.011415929,-0.07393845,0.029338866,0.030541457,-0.012363276,0.019299392,0.018492548,-0.012767826,0.027924344,0.024679,0.077691466,0.07919604,-0.030318119,0.01097679,0.0011686076,0.0559921,-0.0019880452,-0.018332232,0.08392057,0.04715328,-0.024646232,-0.0019085462,0.0014727893,-0.0556613,0.0923779,-0.0602395,0.039844114,0.05226409,-0.032994792,0.006728096,0.04366387,0.034923166,0.019205095,0.03585716,-0.0192343,0.0063955495,-0.05292504,-0.030638898,0.016303636,-0.02043923,0.0035737061,0.025708852,-0.047547583,0.049040623,0.05210313,-0.00080338225,0.042452667,0.018998912,0.016987804,0.045283463,-0.028969143,0.009397426,0.021271843,0.026905715,-0.043138355,0.031625383,-0.027028264,0.05078201,-0.0081736455,0.02801105,0.013533155,-0.04121058,0.0017337094,0.014479585,0.0047594854,-0.02635499,-0.044408653,0.03532547,-0.023235088,0.006687714,0.058489207,0.050609022,0.061223496,0.004977136,-0.017820915,-0.035622183,0.0005212376,0.0022985155,0.008887337,0.03835388,0.012333356,-0.007843671,0.07917307,-0.014646964,-0.015542987,-0.020974595,0.02795517,0.064304255,-0.032035824,-0.04284547,-0.020004243,-0.00042545734,0.08625571,-0.01690873,-0.013289999,0.073113166,0.014798294,-0.04036528,0.03157355,0.051584538,0.021574795,-0.06901972,0.05785054,0.010786377,0.0076605873,-0.00921298,-0.05732439,-0.07743038,0.023855597,-0.0026109675,-0.011831996,-0.032122158,0.019094907,-0.025524998,-0.045809966,-0.034057587,0.04109189,0.026995,0.061360855,0.036632873,0.013142546,-0.019470826,0.019506434,-0.0044218195,-0.028170768,-0.028462466,-0.047906093,-0.041753154,0.027904356,-0.033767726,0.018085264,-0.056536175,0.088053375,-0.0807638,0.021221172,0.047624234,-0.030579738,0.05747069,-0.018404739,-0.012065887,-0.0015542195,0.030192073,-0.047894,-0.045314163,-0.024107654,0.014226145,-0.008098486,0.012287597,0.0013042913,0.030491568,-0.021870274,0.026630774,-0.03644462,-0.031869363,0.008104969,-0.0032220255,0.021343442,0.008332933,0.009800538,-0.037745275,0.008241964,0.0025525407,-0.025334932,-0.0035906218,-0.054123726,0.0376953,-0.010383146,-0.009866562,0.046467077,0.0024977308,-0.0020326898,0.0021192455,0.02309903,0.029474068,-0.012764004,-0.06064331,-0.028295938,0.027124722,0.049579017,-0.020007879,0.048712485,-0.040811885,-0.018311609,-0.03187177,0.014539529,-0.030725287,0.0503098,0.014796991,0.046513185,-0.020275835,-0.046177447,-0.044166066,0.0031390202,0.041041538,-0.004456576,-0.0421415,0.009998593,0.03034612,0.049204573,0.05445005,0.02688585,0.03416012,0.06340687,-0.018395558,0.044874534,0.008961316,-0.0733788,0.039439086,-0.043473497,-0.023470696,0.04313004,-0.006757408,0.02629038,0.018126855,0.018813748,-0.026183618,0.003990114,0.035322633,0.057398524,-0.00095414696,0.012499548,-0.0040120087,0.056614302,0.028349778,-0.075681396,0.019479332,-0.046097845,-0.017342469,0.0106043955,-0.088246755,0.004765299,0.040916413,-0.050623447,0.017812418,0.00014887782,-0.009338788,-0.042158823,0.014602541,-0.028460216,0.06984705,-0.072223336,0.031184943,0.019575618,-0.025782458,0.0970591,0.04895442,-0.013850445,-0.029116029,0.014006947,-0.030915335,0.012667402,0.05950251,-0.018665044,-0.00930598,0.05035145,-0.012038339,-0.0016306682,0.003636153,-0.04452229,-0.0064487876,-0.054949,0.016917285,-0.024249543,-0.038571686,0.043210205,0.0154499635,-0.0100709805,0.04251128,0.016214339,-0.026443321,0.053902887,0.038557664,0.008843695,-0.021945426,0.05459112,0.039452795,-0.022453267,0.06673004,0.009599228,-0.038176984,-0.018271279,0.017437592,0.011239665,0.032488298,0.012105872,-0.008565901,-0.06330481,0.02920047,-0.019196402,-0.017205134,-0.017990284,0.053440116,0.028590161,0.02908336,-0.02810641,-0.0792481,-0.038335767,-0.03966374,-0.043839242,0.040126204,0.001359531,0.00071564235,-0.0068514827,-0.04114485,0.0807063,-0.097491935,-0.028908731,-0.061957497,0.01454407,0.021279009,-0.04193341,-0.0061575524,-0.016296243,-0.029455755,-0.052315123,-0.02094892,0.02298005,-0.024014622,-0.008156767,-0.047284022,-0.014731377,0.0097972425,0.021794753,-0.032582927,-0.014274147,0.038202852,-0.022932734,0.079693764,-0.07147399,-0.009790656,0.008011093,0.015965804,-0.04754751,-0.068503276,0.015311784,-0.034934398,-0.0033374089,-0.025003582,0.007092248,-0.021517416,-0.009210234,-0.019996474,-0.072360344,-0.012959848,0.044975903,0.00580008,0.040890217,0.020557726,-0.0450369,0.0005371715,-0.020296652,-0.019030906,0.04651216,-0.004616174,0.051194064,0.022398207,0.083939016,-0.04405297,-0.070530474,-0.0134494,-0.050489448,0.025026537,-0.011272394,0.045594353,0.044334583,-0.004638055,0.005434273,-0.045539964,-0.003917852,0.03123633,-0.01371591,-0.03758095,-0.008583041,0.0034167978,0.008757241,0.025065081,-0.004278609,0.000478425,0.067259245,0.02408085,-0.047893327,-0.035848074,-0.0115023125,-0.016773561,-0.030610885,0.0062558213,0.021106383,0.0065985927,-0.010744237,0.026387788,0.093103014,0.009323223,-0.046046615,-0.013274572,0.04011707,-0.009032766,0.004978733,-0.02574186,-0.027464928,-0.08514061,0.018751327,-0.008387413,0.0044188746,0.033483334,-0.03247036,-0.018317139,-0.022973076,-0.012738055,-0.01802636,0.0055853026,-0.008098057,0.049465686,0.008480997,-0.006764969,0.046908144,-0.019803297,0.0024205945,0.012774912,0.013943247,0.03212186,0.024654932,0.03167547,0.0034531085,-0.037730694,0.05799756,0.039262883,0.019548964,-0.039493993,0.048784047,0.013608262,0.027346948,-0.00072157977,-0.011576045,0.049374126,-0.014141869,-0.0064395475,-0.030288674,0.04420603,0.043009218,-0.010726839,-0.045012925,-0.026152283,-0.041780423,0.06028693,0.037046243,0.015306335,0.018024104,-0.011128356,-0.053804535,-0.050491113,-0.010216114,-0.035181236,0.04801688,0.047638178,-0.0014373591,-0.07721826,-0.015955906,-0.048676874,0.058463886,0.022049913,0.021508878,0.019681146,-0.0041966178]	Keywords: Layer Normalization, AdaNorm, scaled t normalization, normalization, machine translation, low-resource settings\nKey Objects: Layer Normalization, Input, Normalization Technique, Machine Translation Datasets\nRefers to Images: None\nHypothetical Questions:\n- How does AdaNorm's design contribute to avoiding overfitting compared to traditional Layer Normalization?\n- Why is scaled t normalization considered more parameter-efficient than standard Layer Normalization?\n- In what specific scenarios would scaled t normalization be most advantageous over other normalization techniques?\n---\nSummary:\nAdaNorm replaces Layer Normalization with a normalization technique without learnable parameters, utilizing hyperparameters C and k along with element-wise multiplication (GLYPH) to adjust input x based on its mean and standard deviation. Nguyen and Salazar then propose using scaled t normalization, projecting inputs onto a sphere with a learned radius 'g' for improved parameter efficiency.\nOriginal Text:\nwhere C , k are hyperparameters and GLYPH denotes element-wise multiplication. GLYPH and GLYPH are the mean and standard deviation of input x , respectively.  \nNguyen and Salazar [93] propose to replace the LN module with scaled t$\\_{2}$ normalization. Given any input x of d -dimension, their approach project it onto a d - 1-sphere of learned radius g  \n$$z = g \\frac { \\mathbf x } { \\| x \\| },$$  \nwhere g is a learnable scalar. It is more parameter efficient compared to normal LN and is shown to be effective in machine translation datasets, especially in low-resource settings.\nContextualized Text:\nTo explore alternatives to Layer Normalization, AdaNorm is introduced as a technique without learnable parameters, focusing on modifying input x based on its mean and standard deviation using hyperparameters C and k. Complementing this, Nguyen and Salazar propose scaled t normalization, projecting inputs onto a sphere with a learned radius 'g' for improved efficiency, particularly in low-resource machine translation scenarios.	{"tags": ["normalization", "deep learning", "NLP", "machine translation"], "doc_id": "2d84208d-dcdf-4724-b652-ed43c013af59", "summary": "AdaNorm replaces Layer Normalization with a normalization technique without learnable parameters, utilizing hyperparameters C and k along with element-wise multiplication (GLYPH) to adjust input x based on its mean and standard deviation. Nguyen and Salazar then propose using scaled t normalization, projecting inputs onto a sphere with a learned radius 'g' for improved parameter efficiency.", "doc_type": "text", "entities": ["AdaNorm", "scaled t normalization", "Nguyen", "Salazar"], "keywords": ["Layer Normalization", "AdaNorm", "scaled t normalization", "normalization", "machine translation", "low-resource settings"], "key_objects": ["Layer Normalization", "Input", "Normalization Technique", "Machine Translation Datasets"], "contextual_text": "To explore alternatives to Layer Normalization, AdaNorm is introduced as a technique without learnable parameters, focusing on modifying input x based on its mean and standard deviation using hyperparameters C and k. Complementing this, Nguyen and Salazar propose scaled t normalization, projecting inputs onto a sphere with a learned radius 'g' for improved efficiency, particularly in low-resource machine translation scenarios.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.2 Layer Normalization"}, "hypothetical_questions": ["How does AdaNorm's design contribute to avoiding overfitting compared to traditional Layer Normalization?", "Why is scaled t normalization considered more parameter-efficient than standard Layer Normalization?", "In what specific scenarios would scaled t normalization be most advantageous over other normalization techniques?"]}
565edfef-c341-4cdf-9d42-f50bf78706f9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01755196,0.017048825,0.00036755903,0.026683483,0.029189127,0.0426286,0.032215305,0.04781539,0.017372211,-0.020400714,-0.0024161644,0.00616843,0.025316756,-0.018228684,-0.04180154,-0.0067323805,0.039703224,0.0870651,0.04012384,-0.01281868,-0.000605907,-0.0042415797,-0.016047746,-0.00769626,0.028772693,0.04909659,0.02444841,0.024087721,0.022656107,-0.007968457,0.012281233,-0.048301626,0.013214893,0.014163495,0.013717468,-0.039891146,0.0040219123,-0.04981945,-0.037845857,-0.015162556,-0.020963142,0.031200128,-0.07499962,-0.019631665,0.012222148,-0.012833596,-0.08762434,-0.023567382,0.010465094,0.0010199632,-0.047398925,0.035374723,-0.02617619,0.0025511936,0.0022888521,0.003662194,-0.0040591476,-0.036650106,-0.02385205,-0.07145728,-0.013439,0.015851285,-0.0038732563,0.011716898,0.050081782,-0.00084039714,0.015218697,0.018067364,0.024516039,0.102003686,-0.06297267,0.03957161,-0.015634052,-0.039595462,0.09682943,0.021210197,-0.034330726,-0.013744064,-0.0696106,-0.04602823,0.042183332,0.021184806,-0.056038577,-0.05326819,0.05649835,-0.021193178,-0.07844879,-0.011552202,0.041596204,-0.081917524,-0.018116148,-0.025238488,0.023991313,0.038838923,-0.024677398,-0.032421272,0.017950159,-0.045738924,0.04312776,-0.036631677,0.01635455,-0.034702856,0.08167574,0.111875616,0.039237976,-0.04592732,-0.033515505,-0.0097267395,0.0033515252,0.013290105,0.0033124057,0.0482813,-0.05388604,-0.004637035,-0.0044067195,-0.017710527,-0.01855527,-0.0017552606,0.01717275,0.018097553,0.037020512,0.006445016,-0.00020373792,0.01260794,0.035450734,0.04917407,0.00902213,-0.018054452,-0.038642928,0.017573342,0.036547504,-0.001731346,-0.017895212,0.03424564,-0.043058246,-0.06517105,0.03855284,-0.017785836,-0.011923602,0.011778753,-0.009805139,-0.035708413,-0.082136266,-0.008623087,-0.04738841,-0.050294887,-0.08087323,0.04185238,0.0063840565,0.023509102,-0.0043486096,-0.015709233,0.0071319286,-0.005194355,-0.045948397,0.042920705,-0.0054215696,-0.04207335,-0.008427675,-0.011307617,-0.030713148,-0.04738114,0.0059990967,0.041426975,0.03572274,0.13369493,0.037824914,-0.010708719,-0.009435049,0.02112383,-0.07665967,0.0004916704,0.0057115676,-0.009351502,-0.015882898,-0.013121567,0.0051502963,-0.04884235,0.013241975,0.020722788,-0.010989039,0.002240158,-0.003670575,-0.015955457,-0.03128714,-0.0058686943,0.011847334,0.06547737,-0.019285517,-0.011310248,0.05712235,0.0025852707,-0.014634767,-0.02699495,0.027976042,0.017929949,0.036166724,-0.024816263,-0.004423899,-0.021289771,0.045235403,0.019236853,0.012343177,-0.015918814,0.05680439,-0.059601445,0.022029325,-0.02517227,-0.00043129516,0.053245164,0.014054751,0.046938904,0.0036653725,0.03985757,-0.0021036349,-0.016726004,-0.04128295,0.0024855977,0.020755889,-0.030817194,-0.018981567,0.022863723,0.0048027528,0.058638103,-0.020525536,-0.0046560643,-0.03998758,0.0014534491,0.0071492963,-0.012708083,0.008416671,0.03624586,-0.079480186,0.04067338,-0.024735287,0.026601063,-0.0024777527,0.022608865,-0.03082703,0.01812121,0.08489631,0.025601009,0.005125542,0.084335715,0.014202457,0.023328539,0.055181593,0.020477151,-0.004886602,0.044357833,-0.021774612,0.02905521,0.03256972,-0.011504526,-0.020856448,-0.023907227,0.016546156,0.0629168,0.08233633,-0.021002414,0.009421933,-0.0257879,0.014865096,0.016365306,0.014767436,0.0001907042,0.019598473,-0.0063385675,-0.011025746,-0.039054967,0.0018438182,0.043699235,0.032889567,-0.05278105,-0.04158557,0.061751276,-0.0135878725,-0.017289752,-0.0035322304,-0.06533615,0.014931626,-0.029587977,-0.027917895,-0.0014214745,0.04664388,0.005595753,-0.045129046,-0.012865396,-0.024810014,0.017817227,0.009421135,-0.018626122,0.057515275,0.021765599,-0.034247402,0.003180542,0.0037667814,-0.021862755,-0.013849275,0.0357106,0.10136424,-0.056944437,0.001117046,0.029098302,0.028779436,-0.008813898,-0.056929436,0.005278368,0.017580787,-0.0442457,0.015358279,-0.00056706305,-0.044130784,0.0047229766,0.0033498725,0.10860369,0.08581948,-0.013514912,-0.029166622,0.0075746314,0.029822296,0.0042330576,-0.009673091,0.08724061,0.047713906,-0.030709837,0.010267172,-0.006833105,-0.051758416,0.11793128,-0.048644576,0.046596542,0.05833985,-0.06024671,0.017880404,0.016251879,0.014841781,-0.019678762,0.049066693,0.011971018,-0.02583264,-0.06056698,-0.03599022,0.029318353,0.0012394491,0.0048868367,-0.0006473823,-0.026595866,0.053000446,0.046286445,-0.0063268067,0.061835896,0.031672772,0.014225814,0.033551,-0.026720526,0.021994743,0.027156513,0.006337367,-0.021194868,0.059751388,-0.025087222,0.02900587,-0.024551233,0.008692191,0.03547909,-0.016638625,0.017204713,0.013822287,-0.0033330927,-0.02508644,-0.048874646,0.026678009,-0.041671023,-0.016765641,0.018499017,0.02705433,0.028730974,-0.008973123,0.013602326,-0.025464045,-0.021174807,-0.027893487,0.005378863,0.0106892055,0.0047373874,-0.0017853602,0.06744804,-0.020631071,-0.027751237,-0.048346493,0.026455943,0.04897148,-0.038528107,-0.07141352,-0.026730545,0.013650548,0.061765976,-0.03542458,-0.014226492,0.05460223,0.029498044,-0.026132965,0.031097744,0.05849094,0.0046195574,-0.06383834,0.015487344,0.021626104,0.01871409,-0.028011017,-0.045208145,-0.07029508,0.041169982,-0.010538684,-0.0061920285,-0.008799811,0.022603942,-0.02557307,-0.011314085,-0.01352073,0.03337692,0.029014528,0.0594417,0.031553242,-0.013013393,0.006251457,0.052525822,0.01202674,-0.026620574,-0.020838933,-0.04513074,-0.043782696,0.044722456,-0.005722278,0.024888495,-0.06518021,0.06734468,-0.0779601,0.012947443,0.039409377,-0.025414173,0.04091141,0.0062538553,0.011678384,-0.0015791784,0.021599427,-0.02313027,-0.04588611,0.00093141594,-0.0012511221,-0.012314478,-0.003446333,-0.0021668926,0.031374052,-0.034971107,0.034958445,-0.03260291,-0.020443402,0.012309746,-0.021213692,0.03935157,-0.006592221,0.013197441,-0.014128869,-0.007181568,0.0033780758,-0.024717454,-0.013276354,-0.02310381,0.009664042,0.024777656,-0.015174727,0.07048423,-0.0033729684,-0.012280678,1.5523394e-05,0.032609686,0.025260698,-0.016127607,-0.09025833,-0.03773323,0.021601425,0.039384607,-0.049311686,0.029330164,-0.041677933,-0.00025970503,-0.038743515,0.012685761,-0.030925456,0.07086434,0.039146543,0.026578411,-0.022658138,-0.052921943,-0.057030283,0.0059908796,0.003784153,-0.0060156467,-0.0058222436,-0.012005232,0.027097786,0.041878853,0.02803411,-0.0034271968,-0.0069910362,0.049865086,0.0071563707,0.04337893,-0.037037622,-0.044118114,0.022268653,-0.037572462,-0.04149731,0.021321043,-0.008879319,0.023215476,0.019499531,0.022029623,-0.037099715,0.019051224,0.034830093,0.044357173,0.027313756,0.017648775,0.015673596,0.066014856,0.027953362,-0.06367859,0.032810207,-0.07954336,-0.034397196,0.012083691,-0.09655867,0.020120006,0.03348507,-0.06124511,-0.009430433,0.016499186,-0.029400893,-0.03290132,0.0142490035,-0.055407517,0.052149177,-0.06992278,0.032187037,0.040479697,-0.007429741,0.10872886,0.03705546,0.010521155,-0.058819085,-0.004019209,-0.027831774,0.016734594,0.057429228,-0.028490465,-0.023719354,0.058555223,-0.014143942,-0.032098386,0.0168451,-0.016677476,-0.0035545756,-0.05808636,0.028263815,0.00042100137,-0.025399014,0.042998653,0.01782458,-0.027348552,0.041563563,0.031992577,-0.034355257,0.032530066,0.013391182,-0.0052899537,-0.0016717322,0.034561034,0.036085118,-0.022086848,0.046521533,0.01889924,-0.028397787,0.02391014,0.009558984,-0.0065929587,0.030469289,0.019564016,0.033070248,-0.04625291,0.04167115,-0.012004427,-0.04951934,-0.010710158,0.024396107,0.035273626,0.01980294,-0.045143563,-0.06682555,-0.035174027,-0.06069987,-0.054732885,0.012622343,0.0012509797,0.008319997,0.031162197,-0.052132558,0.06686624,-0.09958802,-0.011076531,-0.056881506,0.04841139,0.046863217,-0.0322149,-0.01838263,-0.026237944,0.011399394,-0.06363708,-0.01500035,0.0392851,0.00024324073,-0.01843073,-0.027328286,-0.034918994,-0.027155014,0.01959151,-0.04331179,-0.009823394,0.051780567,-0.01632722,0.04680475,-0.07515537,-0.012655457,0.029312816,-0.0026407419,-0.030101327,-0.06050231,0.04743644,-0.021088425,0.030379444,-0.010418947,-0.0077876537,-0.011293583,0.0166441,-0.06274158,-0.029103847,-0.011044558,0.022058016,-0.013795711,0.04126175,0.029875016,-0.04769518,-0.015920805,0.011823078,-0.026903868,0.0147211375,0.012933401,0.040286887,0.011306857,0.06788559,-0.05139927,-0.048260536,-0.0380272,-0.07434275,0.025563616,-0.0077734855,0.028416546,0.013333433,0.01161262,0.011704025,-0.02230814,-0.0066809,-0.01150653,-0.01472506,-0.054719195,-0.03365927,0.0034984327,0.0330299,0.028515564,-0.02430559,0.0052475305,0.07780409,0.023624947,-0.027627481,-0.042734813,-0.021712342,0.012815761,-0.015622629,-0.026775116,0.01068178,0.011026129,0.023157192,0.010992949,0.09194152,0.024972118,-0.0410174,0.010645997,0.062218558,-0.017437056,0.010501023,0.00830263,-0.013380658,-0.0776544,0.008790473,-0.010211659,0.008896496,0.04914128,-0.01721409,-0.016038211,-0.038060866,-0.025006985,-0.0356771,-0.015992347,-0.027805945,0.080352865,-0.026276443,0.0050781085,0.007847062,-0.034869246,0.0036654177,-0.0052299495,-0.016409744,-0.027079029,0.0056739175,0.03215475,0.01788374,-0.02954359,0.05558829,0.045322966,0.0060246913,0.010861124,0.056604274,-0.017166305,0.016150657,0.010121297,-0.023026714,0.04985894,-0.0037656236,0.037257105,-0.025354085,0.072510235,0.041153654,-0.0009732314,-0.026197305,-0.029751075,-0.06311684,0.048259247,0.03914731,-0.026056902,0.051268138,-0.0064731557,-0.053695496,-0.06013359,-0.026517157,-0.033315815,0.011916546,0.039621305,0.011400054,-0.09911806,-0.0055579296,-0.0283661,0.042952903,0.0006417072,0.0072047524,0.046747636,0.008858036]	Keywords: Layer Normalization, Batch Normalization, PowerNorm, scaled t normalization, machine translation, low-resource settings\nKey Objects: Layer Normalization, scaled t normalization, Batch Normalization, PowerNorm\nRefers to Images: None\nHypothetical Questions:\n- Why is scaled t normalization considered more parameter efficient than standard Layer Normalization?\n- What are the primary reasons why Batch Normalization performs poorly in Transformers for text data?\n- How does PowerNorm attempt to overcome the instabilities associated with Batch Normalization in Transformer architectures?\n---\nSummary:\nNguyen and Salazar propose scaled t normalization as a more parameter-efficient alternative to Layer Normalization, demonstrating its effectiveness in machine translation, particularly in low-resource scenarios. Sheh et al. subsequently address the poor performance of Batch Normalization in Transformers and introduce PowerNorm as a modified approach.\nOriginal Text:\nwhere g is a learnable scalar. It is more parameter efficient compared to normal LN and is shown to be effective in machine translation datasets, especially in low-resource settings.  \nSheh et al. [121] discuss why Batch Normalization (BN) [58] performs poorly in Transformer for text data and conclude that BN's significant performance degradation stems from the instabilities associated with its batch statistics. They thus propose PowerNorm (PN) that has three modifications over BN: (1) it relaxes the zero-mean normalization; (2) it uses the quadratic mean of the signal, instead of the variance; (3) it uses running statistics for the quadratic mean, instead of using per-batch statistics. Specifically, for the t -th iteration, the PN computes the outputs as  \n$$z ^ { ( t ) } = \\gamma \\circ y ^ { ( t ) } + \\beta, \\\\ y ^ { ( t ) } = \\frac { x ^ { ( t ) } } { \\psi ^ { ( t - 1 ) } },$$\nContextualized Text:\nAs alternatives to Layer Normalization, Nguyen and Salazar introduced scaled t normalization, a more parameter-efficient method particularly useful for machine translation tasks in low-resource settings. Following this, Sheh et al. investigated why Batch Normalization struggles with Transformer architectures for text data and presented PowerNorm, a modified version designed to address instabilities related to batch statistics.	{"tags": ["normalization", "deep-learning", "NLP", "machine translation"], "doc_id": "565edfef-c341-4cdf-9d42-f50bf78706f9", "summary": "Nguyen and Salazar propose scaled t normalization as a more parameter-efficient alternative to Layer Normalization, demonstrating its effectiveness in machine translation, particularly in low-resource scenarios. Sheh et al. subsequently address the poor performance of Batch Normalization in Transformers and introduce PowerNorm as a modified approach.", "doc_type": "text", "entities": ["Transformer", "Batch Normalization", "PowerNorm", "TensorFlow"], "keywords": ["Layer Normalization", "Batch Normalization", "PowerNorm", "scaled t normalization", "machine translation", "low-resource settings"], "key_objects": ["Layer Normalization", "scaled t normalization", "Batch Normalization", "PowerNorm"], "contextual_text": "As alternatives to Layer Normalization, Nguyen and Salazar introduced scaled t normalization, a more parameter-efficient method particularly useful for machine translation tasks in low-resource settings. Following this, Sheh et al. investigated why Batch Normalization struggles with Transformer architectures for text data and presented PowerNorm, a modified version designed to address instabilities related to batch statistics.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.2 Layer Normalization"}, "hypothetical_questions": ["Why is scaled t normalization considered more parameter efficient than standard Layer Normalization?", "What are the primary reasons why Batch Normalization performs poorly in Transformers for text data?", "How does PowerNorm attempt to overcome the instabilities associated with Batch Normalization in Transformer architectures?"]}
e2ee1938-d75c-4025-9ca0-99d44779b834	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.031007795,0.033925872,0.023071885,0.0105977515,-0.016737733,0.045594435,0.03208348,0.07435564,0.0008701417,-0.031006692,0.019896476,-0.004480975,0.03023734,0.0053645642,-0.030526575,-0.008188493,0.015822597,0.084120855,-0.0025378556,-0.028493341,0.059054393,0.006357989,0.0012045639,0.016721517,0.057019025,0.047026783,0.037487727,0.022326961,0.040387213,0.0056533143,-0.010000384,-0.011282638,0.006256797,-0.010133662,-0.01444725,-0.01918742,0.016995052,-0.033874765,-0.0063374205,-0.032267615,-0.017700076,0.028702015,-0.079306215,-0.0022354578,0.018565377,0.0020170105,-0.09043198,0.0014019997,-0.0074592177,-0.018140478,-0.065579616,0.022861462,-0.0429846,0.009420801,-0.030701911,0.008536055,0.011465296,-0.030323008,-0.01363658,-0.060007155,-0.05487716,0.006829247,-0.0028255086,0.022200532,0.02423927,-0.023188379,0.021416605,0.068201445,0.04062566,0.10106806,-0.039006107,0.019716255,-0.018643443,-0.044025082,0.09816003,0.0186588,-0.045000076,-0.03243544,-0.04081971,-0.04467336,0.02927917,0.033347737,-0.014702713,-0.05539313,0.07597474,-0.027820472,-0.04982798,-0.02710272,0.07083543,-0.07066246,-0.017565679,-0.006275996,0.0066385386,0.032041926,-0.0047121895,-0.04461033,0.013721308,-0.055751834,0.026701458,-0.02488454,0.011387429,-0.012122923,0.081390314,0.10937604,0.03140102,-0.049987275,-0.043151826,-0.016627539,0.001230946,0.033662383,-0.01687929,0.035779588,-0.03354693,0.008797377,-0.023325507,-0.0062361015,-0.011848007,0.013164578,0.013048187,0.008758686,0.06187115,0.024856133,-0.0019596769,0.03043838,0.03193702,0.044374067,0.050856978,-0.036530726,-0.02041297,-0.012826757,0.03967981,-0.019055156,-0.014655659,0.0013125979,-0.009255927,-0.022562569,0.049550794,-0.014156182,-0.020428177,0.022555484,-0.011267726,-0.015815044,-0.05067533,-0.018564248,-0.041988805,0.0023243164,-0.06845164,0.030603789,0.0034714704,0.03642813,-0.002288372,-0.047207236,0.025319008,0.018581582,-0.03182969,0.004483672,-0.0031493087,-0.054266006,0.0075271092,-0.009925661,-0.033236984,-0.053169053,-0.033340875,0.04669081,0.021184446,0.12721922,0.031450856,0.00191999,0.00400481,0.03192317,-0.05383922,-0.021084383,-0.014799072,-0.026698954,0.013040803,-0.022154111,0.0036394908,-0.050662007,-0.017044704,0.022463968,-0.01675699,0.012038173,-0.009358989,-0.0016188062,-0.058487296,0.020965328,0.0088480925,0.069026455,0.00179817,-0.0690816,0.022748366,-0.006986408,-0.024151843,0.0040926496,0.011628826,0.021855602,0.03222639,-0.027881151,-0.007270284,-0.032691456,0.028541557,0.021024657,0.031770788,-0.025203455,0.04188398,-0.049354587,0.0048768744,0.01100369,-0.022613294,0.05522322,0.0028411946,-0.0024373217,-0.03820894,0.037941102,-0.0049204845,0.002575102,-0.033508338,-0.0023006513,0.04437574,0.004593633,-0.023812132,0.004238421,0.0019069753,0.04359391,-0.034372263,0.0042527965,-0.026816748,-0.008720184,0.025580792,-0.0059043607,0.05236348,0.027627131,-0.090879865,0.056965776,-0.03489342,0.021650031,0.014062266,0.009633457,-0.021720428,0.0513787,0.07731177,0.029857434,-0.023307856,0.07580157,0.005759103,-0.006740139,0.065938324,0.045388576,0.0031576268,0.06656411,-0.00019184186,0.03802229,0.024426566,0.010376055,-0.0040616747,-0.030607725,0.043351885,0.038773373,0.050268978,-0.0067825154,-0.0023678006,-0.042326774,0.01480429,0.027508097,-0.027739843,-0.004702658,-0.008491525,0.02282792,-0.009399126,-0.048398636,-0.0005393171,0.04619366,0.02174309,-0.045952275,-0.04513708,0.026724175,0.02754219,-0.01566091,0.036720205,-0.061709937,-0.010247459,0.012721273,-0.02590474,0.0027596806,0.045888253,-0.01556038,-0.03826948,0.004010413,-0.040393658,0.0017460684,-0.006478617,-0.043374196,0.002931486,0.028151093,-0.009823142,0.00046873768,0.013271249,-0.04032389,-0.0024188359,0.03916747,0.122648455,-0.06696614,0.010695491,0.001978322,0.028273407,-0.013197255,-0.06065909,0.022246918,0.0540047,-0.040824167,0.032362428,-0.016983332,-0.050041225,0.0074914023,0.015811455,0.101002514,0.08517539,-0.028787535,-0.004211511,0.0001464015,0.029444238,-0.0029661213,0.002510746,0.082325496,0.022690946,-0.05417931,0.0032851375,0.0020720034,-0.05791811,0.092115656,-0.05462021,0.06678285,0.063621804,-0.0032168836,-0.014953277,0.03498607,0.037272643,0.024433551,0.0157832,-0.007308213,-0.01677021,-0.07595443,-0.01662471,0.017162606,0.008871168,0.039333932,0.010585943,-0.026162075,0.070030816,0.04163298,0.016244445,0.040953804,0.02285909,0.0039460575,0.056030065,-0.00072606566,0.02972531,-0.0084929615,-0.00059018104,0.019465629,0.05193881,-0.025057033,0.01778197,-0.025063936,0.0098418,0.0111045195,-0.01692783,0.018155921,0.017808864,-0.037665404,-0.02951945,-0.016200636,0.0519649,-0.03602487,-0.027578412,0.048885655,0.061582193,0.010879712,-0.007864733,-0.011806578,-0.058415223,0.0075837756,-0.04784188,-0.014351889,0.043150406,0.029675247,0.0041556763,0.06858172,0.00042105463,-0.0050946437,-0.017419573,0.02167912,0.01100099,-0.023216207,-0.0661117,-0.006999519,-0.0020597845,0.04884626,-0.019973153,-0.02423297,0.05072733,0.016841399,-0.04593658,0.021740446,0.060741365,0.009653817,-0.06952991,0.02590925,-0.0022136595,0.029960977,0.00048142552,-0.05609611,-0.06566224,0.024370885,-0.012590479,-0.00048291337,-0.0060713147,0.039124135,-0.022499023,0.0022601741,-0.0099913655,0.037070714,0.01111083,0.04896618,0.034670144,-0.01341792,-0.020224426,0.02009939,0.0046619535,-0.010801744,-0.017309558,-0.04576122,-0.048168562,0.04076229,0.04719363,0.0106101325,-0.06582369,0.084425844,-0.06125834,0.009570024,0.034328483,-0.01936595,0.04835301,-0.007005783,-0.012778402,-0.03035723,0.025090508,-0.00777768,-0.036267452,0.001782653,-0.019794455,-0.0040080533,0.014823889,0.0008239804,0.047107954,-0.028219856,0.021722643,-0.016198607,-0.0039694235,0.029656243,-0.011144188,0.04726528,-0.0008106866,0.011661242,-0.031224698,-0.018206796,0.006412134,-0.02266489,-0.0493757,-0.03847635,-0.0030016545,-0.033797972,-0.020681934,0.07341817,0.0017436119,-0.010731001,0.007982599,0.028197441,0.0018077706,0.013336951,-0.068124644,-0.024150496,0.030113554,0.05867757,-0.014001572,0.004230734,-0.04472082,-0.031550847,-0.06206177,0.004203254,-0.0011591659,0.066327,0.044542544,0.008367171,0.023491573,-0.052069,-0.010840275,0.008175613,0.01509059,-0.00958958,0.008343065,0.0015546913,0.054058876,0.04475077,0.030209914,0.021265712,-0.008171711,0.041042827,-0.009166909,0.054033104,-0.02792664,-0.044360325,0.05553475,-0.042754054,-0.039607015,0.033882506,-0.012445753,0.033119284,0.032731082,-0.010262702,-0.035722367,0.010050157,0.03237777,0.043681126,0.020339226,0.028174832,0.0585399,0.03980596,0.041009434,-0.058525503,0.009614474,-0.09474635,-0.011859247,0.0121843,-0.080980204,-0.012247849,0.016576871,-0.062719606,-0.02695266,0.0072971974,-0.018753998,-0.037137583,0.021474764,-0.016600298,0.045240108,-0.058428057,0.043420203,0.050311416,0.003980474,0.100273125,0.034414772,0.0105414605,0.010441245,-0.026756363,-0.032884184,-0.008011091,0.067924455,0.0018296206,-0.019469857,0.058132898,0.0050745406,-0.030785568,0.0022965227,-0.045690857,-0.027514314,-0.059512366,-0.01946969,-0.040662795,-0.010692182,0.02433478,-0.008805637,-0.027706994,0.005870116,0.023685075,-0.01773209,0.02246427,0.0155405365,0.0006039561,0.03324189,0.047759395,0.008179904,-0.005904734,0.023063514,0.03154813,-0.05741062,0.006957247,-0.021963043,-0.0005583344,0.021977633,-0.0036603522,0.01592627,-0.05909481,0.025760412,-0.026600305,-0.063310914,-0.025087703,0.0074405107,0.022526136,-0.008847496,-0.02420956,-0.04196543,-0.04587234,-0.047873434,-0.041691728,0.016713373,0.04706887,-0.0212288,0.04518013,-0.07752952,0.061379366,-0.11211953,-0.025088467,-0.043734755,0.024898175,-0.040711537,-0.03783899,-0.005598359,-0.037029635,0.00805988,-0.052618247,-0.02663392,0.051672235,-0.0015463384,0.006942889,-0.035431802,-0.0058371355,-0.023599314,-0.002023876,-0.015322242,-0.003280515,0.032225486,-0.023146996,0.048990004,-0.057887264,-0.01219283,0.0017135385,-0.0012924061,-0.075496614,-0.07159595,0.018911818,-0.0029887704,0.026075343,0.02699123,0.018353395,0.00054859277,-0.01573245,-0.05537564,-0.058652576,-0.023780758,-0.023682728,-0.026414081,0.026116999,0.012859329,-0.037301693,-0.01139996,0.0063681137,-0.017266367,0.019183531,-0.015185493,0.06058581,0.01187258,0.052787405,-0.06598419,-0.09303068,-0.008089331,-0.0740748,0.031212963,-0.016464034,0.045913745,0.032748297,0.0121931,0.023909373,-0.0051146336,-0.0019428285,0.007359991,-0.012752721,-0.05362158,-0.019265587,-0.0039975676,-0.012887355,0.029952014,-0.012343579,-0.0013987042,0.095348865,0.02290477,-0.047522653,-0.042423222,-0.058772147,-0.010715099,-0.040746752,-0.0038700434,0.017929755,0.0025985914,0.0019745142,-0.010300597,0.051992774,0.008841207,-0.03177734,-0.011283115,0.04766518,0.04177579,0.040877912,0.0005282815,-0.010365733,-0.08851897,0.01739385,-0.0024198764,-0.013926162,0.047824305,0.008468331,-0.03626641,-0.008248264,-0.040206257,-0.019953663,-0.007936822,-0.0005428986,0.08327492,-0.024545046,0.0005752779,0.0012839177,-0.055372898,-0.005114109,-0.023991609,0.016073832,-0.03213884,0.015779836,0.0055523347,0.026483543,-0.04503188,-0.0025859762,0.029129077,0.0145883495,0.023632694,0.05981332,-0.008981769,0.0444377,-0.032999128,0.0025806343,0.050858438,0.005936047,-0.024540674,-0.0036861105,0.07371378,0.050980315,-0.04677742,-0.03245857,-0.03273366,-0.028049406,0.024152962,0.029249974,0.014293233,0.0775323,-0.005494786,-0.057004113,-0.024075128,-0.019197198,-0.013124879,0.03043981,0.035803907,0.0045935935,-0.086688325,0.026835056,-0.0307648,0.065405644,-0.019554885,0.018853016,0.047063123,-0.0075295186]	Keywords: PowerNorm, Batch Normalization, Transformer, normalization, BN\nKey Objects: normalization, quadratic mean\nRefers to Images: None\nHypothetical Questions:\n- Why does standard Batch Normalization perform poorly in Transformers for text data?\n- What are the specific modifications made in PowerNorm compared to Batch Normalization?\n- How does PowerNorm's use of running statistics improve performance?\n---\nSummary:\nPowerNorm (PN) modifies Batch Normalization (BN) to improve Transformer performance with text data by relaxing zero-mean normalization, using the quadratic mean instead of variance, and utilizing running statistics for the quadratic mean.\nOriginal Text:\n$$z ^ { ( t ) } = \\gamma \\circ y ^ { ( t ) } + \\beta, \\\\ y ^ { ( t ) } = \\frac { x ^ { ( t ) } } { \\psi ^ { ( t - 1 ) } },$$  \n$$( \\psi ^ { ( t ) ) } ) ^ { 2 } = \\alpha ( \\psi ^ { ( t - 1 ) } ) ^ { 2 } + ( 1 - \\alpha ) \\left ( \\frac { 1 } { | B | } \\sum _ { i = 1 } ^ { | B | } x ^ { ( t ) } x ^ { ( t ) } ^ { 2 } \\right ),$$  \nwhere 0   , GLYPH are the learnable parameters as in BN formulation.  \n5.2.3 Normalization-free Transformer. Besides LN, there is another mechanism to construct deeper neural network. ReZero [5] replace LN module with a learnable residual connection. For each module F (  ), ReZero re-scales F (  ) in the residual formulation:  \n$$H ^ { \\prime } = H + \\alpha \\cdot F ( H ),$$  \nwhere GLYPH is a learnable parameter with zero-initialization.  \nReplacing LN in Transformer with ReZero mechanism is verified to induce better dynamic isometry for input signals and leads to faster convergence.\nContextualized Text:\nTo address performance degradation in Transformers due to instabilities associated with Batch Normalization (BN), Sheh et al. [121] proposed PowerNorm (PN). PN improves upon BN by relaxing the zero-mean normalization, utilizing the quadratic mean instead of the variance, and using running statistics for the quadratic mean to enhance Transformer performance with text data.	{"tags": ["normalization", "deep-learning", "transformer", "architecture"], "doc_id": "e2ee1938-d75c-4025-9ca0-99d44779b834", "summary": "PowerNorm (PN) modifies Batch Normalization (BN) to improve Transformer performance with text data by relaxing zero-mean normalization, using the quadratic mean instead of variance, and utilizing running statistics for the quadratic mean.", "doc_type": "text", "entities": ["PowerNorm", "Batch Normalization", "Transformer"], "keywords": ["PowerNorm", "Batch Normalization", "Transformer", "normalization", "BN"], "key_objects": ["normalization", "quadratic mean"], "contextual_text": "To address performance degradation in Transformers due to instabilities associated with Batch Normalization (BN), Sheh et al. [121] proposed PowerNorm (PN). PN improves upon BN by relaxing the zero-mean normalization, utilizing the quadratic mean instead of the variance, and using running statistics for the quadratic mean to enhance Transformer performance with text data.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.2 Layer Normalization"}, "hypothetical_questions": ["Why does standard Batch Normalization perform poorly in Transformers for text data?", "What are the specific modifications made in PowerNorm compared to Batch Normalization?", "How does PowerNorm's use of running statistics improve performance?"]}
1b4b8e7b-bef7-45b8-a021-d1116c6450dc	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.019647488,-0.007839132,0.03674027,0.064396955,-0.011643831,0.062368594,-0.0061145723,0.05838885,0.0011570107,-0.021567615,0.0076916185,0.021331146,-0.0006504042,0.026620306,-0.0376562,0.02510656,0.006497919,0.087941974,0.00410822,-0.03600308,0.05084804,0.036917046,0.009868785,-0.00802716,0.021339312,0.029148122,0.06982201,-0.013621844,-0.0014008455,0.041725427,0.030802779,-0.028902126,0.02371825,0.016805882,0.04781186,-0.026992066,-0.010950792,-0.017546635,-0.0037414709,-0.022027278,-0.029057788,0.05760635,-0.09669748,0.003678459,-0.0071703605,-0.072038226,-0.08688355,-0.028463697,0.015131145,-0.023795033,-0.051643156,0.023231732,-0.03253759,-0.028389202,-0.010657739,0.00468133,-0.028719416,-0.038539484,-0.00065368426,-0.076647826,-0.06387423,0.012434643,-0.017126476,0.014320769,0.01876297,-0.02057241,0.045640334,0.0037149452,0.041163642,0.11574585,-0.03198736,-0.0033163542,-0.016026968,-0.049543142,0.10737443,0.035856724,-0.06782733,-0.03685432,-0.044436235,-0.03466888,-0.055735026,0.112137556,-0.0029267028,-0.046580203,0.07843867,0.017081752,-0.009350438,0.005958005,0.07410578,-0.020778073,0.013933706,0.021601941,0.01115166,0.060045574,-0.029043628,-0.07636946,-0.034468338,-0.021994218,-0.007379547,-0.047027953,-0.022623984,0.01843792,0.08063317,0.10190786,0.033309903,-0.014021222,0.0030250656,-0.016646216,0.014016679,0.017555926,-0.020988416,0.011644545,-0.05154467,0.021263696,-0.017341664,-0.004972572,-0.021689828,0.011476761,0.0062166722,0.008413324,0.027902894,-0.01415065,-0.024454651,0.010457858,0.064240165,0.042199787,0.052438356,-0.02960828,-0.015244299,0.0053019696,0.024455557,0.013747456,-0.028100055,0.011706093,-0.019098116,-0.017540233,0.059892174,-0.0029355432,-0.003937005,0.04404845,0.0053234934,-0.028035691,-0.04432563,-0.004592051,-0.035382185,0.06091156,-0.05453406,0.0109220315,-0.046341326,0.024966348,0.041592088,-0.053764917,0.051547848,0.00017719685,-0.030822769,0.0003367909,0.024560738,-0.062111065,-0.0076520485,-0.02785772,-0.03376178,-0.0776178,-0.008489864,0.053447753,0.053706165,0.129712,-0.010541629,0.004043855,0.055104744,-0.012162121,-0.047181744,2.3065679e-05,-0.03328801,-0.047914796,0.02319765,0.004101329,-0.012102315,-0.033026535,0.0004920631,-0.012041821,0.023822172,0.035631854,0.0019024792,0.04052148,-0.023434758,0.013772937,0.019724244,0.060599424,-0.002058277,-0.005804426,0.024816312,-0.034375045,-0.009449683,0.011888206,-0.006268443,0.041995887,0.036956742,-0.053629957,-0.028228864,-0.040149014,-0.034841478,0.035110362,-0.0261451,-0.008870719,0.058603745,-0.013015891,-0.0012123974,-0.027028147,-0.030525696,0.06146077,0.021518731,-0.050142158,-0.017444773,0.069660954,0.028996348,-0.018508462,-0.0022706878,0.010068444,0.052313395,-0.0087253805,-0.041835703,0.011041863,-0.026942085,0.027725348,-0.032776304,-0.0054159896,-0.045759484,0.03421852,0.02790359,-0.017114125,0.045880906,0.026710283,-0.06453748,0.01740485,-0.012510122,0.028811583,0.016611671,0.006227431,-0.014086599,0.027311305,0.059257954,0.05810825,-0.023225872,0.07745336,0.008985647,-0.011216716,0.011145059,0.03496813,0.007942751,0.028504888,-0.019418934,0.03427547,0.029327001,-0.0068396693,0.02041558,-0.053384125,0.031128513,0.01660925,-0.005341844,-0.017533548,0.0127802165,-0.014563826,-0.0062881783,-0.0020123667,0.012588714,-0.021345723,0.008609482,0.013813512,-0.07134767,0.0034902974,0.046131175,0.025517814,0.02565726,-0.021043988,-0.04451614,0.031142933,-0.022017043,-0.016178265,0.033179108,-0.027238294,0.02052866,0.0137673635,0.001327598,-0.01511658,0.0013723519,-0.005546588,0.013193506,-0.039124828,-0.034491174,-0.02547654,-0.010277124,-0.0325432,0.036282044,-0.011596457,-0.05934431,0.002192617,0.0023528298,-0.011496176,-0.02306808,0.019233536,0.12102068,-0.06609305,-0.0140927015,0.015516051,0.029854072,0.012192454,-0.07595624,0.0021276474,0.032747515,0.0060543376,0.055024344,-0.0050528813,-0.009553648,-0.0099580465,0.03634793,0.083019115,0.04427145,-0.04211196,-0.0013915397,0.017209388,0.011376988,0.013171031,0.008154879,0.021605669,0.033704713,0.0022585357,-0.02593782,-0.033413567,-0.09187379,0.06341537,-0.0661038,0.02882809,0.053809334,0.024135511,0.014783472,-0.004276622,0.018965345,0.0429544,-0.0013934032,-0.0010854214,-2.5837438e-05,-0.09126197,-0.019575085,1.1231601e-05,-0.0495566,0.038988434,0.0138818575,0.0034265774,0.057750534,0.022898223,0.03347575,0.035950985,0.018895017,0.013858263,0.019437682,-0.009022114,0.018997498,0.031743526,0.022988811,0.0043223253,0.0919052,-0.06620229,0.032981206,-0.022371085,0.00092994265,-0.01677546,-0.044996325,0.009581291,0.039873324,-0.06359348,-0.029036496,-0.031758673,0.062413387,-0.023742096,-0.011607002,0.043264117,0.024179922,0.049051337,-0.01035737,-0.015880372,-0.0113303615,-0.012899012,-0.01285919,-0.00023275426,0.04310417,0.010702944,-0.0025191857,0.081807196,-2.1483946e-05,0.04905927,-0.050409894,0.017885005,0.015510751,-0.03419303,-0.05936549,0.016701482,0.017422786,0.028969066,-0.02013765,0.0029057383,0.024071697,0.051488295,-0.060846586,0.019779176,0.030010419,0.008011992,-0.09378337,0.045510475,0.035657603,-0.0029403288,0.038038433,-0.084172755,-0.03991924,0.041901354,0.009931432,-0.044941053,-0.03771857,0.067867234,-0.03456468,-0.0017532246,-0.015836088,0.019355182,-0.014772488,0.052289568,0.029563524,0.009765176,-0.017591422,0.037447847,0.005582585,-0.041176576,0.021416865,-0.061320264,-0.026546596,0.050906785,-0.017944029,0.005966055,-0.07213358,0.08912743,-0.06446254,-0.011316702,0.06676767,-0.021690765,0.051905997,-0.0061533265,0.03354373,-0.007589423,0.016582048,0.009756916,-0.029163696,-0.0060589924,0.03610656,-0.017146215,0.0209764,0.013567834,0.020514598,-0.004747775,0.010926209,-0.019765738,-0.0711378,0.011043158,-0.005585382,0.039103393,0.02624667,-0.0063065463,0.013065485,-0.031541307,0.016096333,-0.03209423,-0.02450328,0.0021813929,0.014000788,0.047817804,-0.024974026,0.045420177,0.044181284,-0.013935456,-0.0175539,0.044453498,0.04600756,-0.0059423675,-0.066348664,0.008524359,0.00069382065,0.05793153,0.0082490705,0.002483028,8.667629e-05,-0.01695206,-0.044172358,0.023979114,-0.004967495,0.059517488,0.014914563,0.0005217169,0.028072948,-0.036572754,-0.006546961,0.0028699553,0.027458513,-0.019053519,-0.021875955,-0.009779127,0.06499285,0.04326215,-0.045300525,0.005648575,-0.0036822213,-0.0011722684,-0.0138528375,0.07695537,0.013823778,-0.018851232,0.009181142,-0.024336305,-0.029257353,-0.026386414,0.017831596,-0.029526258,5.667841e-05,0.03095295,-0.0042918893,0.028461317,0.01804146,0.009688156,-0.0050974786,0.0026719002,0.014505601,0.034582682,0.047706276,-0.050075028,0.043416083,0.003921229,-0.050268788,0.014538137,-0.012352667,-0.060459718,-0.004320904,-0.03420183,0.020818407,-6.700373e-05,0.024702951,-0.03653483,0.00067977066,-0.020442495,0.08382553,-0.063686766,0.011909661,0.023971288,-0.0046314076,0.096287236,0.0151584735,-0.016260609,-0.014608016,-0.015447905,-0.028124094,0.018983161,0.08248374,0.029694604,0.006071692,0.042503383,-0.014563196,-0.0037581783,-0.029355304,-0.04425781,-0.005292504,-0.022383578,-0.0051658046,-0.019654902,-0.012618549,0.007205721,-0.024782022,-0.010943202,-0.0048174253,0.034574047,-0.045781218,0.030833932,0.037726376,0.0005206304,0.037900236,0.00071860984,0.06393469,0.020232666,0.025165295,0.017242437,-0.005753306,0.02041847,-0.0062318533,-0.012711147,0.0066002635,-0.014494051,-0.027392983,-0.07499504,0.031285357,-0.008563691,-0.02912994,0.00937858,0.06855444,0.021216236,0.03680946,-0.039075084,-0.039671443,-0.030071273,-0.03744817,-0.0097798705,-0.00028659002,0.021887584,0.004408747,0.012123145,-0.039909773,0.0949604,-0.07845819,-0.0105699,-0.020220315,0.028434753,-0.0076483875,-0.06565099,-0.014555009,-0.009443596,-0.01512464,-0.037451036,0.024308156,0.06004506,-0.01991703,0.0029003215,-0.0018004864,0.034563847,0.0059462558,0.023228534,0.011254948,0.003805628,-0.018027648,-0.030728009,0.07999404,-0.03170682,0.017407771,0.01762173,-0.0011572951,-0.09229998,-0.03229995,0.02286842,0.037531823,0.0026777927,0.023536274,0.0072376914,0.03483739,-0.052051235,-0.0457831,-0.036285117,-0.0046032853,0.003872775,0.03227634,0.048116695,-0.008851986,-0.056815594,-0.033384435,0.043669116,-0.06946832,0.01306021,0.051855497,0.047564622,0.05821302,0.048965666,-0.011123743,-0.05040627,-0.001477938,-0.03271986,0.014348698,-0.028333532,0.04648897,-0.0085838735,0.033727095,-0.03689239,0.008949958,-0.017827773,-0.004451766,0.027354432,-0.0040077744,-0.030989353,-0.02080894,0.008833224,-0.009180555,0.03585405,-0.0087177465,0.06717792,0.024611233,-0.059185963,-0.020165214,-0.052143898,-0.025557859,0.016064802,0.026021985,0.005052434,0.020133767,0.039162423,-0.018033577,0.08785577,-0.0033651083,-0.06639151,-0.04605,0.030150818,0.023460928,0.048735842,-0.0042985836,-0.02977377,-0.08716199,-0.009817933,-0.013771158,-0.04504204,0.060387336,0.016693799,-0.019952362,-0.021928392,-0.0037194404,-0.003677892,0.049415167,-0.016678546,0.07334204,-0.006538194,0.008610067,-0.014925779,-0.054430872,-0.019652892,-0.06564873,-0.01877949,-0.010983866,0.018207049,-0.01920725,0.014005918,-0.032238986,-0.00438514,0.0047988994,0.020674529,0.004969926,0.029876582,0.011051989,0.058682583,-0.019148737,0.052572194,0.060044575,0.004997207,-0.0029823496,-0.0036568986,0.031534173,-0.009277539,-0.018671494,-0.027878141,0.00088302925,-0.057871427,0.019161796,0.020892046,0.036310278,0.027162338,-0.007691998,-0.059679843,-0.041969687,-0.009988188,0.009940704,0.07296826,0.022162171,0.0017761355,-0.067959264,0.011627336,-0.043431282,0.049456947,-0.019762054,0.0139336595,0.004214263,-0.018748702]	Keywords: feed-forward network, FFN, Transformer, rank collapse, token-uniformity, activation function, ReLU\nKey Objects: Position-wise Feed-Forward Network, FFN layers, selfattention modules\nRefers to Images: None\nHypothetical Questions:\n- Why is it problematic for Transformer models to simply stack self-attention modules?\n- What is meant by "token-uniformity inductive bias" and how do FFN layers help?\n- What are some of the approaches researchers have taken to modify the FFN module?\n---\nSummary:\nPosition-wise feed-forward network (FFN) layers are crucial for Transformer performance, mitigating rank collapse and token-uniformity biases. Researchers have explored various modifications to the FFN module, including alterations to the activation function.\nOriginal Text:\n### 5.3 Position-wise FFN  \nDespite its simplicity, the position-wise feed-forward network (FFN) layers are important for a Transformer to achieve good performance. Dong et al. [32] observe that simply stacking selfattention modules causes a rank collapse problem, leading to token-uniformity inductive bias, and that the feed-forward layer is one of the important building blocks that mitigate this issue. Various works have explored modifications on the FFN module.  \n5.3.1 Activation Function in FFN . The vanilla Transformer [137] adopts the Rectified Linear Units (ReLU) activation for non-linearity in between the two FFN layers. Over time, several studies have explored different activation other than ReLU.  \nRamachandran et al. [106] try to replace ReLU in Transformer with Swish function f ( x ) = xsigmoid ( b x ) and observe that it consistently improve performance on WMT 2014 English-German dataset.\nContextualized Text:\nWithin the Transformer architecture, position-wise feed-forward networks (FFNs) play a key role in achieving good performance. Dong et al. found that stacking self-attention modules alone can cause issues, and the FFN layers help to mitigate this problem by introducing diversity and preventing token-uniformity biases. As a result, numerous researchers have investigated modifications to the FFN module.	{"tags": ["architecture", "NLP", "transformer", "optimization"], "doc_id": "1b4b8e7b-bef7-45b8-a021-d1116c6450dc", "summary": "Position-wise feed-forward network (FFN) layers are crucial for Transformer performance, mitigating rank collapse and token-uniformity biases. Researchers have explored various modifications to the FFN module, including alterations to the activation function.", "doc_type": "text", "entities": ["Transformer", "Dong et al.", "ReLU", "Ramachandran et al."], "keywords": ["feed-forward network", "FFN", "Transformer", "rank collapse", "token-uniformity", "activation function", "ReLU"], "key_objects": ["Position-wise Feed-Forward Network", "FFN layers", "selfattention modules"], "contextual_text": "Within the Transformer architecture, position-wise feed-forward networks (FFNs) play a key role in achieving good performance. Dong et al. found that stacking self-attention modules alone can cause issues, and the FFN layers help to mitigate this problem by introducing diversity and preventing token-uniformity biases. As a result, numerous researchers have investigated modifications to the FFN module.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.3 Position-wise FFN"}, "hypothetical_questions": ["Why is it problematic for Transformer models to simply stack self-attention modules?", "What is meant by \\"token-uniformity inductive bias\\" and how do FFN layers help?", "What are some of the approaches researchers have taken to modify the FFN module?"]}
7247201b-d7c9-44d8-84d3-518b21e77340	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.039360657,0.026106153,-0.010257673,0.042703867,0.032360833,0.106579855,0.016681857,0.022368431,0.015554706,-0.026128042,0.04861203,-0.007122225,-0.02783755,0.030302605,-0.042491537,0.02660106,-0.0025550334,0.059310768,-0.028164728,-0.028190756,0.074725695,0.022218945,0.010025437,-0.015464365,0.0069586146,0.04086269,0.05952553,-0.01803391,0.004466124,0.015692877,0.010608155,-0.013444033,-0.007650824,0.018219428,0.04916631,-0.0013795405,-0.005354768,-0.010752277,-0.013144204,-0.02029986,-0.0062958016,0.059329722,-0.089806825,-0.0070792865,-0.002149433,-0.08263875,-0.096488565,-0.033569504,0.010833448,-0.025989912,-0.033055414,0.023012195,-0.022386665,-0.015336088,0.009862005,0.024564397,-0.008031922,-0.0027551975,0.02604008,-0.087646894,-0.08427353,0.016560549,-0.043648627,0.028439676,0.00021754393,-0.017294848,0.05356857,-0.0046178224,0.034327954,0.11066684,-0.05864545,-0.008980221,-0.013806267,-0.061414264,0.10082856,0.04390894,-0.07038147,-0.055805173,-0.04961852,-0.057195753,-0.017318007,0.07659176,-0.009393645,-0.04407098,0.08439071,-0.013837989,-0.0409487,-0.010586232,0.0775281,0.012735629,0.03536588,0.0072688158,-0.01758367,0.058388356,-0.013351034,-0.014218724,-0.03521923,0.0015982364,-0.017690945,-0.033372335,0.006236345,-0.0097685885,0.06577404,0.078878894,0.024232617,0.026835654,-0.017124832,-0.0071129575,0.04628724,0.009873276,-0.01313745,0.030668348,-0.055775464,-0.028045755,-0.06920676,-0.029696872,-0.08072181,0.036243662,0.05826853,0.027083332,0.037556723,0.0019618054,0.004279196,-0.014426653,0.020977143,0.08697363,0.018346269,-0.007816398,-0.03928452,-0.028438527,0.04010572,-0.02707382,-0.041553665,-0.014548401,0.0046608783,-0.019432472,0.03744562,0.0026612955,0.017006356,0.05503177,-0.012978017,-0.016144393,-0.06098369,-0.018291218,-0.06330574,0.049843058,-0.067452975,-0.017942313,-0.025129568,0.08950282,0.014689036,-0.03885762,0.04195743,0.020369703,-0.014873597,-0.015468673,-0.035304137,-0.024882277,-0.014202275,-0.0316122,-0.07607581,-0.08494501,-0.031614836,0.018093491,0.030063665,0.058459744,-0.032427438,0.01876335,0.047279805,0.0396008,-0.022099297,-0.018456047,0.025854424,-0.02465301,0.018085137,0.020586459,0.041811448,-0.041389268,-0.028031183,0.016787233,-0.025354836,0.029321196,-0.0035089445,0.057256408,-0.028598633,-0.0062262616,0.0006850498,0.068777226,-0.019593965,-0.052358586,0.030438231,0.0019699316,-0.060495984,-0.0116502205,0.03188088,0.024442209,0.043627914,-0.03467103,-0.010225573,-0.067913085,0.017553166,0.033978555,-0.02100265,-0.009578454,0.03149542,0.0027083694,-0.004291161,-0.0214917,-0.03177262,0.055304553,0.045183282,-0.019146347,0.015503861,0.056005057,0.029223405,-0.0025415968,-0.05371068,0.031951644,0.071600474,-0.029658206,-0.062554754,-0.023465127,0.0017180643,0.0073006833,-0.031203004,-0.011551212,-0.04293787,0.04610692,0.025213676,-0.020829514,0.032210175,0.015977945,-0.008424385,0.044158943,-0.002917518,0.021750921,0.028569344,-0.0017224995,0.043105837,0.03158073,0.073173,0.0056347754,-0.017616188,0.05196435,0.0088790255,-0.004404776,0.04790468,0.05169348,-0.02902049,0.037058093,-0.014357139,0.0045712357,0.0012608141,0.0018954633,0.0038276196,-0.014283448,0.04034099,0.0059001185,0.009035187,-0.014390058,-0.015130847,8.291818e-05,0.0029798816,0.0026998618,-0.0038999596,-0.0025429523,0.02621161,0.0063288924,-0.04772911,0.0034105396,0.06333202,0.03435034,0.020694869,-0.026356837,-0.020315258,0.049409915,0.004183308,0.0006911167,0.023753537,-0.018401315,-0.01617888,-0.0007502916,0.017607262,0.0017790552,0.011236436,0.035977624,-0.013774947,0.018973913,-0.029061407,0.017545916,0.0024619664,-0.06071973,0.023723308,-0.01869695,-0.017774783,-0.029490216,-0.014026521,-0.042285398,-0.0037710257,0.02939133,0.1312396,-0.048842397,-0.016581181,0.009385917,-0.024791773,0.040171243,-0.07921182,0.014775719,0.051391106,-0.01934528,0.023714786,0.00092967594,-0.026050683,-0.008190073,0.040713664,0.0753936,0.029904272,-0.051862486,-0.010240947,0.004680497,0.010889183,-0.011504953,-0.043596853,0.03625417,0.023047175,-0.01735685,-0.032435104,-0.053350665,-0.06205747,0.06672921,-0.036737565,0.036287818,0.055427007,-0.00050732895,0.017977849,0.0040003844,0.030715099,0.06497355,0.0120196,0.036074586,-0.039757885,-0.06344102,0.0023383673,0.023193428,-0.034584187,0.007970039,0.054164115,-0.021564685,0.05644334,0.024585653,0.013724731,0.065600656,-0.0038464926,0.020533212,0.0254806,-0.008759179,0.023411714,0.04598698,0.007986973,0.02332034,0.061653085,-0.03193117,1.0708811e-05,-0.025432035,0.0046922085,-0.056441493,-0.044494655,-0.011142229,0.030112593,-0.04450402,0.0014372395,-0.012932659,0.049920797,-0.03669121,-0.003928013,0.033130836,-0.0044309897,0.050439585,-0.023628537,-0.032269564,-0.0467622,0.0052373614,-0.06123311,0.004503402,0.005537925,-0.013526952,-0.00032371524,0.07711709,0.027903497,0.02270232,-0.019187707,0.023672676,0.04715842,-0.050834447,-0.09232563,-0.0007487214,0.0212229,0.012834546,-0.015234381,0.009388483,0.014908392,0.008246908,-0.05993916,-0.0023187578,0.040284183,0.06609824,-0.06337347,0.03712184,0.03546614,0.01097727,0.041067462,-0.05556721,-0.046665665,0.03243526,0.025339296,-0.027470607,-0.023775626,0.012892397,-0.02525182,-0.015579511,-0.039518684,0.045501903,0.001158625,0.050910577,0.031533007,0.0044382974,-0.045841068,-0.001009675,-0.008602032,-0.0810018,0.011219816,-0.047135275,-0.019112146,0.05006389,0.0023130113,-0.0066742282,-0.017868906,0.057671636,-0.05647337,0.010613295,0.041466434,0.01973365,0.045800094,-0.013794889,-0.018134242,-0.0051203165,0.058691356,0.0060356883,-0.031152168,-0.008243091,0.05372598,-0.009683973,0.033477888,-0.02604091,0.043668974,-0.0011804055,0.029495697,-0.022271587,-0.054131206,0.0075386125,-0.017514743,0.042370163,0.006345532,0.036261734,-0.015054957,-0.042068843,0.028642096,-0.021774858,-0.0038837618,-0.054348763,-0.025804011,0.060948763,-0.057491515,0.01161167,-0.023036808,0.016036766,0.01042992,0.053141676,0.014740257,0.028204711,-0.051744636,-0.0029786294,0.0045173545,0.03230164,-0.06092837,-0.0057589523,-0.022819426,-0.028172925,-0.025708986,0.04114833,-0.019029688,0.021249859,-0.006412711,0.0032118687,0.017902184,-0.03141331,-0.0121208485,-0.029129628,0.017615179,0.018148603,-0.036501866,-0.008726493,0.028500456,0.045342598,-0.01467751,0.020626022,0.03072501,0.043683592,0.0023308117,-0.0013219451,0.024630625,-0.0342157,-0.022142489,-0.0335743,-0.024593066,-0.017402418,0.005284796,0.015450547,0.0019707186,-0.0025776264,-0.010766361,-0.0015651463,-0.024690354,-0.024785444,0.018595533,0.0443908,0.056184217,0.042659283,0.07034996,-0.054857627,0.026521234,-0.019167421,-0.03676394,0.01858801,-0.023550058,-0.017638277,-0.0025671604,-0.023918556,0.03554164,0.012333674,0.0037231003,-0.061438728,0.018249247,-0.017996922,0.08464565,-0.032724734,0.013453409,-0.02707994,0.04192416,0.084046386,-0.0016111811,-0.058567464,-0.009920845,-0.023152804,-0.034734067,0.004529758,0.053366844,0.018116917,0.0061285137,0.047061365,-0.024825659,0.042743858,-0.007094032,-0.04215851,-0.027512925,-0.0018886207,-0.0275627,-0.0067854263,0.02249827,0.015907556,-0.0059987614,-0.025440501,0.025820939,0.036313053,-0.04885956,0.032268442,0.043464407,-0.012374399,0.016884834,0.05936006,0.039564583,0.009658727,0.020242536,0.024169056,-0.03634898,0.0175804,-0.048416346,-0.033157438,0.003336842,0.01944205,-0.03348458,-0.04533561,0.031355202,-0.016644211,-0.011077954,-0.027198268,-0.0024320327,0.041474722,0.01877513,-0.040418305,-0.08945986,-0.0106525775,-0.07350695,-0.07732475,0.02960436,0.007032263,0.0068776417,0.071289554,-0.08545031,0.111783564,-0.050859783,-0.040651344,-0.044928536,0.015125578,-0.024813063,-0.014231659,0.004762986,-0.030767927,0.011442442,0.016433308,0.039138842,0.056274343,-0.014600049,0.023281774,-0.020323263,0.039166477,-0.0027670853,0.003551242,0.017079918,-0.017125303,0.010626133,-0.02004451,0.054639358,-0.0527114,-0.0057083694,0.016843745,0.01376989,-0.09192699,-0.055492673,0.025222104,-0.0042062257,0.006489453,-0.012903158,0.016072836,0.033243768,-0.04596727,-0.038394,-0.036287587,0.020553745,-0.0074317586,0.06852158,0.00050526665,0.006922087,-0.060123708,0.00502073,0.02971814,-0.026810283,0.0104232095,0.028176174,0.029744506,0.041818548,0.021596817,-0.016794465,-0.06521268,-0.009900755,-0.026887985,-0.03020793,-0.034840737,0.04013653,0.035503387,0.00055677816,0.002870999,-0.0016830374,-0.018704297,-0.017748177,-0.04748966,0.033935618,-0.04157272,-0.007745322,-0.036664262,-0.010351801,0.04921325,-0.012624157,0.09047248,0.026181675,-0.014833323,-0.006557303,-0.05196834,-0.0071661994,-0.029082071,0.01717356,0.01712759,0.03512467,0.06808085,0.016637016,0.08959012,0.03075803,-0.046972692,-0.029168505,0.06686462,0.037568554,0.03952187,0.010210248,-0.010915984,-0.084513046,0.03592136,-0.010248802,-0.042912807,0.013811666,0.049102154,-0.017224422,-0.02480838,-0.01011353,0.0004351054,0.00703949,0.0173709,0.043411333,-0.012769684,0.009738612,-0.012648718,-0.029792571,-0.018940374,-0.028726809,-0.011583601,0.006341737,0.020119386,-0.0006378004,-0.022401404,-0.007766435,-0.014688412,0.028613439,0.040943034,-0.00043464702,0.009731106,0.017743146,0.056225445,-0.021905186,0.038107615,0.027756887,-0.013938919,0.0037403633,-0.01592421,0.0254884,-0.0014405906,-0.021822402,-0.029306473,0.001139273,-0.06153625,-0.008748762,0.016272787,-0.008247364,0.020214535,0.036297176,-0.03366532,-0.006597038,-0.00072782557,-0.036601376,0.06122663,0.05422915,0.0522331,-0.045371186,0.022633895,-0.0146917915,0.049131315,-0.022375312,-0.007316703,0.024121676,-0.055939395]	Keywords: ReLU, Swish, GELU, GLU, FFN, activation function, Transformer\nKey Objects: ReLU, Swish, GELU, GLU, FFN, activation function\nRefers to Images: None\nHypothetical Questions:\n- Why would replacing ReLU with a different activation function like Swish or GELU improve performance?\n- What are the potential drawbacks of using GLU, given that it introduces extra parameters?\n- How does GELU contribute to the prevalence of pre-trained language models?\n---\nSummary:\nResearchers have explored various replacements for the ReLU activation function within the feed-forward network (FFN) layers of Transformers, including Swish, GELU, and Gated Linear Units (GLU), to improve performance and efficiency.\nOriginal Text:\nRamachandran et al. [106] try to replace ReLU in Transformer with Swish function f ( x ) = xsigmoid ( b x ) and observe that it consistently improve performance on WMT 2014 English-German dataset.  \nGPT [101] replace ReLU with Gaussian Error Linear Unit (GELU) [52] on language pre-training. It becomes the default practice for many pre-trained language models [28, 50].  \nShazeer [118] explore using Gated Linear Units (GLU) [25] and its variants as a drop-in replacement for ReLU in FFN. Their pre-training experiments show that the GLU variants consistently improve vanilla Transformer with ReLU activation. Note that GLU introduces extra parameters and the experiments are conducted with the intermediate dimension of FFN reduced to match the parameter count with baseline.  \n5.3.2 Adapting FFN for Larger Capacity . Several works have focused on expanding FFNs in order for a larger model capacity. The basic idea is to replace FFNs with similar structures with much more parameters.\nContextualized Text:\nWithin Transformer models, the feed-forward network (FFN) layers often utilize ReLU activation functions. However, several studies have investigated alternatives like Swish, Gaussian Error Linear Unit (GELU), and Gated Linear Units (GLU) to enhance performance. For instance, Ramachandran et al. replaced ReLU with Swish and observed improvements on the WMT 2014 English-German dataset. GPT utilized GELU for language pre-training, which has become a standard practice in many pre-trained language models.	{"tags": ["architecture", "optimization", "FFN", "Transformer"], "doc_id": "7247201b-d7c9-44d8-84d3-518b21e77340", "summary": "Researchers have explored various replacements for the ReLU activation function within the feed-forward network (FFN) layers of Transformers, including Swish, GELU, and Gated Linear Units (GLU), to improve performance and efficiency.", "doc_type": "text", "entities": ["Transformer", "GPT", "GELU"], "keywords": ["ReLU", "Swish", "GELU", "GLU", "FFN", "activation function", "Transformer"], "key_objects": ["ReLU", "Swish", "GELU", "GLU", "FFN", "activation function"], "contextual_text": "Within Transformer models, the feed-forward network (FFN) layers often utilize ReLU activation functions. However, several studies have investigated alternatives like Swish, Gaussian Error Linear Unit (GELU), and Gated Linear Units (GLU) to enhance performance. For instance, Ramachandran et al. replaced ReLU with Swish and observed improvements on the WMT 2014 English-German dataset. GPT utilized GELU for language pre-training, which has become a standard practice in many pre-trained language models.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.3 Position-wise FFN"}, "hypothetical_questions": ["Why would replacing ReLU with a different activation function like Swish or GELU improve performance?", "What are the potential drawbacks of using GLU, given that it introduces extra parameters?", "How does GELU contribute to the prevalence of pre-trained language models?"]}
df8e6b6d-65be-493e-8e9a-8093a12f3b18	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.034328584,0.02021409,-0.0016011556,0.048287757,-0.03214026,0.07505032,-0.0021256416,0.02722168,0.037438877,0.0063382713,0.045945067,-0.010116326,-0.023867175,-0.0057613864,-0.038047094,-0.014878942,0.0071081626,0.09171883,0.026739785,-0.04422425,0.03603855,-0.0045321253,-0.019540856,0.019144947,0.0048026005,0.015897246,0.06706995,0.005702899,0.0045987046,0.0075183907,0.009852162,-0.044409476,0.057585336,0.02976834,0.053834412,-0.07893319,-0.016192157,-0.03123214,-0.01980799,0.00477549,-0.014124003,0.02967215,-0.045054324,-0.008254197,-0.02112245,-0.024191653,-0.053835675,-0.02570474,0.015538484,-0.017494857,-0.005286115,0.036393072,0.0069585135,-0.025851978,-0.0009340105,-0.009193793,-0.004527714,-0.057644073,0.018100085,-0.09565047,-0.03605165,0.014976058,-0.04199199,-0.023422448,0.033174615,-0.025537234,-0.028608425,0.022365062,0.055317234,0.14089568,-0.022884853,0.0113478135,-0.005056486,-0.01459326,0.1080838,0.04066015,-0.045316774,-0.04364934,-0.05798195,-0.010241777,-0.024988499,0.08769015,-0.03416429,-0.019642374,0.061297998,-0.00652536,-0.06361435,0.01959206,0.06330866,-0.026569549,0.038525548,0.023188444,-0.06074372,0.066842854,0.004425305,-0.042982105,-0.031264678,-0.02557464,-0.031148618,-0.048738506,0.027555432,0.0003104002,0.07128773,0.10627403,0.02225336,-0.005255157,-0.01759761,-0.023899099,0.01998949,-0.013386315,-0.023410557,0.0502142,-0.059753995,0.0028309673,-0.08341941,-0.0006733986,-0.058039017,0.029135965,0.03907383,-0.016263124,0.048884094,-0.0017092396,0.0032912935,0.0033754385,0.026848711,0.025426246,0.025530105,-0.030792803,-0.021609923,0.007732793,0.07340748,0.016168298,-0.019274704,0.018420817,-0.03170218,-0.01854566,0.04409407,-0.04096612,-0.012781772,0.023366507,-0.015483892,-0.024865983,-0.045389276,0.018294586,0.0026771084,0.029502178,-0.101297624,0.008827951,-0.03398926,0.041197654,0.04390354,-0.044373505,0.050226483,-0.027761612,-0.032079086,-0.007293646,-0.011026606,-0.03885013,0.024057016,-0.028360838,-0.07372649,-0.08472106,-0.012271643,0.056635275,0.051575083,0.09664155,0.0056335395,-0.025626108,0.03719178,0.025654122,-0.055112552,-0.0036141537,0.005155882,-0.04879022,0.01749551,0.03257171,0.013687332,-0.045489326,0.008396373,0.00870535,-0.003289837,0.018576099,-0.024576694,0.03451749,-0.025542375,0.068582445,0.007383954,0.06867168,-0.026833098,-0.051549412,0.011465067,0.018653627,-0.0049455017,-0.012006274,0.031726174,0.04372024,0.05596396,-0.065776855,-0.040943556,-0.024247326,0.02129844,0.048884273,-0.0142748235,-0.015620061,0.01303573,-0.012279946,0.009151805,-0.043338135,-0.0281338,0.016550144,0.002676227,-0.0477172,0.0031012,0.034507517,0.007107597,0.01457512,-0.02656254,0.02566523,0.054748703,-0.008991218,-0.048202034,0.024524922,-0.0028449344,0.035443507,-0.066450715,0.031899,0.009103061,0.026177026,0.024635725,-0.0137123745,0.060503125,0.022407802,-0.016618198,0.04839603,-0.067029305,0.009075815,0.027137944,0.03305463,-0.015669648,0.049537685,0.045044515,0.053975098,-0.030100366,0.07129147,-6.100882e-05,-0.016139401,0.022343911,0.012728575,0.0013338447,-0.018566716,-0.023380198,0.034249082,0.022237983,-0.00470029,0.009208533,0.014425373,0.0019101123,0.018703176,-0.009108704,-0.032002576,0.022461638,0.0030467357,0.012204396,0.061878383,0.01495571,0.015204327,0.032091554,0.00272164,-0.031065965,0.01011547,0.039780904,0.04195103,0.021990316,-0.005150168,-0.038078535,-0.0046922076,0.0026489762,0.02970657,-0.0041244575,-0.01539906,-0.029514037,-0.011058593,-0.042237345,0.0033196555,-0.03873078,0.0064674243,0.040420786,-0.0025920642,-0.03211738,0.025225662,0.021271136,-0.01860361,0.063680254,-0.0467055,-0.011917198,-0.048024427,-0.018883022,-0.052974712,-0.024413386,0.003732591,0.12428169,-0.023610057,0.024306552,0.017969673,0.018964157,0.0193974,-0.06263538,0.05871137,0.008228717,-0.0013804232,0.004599845,-0.0005556701,0.0058906935,-0.0020603586,0.06979014,0.068421364,0.042674206,-0.059493016,-0.0066082114,0.025176428,0.041128453,0.029086629,-0.02071532,0.036393177,0.02800631,0.0024959794,0.0049139042,-0.024985071,-0.064291365,0.070622176,-0.033277437,-0.016228676,0.068265505,-0.007662055,-0.019963128,-0.041654296,0.013432733,0.031713508,0.013292942,0.022876436,-0.004912725,-0.072902724,0.0028099816,-0.012305053,-0.03416441,0.045733556,0.0013632486,-0.00597257,0.016529504,0.025714252,0.006523599,0.0321732,-0.008497836,0.019505251,0.01653898,-0.03509633,0.0026460188,0.029122721,0.0098946495,0.014811629,0.043420825,-0.07738974,0.046301886,-0.040771108,0.019470887,-0.034177247,-0.049034815,0.017215878,0.08868656,-2.6043808e-05,-0.038298428,-0.026815148,0.084401384,-0.045411356,0.036169928,0.048361164,0.015764197,0.056898464,-0.03674734,0.022886815,-0.0055093933,0.00036553195,-0.008800765,0.01597233,0.020837659,0.004451138,0.02765343,0.055374503,-0.06724762,0.047405675,-0.05251105,-0.013911628,0.05777205,-0.012762499,-0.027113669,-0.009350088,-0.001997726,0.026792562,0.007077128,0.021175228,0.037481416,0.040138308,-0.06348264,0.014288345,0.0026206695,0.02956704,-0.08972868,0.07012801,0.039383784,0.039747078,0.038522206,-0.0028897377,-0.020519037,0.006354819,3.7509744e-05,-0.057572443,0.0048023467,0.026541099,-0.024206538,0.03813355,-0.0029913033,0.060601585,0.019192593,0.010264506,0.04053939,0.020461097,-0.014621252,0.057511285,-0.046243288,-0.07102526,-0.007114041,-0.044002578,0.012656517,0.057144653,-0.03560424,0.0016448322,-0.044988837,0.0834391,-0.014140335,0.013705346,0.0055962005,-0.03042724,-0.004349868,-0.055363376,0.00018683508,-0.0223457,0.031779546,0.036427338,-0.043666765,-0.0057229535,0.076173574,0.023235003,0.043855753,-0.0059644664,0.06976167,-0.0062109437,0.007957781,-0.029444454,-0.060193077,-0.005651663,0.044163655,0.040875457,-0.006716969,-0.014119815,-0.008943361,-0.016257526,-0.025210863,-0.018034687,-0.03582193,-0.031755447,0.03141738,0.039828204,0.023192732,0.026984053,0.020430863,-0.04093711,-0.03029138,0.051411234,0.040457953,0.011668847,-0.0593915,-0.0090033375,-0.021928832,0.038921792,0.002497294,-0.017604845,-0.02343641,-0.0269012,-0.03732305,0.05704796,0.013473088,0.09025077,-0.0033747875,0.031766627,0.008071917,0.00855376,0.029674672,-0.005666374,0.017619822,-0.009564871,-0.037098955,0.013734189,0.039704926,0.07617053,-0.004831694,-0.01387576,-0.0037308168,0.0182196,-0.01962739,0.020530932,0.0512936,-0.037237465,0.018864151,-0.034869254,-0.05235302,0.017463012,-0.010249226,0.04418784,-0.014505189,0.043909885,0.022866484,0.021603238,-0.0038375137,0.010611523,0.02686474,0.0036297988,0.019180536,0.0074169985,0.021901762,-0.061906446,0.013595736,-0.0401354,-0.031929806,0.013460166,-0.062485006,-0.054687552,-0.0063574305,-0.03614718,0.023570148,-0.011795908,-0.0051376973,-0.0428554,0.011664772,0.005217189,0.064769164,-0.03120591,0.0047544846,-0.0024504147,-0.010157896,0.061432675,-0.00018388344,-0.00667476,-0.007729259,-0.012839937,-0.022741448,-0.010847911,0.031596612,0.0034618136,-0.01062192,0.020591367,-0.0163578,0.009552115,0.0021476331,-0.02704897,-0.025942355,-0.052983698,-0.020743519,-0.04013323,-0.002823826,0.061051153,-0.024701618,-0.025616689,-0.02273651,-0.009103851,-0.0111456085,0.019414134,0.038137525,0.018682836,0.027394265,-0.0043703276,-0.009056408,0.033497896,0.058470525,0.01932023,-0.037597,-0.013729437,0.008589447,0.018809615,-0.005135856,-0.027351417,-0.006856919,-0.06675869,0.012523284,-0.002141901,-0.017936042,-0.026790008,0.09477171,0.0611227,0.009486133,-0.040339492,-0.042671822,-0.03888164,-0.010592168,-0.033597264,0.010177715,-0.01947974,-0.0062058526,0.025248555,-0.023398139,0.077248886,-0.089028955,0.006658422,-0.081884414,-0.017305644,-0.02454811,-0.040961802,-0.04187442,-0.046057235,-0.0121209435,-0.04593654,0.041038554,0.058669843,-0.04337453,0.036594726,-0.019095846,0.036056567,0.0053765085,0.006267062,-0.007496322,0.0032171633,-0.021849358,0.0022031178,0.092885844,-0.062278457,0.013781408,-0.0038750912,0.003817562,-0.07927955,-0.024221336,-0.021598484,-0.0062616733,-0.022019668,-0.012803666,0.022232601,0.025845502,-0.050097153,-0.027799288,-0.06661411,-0.007593078,0.027657775,0.0053652944,-0.003085369,-0.0018460448,-0.0552161,-0.032647755,0.036974285,0.014292423,0.04199446,0.014886333,0.0014354875,0.040766977,0.029011635,0.0041243588,-0.053451434,-0.030575614,-0.01724839,-0.045353793,-0.04806215,0.07755484,-0.04922696,-0.011263987,0.0022806898,-0.026812993,-0.043252703,-0.02264265,0.015643196,-0.0014144445,-0.027987367,-0.012287778,0.047026385,0.012150867,-0.0017717906,-0.04585015,0.07194511,-0.002448847,-0.025382947,-0.03227397,-0.0547823,-0.006648864,0.009570795,0.0035834336,0.010527706,0.054630592,-0.016121058,-0.025747208,0.05543117,0.056155045,-0.03288627,-0.020584727,0.023851283,0.02760179,0.013906537,-0.016987262,-0.029049367,-0.0991543,0.01843828,-0.009071261,-0.038385957,0.025758177,-0.047957562,-0.04285158,-0.017173022,0.0018643234,-0.017854579,-0.0074342843,-0.022762502,-0.01345751,0.00963245,0.02323746,-0.002905362,-0.04797095,-0.0022493515,-0.002049614,0.005982077,-0.01971044,0.021617493,0.010049923,-0.021792844,-0.01150045,0.009495509,0.027315024,0.049037695,-0.0346862,0.023246178,0.03206331,0.023003919,-0.0014589265,-0.0074076043,0.04033032,0.04096498,0.000353431,-0.010984583,-0.019688658,-7.238998e-05,-0.018545702,-0.010756907,-0.031481728,-0.043879725,0.04591073,0.06692985,0.0150915,0.0021859275,0.022665532,-0.041468956,0.00910951,-0.030877165,0.0057043694,0.088103905,-0.0038638334,0.0036764974,-0.09238636,0.02174255,-0.03961112,0.06359448,-0.0064402036,0.05568134,-0.038236376,-0.020249091]	Keywords: product-key memory, FFN, key-value pairs, attention mechanism, large-scale language modeling\nKey Objects: FFN, Product-Key Memory, Query Network, Key Selection Module, Value Lookup Table\nRefers to Images: None\nHypothetical Questions:\n- How do product-key memory layers mimic the attention mechanism?\n- What is the purpose of the query network, key selection module, and value lookup table in product-key memory layers?\n- Why would replacing FFN layers with product-key memory layers improve performance in large-scale language modeling?\n---\nSummary:\nLample et al. [69] explored replacing some Feed-Forward Network (FFN) layers with product-key memory layers to enhance model capacity. These memory layers utilize a query network, key selection module, and value lookup table, mimicking an attention mechanism and demonstrating performance improvements with minimal computational overhead in large-scale language modeling.\nOriginal Text:\nLample et al. [69] replace some of the FFNs with the product-key memory layers. A product-key memory is composed of three components: a query network, a key selection module containing two sets of sub-keys, and a value lookup table. The model first projects an input to a latent space using the query network, and then compares the generated query to keys that are Cartesian product of the two sets of sub-keys from key selection module to get k nearest neighbors, and finally finds the corresponding values in a value lookup table using the k nearest keys and aggregates them to produce the final output. This process resembles the attention mechanism, in that the generated query attends to a large number of global key-value pairs. They thus propose a multideck mechanism for the key-product memory to further enlarge the capacity of this module. The experiments on large-scale language modeling suggest that this mechanism significantly improves performance with negligible computational overhead.\nContextualized Text:\nTo increase model capacity, Lample et al. [69] investigated replacing some Feed-Forward Network (FFN) layers with product-key memory layers. These layers, composed of a query network, key selection module, and value lookup table, function similarly to an attention mechanism, allowing the model to attend to a large number of global key-value pairs. Experiments showed this approach significantly improved performance in large-scale language modeling with only a negligible increase in computational cost.	{"tags": ["architecture", "NLP", "Transformer", "memory", "attention"], "doc_id": "df8e6b6d-65be-493e-8e9a-8093a12f3b18", "summary": "Lample et al. [69] explored replacing some Feed-Forward Network (FFN) layers with product-key memory layers to enhance model capacity. These memory layers utilize a query network, key selection module, and value lookup table, mimicking an attention mechanism and demonstrating performance improvements with minimal computational overhead in large-scale language modeling.", "doc_type": "text", "entities": ["Lample et al."], "keywords": ["product-key memory", "FFN", "key-value pairs", "attention mechanism", "large-scale language modeling"], "key_objects": ["FFN", "Product-Key Memory", "Query Network", "Key Selection Module", "Value Lookup Table"], "contextual_text": "To increase model capacity, Lample et al. [69] investigated replacing some Feed-Forward Network (FFN) layers with product-key memory layers. These layers, composed of a query network, key selection module, and value lookup table, function similarly to an attention mechanism, allowing the model to attend to a large number of global key-value pairs. Experiments showed this approach significantly improved performance in large-scale language modeling with only a negligible increase in computational cost.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.3 Position-wise FFN"}, "hypothetical_questions": ["How do product-key memory layers mimic the attention mechanism?", "What is the purpose of the query network, key selection module, and value lookup table in product-key memory layers?", "Why would replacing FFN layers with product-key memory layers improve performance in large-scale language modeling?"]}
77840b45-4cf9-4c53-a1c7-c98ed8b00aba	abe8c200-bfa1-4355-947e-23ea618c310d	[0.00048893335,0.010396827,0.041382805,0.07483032,-0.011587115,0.07242737,0.028751984,0.038629927,-0.013188451,-0.04554998,-0.0048800856,0.033897754,0.049857415,0.010939792,-0.0365969,0.013515997,0.019627633,0.08487195,0.013052716,-0.037588634,0.049127225,0.009600111,0.08454754,0.030094694,0.024958149,0.026336705,0.03628713,-0.04425262,0.018740905,-0.011999337,0.028208395,-0.056960393,0.09179182,0.013469737,0.061391726,0.025683649,0.06351918,-0.05144544,-0.01573116,0.0075476184,-0.021116305,0.042824276,-0.02511582,-0.0017027586,-0.054737587,-0.0065270243,-0.06172773,-0.032183334,0.024811625,-0.0027500065,0.00094377185,0.073982835,0.005591826,-0.014420981,-0.022376556,0.010870753,-0.008994835,-0.027523104,-0.020781148,-0.094088934,-0.031158235,0.011563517,0.0041441964,-0.018804496,-0.0022887643,-0.023208072,-0.023715787,0.03850564,0.04658805,0.12779428,-0.011571039,0.0018087145,0.012933462,-0.09322312,0.120861515,0.059529733,-0.062467754,-0.024544563,-0.029545268,-0.027301634,-0.032529686,0.05378819,-0.013683351,-0.031869013,0.10342903,-0.01175063,-0.01944483,-0.02338896,0.022698136,-0.03152247,0.025562333,0.044137765,-0.0074953577,0.07122057,0.0018227624,-0.055100705,0.013481063,-0.025547722,0.008007556,-0.035151,-0.010546558,0.0343835,0.05779632,0.1264838,0.03316087,-0.01869966,-0.0004452626,0.004312322,0.0018416051,0.027851865,-0.0011135602,0.020095592,-0.07595194,-0.007024132,-0.007448259,0.0020390912,-0.050020453,0.004010375,-0.0018827053,0.024545383,0.025185663,0.003039377,0.009117325,0.027395641,0.01716597,0.020518279,0.037083767,-0.053113658,-0.018476738,-0.013869929,0.07276515,0.0041338266,-0.02124973,0.015587667,-0.037703406,-0.018745288,0.048035875,-0.048752863,0.005609136,0.051335506,-0.030038152,-0.032312103,-0.04609801,0.00076907314,-0.014439996,0.042332813,-0.05936501,0.0028734633,-0.0055298316,0.023949573,0.062702775,-0.009125445,0.0463898,-0.020806303,-0.054016985,-0.052660737,0.003303858,-0.10994873,0.031167438,-0.019497948,-0.076145515,-0.052776363,-0.043706976,0.0655571,0.058506258,0.080239795,0.030236835,-0.007256146,0.011622458,0.060857724,-0.03146351,-0.016782898,-0.032222044,-0.05878808,0.00452938,-0.002018614,0.022387136,-0.0076984516,-0.02055416,-0.009876979,-0.010105724,-0.014756483,-0.001276446,0.032124452,-0.03842339,0.03113614,0.007846416,0.04720576,-0.049114864,-0.055211447,0.020670079,-0.05754321,-0.0094005065,0.03592367,0.0235639,0.062354613,0.051397424,-0.062471762,-0.031659342,-0.03551752,-0.016013408,0.015642967,0.033178862,-0.0072437916,0.003342081,0.016152458,0.0015013308,-0.004150626,-0.043254014,0.044000354,-0.013247179,-0.029351767,-0.019682977,0.07076048,0.06122526,-0.027419755,-0.054633234,0.01483862,0.041113522,0.010686066,-0.020328332,0.010158135,-0.01923155,-0.023783458,0.021150818,0.031447604,-0.0513377,0.019874014,-0.01996479,0.022442736,0.05270241,0.010606783,-0.044012286,0.05477156,-0.034934837,0.024377415,0.016707845,0.04850056,0.003777448,0.07394849,0.045595374,0.08854098,-0.03639756,0.053617336,-0.009843959,-0.07419413,0.022716694,0.0527885,0.032971624,0.00859586,-0.024857605,0.043890383,0.019218126,-0.010682318,-0.0017069727,0.010569456,0.017832356,-0.0018882895,-0.015665308,-0.0054952186,0.024315514,-0.023352457,0.03278206,-0.0076551265,0.027899005,-0.026108129,0.009935904,-0.006913371,-0.020916684,0.030988475,0.023930864,0.011428912,-0.009021218,-0.033240605,-0.03146205,0.008876128,-0.004710164,0.024585372,-0.005810547,-0.0523998,-0.009524026,0.01495239,-0.02072318,0.005898839,-0.0060764407,0.014865633,0.028099705,0.014200622,-0.044934016,0.0012215656,0.004832945,0.0036190345,0.040602285,-0.0008350027,-0.01989642,-0.016247023,-0.015699312,-0.035619043,0.0011485183,0.033150762,0.10977972,-0.027824279,-0.004436321,0.005402149,-0.02070247,0.0030104092,-0.058529492,0.046266206,0.049357373,0.005440927,0.06949547,0.015562627,-0.0012713816,-0.010745515,-0.0011258228,0.058989424,0.066868536,-0.047078103,-0.0028214457,0.044402055,0.0037896035,-0.021064058,-0.05575224,0.039165724,0.0245595,0.024098018,-0.046855778,-0.033193488,-0.05097919,0.03205864,-0.054023266,0.019387487,0.06651754,0.0059084753,-0.042598367,-0.00539693,-0.014108167,0.0023829883,-0.0050049284,0.012378446,-0.005080285,-0.05477907,-0.046072315,-0.031825624,-0.012422379,0.025308102,-0.011887536,0.016789326,-0.03602442,0.005101429,-3.1176813e-05,0.028837886,0.03223886,0.010243482,0.031477675,-0.02218748,-0.003556411,0.0038549385,0.004068979,-0.029331049,0.06135104,-0.045821413,0.027939187,-0.030250775,0.011194015,-0.0061857407,-0.027919602,0.007677949,0.045628887,-0.0037563187,0.008529418,0.019397467,0.033812426,-0.014857564,0.022175476,0.0262085,0.048150335,0.031771302,-0.027422855,0.0109902015,-0.00844836,0.037068777,-0.046818383,0.011259992,-0.005324255,-0.008720922,0.041173838,0.07033415,0.007933267,0.046637036,-0.028836986,0.022890138,0.037182447,-0.027031288,-0.0056732614,-0.011937721,0.021059198,0.045580477,0.024612602,0.007565011,0.03781826,0.024267955,-0.058436163,0.029098848,0.041553758,0.071811266,-0.11744197,0.0630857,0.018606152,0.026016196,0.02693813,-0.04238928,-0.025609715,0.01820923,-0.0062993793,-0.0108221015,-0.011230385,0.044773135,-0.032953516,-0.010178607,-0.027379593,0.004163273,-0.013001266,0.03096716,0.045653176,0.01905112,-0.041829586,0.022853244,0.029424263,-0.047488842,0.0007771516,-0.05829524,-0.005714615,0.049102683,0.040060233,-0.013323904,-0.045943324,0.07484352,-0.019627692,0.035507157,0.055133574,-0.009417691,0.043489147,0.020375744,-0.017483443,0.012982924,0.0030327353,0.024380665,-0.020952981,-0.015506142,0.01173931,0.022414733,-0.020181445,0.002617409,0.08438068,-0.05154625,0.022515882,-0.02117774,0.0030140122,0.025722587,0.0104463305,0.06437981,0.01831503,0.013141572,0.007815748,-0.0025516513,-0.039683376,-0.007340034,-0.01240067,-0.023343623,0.025673417,0.041188426,-0.045425024,0.02656959,0.00697397,-0.0054182415,0.00041782914,-0.0060806596,0.039572604,-0.009893899,-0.041570146,0.0137385875,-0.017896807,0.035071503,0.0101283975,-0.019445375,0.008126373,-0.014539347,-0.03741653,0.059587654,0.014299427,0.08204076,0.03653851,0.033513024,0.008473028,0.012082273,0.012360381,0.04451167,0.029052647,0.02075331,-0.044850152,-0.015030699,0.0026798232,0.040949438,-0.042401273,-0.0018648388,-0.016369535,0.030347819,-0.028540583,0.04975467,0.014456572,-0.057135828,0.011321222,-0.01667711,-0.019573133,0.00038605568,0.0035809362,0.022730978,-0.016152818,0.08598212,0.04668083,0.010003934,0.028518915,0.024014052,-0.0049345815,-0.019557213,-0.02981408,0.008816428,0.027875805,-0.0012882308,0.014908853,-0.036139976,-0.024465991,0.029183824,-0.066083625,-0.030015234,0.015878318,-0.025718924,-0.048600003,-0.0048604533,0.052507326,-0.0311436,0.026385361,-0.043182064,0.04936606,-0.070324644,0.0139095625,0.032609995,0.032005038,0.065454826,0.04010514,0.029654978,0.00859394,-0.018232835,0.0036181307,0.0031149902,0.05836238,-0.0015535378,-0.039010607,0.046501115,0.010658992,0.022374302,-0.0017649506,-0.052573156,-0.026742537,-0.016217059,0.019979399,-0.0311237,-0.0045749303,0.009156093,-0.049009945,0.01332442,-0.054863803,0.02779807,-0.035929754,0.04471303,0.00394509,-0.021724103,0.03248165,-0.011947759,0.025291966,0.044073842,0.048507806,0.0038207446,-0.035214104,0.006013634,-0.02821337,0.0050053215,0.03324655,-0.00046436748,-0.04861259,-0.07128233,-0.02157541,0.010917656,-0.016627248,0.008003113,0.06304999,0.029389482,-0.0026474523,-0.023242882,-0.04268578,-0.028328415,-0.023069995,-0.00937253,0.003290946,-0.003151771,0.018350063,0.047678754,-0.026223375,0.07663085,-0.07382065,0.0148657365,-0.043196436,0.04786922,-0.035749312,-0.07026134,0.016729865,-0.031319637,-0.011134776,-0.028127562,0.07073441,0.052528977,-0.043431886,0.020409027,-0.02436897,0.011324531,-0.0394104,-0.040226292,-0.005930663,0.007486194,0.036544535,0.0060374923,0.07631514,-0.006921787,-0.008847218,-0.031272862,-0.032002073,-0.08909816,-0.015375668,0.00630396,0.005388186,0.010493132,-0.002170401,0.05071689,0.045626853,-0.049068112,-0.042598058,-0.09108514,-0.0004995561,0.040894687,0.020678205,0.019559907,-0.012011249,-0.050499845,-0.009289177,0.016467327,-0.0014613016,-0.0034809266,0.018759454,0.02723359,0.052474976,0.025538303,-0.049896248,-0.022880709,-0.022905272,-0.04745681,-0.0068296418,-0.03436602,0.052769057,0.004881969,-0.041556984,-0.045096915,0.035553794,0.014480735,-0.01833754,0.011303174,-0.042264465,-0.038145147,-0.039441235,-0.0002282584,-0.010224069,0.01772684,0.007319751,0.067961395,0.016005445,-0.10001301,-0.039582606,-0.040007763,-0.022309195,0.033625267,0.035939034,0.03385407,0.034618393,-0.003549255,0.025613377,0.07549033,0.041019157,0.008100719,-0.019267676,0.038674906,0.021628743,0.026005369,-0.019230634,-0.048429728,-0.08737509,0.01456505,-0.011978061,-0.057049625,0.012049991,-0.04598406,-0.031849556,-0.038581062,-0.033015937,0.008306027,0.019092875,0.011296735,0.01052491,-0.008653007,-0.010635306,0.049819,-0.020923011,-0.019737486,-0.030398963,-0.039719272,0.033351287,-0.0138013,-0.006745496,-0.0057664234,-0.024931364,0.01168825,0.019970017,0.019944275,-0.02748432,0.011064304,-0.020632694,0.031133901,0.0077016572,0.030351644,0.087007165,-0.040624708,0.024981365,0.020739397,0.047758725,0.025770314,0.009359038,-0.036856424,-0.036478147,-0.045868397,-0.018319948,0.010906329,0.029865485,0.036778066,-0.018271621,-0.03742604,-0.06545742,0.025324289,0.0039520604,0.08108866,-0.0005091954,-0.010135933,-0.07126018,0.0007858004,-0.023143511,0.034563728,0.011922231,0.009535813,-0.006795625,-0.0014140563]	Keywords: Mixture-of-Experts, FFN, MoE, routing function, experts\nKey Objects: FFN, MoE Layers, Experts, Routing Function\nRefers to Images: None\nHypothetical Questions:\n- How does the routing function determine which experts are activated?\n- What is the purpose of the auxiliary loss function used in conjunction with MoE layers?\n- What are the advantages and disadvantages of using MoE layers compared to standard FFNs in Transformers?\n---\nSummary:\nTo increase the capacity of Feed-Forward Networks (FFNs) in Transformers, researchers have explored using Mixture-of-Experts (MoE) layers, where multiple FFNs (experts) are used and a routing function determines which experts are activated for each token.\nOriginal Text:\nSeveral studies exploits the idea of Mixture-of-Experts (MoE)[120] to increase the capacity of FFNs. Gshard[71] uses sparsely-gated MoE layers to replace FFNs in Transformer. Each MoE layer consists of several FFNs (each called an expert) that are the same structure as position-wise FFNs in vanilla Transformer. The output of the layer is a weighted sum of the outputs of the FFNs, using gate values computed by a routing function g (  ) . They design a learnable routing function that assigns tokens to experts, with auxiliary loss to satisfy balanced loads between experts and efficiency at the scale of length such that the experts can be distributed across multiple devices. For each forward pass of the MoE layer, only the experts with top-k gate values are activated.\nContextualized Text:\nResearchers have explored using Mixture-of-Experts (MoE) layers to increase the capacity of Feed-Forward Networks (FFNs) within a Transformer. In this approach, an MoE layer replaces the standard FFN and consists of several FFNs, referred to as 'experts'. A routing function then assigns tokens to these experts, and for each forward pass, only the experts with the top-k gate values are activated, allowing for increased model capacity.	{"tags": ["MoE", "architecture", "transformer", "scaling"], "doc_id": "77840b45-4cf9-4c53-a1c7-c98ed8b00aba", "summary": "To increase the capacity of Feed-Forward Networks (FFNs) in Transformers, researchers have explored using Mixture-of-Experts (MoE) layers, where multiple FFNs (experts) are used and a routing function determines which experts are activated for each token.", "doc_type": "text", "entities": ["Gshard", "Transformer"], "keywords": ["Mixture-of-Experts", "FFN", "MoE", "routing function", "experts"], "key_objects": ["FFN", "MoE Layers", "Experts", "Routing Function"], "contextual_text": "Researchers have explored using Mixture-of-Experts (MoE) layers to increase the capacity of Feed-Forward Networks (FFNs) within a Transformer. In this approach, an MoE layer replaces the standard FFN and consists of several FFNs, referred to as 'experts'. A routing function then assigns tokens to these experts, and for each forward pass, only the experts with the top-k gate values are activated, allowing for increased model capacity.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.3 Position-wise FFN"}, "hypothetical_questions": ["How does the routing function determine which experts are activated?", "What is the purpose of the auxiliary loss function used in conjunction with MoE layers?", "What are the advantages and disadvantages of using MoE layers compared to standard FFNs in Transformers?"]}
7fea08c4-5932-4a1b-bd5b-6710f1ea358e	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0016098899,0.014877201,0.0007522286,0.071474105,-0.018274548,0.088430665,0.028302003,0.03016713,0.00573783,-0.026807353,-0.024736753,0.032732345,0.018604679,0.024597889,-0.050990578,0.053933095,0.017742306,0.0935178,0.021429092,-0.03311038,0.022214266,0.015830943,0.050019994,0.02072912,0.010005953,0.024617692,0.027772713,-0.01517047,0.020254875,-0.036543153,0.036380127,-0.030068787,0.077982575,0.0024423776,0.056536295,-0.012242932,0.051104642,-0.044087876,-0.03672814,-0.027258227,-0.006459981,0.060160607,-0.025212433,-0.015847879,-0.044074137,0.008261507,-0.06917322,-0.062406633,0.03289863,-0.013321645,-0.012626344,0.06702646,-0.010300689,-0.0064505525,-0.034079634,-0.0013857026,-0.008522772,-0.024343576,-0.0041710855,-0.08503405,-0.04747053,0.016170006,0.020245558,0.004547666,0.00085139007,-0.013973459,-0.03085102,0.023003995,0.03262798,0.1293516,-0.0023079487,0.011030811,-0.011029446,-0.090606,0.10100268,0.062909916,-0.07648194,-0.009053578,-0.056145947,-0.008897042,0.012361442,0.05651277,-0.01937465,-0.052317355,0.06837397,-0.012360248,-0.048609037,0.0005519328,0.048905652,-0.043916963,-0.01054604,0.021903703,-0.010812754,0.06070776,-0.010115443,-0.061790045,0.014413325,-0.033170406,0.015202714,-0.046602473,0.0057133427,0.0069107525,0.040820397,0.13061403,0.018615173,-0.0069589606,-0.009343986,-0.005392988,-0.0041375654,0.00982974,0.0028303775,0.026243893,-0.056223646,-0.016687527,-0.038002778,-0.02176548,-0.04196609,0.0028752508,0.0029352796,0.0027212356,0.021669008,-0.020904504,0.011920026,0.032215938,0.027302934,0.04059896,0.047361337,-0.03158958,-0.026590673,0.005723497,0.06770363,0.013493319,-0.05273969,0.022704283,-0.045176424,-0.008112084,0.04024937,-0.04138566,0.00076458376,0.026636247,-0.03651484,-0.028244654,-0.04553706,0.015698085,-0.0064673927,0.020834692,-0.047394197,0.028265435,0.0020167122,0.05684211,0.054089256,-0.0152022885,0.04595351,-0.007998992,-0.023862118,-0.029203188,0.0006632811,-0.08551416,0.02149928,-0.003364899,-0.06349839,-0.06066425,-0.015615055,0.054677736,0.08882079,0.0849339,0.0056084804,0.025687952,0.01117912,0.021615421,-0.03351332,0.01036163,0.001994167,-0.017513998,0.0044708913,-0.031951897,0.02449803,-0.031207118,-0.034212906,-0.024675192,-0.020617805,0.012500263,-0.0024745064,-0.007666362,-0.021353344,0.028498963,0.007833895,0.062323317,-0.045088243,-0.06402516,0.046244703,-0.039512593,-0.00541111,0.04089616,-0.00067563815,0.09498422,0.03453693,-0.077600226,-0.021709664,-0.034257557,0.0066618696,0.020726977,-0.007814347,-0.0038637698,-0.001078873,0.013333048,0.017267212,-0.041450355,-0.02032995,0.0077610966,-0.0104881385,-0.012226212,-0.033323452,0.11351314,0.038080685,-0.0072081164,-0.02951008,0.0048514483,0.067934155,-0.012875504,-0.035089314,-0.014559683,-0.01605783,-5.703096e-05,-7.192249e-05,-0.008834827,-0.03236283,0.023968384,-0.024604866,1.4929426e-05,0.046323862,-0.0068223444,-0.051716376,0.044693526,-0.013200735,0.045651853,0.005583494,0.028010326,-0.016749974,0.05387728,0.065189466,0.073682696,-0.00051488774,0.054639652,-0.0183453,-0.025328072,0.014243105,0.048847694,0.049736116,0.024447614,-0.01101613,0.010028714,0.01591798,-0.041909605,0.011421507,-0.022240987,0.042012967,-0.007184106,0.0028394691,-0.008590827,0.03782436,-0.008169977,0.027539896,0.01992686,-0.0059414706,-0.018607443,0.0043192017,0.010205541,-0.022481244,0.017715098,0.039180376,0.029744275,0.0047236946,-0.02776068,-0.05127523,0.031603288,-0.008132541,0.014806313,0.00055322057,-0.047527965,-0.015493263,0.020558678,0.012863666,-0.01945418,-0.00074175274,0.000366713,0.036320455,0.0017530287,-0.051506285,-0.0014134092,-0.009722393,-0.01901107,0.01905933,0.0038304087,-0.019959489,-0.03000068,-0.007878744,-0.075825885,0.00813017,0.03850179,0.11549551,-0.02353179,0.025331054,0.01733966,0.0020774934,0.011787047,-0.046007365,0.048307393,0.03316376,0.0031903596,0.041560724,0.02199182,-0.007845753,0.0048994888,0.012182604,0.08830006,0.086228356,-0.036010962,0.015091163,0.035389584,0.020880409,-0.0075409953,-0.04508575,0.051453125,0.05004495,0.020742603,-0.0303224,-0.03176917,-0.059960872,0.06407137,-0.05492944,0.03296859,0.03606849,-0.008751665,-0.020630049,-0.016116839,-0.012930778,0.026638305,0.02935467,0.037863083,-0.010679704,-0.060111202,-0.011611709,-0.01320595,-0.039233916,0.039825454,-0.02007587,0.0069207726,-0.0068699294,0.031739816,0.0038291735,0.012329715,0.05635111,0.015125487,0.045065872,-0.036477618,0.0018747988,0.0081075,0.011391298,-0.006402115,0.04968769,-0.065767184,0.033920493,-0.015222111,0.019199463,0.014507782,-0.03776925,-0.003147986,0.05014671,-0.0071289404,-0.009229527,-0.0012063863,0.03396447,-0.03037593,0.020936305,0.029773155,0.041418277,0.060469367,-0.008142205,-0.0072367834,-0.008703943,0.035919193,-0.046168428,-0.0027714535,0.0074781496,0.009463311,0.03963271,0.067096554,0.0068425788,0.028956113,-0.01643008,0.02648288,0.045576584,-0.034962233,-0.025462067,-0.027623003,0.017916638,0.060020138,0.018434577,0.026832988,0.056811832,0.021951405,-0.06352767,-4.0604587e-05,0.012518881,0.05194732,-0.110738106,0.033726133,0.032814894,0.009741913,0.012477019,-0.0509785,-0.02287504,0.005869577,0.0071191792,-0.021705667,-0.016229061,0.020178072,-0.013320714,-0.002333015,-0.00030753753,0.027236832,0.021643393,0.03181213,0.03605331,0.012982366,-0.024962965,0.03204842,0.00036837417,-0.056404915,-0.0054949587,-0.044362802,-0.018402847,0.07457686,0.021332763,-0.0037863832,-0.057811633,0.063458785,-0.03568472,0.020941423,0.05088461,-0.008228357,0.024967365,0.0060861777,-0.015909234,0.009856678,-0.018384961,0.02129858,-0.052270044,0.0057484536,0.006214186,0.014144958,0.0044000424,-0.002329344,0.053368162,-0.042904746,0.0054371106,-0.0061054965,-0.021686532,0.012922824,0.024396889,0.080965236,0.002219953,0.025776546,0.030403793,-0.004596725,-0.030768735,-0.03807947,0.0072112316,-0.008204391,0.022985961,0.04111427,-0.041017924,0.054121826,-0.0032988742,-0.018655926,-0.008148405,0.0042878967,0.027784884,-0.011460958,-0.046658233,-0.0049449117,-0.016813975,0.038031287,-0.0008986417,-0.040333796,-0.021664819,-0.030653741,-0.040614773,0.0405058,0.0077177435,0.07018901,0.005937629,0.021792453,-0.0007834926,-0.027750008,0.009550738,0.020357523,0.016801907,0.01155293,-0.011671395,-0.021337854,0.05969409,0.041648638,-0.01661005,-0.010734574,0.0038174645,0.03424962,-0.030299079,0.0665785,-0.002859452,-0.033257242,0.022263996,-0.028592547,-0.04387312,0.027521433,-0.0011475115,0.03623965,-0.03684485,0.048588447,0.014278608,-0.011832903,-0.0026415335,0.029960789,0.020273326,0.0067627537,0.0071129883,0.011635036,0.036753476,-0.042881794,-0.0012408108,-0.047110405,-0.01564202,0.022371247,-0.09258941,-0.024137951,0.0037296202,-0.027953476,-0.010586901,-0.0067085912,0.035815127,-0.054651182,-0.0067650517,-0.037037943,0.035802606,-0.073928356,0.0021693325,0.034485333,0.016657835,0.06705465,0.033367354,0.00083464547,0.00062085706,-0.011451109,-0.0010063628,0.024587715,0.07237156,-0.018842071,-0.025203066,0.059768792,0.024559202,0.022594443,-0.038812127,-0.083608046,-0.011139167,-0.029360693,0.018006884,-0.048129402,-0.021222303,0.035014138,-0.01953621,0.018198488,-0.023876734,-0.011310553,-0.035160493,0.0876991,0.02353858,0.0058256616,0.04943831,-0.0040628123,0.04606958,0.0353263,0.05082234,0.008326903,-0.020957004,-0.012242297,-0.02127049,-0.0048877406,0.040939666,0.0015948077,-0.030858625,-0.04516942,-0.014206019,0.01670113,-0.026191568,-0.023952147,0.07508027,0.037725672,0.036702313,-0.031699784,-0.061654944,-0.020971037,-0.019662542,-0.010331535,-0.036557127,-0.0075883525,0.024391564,0.04412625,-0.05134275,0.04751234,-0.06724762,-0.013032187,-0.050962653,0.037280362,-0.025277136,-0.052928295,-0.0027576392,-0.014994283,-0.030670721,-0.03890383,0.062330786,0.047213834,-0.042918164,0.009703154,-0.0007091557,0.020700607,-0.025773726,-0.05577044,-0.040029593,0.012298768,0.03155045,-0.012419876,0.078686215,-0.044417642,-0.02575009,-0.0062105767,0.008493457,-0.076478064,-0.0008721224,0.0004093472,0.035254076,0.02735392,-0.0006387699,0.061075795,0.024179567,-0.05687109,-0.03336278,-0.07158319,-0.0051432806,0.035459403,0.014584827,0.04483131,-0.006885843,-0.05822536,-0.012447147,0.011377735,-0.007003285,0.014617063,0.032034874,0.060488574,0.075807326,0.045201927,-0.037518084,-0.045983728,-0.024569476,-0.019412797,-0.022200301,-0.046017185,0.034043424,-0.018522577,-0.01570984,-0.029172119,0.020184442,-0.0054154983,-0.010603384,-0.03131467,-0.060504515,-0.040002782,0.0023634802,-0.0016321442,0.01184358,0.0072051636,0.0019221089,0.09725112,0.0248924,-0.057193305,-0.0468497,-0.066385865,-0.029584436,0.041235227,0.040005993,0.0120283505,0.01696701,-0.015395711,0.011614543,0.06965242,0.038570087,0.0119098835,-0.026889265,0.037780613,0.008250685,0.033745334,-0.041473642,-0.03719763,-0.089318134,0.037102774,-0.02892331,-0.07154086,0.0029393088,-0.018536754,-0.0021284074,-0.044393502,-0.03915668,-0.001515211,-0.0064545027,0.018537883,0.008218741,-0.0149357775,-0.05131856,0.022742527,-0.03409366,-0.016439296,-0.029196048,-0.018941518,0.036771473,-0.012440382,0.008964812,-0.013728229,-0.018860847,0.023563666,0.025601903,0.010996035,-0.021672549,0.012475619,-0.012228601,0.034821063,0.009951283,0.04656131,0.087727524,-0.051911235,0.026485069,0.009674826,0.05214345,0.003270159,-0.0028100235,-0.03318515,-0.059303883,-0.047040734,-0.012803257,0.02436371,0.01839024,0.032462668,-0.009188109,-0.06491466,-0.06646014,0.01499227,-0.026183525,0.050541006,0.043309826,0.014415059,-0.067196555,0.0014929327,-0.038278587,0.08024242,-0.017566778,0.0153221125,-0.0108971195,0.00038323583]	Keywords: Mixture-of-Experts, routing, expert prototyping, Switch Transformer, MoE\nKey Objects: experts, routing function, MoE layers\nRefers to Images: None\nHypothetical Questions:\n- How does routing tokens to a single expert in Switch Transformer improve pre-training speed?\n- What is the purpose of the auxiliary loss function in Switch Transformer and Yang et al.'s expert prototyping strategy?\n- Why is it beneficial to divide experts into groups when applying routing?\n---\nSummary:\nSwitch Transformer optimizes Mixture-of-Experts (MoE) layers by routing tokens to a single expert with the highest gate value, which significantly speeds up pre-training while maintaining a similar computational cost. Subsequently, Yang et al. propose expert prototyping, splitting experts into groups and applying top-1 routing within each, improving model quality with consistent costs.\nOriginal Text:\nInstead of using k experts for each forward pass, Switch Transformer [36] proposes to route using only a single expert with the largest gate value, leading to a much smaller computational footprint. The authors also design an auxiliary loss to encourage load balance between experts. It is reported to speed up pre-training by a large margin compared to the non-MoE counterpart while having a similar number of FLOPS.  \nYang et al. [155] propose to replace top-k routing with expert prototyping strategy. Specifically, the proposed strategy splits experts into k different groups and applies top-1 routing within each group. The outputs of prototype groups are combined linearly to form the final output of the MoE layer. This strategy is proved to improve the model quality while maintaining constant computational costs.\nContextualized Text:\nTo increase the capacity of Feed-Forward Networks (FFNs), several works explore Mixture-of-Experts (MoE) layers. Switch Transformer optimizes this approach by routing each token to the single expert with the largest gate value, which dramatically improves pre-training speed.  Yang et al. then introduce expert prototyping, dividing experts into groups and applying top-1 routing within each to enhance model quality while maintaining consistent computational costs.	{"tags": ["architecture", "NLP", "transformer", "MoE"], "doc_id": "7fea08c4-5932-4a1b-bd5b-6710f1ea358e", "summary": "Switch Transformer optimizes Mixture-of-Experts (MoE) layers by routing tokens to a single expert with the highest gate value, which significantly speeds up pre-training while maintaining a similar computational cost. Subsequently, Yang et al. propose expert prototyping, splitting experts into groups and applying top-1 routing within each, improving model quality with consistent costs.", "doc_type": "text", "entities": ["Switch Transformer", "MoE"], "keywords": ["Mixture-of-Experts", "routing", "expert prototyping", "Switch Transformer", "MoE"], "key_objects": ["experts", "routing function", "MoE layers"], "contextual_text": "To increase the capacity of Feed-Forward Networks (FFNs), several works explore Mixture-of-Experts (MoE) layers. Switch Transformer optimizes this approach by routing each token to the single expert with the largest gate value, which dramatically improves pre-training speed.  Yang et al. then introduce expert prototyping, dividing experts into groups and applying top-1 routing within each to enhance model quality while maintaining consistent computational costs.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.3 Position-wise FFN"}, "hypothetical_questions": ["How does routing tokens to a single expert in Switch Transformer improve pre-training speed?", "What is the purpose of the auxiliary loss function in Switch Transformer and Yang et al.'s expert prototyping strategy?", "Why is it beneficial to divide experts into groups when applying routing?"]}
c8b5b551-5ad5-4ad4-8814-f5f44a412348	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.020896338,0.010072875,-0.01102358,0.03688507,-0.019058825,0.04093505,0.0033120145,0.035240687,0.0034004955,-0.03281743,-0.017172467,-0.014864686,0.0019093985,-0.00040983065,-0.039214313,-0.016600003,0.050885715,0.09248626,-0.005311134,-0.057928454,0.02216998,0.021603351,0.031022731,0.0090392325,0.029103678,0.05163103,0.05336159,-0.049177695,-0.026438247,-0.04779436,-0.0013475593,-0.03539376,0.04022051,-0.0041453172,0.07317091,-0.03974922,0.036534753,-0.030642573,0.01806955,0.00021390221,-0.039117966,0.02175078,-0.008289435,-0.024582395,-0.04262241,-0.039192483,-0.062961735,-0.03654997,0.028712625,-0.009580495,0.021579854,0.06700969,-0.050496303,-0.018233016,-0.03328077,-0.04259068,0.004700061,-0.04264384,0.014899038,-0.09529907,-0.04978699,0.055642955,-0.023181362,-0.028825203,0.01523845,-0.0025261973,-0.022183804,0.00417111,0.0749198,0.13435456,-0.013679832,-0.027824866,-0.01614696,-0.06695712,0.13289626,0.044105317,-0.03544271,-0.018976824,-0.05771942,0.024878407,0.036618613,0.07071941,-0.009694821,-0.003334353,0.082230516,0.014688455,-0.04279772,0.010781225,0.041110214,-0.032834556,0.012221858,0.017674757,-0.008895627,0.06919781,-0.021871706,-0.04964927,-0.012004144,-0.014738612,0.019094696,-0.057266135,0.0043210723,0.022486858,0.031313013,0.103771485,0.031988654,-0.029592063,-0.014504993,-0.012906124,-0.0034299833,0.05808348,0.008502438,-0.013546801,-0.024136895,0.039465122,-0.050349146,-0.0012877724,-0.028295834,-0.025111828,-0.008960971,0.011613736,0.034215655,0.023021711,0.008633497,0.036257174,0.0023754924,0.04764139,0.034826685,-0.047108065,0.011574845,0.008651196,0.04385161,0.0066778213,-0.053015985,0.0056276177,-0.018968917,-0.015615681,0.03742039,-0.032843705,-0.015629675,0.0057549137,-0.012789132,-0.022970833,-0.021614106,0.038383354,-0.008317627,-0.003579575,-0.062622756,0.009005852,-0.011903916,0.074349925,0.038219742,-0.008048223,0.030208997,-0.021395983,-0.013909629,-0.049333192,0.0062736263,-0.045140214,-0.020800103,-0.07040485,-0.045766026,-0.0828442,-0.0237059,0.055792492,0.054344755,0.087237,-0.02847145,-0.0013897081,-0.0013535083,0.01238016,-0.0054518166,-0.016335197,-0.02394176,-0.031360038,-0.001270366,0.009087189,0.019060872,-0.047708098,0.0049450696,0.024265101,-0.04602276,0.032580543,0.0053486703,0.034294195,-0.009278705,0.041714597,0.02043934,0.045311935,-0.017003747,-0.045320027,0.03477725,-0.051172573,-0.022113401,0.02159434,0.0030951553,0.0555566,0.0474672,-0.031568397,-0.04426007,0.002356034,-0.006268124,0.017227778,0.021391526,-0.023877371,0.010985189,-0.028018454,0.0048158877,-0.012607989,-0.03611074,0.021253442,-0.007084474,-0.038793497,-0.0061739436,0.046336483,0.023260454,0.04647712,-0.03392183,0.0037401435,0.047102273,-0.014906247,-0.041054294,-0.017560447,-0.061241467,0.01441461,-0.015802106,0.016332718,-0.015064648,0.058832046,0.026485069,0.03140419,0.03657049,0.020127121,-0.007846059,0.033158027,-0.019924225,0.031112166,0.0010346174,-0.008256975,-0.025398724,0.05770982,0.04273494,0.060729884,-0.02939899,0.06656657,-0.015480989,-0.019582113,0.039538793,0.04717528,0.025665069,0.038503446,-0.022072904,0.012689074,0.051985595,-0.04789996,0.006770358,-0.011421696,-0.0023186475,0.0009422297,0.0047227116,-0.009964429,0.025271393,-0.012186955,0.017222293,-0.021298364,0.04031754,-0.0035035228,0.014428804,0.02058065,-0.02871216,0.019073656,0.045119993,0.05018661,0.01473677,-0.004917809,-0.01879479,0.055725425,-0.01693814,0.0044676564,0.034950048,-0.04303169,-0.019234888,0.029505722,-0.012249825,0.004883087,0.03556599,-0.006821874,0.014907919,-0.018959923,-0.050042406,-0.029355139,-0.03375911,-0.019033737,0.046137575,-0.009976537,0.010933813,-0.028265247,-0.021779764,-0.09610614,-0.03329577,0.02509812,0.14481564,0.010125243,0.0063672033,0.01775869,0.011595722,-0.038546395,-0.0718714,0.008539378,0.04495052,0.0024260501,0.04917179,0.011018714,-0.0797373,0.0072882306,0.01580624,0.06284779,0.06437752,-0.04718354,0.007716656,0.050280586,0.014152369,0.0071956227,-0.03550438,0.054355305,-0.013668138,-0.004173094,-0.0005607879,-0.0072710444,-0.06820229,0.08036788,-0.052723978,0.008934939,0.057442464,0.015252357,-0.026683746,0.003426432,-0.074615344,0.042130716,0.009694664,0.045284435,0.011770746,-0.04623647,-0.015774604,-0.018381998,-0.054074787,0.04765918,-0.018565973,-0.018059062,0.0007672913,0.01297845,-0.041824475,0.062769435,0.051342662,0.05043748,0.009642619,-0.028622648,-0.009565478,0.007516106,0.020358494,-0.023960209,0.04829432,-0.10059083,0.07526254,-0.0031773075,-0.0016487561,0.017727204,-0.025187192,0.0041647526,0.05200355,0.0195847,0.0022807904,0.008852408,0.08359282,-0.060609363,0.0069984565,0.024720663,0.03933366,0.068010785,-0.024549609,-0.011418227,0.0057082907,0.01892673,-0.034463424,0.0016724792,0.006029999,-0.022138515,-0.0068137115,0.03506506,0.005183443,0.026251245,-0.029388402,0.04522798,0.008826336,-0.012082715,-0.06522745,-0.012323614,-0.017627126,0.019159693,-0.008800252,0.005350287,0.042259753,0.023685524,-0.069574535,0.011806627,0.029287735,0.047663596,-0.08856884,0.038156975,0.019257713,0.022981727,0.03579189,-0.04391114,-0.0021762066,0.0023783525,0.010583173,-0.0011392372,-0.01800323,-0.0033198246,-0.014351461,0.036564067,-0.024121875,0.029696891,0.01394627,-0.0022874353,0.056623623,0.0043914486,-0.053380497,-0.008166643,-0.010145449,-0.06555703,0.019381527,-0.057279594,-0.011806567,0.06853697,0.001405645,0.0058004735,-0.07026089,0.06595287,-0.015546947,0.037798714,0.051783334,0.00045753625,0.0022019832,0.0022752176,-0.012913157,0.017849183,-0.011550732,0.03100037,-0.0076100235,-0.001678299,0.012401674,-0.010322741,-0.012073973,-0.0062855864,0.03631797,-0.048767988,0.026017344,-0.003606113,-0.022017058,-0.0033290852,0.04078782,0.026647368,0.023615062,0.0035822627,0.05023646,0.0013588901,-0.013169365,-0.02436731,-0.012839838,-0.00631146,0.010422367,0.037352897,-0.030086143,0.043586683,-0.016265118,0.014154125,-0.0001311917,0.010244971,0.066502415,0.03651944,-0.04114782,-0.04271005,-0.030879002,0.046409123,-0.02386324,-0.017020058,-0.010180808,-0.021123867,-0.045706082,0.04643508,0.0004785137,0.05211107,0.025084015,0.010477782,0.035526555,0.007962925,0.015175309,-0.0038340932,0.034680247,0.026666688,-0.025160206,-0.042741306,0.053686876,0.07180807,-0.033856917,0.011510463,-0.010967945,0.02517241,-0.0245263,0.05186092,-0.02140691,-0.01344153,0.02783593,0.015262035,-0.0050226864,0.034759853,-0.0019880414,0.0031361373,-0.0063041607,0.016658353,0.018273674,-0.017493898,0.03680228,0.013886246,-0.00031665072,0.0054804203,-0.019361675,0.0060957004,0.053636044,-0.017207269,0.017970389,-0.055427607,-0.019424168,-0.0004990136,-0.028462755,-0.0024794121,-0.021363229,-0.03579004,0.006192674,0.017919386,-0.0006769593,-0.04446534,0.008342904,-0.036569573,0.05930386,-0.08550592,0.012807837,0.019105777,-0.0038710593,0.104237854,0.053643215,-0.014992263,0.004088685,0.009736858,-0.017577767,0.017316336,0.047827374,-0.005282737,-0.01735673,0.041731123,0.013998251,0.0029872528,-0.015003486,-0.038936883,-0.046453428,-0.03862283,0.012666405,-0.045435004,-0.03686114,0.052067094,-0.018872684,-0.00024062584,-0.0046766857,-0.03560004,-0.057947833,0.040952776,0.0033086508,0.0006230309,-0.017456193,-0.00096258236,0.044228245,0.0032402703,0.03296985,0.00516792,-0.0146444775,-0.009273895,0.0056024147,-0.0007642158,0.02773915,0.010066089,-0.06471277,-0.046783358,-0.0020256855,0.007287744,-0.018249007,-0.009388145,0.06501153,0.008137794,0.025161974,-0.04412182,-0.05587431,-0.02783721,0.004961054,-0.036083266,-0.016107175,-0.02552615,0.033634353,0.058830347,-0.067369066,0.049193345,-0.087462075,-0.029303983,-0.025248714,0.030821325,-0.024376575,-0.03202345,-0.0071514677,-0.01091451,-0.03273052,-0.027466055,0.038562797,0.04735444,-0.039206326,0.027600626,-0.045933295,0.031175433,0.008078201,-0.026509134,-0.055422552,0.014762152,0.026523234,-0.0058881594,0.08476144,-0.051047288,0.011620598,-0.06808974,-0.047202382,-0.09228998,-0.04657897,0.008148944,-0.0043675774,-0.01162406,-0.019542417,-0.0030873471,-0.004688175,-0.054014403,-0.039568044,-0.07547852,-0.0076781716,-0.010820677,0.014330759,0.027575875,0.01005697,-0.04636807,-0.01564633,0.007841923,-0.041641563,0.06351711,0.04051441,0.039268855,0.06580082,0.028726993,-0.021686617,-0.067025766,-0.0049077896,-0.01376932,-0.0009963484,-0.01807458,0.07030134,0.0055254004,-0.03330092,-0.020617317,0.013814391,0.030778524,-0.0029637571,-0.015750606,-0.027100574,-0.017229302,-0.025529196,-0.02817157,-0.016214497,-0.00014429768,-0.011975161,0.079799265,0.060403705,-0.05377354,-0.042490244,-0.060547363,-0.009483644,0.027621057,0.004858679,0.009201885,0.04777419,0.00019670406,-0.014296236,0.07584133,0.06918941,-0.019352455,-0.037991382,0.0017776645,-0.0080223875,0.028637014,-0.0026070003,-0.079789214,-0.10755112,0.0016200185,0.0038576762,-0.044547107,0.018480998,-0.014174485,-0.037447095,-0.036928926,-0.018826727,0.0025865356,-0.0026986357,0.00357825,-0.013294825,0.015451352,-0.023922263,0.001584492,-0.031659603,-0.03212525,-0.04617552,0.020476246,-0.01681488,0.02950786,0.03921693,0.017173544,-0.012189324,0.043046873,0.06824774,0.037195425,-0.029579688,0.029450325,0.009595909,0.020208994,-0.005086222,0.049616802,0.08688773,-0.053205777,0.013437961,0.00528549,0.00712101,0.007526284,-0.039113548,-0.014788549,-0.044847757,0.003498133,0.0128464755,0.04499151,0.011858767,0.038873147,-0.0053594084,-0.04648432,-0.06562224,0.0022525166,-0.021389095,0.06682252,0.014666453,-0.008255987,-0.078877605,-0.017871682,-0.06551544,0.03571972,0.006370251,-0.028292583,-0.005577943,-0.00527645]	Keywords: hash layers, expert assignment, Mixture-of-Experts, routing function, Switch Transformer\nKey Objects: hash layers, routing function, experts\nRefers to Images: None\nHypothetical Questions:\n- What are the advantages of using hash layers compared to learnable routing functions?\n- Why would a routing function require auxiliary loss functions?\n- How does the use of hash layers contribute to the overall efficiency of the Transformer architecture?\n---\nSummary:\nRoller et al. [110] proposed using hash layers as an alternative to learnable routing functions for assigning tokens to experts within a Mixture-of-Experts (MoE) architecture, which eliminates the need for routing parameters or auxiliary loss functions and demonstrates competitive performance compared to methods like Switch Transformer [36].\nOriginal Text:\nAs opposed to using a learnable routing function for expert assignment, Roller et al. [110] design hash layers where tokens are hashed into a fixed number of buckets, each bucket corresponding to an expert. This approach requires no routing parameters or any auxiliary loss function, while showing competitive results with existing methods such as Switch Transformer [36].  \n5.3.3 Dropping FFN Layers. Notably, one might argue that under some circumstances, FFN layers can be dropped completely, resulting in a simplified network.\nContextualized Text:\nTo avoid using learnable routing functions for expert assignment in a Mixture-of-Experts (MoE) Transformer, Roller et al. [110] designed hash layers. These hash layers assign tokens to experts based on fixed buckets, which simplifies the process by removing the need for routing parameters and auxiliary loss functions, and showing competitive results compared to methods like Switch Transformer [36].	{"tags": ["MoE", "architecture", "optimization", "Transformer"], "doc_id": "c8b5b551-5ad5-4ad4-8814-f5f44a412348", "summary": "Roller et al. [110] proposed using hash layers as an alternative to learnable routing functions for assigning tokens to experts within a Mixture-of-Experts (MoE) architecture, which eliminates the need for routing parameters or auxiliary loss functions and demonstrates competitive performance compared to methods like Switch Transformer [36].", "doc_type": "text", "entities": ["Roller", "Switch Transformer"], "keywords": ["hash layers", "expert assignment", "Mixture-of-Experts", "routing function", "Switch Transformer"], "key_objects": ["hash layers", "routing function", "experts"], "contextual_text": "To avoid using learnable routing functions for expert assignment in a Mixture-of-Experts (MoE) Transformer, Roller et al. [110] designed hash layers. These hash layers assign tokens to experts based on fixed buckets, which simplifies the process by removing the need for routing parameters and auxiliary loss functions, and showing competitive results compared to methods like Switch Transformer [36].", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.3 Position-wise FFN"}, "hypothetical_questions": ["What are the advantages of using hash layers compared to learnable routing functions?", "Why would a routing function require auxiliary loss functions?", "How does the use of hash layers contribute to the overall efficiency of the Transformer architecture?"]}
c81a9085-4fd4-41e6-9c4e-c90361888adf	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0565169,0.03831199,0.02242921,0.039403047,-0.02532804,0.056880474,-0.0006263769,0.054491162,0.030863008,-0.038006265,0.00038898172,0.0053394544,-0.032673985,-0.0071806903,-0.045000236,0.0153074125,0.011126738,0.084938385,0.002306525,-0.036692988,0.05447454,0.017388841,0.051794175,0.0031544364,0.011178009,0.04508804,0.04115206,0.0153710935,0.013433453,0.050020155,0.03948937,-0.0027758041,0.033097673,0.009500313,0.03334655,-0.09513291,-0.010291443,-0.014290562,0.03801811,-0.0030278047,-0.020963965,0.02540442,-0.10449749,-0.0022916582,-0.021435356,-0.02685426,-0.10032897,-0.027603112,-0.004364286,-0.010896188,-0.005074728,0.03301348,0.031350266,0.0056673796,-0.017780779,0.016255205,-0.03313142,-0.031249953,0.0034828417,-0.068161905,-0.0572904,-0.009541182,-0.0018550705,-0.0036355155,0.0074657435,0.026629677,0.026073864,-0.014309421,0.05652925,0.11907023,0.023789074,-0.0075763995,-0.023093186,-0.013908893,0.112066455,0.013562504,-0.068158925,-0.058190793,-0.03173611,-0.034244526,-0.03488926,0.10487352,-0.009990324,-0.021069426,0.053715218,0.027425429,-0.03649741,-0.005231381,0.07067273,0.0026430634,0.03792424,0.041753225,-0.0013280798,0.012960902,-0.014491631,-0.033593453,0.00022224622,-0.04114124,-0.019914947,-0.03195706,0.029382888,0.052132677,0.064161405,0.10452431,0.047293685,-0.0032604963,0.011965371,0.018764691,0.014625804,0.045966092,-0.025946341,0.013717266,-0.04202135,-0.021214081,-0.02923097,-0.006867132,-0.040996503,-0.0006029822,-0.03120186,-0.032305185,0.073899955,0.009462404,-0.015665527,0.00668964,0.039194603,0.03339219,0.02467007,-0.03958926,-0.007913273,-0.012856354,0.064731665,0.029822543,0.003994731,0.042709265,-0.023329541,-0.022074716,0.042248677,-0.02564461,0.015812486,0.030200794,-0.0053709154,-0.02438218,-0.049908426,-0.008327595,-0.02776673,0.035868395,-0.08481469,0.030223228,0.0052799513,0.046677902,0.02965332,-0.03457599,0.03776794,0.004242568,-0.03280459,0.014853835,0.035076372,-0.049487386,0.011671348,-0.03763888,-0.066081814,-0.0818463,0.026304796,0.034469325,0.024643252,0.09092338,-0.013001192,-0.0052929763,0.068540625,0.016176024,-0.02997731,-0.020134892,-0.012541888,-0.016661014,0.03668337,-0.016077545,0.02865802,-0.021435192,0.00073345605,0.0023392509,-0.0022861634,0.017167965,-0.012527753,0.049558822,-0.04705909,-0.0011140702,0.036709785,0.07300185,-0.0061711394,-0.031550266,0.0480108,-0.017868511,-0.008394867,0.008442395,0.0605105,0.039373085,0.036834996,-0.03411694,-0.06016822,-0.036119472,-0.00458567,0.012929816,-0.031506922,-0.014345841,-0.004071427,0.015932968,-0.02448031,-0.020335339,-0.05175756,0.04593666,0.03101516,-0.027254203,0.004074985,0.056174893,0.022128806,-0.010564712,0.00096530415,0.031541876,0.049082216,-0.016141219,-0.026398504,0.016130166,-0.04130203,-1.2191388e-05,-0.05899518,0.017107988,-0.03010276,0.013146171,0.057274375,0.0014352901,0.04396299,0.072964005,-0.02733247,0.04527668,0.016123977,0.052253745,0.012412232,0.013831904,-0.0013767034,0.05330975,0.07326156,0.07811568,-0.023659263,0.049264047,0.030019468,-0.033307236,0.0407872,0.019483313,-0.025027925,0.016077269,-0.017402375,0.044007923,0.04974437,-0.0057076984,0.017002676,0.006165902,0.03985226,0.0074306703,0.02139813,-0.035845924,-0.019057577,-0.009615427,0.00083272136,0.025624575,0.028441433,-0.014383505,0.020212272,0.0028771486,-0.076647446,-0.005843101,0.02208561,0.027166111,0.014139042,-0.034166645,-0.052050818,0.040347815,-0.0075390683,0.021964934,0.0017399446,-0.032832988,0.002195684,0.018333385,-0.0044214544,0.00085380883,0.0009919278,0.022770742,-0.015052318,-0.010418839,-0.045352634,-0.020545421,-0.012962936,-0.057383347,0.027853863,-0.0033536935,-0.033921994,0.02152964,-0.009140398,-0.0042278785,-0.0040965863,0.033444025,0.10179587,-0.036458194,-0.033127192,-0.025779484,0.03962968,0.022285234,-0.045870714,0.02859222,0.03709294,0.012355379,0.032743067,0.028620748,-0.018079609,-0.0090737045,0.0626411,0.04297137,0.063053876,-0.068993114,-0.008330731,0.03780443,0.06978615,0.0050597857,0.017386492,0.023569621,0.025745582,-0.0054713655,0.0011506964,-0.01392372,-0.051458832,0.077871025,-0.029984428,-0.007895122,0.050650306,0.024202427,-0.007124738,-0.017303683,0.014420145,0.035769887,-0.017869543,-0.036421172,-0.020230217,-0.064809434,-0.04163529,0.009956554,-0.055465482,0.031925146,-0.0028769844,0.0008167154,0.016143955,0.026735485,-0.02315396,0.025077572,0.029044319,0.024838276,0.0366693,-0.010644231,-0.033195633,0.003939625,0.0340241,0.017354479,0.083911955,-0.036107767,0.0032781204,-0.013152895,0.014852439,0.00284854,-0.06516392,0.017052362,0.05711006,-0.085454844,-0.010665919,-0.03676112,0.06483743,-0.004811261,0.03862101,0.05853116,-0.035916504,0.014497599,-0.0008646509,0.019148625,-0.0026985398,0.022171626,-0.015657134,0.026280917,0.019734984,-0.006625242,0.017888041,0.06447888,-0.022206256,0.061953597,-0.056100655,0.022927921,0.038672246,-0.006548565,-0.05408738,-0.038418163,0.014904095,-0.005147701,-0.019415056,0.004134086,-0.00216724,0.026170088,-0.05234411,0.04080199,0.010624537,0.016125213,-0.08246327,0.04529724,0.042411335,0.010548369,0.06485439,-0.05060369,-0.04929595,0.043135665,0.0027329503,-0.047736064,-0.016125787,0.063399345,-0.024149379,-0.0025725327,-0.032716382,0.0065580793,-0.0011286322,0.04973088,0.012586031,0.0008322168,0.005328146,0.017124616,-0.011181412,-0.032889143,0.0124965785,-0.06039849,-0.018365677,0.054425817,-0.019832402,-0.00988709,-0.06612181,0.09469587,-0.07558951,-0.015212022,0.035768684,0.0154058775,0.029142253,-0.0005174912,0.0067967256,0.01953511,-0.023827933,0.027635446,-0.07316896,0.0088329,0.054274794,-0.016485073,0.031200888,0.039497703,0.019475522,-0.02544522,0.014517451,-0.0053587947,-0.06895096,-0.03391565,-0.0011757354,0.033146325,0.03219051,0.027069494,0.0065371245,-0.03517906,0.028421044,0.005139014,-0.0237361,0.0062019294,0.00015514529,0.018069241,0.0014734388,0.045602687,0.049668163,-0.026376478,0.0020798733,0.04154451,0.02917825,-0.00097497663,-0.047335923,0.019833984,-0.008037892,0.05086903,-0.033003088,-0.0009856086,-0.021810303,0.018249286,-0.03203495,0.05842608,-0.0026739638,0.077601835,0.017165877,0.006525754,0.049036894,-0.015424211,0.014317687,-0.022621179,0.027822286,-0.008249131,-0.04118564,-0.014979688,0.059607185,0.06345199,-0.009876379,-0.023837404,0.0023173115,0.0540812,-0.0029098573,0.074097246,-0.0016008242,-0.048955396,0.037985288,-0.013243169,-0.013318653,-0.063768506,0.04604219,-0.017145237,0.019632887,0.03618916,-0.024355512,0.036797643,-0.001788785,0.0148124555,-0.018131902,-0.0023515574,0.010500418,-0.015783044,0.0431122,-0.034900386,0.03339101,-0.014937298,-0.052318152,0.0129144555,-0.002000726,-0.0120086465,0.018459527,0.017804064,0.05348274,0.008750228,-0.012979985,-0.021249602,0.0008385448,-0.012698312,0.07793404,-0.07043489,0.02513673,-0.005477057,0.007425491,0.07685436,-0.040993728,-0.011387622,-0.023248179,-0.010396194,-0.02080658,0.04389069,0.074315995,0.013269465,-0.013925339,0.03476169,0.009054553,-0.025356626,-0.0050998153,-0.026790457,-0.031226091,-0.057113137,-0.036391415,-0.0023480025,-0.0076286006,0.04225812,-0.014928547,-0.0057064164,-0.002331668,0.037526928,-0.052166503,0.026054956,0.0447866,-0.013745001,0.0669015,0.009105124,0.034058858,0.028661406,0.015790619,0.015494563,-0.055647045,0.009998026,-0.0031850266,-0.022343706,0.010470802,0.0026239702,-0.02117977,-0.05379995,0.038173214,-0.01699944,-0.042406563,-0.006722473,0.060418695,0.030188045,0.018987238,-0.013046714,-0.032651335,0.005683158,-0.04824076,-0.004425345,0.025514035,0.044630237,0.008314358,0.023369007,-0.054897096,0.089833274,-0.060439527,0.030093234,0.01887667,0.019412117,-0.01871483,-0.04725374,-0.025898369,-0.01382605,-0.0016567382,-0.06426907,-0.003969684,0.0876537,-0.034457054,-0.0022536784,0.017396292,0.07279775,0.024212835,0.027838277,0.0028890676,0.02328579,0.022681352,-0.011445456,0.081379294,-0.077084266,0.00045353812,-0.01891361,0.006327074,-0.108868964,-0.003286662,0.02027197,0.045362722,-0.0067533753,0.006080481,0.054428376,0.019461919,-0.063535295,-0.02768256,-0.042091098,-0.017506743,0.0015173346,0.024236752,0.016926011,-0.0042715697,-0.048918247,-0.0009190819,0.042579237,-0.05094085,-0.0037847192,0.029665083,-0.0031523819,0.033156537,0.024789548,-0.025446825,-0.027612062,0.0030894338,-0.030175313,-0.0063672536,-0.044009253,0.042659175,0.0007088809,0.026280569,-0.017292136,0.01709782,-0.03455556,-0.01558145,0.008546195,-0.029734762,-0.004005001,0.007115977,-0.006413836,0.009830604,0.0031357307,0.01319428,0.06945329,0.040124785,-0.008970896,-0.021555636,0.0008709759,0.0038324883,-0.00068792276,0.048363414,0.0059833415,0.024907537,0.045681268,-0.006295976,0.08753588,0.060553513,-0.046449747,-0.04923616,0.04689171,0.025301041,0.027265176,-0.02561202,-0.03644021,-0.08336184,-0.006855655,-0.00685179,-0.039758034,0.030014705,0.003841119,-0.029411368,-0.008309258,-0.039285768,-0.007956616,0.026946204,-0.012865422,0.053200446,0.04444299,0.016144712,0.008063698,-0.020664122,0.002616976,-0.077963404,-0.017533679,0.01829424,0.0061774435,-0.040569276,0.004123015,-0.06632302,-0.015496956,0.024930445,0.03874356,-0.0034410178,-0.0023065803,0.03158312,0.0475945,0.0071573444,0.06261967,0.051677108,0.014070286,0.024909528,-0.066971466,0.035610337,0.015151621,-0.008197845,-0.045804463,-0.0065528774,-0.0763011,-0.013992194,0.01588625,0.0027281283,0.051957674,0.009599047,-0.049642693,-0.03364919,-0.0072668064,-0.012500142,0.046434257,-0.0037665137,-0.0030491038,-0.055541042,0.014015996,-0.03754035,0.032548282,-0.052373823,0.033248305,-0.0046210587,-0.04884837]	Keywords: FFN, Feed-Forward Network, Transformer, Dropping layers, training speed, inference speed\nKey Objects: FFN layers, Transformer decoder\nRefers to Images: None\nHypothetical Questions:\n- Why would someone consider dropping FFN layers in a Transformer model?\n- How does dropping an FFN layer effectively turn it into an attention module?\n- What are the potential benefits of removing FFNs in the decoder of a Transformer?\n---\nSummary:\nResearchers have explored dropping Feed-Forward Network (FFN) layers in Transformers, demonstrating that under certain conditions, these layers can be removed without significant performance loss, and even improving training and inference speed.\nOriginal Text:\n5.3.3 Dropping FFN Layers. Notably, one might argue that under some circumstances, FFN layers can be dropped completely, resulting in a simplified network.  \nSukhbaatar et al. [127] demonstrate that replacing the ReLU activation with Softmax and dropping the bias term in FFN effectively turns FFN into an attention module where position-wise inputs attend to a global key-value memory of D$\\_{fin}$ slots. They thus propose to drop the FFN module and add to the attention module a set of global key-value pairs, which are learnable parameters concatenated with key and values generated by inputs. This approach simplifies the structure of the network with no loss of performance.  \nYang et al. [157] empirically show that FFNs in the decoder of Transformer, despite its large number of parameters, is not efficient and can be removed safely with only slight or no loss of performance. This approach significantly boosts the training and inference speed.\nContextualized Text:\nIn Transformer models, researchers have investigated the possibility of eliminating Feed-Forward Network (FFN) layers. Sukhbaatar et al. [127] showed that by replacing the ReLU activation with Softmax and dropping the bias term, FFN effectively transforms into an attention module. Yang et al. [157] further confirmed that FFNs in the Transformer decoder, despite having many parameters, can be removed without significant performance loss and boosts training and inference speed.	{"tags": ["architecture", "optimization", "efficiency", "transformer"], "doc_id": "c81a9085-4fd4-41e6-9c4e-c90361888adf", "summary": "Researchers have explored dropping Feed-Forward Network (FFN) layers in Transformers, demonstrating that under certain conditions, these layers can be removed without significant performance loss, and even improving training and inference speed.", "doc_type": "text", "entities": ["Transformer", "ReLU", "Softmax"], "keywords": ["FFN", "Feed-Forward Network", "Transformer", "Dropping layers", "training speed", "inference speed"], "key_objects": ["FFN layers", "Transformer decoder"], "contextual_text": "In Transformer models, researchers have investigated the possibility of eliminating Feed-Forward Network (FFN) layers. Sukhbaatar et al. [127] showed that by replacing the ReLU activation with Softmax and dropping the bias term, FFN effectively transforms into an attention module. Yang et al. [157] further confirmed that FFNs in the Transformer decoder, despite having many parameters, can be removed without significant performance loss and boosts training and inference speed.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "5 OTHER MODULE-LEVEL MODIFICATIONS", "h3": "5.3 Position-wise FFN"}, "hypothetical_questions": ["Why would someone consider dropping FFN layers in a Transformer model?", "How does dropping an FFN layer effectively turn it into an attention module?", "What are the potential benefits of removing FFNs in the decoder of a Transformer?"]}
45b2419a-cd56-4830-ad3b-5dd36cadaf46	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.040293112,0.0056533352,-0.0130518945,0.0050372276,-0.011157934,0.004592513,0.008490853,0.016784867,-0.01363322,0.012112833,0.013612878,0.009908127,-0.0046232725,0.034791764,-0.0060469313,0.041677244,0.040614065,0.038834766,-0.021616438,-0.0457678,0.03145319,-0.035468753,0.06033035,-0.008592392,0.039656185,0.0059216195,0.0044388087,-0.027736679,-0.0014035715,-0.0036255452,-0.008945951,-0.028301332,0.036765855,0.03605788,0.034710564,0.06862072,-0.0004511137,-0.0195266,-0.0068009603,-0.010974498,-0.05282344,0.052325327,-0.071618326,-0.004078294,-0.049122028,-0.048866514,-0.08066937,-0.022106117,0.0020539365,-0.02015023,-0.043609884,0.034353778,-0.095920846,-0.026886677,-0.034665905,-0.041813906,-0.018760314,0.0336308,0.0031496761,-0.051302295,0.0055676554,-0.0073878285,-0.032498755,-0.041422624,0.011689355,-0.016567523,-0.007322578,0.012916667,-0.013463567,0.123721875,0.0031647282,-0.049735703,0.02588451,0.019830538,0.12413588,0.026729815,-0.06406933,0.032495756,-0.017035574,-0.021952596,0.0009999947,0.08696751,-0.08168991,-0.030127496,0.08222348,0.06185578,-0.041783288,-0.044330094,0.052180555,-0.05957673,0.00954922,0.026490694,0.02247861,0.011450459,0.00892324,-0.079970025,-0.028270965,-0.03818872,0.008241374,-0.062787354,0.0076119495,-0.016543357,0.049687367,0.028929295,0.00091444113,-0.0073488336,-0.016629318,-0.031774577,0.016954638,0.02260969,-0.06749915,0.008747464,-0.0018119669,0.0031443785,-0.04576281,-0.02796254,-0.017786473,0.0065770126,0.024244295,0.030529607,0.00843699,-0.018849395,-0.06239709,0.027050791,-0.005417608,-0.033953767,-0.00053151895,-0.02790522,0.0064188326,-0.010884975,0.03447431,-0.016799137,-0.00951351,-0.0048103496,-0.0016712638,0.002973949,0.09365478,0.018581597,0.01347984,0.013862144,0.04077963,0.0028447818,-0.006001585,-0.026470581,-0.027302966,0.021038847,0.011779052,-0.018929984,-0.019022658,0.0772966,0.063532844,-0.03679013,0.029207058,-0.024038423,-0.04653566,0.023180887,-0.022116488,-0.044581078,0.018714333,-0.0073062447,-0.013281891,-0.052194647,-0.018204773,-0.00807159,0.012275842,0.031056779,0.0057245297,0.06625598,-0.004041556,0.031431608,-0.04350813,-0.0077787475,-0.021883637,0.009099334,0.007437468,0.009838518,0.0013670453,-0.029177476,-0.015693035,-0.013314298,0.0060462235,-0.008449797,0.020901468,0.068544865,0.04005798,0.02998362,-0.045842648,0.03548695,-0.02529412,-0.008660003,0.025993668,-0.03167175,-0.039816886,0.033111144,0.045414634,0.05907335,0.057435144,-0.030702349,-0.073712945,0.0003619722,0.01881883,0.010895474,-0.0071388837,-0.038109694,0.01212879,0.0028548369,-0.03362367,0.020949963,-0.0132875545,0.041233376,-0.023616629,-0.03853242,-0.022443967,0.06549032,0.044149704,-0.013413368,-0.0040290197,-0.032526802,0.05079557,-0.035288364,-0.0049823443,0.039171517,-0.031847622,0.006832284,0.0015718326,-0.018538151,-0.036152456,0.0041710543,0.018263299,-0.015293714,0.054755017,0.0076620867,-0.0004117617,-0.009606937,0.04692028,0.02025377,-0.028286388,0.0110427495,0.050813954,0.006948696,0.011176381,0.03239561,-0.060482863,0.045941282,0.04973636,0.010774484,-0.011678656,0.037777413,0.016380874,0.0025620256,0.004642947,0.057088844,0.0727156,0.016851794,-0.01066414,-0.028720299,0.0015478517,-0.008192859,0.018952932,-0.011760207,-0.018757945,-0.024208106,0.0047290656,0.03127527,0.026601372,-0.020897472,-0.010525467,0.021702917,-0.031911075,-0.057859715,0.037301935,0.0025649266,0.04382067,-0.029317776,0.023116572,-0.029947309,-0.013412782,0.023562375,0.037002563,-0.007431236,-0.042288482,-0.04903099,0.056653608,0.010292871,0.019047692,-0.0043675946,0.023321673,-0.096078,-0.06374632,-0.04167943,-0.05701729,-0.02637027,-0.025076522,-0.019541537,-0.005472066,-0.0033522549,-0.03505823,0.016198676,-0.017919533,-0.05278105,0.12794629,-0.022800699,-0.008073335,-0.045697622,-0.028891642,-0.030725962,-0.050441336,-0.07579859,0.03067857,-0.044333067,0.004688069,-0.0082257185,-0.05977512,0.016834345,0.045013633,0.092378765,0.008585631,-0.10716257,0.008515981,-0.018568201,0.060142573,-0.017702065,-0.056742895,0.010403794,0.059667494,-0.003249506,0.0060942494,-0.011067415,-0.03130505,0.06747052,-0.07066425,0.03902521,0.04203131,0.005867562,0.0068760696,0.029566785,0.022706717,0.026545955,0.059166975,-0.031172331,-0.008011588,-0.023538109,-0.023292711,-0.0057714595,-0.02229036,0.038283836,-0.064653926,0.039801702,-0.021925138,-0.032699972,0.03609771,0.02386331,0.025651649,0.016981509,-0.016606452,-0.0327851,0.044366386,-0.054942705,0.011868946,-0.017087795,0.021314496,-0.043261386,0.035089675,0.02660502,0.029365953,0.031482466,-0.08380586,-0.009048933,-0.034063425,-0.012972209,-0.0211499,-0.011258134,0.036182057,-0.017818842,-0.04620419,0.010537322,0.044265233,0.04211692,0.02799282,0.0294722,-0.004267959,0.0019558517,-0.06264652,0.0037044229,0.000967274,0.051652767,-0.004158617,0.013747115,-0.03459679,-0.008953943,-0.05129799,-0.04737092,0.012456221,-0.008487678,-0.014173737,0.011803394,0.054769356,0.06898045,0.016224395,-0.023514539,0.015102832,0.06776084,-0.051969785,0.04263902,0.063097924,0.013684544,-0.04859539,0.034846425,0.05485237,0.02070912,0.002670765,0.003245509,-0.019534217,0.039785363,-0.03364958,-0.016023828,-0.041438513,0.07541236,-0.03419762,0.02302526,-0.005960934,0.0041473783,-0.0052968054,0.03081727,0.017712597,0.016627502,-0.0124565745,0.00883996,0.043122694,0.010049889,0.012027332,-0.05986747,-0.06924585,0.07124813,0.09037001,-0.0042420127,-0.07596597,0.062841505,-0.09122638,0.037040267,0.07072601,0.033008955,0.037440814,-0.0055918796,0.0006023605,-0.049181573,-0.0055776406,0.038459312,0.009256253,0.0049054227,-0.0015400915,0.024372783,-0.014675799,0.03112859,-0.028989026,-0.037326694,0.019518074,-0.019611103,-0.023488348,0.056751635,0.0089610815,-0.011132385,-0.022984024,-0.01566395,-0.0061698984,-0.011721942,0.06011859,-0.03345484,-0.03544775,0.020629827,0.0009979064,-0.017074099,-0.0039259386,0.034614637,0.003907137,-0.004229826,-0.020839538,0.049025074,0.012298195,0.003389262,-0.019665176,0.020480365,0.053294808,0.043293703,-0.014288348,0.0028111504,-0.03393549,-0.0049082898,-0.03370467,0.0036157973,0.012354981,0.093635984,0.045641586,0.056772,0.031862427,-0.038065527,-0.0278436,0.004461694,-0.008911679,-0.036188394,-0.0068407287,0.013494851,0.059087474,0.018378578,-0.02639785,0.029670304,-0.043296773,-0.019410407,-0.0036746424,0.057922475,0.033425406,0.004402071,-0.007948733,-0.0110049,-0.037715245,-0.037079133,-0.06003508,-0.029843874,0.013554484,0.05021247,-0.028882286,-0.012131053,0.026688106,0.021679787,-0.01470605,-0.0043289657,0.026443789,0.04653566,0.031395655,0.00695358,0.03017864,-0.030645091,0.019852957,-0.014103655,-0.0074071675,-0.0002759397,0.05093403,-0.004074628,-0.02004207,0.0385719,0.0150256315,-0.017514711,0.0021867927,0.02763237,0.09689347,-0.0328939,0.014045824,-0.05792125,0.035057,0.05875146,0.07135703,0.0013367981,0.03434591,-0.014475453,-0.0063935923,0.035250425,0.050925195,-0.0033259776,-0.04811746,0.046213977,0.011869561,0.014381153,-0.0021404186,-0.05091219,0.0066295858,-0.0068761706,0.04397052,-0.03805187,-0.01885126,0.02626694,-0.0071056336,0.006184294,0.029080482,-0.011594779,0.016330265,0.06713182,0.043791387,0.00029232376,0.07003188,0.06689797,0.093206584,0.01028356,0.0007723701,0.045861278,0.006316403,-0.030028218,0.049709827,-0.008326694,-0.0150501635,0.022243816,0.0064720274,-0.03959267,0.02323035,0.018627336,0.005124988,-0.0015286566,-0.023200726,0.057692572,0.044236366,-0.04085747,-0.013610452,-0.0015020671,-0.01518583,-0.033665236,-0.01998226,-0.07636595,0.05591823,0.014120014,-0.04339088,0.09603541,-0.05545496,-0.033185013,0.0040066387,0.0011252239,0.03595711,-0.046667047,-0.016245354,-0.0267959,-0.03587843,0.011804641,0.060602196,0.017018963,0.038618665,-0.022619696,-0.015322239,-0.004734341,0.024797192,-0.043493025,0.008694445,0.0075485255,0.02143099,-0.0105090225,0.049879603,0.007341965,0.006655437,-0.008142853,-0.02976145,-0.0023801734,-0.0060540778,0.01252185,0.059217777,0.02326801,-0.012466316,0.031970892,0.013607278,-0.021882568,0.002579921,-0.002306179,0.004111192,-0.015773425,0.0031899977,0.034815963,-0.03599626,-0.09744153,-0.030541435,-0.017328646,-0.021183234,0.024919705,-0.0009091099,0.080412626,0.023930456,0.012439718,-0.005394382,-0.088975556,-0.017297132,-0.023736088,0.049774222,-0.10864043,0.07688711,0.035175703,0.03526194,-0.045965917,-0.011724716,0.014772191,-0.018603018,-0.01456843,-0.017270003,-0.04940157,-0.0034113075,-0.025302203,0.018554568,-0.10028389,-0.018121975,0.01420966,0.061042033,-0.07636368,-0.0041467673,-0.017287144,0.0018157493,0.03599476,0.0010543016,0.026374679,-0.014104654,0.00835392,0.0021303436,0.046889514,0.049339175,-0.050978556,-0.0100704245,0.062484443,-0.022448197,0.011259801,0.0076175234,0.02937055,-0.02506223,0.04633572,0.0038082616,-0.021873254,0.074883476,-0.031090755,-0.016018022,-0.015726859,-0.01933026,0.022957964,-0.01212569,0.03202527,-0.0059223967,0.01001111,0.025913246,0.048964087,0.020909498,0.024909932,-0.004077193,0.009153334,0.0022843576,0.032368768,0.0030090467,-0.0151387,0.0023430302,0.04999867,0.036303923,-0.039440386,0.07077468,0.08016,-0.03985405,0.01897729,-0.033182066,0.02019609,0.024842592,0.005556405,0.018480575,-0.030471707,0.006559869,-0.0015295789,-0.020230029,-0.005635781,-0.013799341,-0.043704808,0.03505713,-0.0071086334,0.03360457,0.011722014,0.00273812,-0.013136861,0.005598698,0.03266557,-0.0124656465,0.030610254,0.022088688,-0.029356187,-0.031151973,0.046995457,-0.0110889645,0.043969702,-0.04020194,-0.058785785,-0.03225577,-0.07331363]	Keywords: X-formers, Transformer architecture\nKey Objects: X-formers, Transformer architecture\nRefers to Images: None\nHypothetical Questions:\n- What distinguishes X-formers from other Transformer modifications?\n- How do X-formers differ from standard Transformer modules?\n- What types of modifications do X-formers make to the Transformer architecture?\n---\nSummary:\nThis section introduces X-formers, which are modifications to the standard Transformer architecture that go beyond simply altering individual modules.\nOriginal Text:\n## 6 ARCHITECTURE-LEVEL VARIANTS  \nIn this section, we introduce the X-formers that modify the vanilla Transformer beyond modules.\nContextualized Text:\nThis section details X-formers, which represent variations of the Transformer architecture. These X-formers go beyond modifications to individual modules and offer broader changes to the overall structure of the Transformer.	{"tags": ["architecture", "transformers", "variants"], "doc_id": "45b2419a-cd56-4830-ad3b-5dd36cadaf46", "summary": "This section introduces X-formers, which are modifications to the standard Transformer architecture that go beyond simply altering individual modules.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["X-formers", "Transformer architecture"], "key_objects": ["X-formers", "Transformer architecture"], "contextual_text": "This section details X-formers, which represent variations of the Transformer architecture. These X-formers go beyond modifications to individual modules and offer broader changes to the overall structure of the Transformer.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS"}, "hypothetical_questions": ["What distinguishes X-formers from other Transformer modifications?", "How do X-formers differ from standard Transformer modules?", "What types of modifications do X-formers make to the Transformer architecture?"]}
7c8cb13d-c699-46b2-aba7-20f16e77bb50	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03764137,0.0025943313,0.050019752,0.069507904,-0.04305422,0.056568984,0.012202268,0.037314463,0.013900079,-0.00717255,-0.019169798,0.022931071,0.029923089,-0.01343834,-0.03267945,-0.023192663,0.028442308,0.092052445,0.024556726,-0.04627217,0.04357377,0.010863686,0.039552335,0.0066440217,0.016778478,0.027617987,0.043094333,-0.0103575885,0.0011909385,-0.018483784,0.013258776,-0.06500812,0.022297177,0.052956574,-0.010894495,-8.453746e-05,-0.018030562,-0.055438444,-0.014438593,-0.029442139,-0.040544286,0.046680823,-0.05125788,-0.020528521,-0.010129317,-0.029018685,-0.08199578,-0.042411484,0.022965446,0.014388482,-0.049527444,0.05202479,-0.02491003,-0.0051178792,-0.07686756,-0.013206912,-0.012070909,-0.025654634,0.01612809,-0.066351764,0.012367542,0.00941073,-0.033033468,0.0013147042,-0.0023296247,-0.03167097,-0.017904885,0.022253629,0.033907436,0.12231289,-0.026560243,-0.022069752,0.003733711,-0.049110238,0.103452854,0.043136485,-0.035468295,-0.02803498,-0.070323214,-0.0148565015,-0.0431184,0.049315114,-0.07757367,-0.036781687,0.07923432,0.0027988597,-0.028097186,0.025823394,0.09065482,-0.0036737917,-0.009239986,-0.0040966305,0.015274345,0.044831634,-0.048421178,-0.04029955,-0.032438613,-0.036396697,0.05922213,-0.042658594,0.023060037,-0.024057701,0.080529794,0.07668974,0.013922635,-0.05190006,-0.028097592,-0.036269043,0.023288423,0.015830403,-0.015962983,0.03900376,-0.04361929,0.01786163,-0.023507837,-0.017239597,-0.037258424,0.024817802,0.01494108,-0.018490918,0.03326808,-0.024962343,-0.04003949,0.09511704,0.05472013,0.0347412,0.019206898,-0.04558716,-0.01973662,0.026661083,0.04991017,-0.006782366,0.013432717,-0.028921928,-0.023501,0.0064753233,0.06139427,0.030560046,0.009618547,0.031216074,0.006550233,-0.04382622,-0.049467463,0.019620175,-0.0065368777,0.030166224,-0.049182966,0.008613998,-0.015908567,0.05151422,0.00034902003,-0.033441342,0.037688546,-0.021918602,-0.02401224,-0.009056775,-0.0043018586,-0.0382854,-0.02402407,-0.038806204,-0.05438893,-0.041866455,-0.017341536,0.04858372,0.042737063,0.08596254,0.0020455243,-0.004039754,0.021223685,0.049961768,-0.014266511,-3.6588644e-05,0.018322928,-0.011912571,0.03881259,-0.014799544,-0.00062501605,-0.0073003666,-0.008867463,-0.025390895,0.026882006,0.07108597,-0.007931958,0.032916553,-0.04252519,0.035591114,0.0544741,0.03970408,-0.025633674,-0.010961867,0.014111913,-0.018009678,-0.010936498,-0.01822418,0.067622714,0.045808945,0.06441031,-0.08016454,-0.027347866,-0.056211762,-0.027484413,0.008441244,-0.0024210666,-0.011931512,0.004395969,-0.05299037,0.012625545,-0.05118918,-0.028307877,0.030207636,-0.026283065,-0.018119615,-0.016989391,0.076639704,0.029202832,-0.012937522,-0.02614274,0.00600182,0.050171632,-0.07640259,-0.045245577,-0.034481913,-0.064376645,0.04645017,-0.033083033,0.028317854,-0.026332917,0.0335711,0.031053282,-0.043108705,0.029617067,0.019047419,-0.016267696,-0.0016996311,0.009530449,-0.007622437,0.017763255,0.015746936,0.004214695,0.050286643,0.05264821,0.069438756,0.017320493,0.07357888,0.0011149948,0.029909885,-0.016315011,0.059812218,0.021521207,0.017757766,-0.023139305,0.067305535,-0.012993521,0.006889283,-0.016718606,-0.018476203,-0.009075747,-0.0076786913,0.026077505,-0.021981891,0.01628114,-0.011196158,0.033313926,0.0024387615,-0.003911444,0.0018388588,-0.029913122,0.019012667,-0.017521014,0.023876272,0.006277993,0.032479815,0.028796924,-0.028620388,-0.03662027,0.04870055,-0.0048511587,0.0065598083,0.028949732,-0.022613985,-0.009054733,-0.005949777,0.06723229,0.006589447,0.042463068,-0.0025829615,0.051865537,0.01232751,-0.016415698,-0.019929206,-0.022604913,-0.032260448,0.07440111,-0.039176892,-0.021371027,0.018867956,-0.014715943,-0.04581101,-0.0073568323,0.0433674,0.09573001,-0.0060030627,-0.04191675,0.046880774,-0.015703455,0.022980273,-0.06784672,0.031500176,0.02372311,0.012356466,0.025611011,-0.0133601995,-0.009170935,-0.017417042,0.019873716,0.080373466,0.066914804,-0.022085765,-0.00027105372,0.035479512,0.04285479,-0.008313536,-0.02532676,0.032670274,0.07386385,-0.022500345,0.00062613876,-0.04725916,-0.09637705,0.066368416,-0.10313511,0.0069261473,0.059657548,-0.013314216,0.007956643,-0.021676725,-0.015511698,0.019456556,0.020216705,0.015972823,-0.0035714302,-0.07984009,0.013041623,-0.035281885,-0.038322087,0.014590317,0.007511559,-0.007067411,0.013166311,0.008511239,0.0011158574,0.012497974,-0.0031802547,0.008871748,0.0033553548,-0.018059969,-0.024237555,-0.006410751,-0.0012392902,-0.020187605,0.027174314,-0.045014683,0.04979459,-0.051371668,0.016113648,0.018878901,-0.06407617,0.0051149586,0.0811773,0.008694444,-0.031081825,-0.06173158,0.050687846,-0.0565827,-0.03030745,0.051420826,0.047269817,0.061058544,-0.026056807,-0.02782695,0.014878858,-0.016316196,-0.022003487,0.007456329,0.026499672,0.0046087857,0.0059574563,0.039387852,-0.035321534,0.046864912,-0.008714732,0.0028491896,0.044377763,0.03056833,-0.027328365,0.015617113,0.02648944,0.05386107,-0.016260993,-0.017360587,0.0038033416,0.04762533,-0.04815054,0.0066951006,0.039487295,0.004433409,-0.09325816,0.060684692,0.040494815,0.039142914,-0.01626538,-0.028671768,-0.029901205,0.011160918,0.037822526,0.0029867492,-0.055900794,0.028015848,-0.028049605,0.04531579,0.008677314,0.029149298,-0.02036728,0.043008585,0.03019994,-0.009806489,-0.030218035,0.01567485,-0.020012181,-0.022383487,-0.009467182,-0.07811717,-0.022040343,0.020154793,0.020006545,0.021483952,-0.055004053,0.05392637,-0.041925326,-0.007575187,0.054248188,-0.024048122,0.012794096,-0.015674148,0.03431685,-0.0120859705,0.0060464498,-0.0035942083,-0.007943588,0.012907194,0.0515269,-0.0024774664,-0.0036577878,0.027474834,0.05067616,0.005562936,-0.0066206953,-0.003043988,-0.07168352,0.0019496616,-0.03443822,0.049173776,0.031545464,-0.023463149,0.016882755,-0.026238142,-0.034675885,0.00031321388,-0.030867506,-0.005154335,0.014995791,0.031340197,-0.0040067984,0.03466518,0.007234961,-0.006017142,-0.019897953,0.082717955,0.04746508,0.021733966,-0.067525424,-0.039013807,0.0023308457,0.040836316,-0.04458379,-0.022621496,-0.013902863,-0.034973085,-0.013414778,0.013572085,0.020203382,0.054377273,0.036429957,0.07746855,-0.007476897,-0.03134634,-0.0003758069,0.028163133,0.016760126,-0.011415583,-0.06930331,0.029830687,0.06482147,0.052537877,0.0037143864,0.021713102,0.029934896,0.014196162,-0.04491281,0.07735032,0.07017668,-0.017531788,-0.0044585112,-0.009151244,-0.04839829,-0.008780454,-0.03976182,0.01322662,-0.022200631,0.024165563,-0.0030164553,0.013250088,0.011973747,-0.025417643,-0.0022275527,0.0035225828,0.026484726,0.03649846,0.018571263,-0.08754111,0.043963704,-0.027796585,0.02183069,0.016088331,-0.027344806,-0.061520953,0.05914489,-0.037535194,-0.013015683,0.006001157,0.012561636,-0.051133573,0.0064844484,-0.018634142,0.061769202,-0.08641705,-0.007751234,0.019209938,-0.023864659,0.08406629,0.049316622,-0.004811405,0.00037005587,0.03371463,-0.025206588,0.048003603,0.01850928,0.025702737,-0.038158733,0.07173359,0.006694758,-0.016986113,-0.011827659,-0.0692564,-0.023982717,-0.03329838,-0.022285977,-0.015136722,-0.03572626,0.037970185,0.031102916,0.016319372,0.040213488,0.019697022,-0.01859816,0.04332884,0.03325465,-0.0230913,0.038401753,0.00695722,0.0074338443,0.024204938,0.038188573,0.036278967,0.03366142,0.0007707668,0.0063818726,0.012423824,0.032191988,-0.042038195,-0.035961278,0.018588265,-0.02775669,0.005558029,-0.005468759,-0.0016092611,0.02267963,0.036802728,0.022028217,-0.027712086,-0.06747603,-0.028972935,-0.025659978,-0.035537038,0.019239057,-0.014715647,0.023304438,0.0143597005,-0.017744467,0.085479654,-0.11807142,-0.02688569,-0.058829848,-0.0016761164,-0.008461151,-0.03253297,-0.011730854,-0.00881753,0.013846664,-0.069212966,0.07366047,0.019998822,-0.036616158,0.029468888,-0.032738034,-0.012478444,-0.013095847,-0.0031540936,-0.0026762448,-0.0037621325,0.014046372,-0.010424491,0.05605314,-0.024228977,-0.011183855,0.024894435,-0.018810593,-0.04805081,-0.04224895,0.008357337,0.048802968,-0.032165587,0.0047672754,0.019844498,0.01355581,-0.010280452,0.009329664,-0.019046254,-0.011186615,0.029511152,-0.0020577882,0.08072896,-0.010087214,-0.04640482,0.02773643,0.020628553,-0.020997353,0.04305696,0.016505992,0.069760166,0.044114366,0.019730458,0.019517543,-0.059330188,-0.009772212,-0.015006792,0.02320148,-0.03646832,0.09647933,0.057018157,0.002370925,-0.016218882,-0.04463758,-0.007842978,0.00036172668,0.052562743,0.017318966,-0.02292667,-0.010977974,-0.012562677,-0.015410728,-0.017026888,-0.043363377,0.048094258,0.04589128,-0.035112843,-0.045139935,-0.062086668,-0.013268782,0.03325189,0.008976333,0.008491463,0.055938788,-0.021205325,0.0138273155,0.029626641,-0.025480956,-0.04397687,-0.011688694,0.021292076,0.021922182,0.017222462,0.007838246,0.0030054692,-0.09361261,0.023596035,0.0015820237,-0.068432264,0.007541023,-0.014752143,-0.018748198,-0.017800966,0.057613887,0.00651273,0.0065599116,0.014602605,0.03472274,0.04201966,-0.00020573713,0.0028573368,-0.05328755,0.011500883,-0.022262493,-0.02233567,0.0034461496,0.04184853,0.03787972,8.7311826e-05,0.01636343,0.023201384,0.029526684,0.020113075,-0.00703228,0.06542169,-0.012729433,0.014604878,-0.0066214195,-0.016506137,0.07835886,0.0047245724,0.023770906,0.0057682265,0.0014955368,-0.019512085,0.024831332,-0.0029446573,-0.009794611,-0.05413156,0.014300904,0.037070584,0.043832753,0.024869924,0.01896974,-0.034572273,-0.021546211,0.021217722,-0.020707937,0.07313591,0.03500114,-0.023066252,-0.03611815,0.018772332,-0.035344318,0.060813714,-0.011068617,0.004581993,-0.02894744,-0.0049125906]	Keywords: Transformer, attention module, low-rank self-attention, depth-wise convolution, mobile devices\nKey Objects: Transformer architecture, attention modules, two-branch structure\nRefers to Images: None\nHypothetical Questions:\n- What is the primary motivation for adapting the Transformer architecture?\n- How does the two-branch structure improve the Transformer's efficiency?\n- What are the advantages of using depth-wise convolution in the alternative architecture?\n---\nSummary:\nResearchers have explored adapting the Transformer architecture for lightweight designs by making modifications at a higher level, such as replacing attention modules with two-branch structures.\nOriginal Text:\n### 6.1 Adapting Transformer to Be Lightweight  \nApart from the efforts made at the module level to alleviate computation overheads, there are several attempts to adapt Transformer to be lightweight by modifications at a higher level.  \nSimilar to low-rank self-attention [45] that decomposes attention into a locality-constrained attention and a low-rank global attention, Lite Transformer [148] proposes to replace each attention module in Transformer with a two-branch structure, where one branch uses attention to capture long-range contexts while the other branch uses depth-wise convolution and linear layers to capture local dependencies. The architecture is lightweight both in terms of model size and computation, and is thus more suitable for mobile devices.\nContextualized Text:\nTo alleviate computation overheads, several attempts have been made to adapt the Transformer architecture for lightweight designs.  One such approach, similar to low-rank self-attention, involves replacing each attention module in the Transformer with a two-branch structure. One branch uses attention to capture long-range contexts, while the other utilizes depth-wise convolution and linear layers to capture local dependencies, making the resulting architecture suitable for mobile devices.	{"tags": ["architecture", "optimization", "NLP", "transformer"], "doc_id": "7c8cb13d-c699-46b2-aba7-20f16e77bb50", "summary": "Researchers have explored adapting the Transformer architecture for lightweight designs by making modifications at a higher level, such as replacing attention modules with two-branch structures.", "doc_type": "text", "entities": ["Lite Transformer"], "keywords": ["Transformer", "attention module", "low-rank self-attention", "depth-wise convolution", "mobile devices"], "key_objects": ["Transformer architecture", "attention modules", "two-branch structure"], "contextual_text": "To alleviate computation overheads, several attempts have been made to adapt the Transformer architecture for lightweight designs.  One such approach, similar to low-rank self-attention, involves replacing each attention module in the Transformer with a two-branch structure. One branch uses attention to capture long-range contexts, while the other utilizes depth-wise convolution and linear layers to capture local dependencies, making the resulting architecture suitable for mobile devices.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.1 Adapting Transformer to Be Lightweight"}, "hypothetical_questions": ["What is the primary motivation for adapting the Transformer architecture?", "How does the two-branch structure improve the Transformer's efficiency?", "What are the advantages of using depth-wise convolution in the alternative architecture?"]}
dfdac0d1-68e9-4b51-bd9f-175a427c6634	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.053536657,0.019698136,0.036444712,0.071425796,-0.0061407955,-0.0043792715,-0.04017822,0.03530676,0.033936772,-0.016154915,-0.016185502,0.04348721,0.02480874,-0.014173855,-0.055786856,0.031559456,0.042136583,0.0607119,0.013327515,-0.021314248,0.055039644,0.0055688103,0.03548529,-0.004989098,0.023281578,0.041260056,0.04140457,-0.0028642595,-0.0068055103,0.04632674,0.01591961,-0.0378458,0.010738328,0.012831544,0.018893495,-0.023592487,0.013043007,-0.024458531,0.012428847,0.023015223,0.012364552,0.052085582,-0.041970428,-0.015570039,-0.0028294318,-0.053245254,-0.047129292,0.015618374,0.049441736,-0.00925715,-0.04614209,0.028020274,-0.055791833,-0.0014648965,-0.018795276,-0.029784635,-0.031597428,-0.03459538,-0.0071689133,-0.06147982,-0.036032733,0.022511775,-0.0014358315,-0.023442492,0.017213695,-0.00025294142,0.027199902,0.014868151,0.04284753,0.15064798,-0.008149474,-0.011949902,-0.031747654,-0.04357526,0.11054229,0.071304,-0.024406279,-0.0018884442,-0.0572129,-0.038187042,-0.041198075,0.02072595,-0.0286207,-0.05913607,0.06129691,-0.012739977,-0.029393697,-0.017971257,0.0783951,-0.023804829,-0.010736889,0.072692625,0.015322782,0.048132755,-0.038610704,-0.06583725,-0.023913303,-0.044305533,0.0068215705,-0.032185376,0.04092422,-0.03424426,0.07876564,0.124446906,0.03742955,-0.03008761,-0.030602485,-0.0232103,-0.012569812,0.05128338,-0.06399485,0.024228374,-0.065140195,0.046998497,-0.025805369,-0.031333264,-0.029878736,-0.0070472457,0.014887677,-0.0040761456,0.036865316,-0.018334858,-0.008067242,-0.0027219905,0.06884586,0.040344816,0.017659727,0.0035214748,-0.022528827,-0.008101841,0.059718303,-0.0016066894,-0.015033222,-0.0103021925,0.0028100964,-0.029116372,0.026204554,0.027221037,-0.009870441,-0.008653089,0.023755746,-0.05902807,-0.046592284,0.016400311,-0.011020867,0.075412236,-0.08220244,0.060255148,-0.02797843,0.043507446,0.008427833,0.013010011,0.053865917,-0.002519497,-0.0036295475,0.020867158,-0.0029159787,0.00022036965,-0.02060025,-0.058219537,-0.053940877,-0.03685837,0.0128084095,0.08514304,0.018642474,0.08516852,-0.012238006,0.008084071,0.021477567,-0.00748482,-0.03269308,-0.01773932,-0.04091131,-0.04216783,0.03681498,0.0043325596,0.05213835,-0.018560026,0.014676591,0.012642412,-0.011430465,0.02031319,-0.025515681,0.0134734465,-0.017095719,0.022689935,-0.0053641903,0.03875525,-0.030620601,-0.063531324,0.030298954,-0.039510183,-0.03917353,-0.012716177,0.015463915,0.04817381,0.050911702,-0.045783456,-0.08462178,0.0041609067,-0.0022297935,-0.02463237,-0.014314623,-0.047387436,0.049013313,0.007790316,0.0120211905,-0.038321875,-0.02107285,0.013122905,0.013733781,-0.017686596,0.00469922,0.028972572,0.019141098,0.027263846,0.024908096,-0.0017416612,0.012084429,-0.018757464,-0.08140745,-0.01278277,-0.056366544,0.057660833,-0.02905385,0.03921487,-0.07588034,0.02456321,0.04808281,-0.035692886,-0.011776329,-0.0070477384,-0.031304725,0.007963189,0.0050340192,0.055464584,0.04926741,0.009592629,-0.0052614915,0.048699755,0.046485234,0.04563039,-0.07789288,0.030941859,0.0002464488,-0.031174676,-0.028649578,0.030666377,-0.0024499872,0.022325803,-0.032365885,0.06509624,0.013694686,-0.0046208426,0.004332075,-0.015620191,0.018861042,-0.008775564,-0.035301488,0.022811508,0.009222383,-0.01977593,-0.0006209627,-0.024579892,0.0014036697,0.0068527954,0.022516208,0.013619883,-0.00629021,-0.027939042,0.0065816683,-0.004049351,0.02787859,-0.0031411366,0.020404212,0.043153953,-0.006973359,-0.0039348695,0.03450901,-0.060937393,0.013530287,0.0039939242,0.0012400772,0.0063458458,0.049454253,0.008389603,0.03789265,-0.005296862,-0.067902185,-0.022340558,0.01825691,-0.03774983,0.02541692,-0.0029395865,-0.021148402,-0.009033045,-0.015431966,-0.04141666,-0.020435207,0.0076152836,0.10614551,-0.035991255,-0.040942077,-0.0010208036,0.023476014,0.03540887,-0.034003347,0.04408736,0.03360355,0.037928477,0.03823157,-0.047738656,-0.025070358,0.013995534,0.023680106,0.045490317,0.04302035,-0.01984924,0.00055462687,0.04627375,0.046215504,-0.03821989,-0.015687004,0.028249314,0.015744893,-0.052575774,0.045754768,0.002907217,-0.03391276,0.06683823,-0.07243099,0.04730571,0.06368684,0.027702576,-0.02008967,0.0006353758,0.013196716,0.017913181,0.031066757,0.007195086,0.010783633,-0.05453858,-0.012275491,0.028110133,0.0031530045,0.02561779,-0.0350806,-0.0052957656,0.01805845,-0.009861278,-0.0077535748,0.04229432,0.016885804,0.03951373,0.007723407,-0.032815315,0.010681021,0.028963171,0.027929008,0.005091096,0.048594553,-0.07719944,0.047293164,-0.0366532,-0.0067108585,-0.014573942,-0.049215738,0.043945625,0.020189209,0.004674304,-0.05039521,-0.030996436,0.08997252,-0.042019676,0.031203317,0.030634973,0.023159612,0.05573833,-0.027151123,-0.027840756,0.013453486,0.00057455315,-0.02163689,-0.05233108,-0.002181762,-0.048560206,0.013619495,0.10200547,0.0074028615,0.0545054,-0.030627415,-0.013602017,0.058352023,0.022663992,-0.0444431,-0.015601516,0.035679273,0.06929384,-0.023311144,-0.01065311,0.018701203,0.036872182,-0.055983197,0.016131725,0.042211812,0.016807377,-0.052004617,0.03913779,0.0072803125,0.0076117343,0.048566084,-0.049188,-0.036516197,0.022727583,0.0059423787,0.006341547,-0.0054662507,0.06738543,-0.025018563,-0.0022246714,-0.026173588,-0.0040248353,0.007936337,0.026577635,0.051948242,3.9142014e-05,-0.018410312,-0.008898646,0.013979598,-0.021248374,-0.0029477107,-0.049564514,-0.004476095,0.05061726,0.026049884,-0.0014090925,-0.07487636,0.06670476,-0.059564672,0.019366859,0.084703594,-0.010741699,0.030927025,-0.010604165,0.007105409,0.005407873,-0.009413563,0.026628999,-0.011518214,-0.027351478,0.020108005,0.011685198,0.0543128,0.004137051,0.056046996,-0.0049379407,0.027961928,0.009667862,-0.07177789,0.010752038,0.017625723,0.04097465,0.039224055,0.019972574,0.019542385,-0.0052100183,-0.011308895,-0.021717157,-0.046378065,0.018247025,0.013527399,0.03438805,0.0007574029,0.07031929,0.010631263,-0.0043410845,-0.031266037,0.07916543,-0.003465825,-0.00748374,-0.05672599,-0.017345922,0.008037044,0.053462856,-0.023016682,0.004954138,0.00085328077,-0.027742928,-0.028346293,-0.0173597,0.002768182,0.011700803,0.039145958,0.02912963,0.04989619,-0.038489275,-0.0075834626,-0.02026495,0.031975027,-0.03791541,0.012577941,-0.022672575,0.06377013,0.060445137,-0.023184959,0.0099937385,0.01854968,0.024468472,-0.048729584,0.039026584,-0.019652467,0.017940158,0.022486322,0.041683473,-0.041963093,-0.007233231,0.023660406,-0.007913691,0.018691657,0.03212479,0.022724431,-0.007444706,0.035494838,0.0014579843,-0.012766867,-0.02374271,0.036447253,0.019356575,0.04881268,-0.07550333,0.022858385,-0.06306425,-0.043539856,-0.012622137,-0.018599892,-0.032968085,0.023444686,0.0017588024,-0.006378028,0.01304759,0.0044249473,-0.02862576,-0.033013202,0.010138988,0.06616897,-0.06277922,-0.007682777,0.04511985,0.024532985,0.07663475,0.015299957,-0.0012137411,-0.005968237,0.013259531,0.00089749834,0.042664435,0.02603509,0.01655352,-0.022312677,0.053064995,0.05280374,-0.041520726,-0.027790029,-0.058049724,-0.05173394,-0.019925779,0.022550087,-0.058522787,-0.018028606,0.031125741,-0.0076776957,-0.0013472453,-0.01842945,0.04309785,-0.088711806,0.08375342,0.01686149,0.008873458,0.055655625,0.021131774,-0.015386313,0.020389512,0.040255554,0.020591453,0.007124837,0.058487944,0.0127359,-0.034382988,0.0153372465,0.0016300614,0.007836287,-0.0740909,0.028038409,0.034840144,0.056124937,-0.032190245,0.07502612,0.04827148,0.058249224,-0.01709811,-0.049576603,-0.04301098,-0.014427595,-0.049389534,0.015190395,0.0367923,0.036380585,0.045472555,-0.0403653,0.060795564,-0.09199141,-0.020430451,-0.010935451,0.025601966,0.022228837,-0.026022905,0.014830738,-0.020687401,-0.007737106,-0.07258448,0.015847389,0.04320772,-0.0018084658,0.025578754,0.012579547,-0.0006002679,0.024015604,0.03735801,-0.027984543,0.000688688,0.007231768,-0.0061631757,0.039936833,-0.01576555,-0.022996929,-0.0032100396,-0.019540956,-0.037602663,-0.015755855,-0.0027484598,0.03990087,0.014745305,0.029318329,0.009472659,0.0059261653,-0.06493813,0.002918941,-0.016806936,-0.042305544,-0.009149673,-0.045977082,0.01614812,0.020500746,-0.041019503,0.003907422,0.012476858,-0.0319448,0.05475721,0.044409193,0.01447614,0.03928384,0.0364592,-0.042513236,-0.06986325,-0.048142605,-0.023012996,0.010285726,-0.06304559,0.02583664,0.014210114,0.036162667,-0.052312143,0.005683404,-0.018651295,-0.026475517,-0.02136799,0.01123113,-0.02183155,-0.0187666,-0.018397601,-0.00045831103,-0.014078445,-0.03935394,0.044910666,0.044025708,-0.026198588,-0.06542684,-0.023556631,0.006792317,0.0122631965,0.002490401,0.023457052,-0.00996557,-0.026181739,0.0050391606,0.076384194,0.034014136,-0.066486515,-0.04315394,0.029284766,0.029771842,0.05806524,-0.012201562,-0.012338663,-0.103590794,0.052824747,-0.034494553,-0.054745812,0.06649192,0.01913199,-0.02615206,-0.015874747,0.012424985,0.006313073,-0.004662161,-0.011289391,0.022904959,0.008577217,-0.004536152,-0.0070477827,-0.011746441,-0.03005358,-0.007204787,-0.021824155,-0.027420042,0.06744065,-0.04618651,-0.012624455,-0.015312446,-0.005051259,0.05200934,0.015959958,-0.025615752,0.03738067,-0.024291912,0.004247754,-0.0058374186,0.019012403,0.095887065,-5.7708818e-05,0.018768622,-0.03553455,0.023287795,-0.014436976,-0.01845583,-0.045539457,-0.033241186,-0.054063056,-0.0037496872,0.012960662,0.07110821,0.036979422,-0.015684193,-0.030337092,-0.04540389,0.002026828,-0.007567531,0.03393771,0.017989792,0.033377472,-0.05849969,0.016964208,-0.018783174,0.09322695,-0.027319072,-0.018500118,-0.012883607,-0.0062552067]	Keywords: funnel-like encoder, pooling, up-sampling, FLOPs, memory\nKey Objects: encoder architecture, hidden sequence\nRefers to Images: None\nHypothetical Questions:\n- How does pooling along the sequence dimension reduce computational complexity?\n- What is the benefit of reconstructing the hidden sequence after pooling?\n- In what scenarios would using a Funnel Transformer be advantageous over a vanilla Transformer?\n---\nSummary:\nFunnel Transformer reduces computational cost and memory usage by employing a funnel-like encoder architecture that progressively shortens the hidden sequence using pooling and then reconstructs it with up-sampling.\nOriginal Text:\nFunnel Transformer [23] utilizes a funnel-like encoder architecture where the length of the hidden sequence is gradually reduced using pooling along the sequence dimension, and then recovered using up-sampling. The architecture effectively reduces the FLOPs and memory compared to the vanilla Transformer encoder. Naturally, one can use this architecture to build a deeper or wider model using the same computation resources.\nContextualized Text:\nTo create lightweight Transformer models, Funnel Transformer [23] introduces a funnel-like encoder architecture. This design gradually reduces the length of the hidden sequence through pooling operations along the sequence dimension and then recovers the full sequence using up-sampling. This technique effectively reduces the number of floating-point operations (FLOPs) and memory requirements compared to a standard Transformer encoder, allowing for the creation of deeper or wider models without increasing computational resources.	{"tags": ["architecture", "optimization", "NLP", "transformer"], "doc_id": "dfdac0d1-68e9-4b51-bd9f-175a427c6634", "summary": "Funnel Transformer reduces computational cost and memory usage by employing a funnel-like encoder architecture that progressively shortens the hidden sequence using pooling and then reconstructs it with up-sampling.", "doc_type": "text", "entities": ["Funnel Transformer"], "keywords": ["funnel-like encoder", "pooling", "up-sampling", "FLOPs", "memory"], "key_objects": ["encoder architecture", "hidden sequence"], "contextual_text": "To create lightweight Transformer models, Funnel Transformer [23] introduces a funnel-like encoder architecture. This design gradually reduces the length of the hidden sequence through pooling operations along the sequence dimension and then recovers the full sequence using up-sampling. This technique effectively reduces the number of floating-point operations (FLOPs) and memory requirements compared to a standard Transformer encoder, allowing for the creation of deeper or wider models without increasing computational resources.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.1 Adapting Transformer to Be Lightweight"}, "hypothetical_questions": ["How does pooling along the sequence dimension reduce computational complexity?", "What is the benefit of reconstructing the hidden sequence after pooling?", "In what scenarios would using a Funnel Transformer be advantageous over a vanilla Transformer?"]}
70d5b7e8-8d15-4120-a802-35c77c10b00b	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.035018988,0.038246002,0.032131217,0.02734643,-0.016094929,0.009675595,0.009838756,0.065843165,-0.029775297,-0.027265029,0.0073566954,0.05806921,0.009887031,0.019121187,-0.018233605,0.014930078,0.03980441,0.103958875,0.014326807,-0.032337323,0.04459061,0.005289649,0.035848998,0.031464983,0.014764491,0.0413952,0.06687303,0.04663538,-0.024254978,0.00047302703,-0.0025148266,-0.06563952,0.032469187,-0.0022913904,0.03319487,-0.026974192,-0.02633832,-0.035752825,-0.0045527443,-0.033432953,-0.008525433,0.051096164,-0.05274215,-0.015999502,0.009421327,-0.02053299,-0.11940839,0.0089816805,0.011993415,-0.020667307,-0.041579407,0.055947993,0.024329811,0.02194068,-0.039286718,-0.017275803,-0.00090197945,-0.002730517,-0.023584602,-0.08389053,0.030547315,-0.0012327249,0.011143847,0.00020849261,-0.0025890367,0.02363601,-0.024039999,0.029267883,0.080939665,0.11522373,-0.026715143,-0.013267926,-0.019558793,-0.0117432345,0.09165808,0.06699166,-0.0133395735,-0.0439535,-0.062401544,-0.023354711,-0.04338007,0.09060048,-0.038384303,-0.018592969,0.08547504,0.016994998,-0.0055646095,0.018379586,0.06952429,-0.0006633848,-0.022292972,0.022027615,-0.004997712,0.03639059,-0.04998491,-0.08480306,-0.03740777,-0.02405317,0.028420908,-0.052415006,0.040867195,0.0275092,0.09414154,0.08138007,0.03770081,-0.04212429,-0.005860008,-0.019678723,-0.013575186,0.027700895,-0.05320194,0.0285755,-0.03602404,0.0152000515,-0.0042469804,-0.011173116,-0.019471008,0.011705518,0.010478888,0.011000031,-0.0013143738,-0.011625019,-0.005441382,0.019511903,0.07194725,0.01924234,0.009537007,-0.047798257,-0.05460333,0.027381163,0.07941271,-0.024867684,-0.0047611375,0.04241237,-0.03230487,0.03208324,0.044436272,0.05657607,-0.003669538,0.06619062,0.003645202,-0.023260266,-0.04734032,0.002112822,-0.0029723798,0.031211337,-0.07155536,0.007919751,-0.081486225,0.05963937,0.044744972,-0.0059290957,0.040071663,0.034824643,-0.03273874,0.005116115,-0.02742711,-0.0014029884,-0.052262682,-0.028974926,-0.06666956,-0.061577994,-0.020587552,0.032829415,0.04559345,0.06779726,-0.027860753,0.04010577,0.024672702,0.040487852,-0.028316157,-0.012918276,-0.008138391,-0.03670972,0.012353251,0.019414913,0.01535841,-0.031873252,-0.023023179,0.01519543,0.008657569,0.065074556,0.010081512,0.058990255,-0.0014116867,0.022868667,0.014597162,0.020388434,-0.008256446,-0.032200254,-0.007934048,-0.009658066,-0.024559656,-0.021918504,0.061012007,0.017905932,0.05273742,-0.061476685,-0.024677772,-0.027768275,-0.00012129978,0.012299793,0.009216078,-0.005703827,0.051566176,-0.02635818,0.008630064,-0.022802344,-0.010794384,0.050957866,0.00016022952,-0.03518028,-0.0009992808,0.07234757,-0.0017656356,-0.005465189,-0.037407253,0.036409214,0.039638095,-0.023095846,-0.05291969,-0.007317605,-0.025054444,-0.007716794,-0.041666392,0.03205242,-0.023020813,0.001921094,0.019417606,-0.02291693,0.014621925,-0.0016763867,-0.030730668,0.0064026345,-0.031996183,0.018096378,-0.006061356,0.016420538,-0.035864063,0.03080079,0.065163836,0.051764045,0.016101087,0.07138508,0.051573306,0.02306599,-0.019759476,0.051196743,0.011873368,-0.0129666235,0.012520868,0.052655946,0.03626782,0.02710031,-0.033624236,-0.019127114,-0.009695832,0.006112326,0.00022708552,-0.016031835,-0.023178624,-0.027471742,0.03888709,0.0024131418,0.013487588,0.0029687993,0.018242447,0.032380145,-0.06931258,-0.0017178474,0.02379112,0.06574336,0.018028263,-0.0329327,0.012399613,-0.009645278,-0.006795778,-0.030697148,0.050486524,-0.0562607,0.031835776,0.0075432556,0.028320955,-0.006369185,0.0074099796,0.0011101385,0.011128512,-0.012663378,-0.018670335,-0.042490315,-0.009583049,0.00029561168,0.012712688,-0.021189256,0.0094312765,0.011508006,-0.012494432,-0.023509406,0.014151381,-0.015006074,0.13579874,-0.063094355,-0.03474344,0.045092445,-0.05449837,0.025401378,-0.046470024,0.057689354,0.03545579,0.00502026,0.006573577,0.008236347,-0.03288563,-0.01132926,0.029526673,0.095993094,0.06857015,-0.06560568,-0.0127221225,0.0072385687,0.026299927,-0.031420805,-0.035671424,0.04215094,0.02338571,-0.046242908,0.015158921,0.005326868,-0.055764254,0.09382295,-0.04365229,-0.009640714,0.045781326,-0.008038464,0.01417063,0.0037581527,0.006568032,0.014016127,-0.014711122,-0.026361255,-0.014529271,-0.07949518,-0.00015974765,0.024631018,-0.01861481,-0.0014505624,0.012445022,-0.029771794,0.0420398,0.015414532,-0.002089271,0.018332152,0.014466422,0.0014149369,-0.004200413,-0.027040266,-0.013677687,0.0006475282,0.0017893332,-0.026046555,0.066521324,-0.030461567,-0.011578166,0.002779543,0.03257187,-0.032880567,-0.04467655,0.018248005,0.059202686,-0.006421221,-0.047651917,-0.05431053,0.03653239,0.016501136,-0.020481974,0.027996173,0.09240288,0.03425741,-0.017173477,-0.017573725,0.0005818769,0.01769104,0.0066168928,-0.018687684,0.018518891,0.002465545,-0.005049527,0.06734444,0.008792841,0.051027037,-0.020573555,0.021496294,-0.014628818,0.022815736,-0.06522304,-0.002226607,0.007166916,0.04341107,-0.0065588783,-0.02136792,-0.0076824017,0.021979757,-0.053563725,0.048204534,0.02419099,-0.026404422,-0.11361929,0.06282163,0.05828871,0.014594372,0.011800737,-0.05502442,-0.010438018,0.02971663,-0.013444042,-0.030231567,-0.038060114,0.014709545,-0.018361935,-0.015650153,0.033384155,0.0067710383,-0.014335937,0.04959167,0.025590438,-0.016761718,-0.046532474,0.03534288,-0.012135652,-0.057597395,-0.0080160275,-0.07849143,0.022596385,0.070183285,-0.00089285674,-0.023302332,-0.0756439,0.06396173,-0.043846678,0.02075992,0.07628943,0.013257674,0.019483952,0.031183153,-0.021013075,-0.058245968,0.0031681757,0.031991698,-0.008777357,-0.020134661,0.029149858,0.018508658,0.015941527,0.044038642,0.04085607,0.005365078,4.7259186e-05,-0.056459863,-0.056352224,-0.0071560447,-0.007067871,0.025224278,0.050387196,-0.039190445,0.018245917,-0.007219607,-0.0036105616,-0.048838314,-0.005135168,0.011652016,0.008917814,0.0024650237,0.019855263,0.009667869,0.010356856,-0.05162264,-0.007363686,0.069541074,0.030830229,-0.043007616,-0.062841766,-0.0053332043,0.03944659,0.0338868,-0.026789943,0.017941251,-0.028722955,0.0074221836,-0.004054684,0.012994181,0.0075732446,0.082236305,0.05251857,0.02802477,0.04779111,-0.00825342,0.022528023,0.00919639,0.016765377,-0.025845261,-0.041446585,-0.047801625,0.0722059,0.027456881,-0.010664617,-0.017464798,0.0173148,0.009630999,-0.019906485,0.026795855,0.007539247,-0.035806086,0.05533642,-0.030336766,-0.021143936,-0.003648896,-0.004746567,-0.0069993716,-0.020780394,0.03865388,-0.021017263,0.024636418,0.018163102,0.02901128,0.017110653,-0.004393348,0.06488608,0.034975894,0.041889906,-0.043085176,0.009138557,-0.021777451,-0.029691715,0.030583318,-0.036933918,-0.08162963,0.02519256,-0.033068474,0.043344103,0.0043604034,-0.0277138,-0.044411454,-0.014147156,-0.004507458,0.06893757,-0.045679826,0.042803694,0.0042935023,-0.043125957,0.08393108,0.005534804,-0.0069269617,-0.0061241295,-0.006886401,-0.018611621,0.020975724,0.039137557,0.03364792,-0.031579282,0.06702979,0.026068194,-0.026557505,-0.026597543,-0.049196087,-0.040380307,-0.032916725,-0.05031124,-0.057817448,-0.0025845338,0.0531003,-0.012171903,0.0310854,0.0032432554,0.021978639,-0.041222304,0.03163808,-0.007957174,-0.024032917,0.034813207,-0.0017438335,0.058483932,0.031794626,0.0360343,0.029334709,-0.030477777,0.014274231,0.03541986,-0.04716172,-0.0012708696,-0.030065015,-0.03144311,-0.025572356,0.00047600755,0.02347766,0.020119889,-0.01953053,0.051524688,0.058617506,0.041754603,-0.028471308,-0.08380634,-0.03807851,-0.025898425,-0.035047643,-0.013355898,0.02999673,-0.0011021103,0.016717928,-0.010245391,0.05298956,-0.07774234,-0.015495299,-0.010265841,0.02157581,0.034257617,-0.027522417,0.033446766,0.024278857,-0.0006993815,-0.038986757,0.03856943,0.044769548,0.008730863,-0.00088115665,-0.017253254,-0.029152168,-0.033164836,0.037415057,-0.008763489,0.010078493,0.016445335,-0.019795887,0.03095142,-0.0077781673,0.04159009,-0.017087143,0.0018398975,-0.103091,-0.032349844,0.02444252,0.04211975,-0.0047522443,0.022718962,0.034942944,-0.0024719716,-0.0177055,0.008366157,-0.036253646,-0.013948252,-0.021728098,0.046995923,0.047726147,-0.045768462,-0.065225594,-0.020934826,-0.0008460204,-0.008514995,0.050571717,-0.01890519,0.060155377,0.016411768,0.054304756,0.01847272,-0.10439847,-0.03977665,-0.050903287,-0.023143595,-0.025574658,0.075413235,-0.0054832776,0.04669966,-0.01931775,0.0066221883,0.021210885,0.013750699,0.012568906,-0.004599483,-0.031606127,0.0028212622,-0.028872551,-0.01865088,-0.008873344,-0.03672312,0.067502946,0.03501804,-0.051488053,-0.022593617,-0.054495335,0.01990332,0.007940812,0.014181074,-0.011443149,-0.013113899,0.022209028,0.0023996404,0.058919102,-0.008935709,-0.029014282,-0.05750953,0.031319376,0.017051106,-0.016762508,-0.020931594,0.0062076002,-0.07959333,0.0011425073,-0.01825129,-0.04710656,-0.0076717,0.0076728864,-0.031374134,-0.028719822,0.023268411,-0.007098409,0.0255601,-0.004141381,-0.0131418435,0.017858358,-0.03855381,0.029194016,0.0047493186,-0.0027284583,-0.002380387,-0.0023776407,-0.015151897,0.03518881,0.047938354,0.0138144335,0.034698922,-0.01936042,0.003954934,0.003689747,0.017791864,0.055259164,-0.0059651504,-0.016496465,0.022614432,0.023512913,0.08264446,0.026394676,0.01614688,-0.005354322,0.027329372,0.023690725,0.015552142,-0.017074056,-0.047847353,-0.057220113,0.048809335,-0.013207057,0.04369411,0.014978092,0.004855274,-0.048040602,-0.025078593,-0.010494075,-0.023644513,0.075568415,0.0544625,0.024006464,-0.06892002,0.022886304,0.01718636,0.045982726,-0.01549402,0.041620143,0.018303003,-0.030451143]	Keywords: DeLight, Transformer block, self-attention, FFN, block-wise scaling\nKey Objects: DeLight block, Transformer block, FFN\nRefers to Images: None\nHypothetical Questions:\n- How does the 'expand-and-reduce' DeLight transformation module contribute to learning wider representations?\n- What is the purpose of the block-wise scaling strategy in DeLight?\n- In what ways does the DeLight block differ from a standard Transformer block in terms of its structure and operation?\n---\nSummary:\nDeLight [91] introduces a modified Transformer block, the DeLight block, that utilizes three sub-modules and a block-wise scaling strategy to achieve a deeper network with fewer parameters and operations.\nOriginal Text:\nDeLight [91] replaces the standard Transformer block with DeLight block, which consists of three sub-modules: (1) a expand-and-reduce" DeLight transformation module to learn wider representations with low computation requirements; (2) a single-head self-attention to learn pairwise interaction; (3) a lightweight "reduce-and-expand" FFN (as opposed to vanilla Transformer that first expands the dimension of hidden representations and then reduces them back to D$\\_{m}$ ). They also propose a block-wise scaling strategy that allows for shallow and narrower blocks near the input and wider and deeper blocks near the output. The induced network is much deeper than the vanilla Transformer but with fewer parameters and operations.\nContextualized Text:\nTo reduce computational overhead in Transformer models, DeLight [91] replaces the standard Transformer block with a DeLight block. This block comprises three sub-modules: (1) an expand-and-reduce DeLight transformation module for learning wider representations; (2) a single-head self-attention mechanism to capture pairwise interactions; and (3) a lightweight reduce-and-expand Feed Forward Network (FFN), differing from the standard Transformer which expands and then reduces dimensions back to Dm. A block-wise scaling strategy is also implemented, allowing for shallow and narrow blocks near the input and wider, deeper blocks near the output, resulting in a deeper network with fewer parameters and operations.	{"tags": ["architecture", "optimization", "transformer"], "doc_id": "70d5b7e8-8d15-4120-a802-35c77c10b00b", "summary": "DeLight [91] introduces a modified Transformer block, the DeLight block, that utilizes three sub-modules and a block-wise scaling strategy to achieve a deeper network with fewer parameters and operations.", "doc_type": "text", "entities": ["DeLight"], "keywords": ["DeLight", "Transformer block", "self-attention", "FFN", "block-wise scaling"], "key_objects": ["DeLight block", "Transformer block", "FFN"], "contextual_text": "To reduce computational overhead in Transformer models, DeLight [91] replaces the standard Transformer block with a DeLight block. This block comprises three sub-modules: (1) an expand-and-reduce DeLight transformation module for learning wider representations; (2) a single-head self-attention mechanism to capture pairwise interactions; and (3) a lightweight reduce-and-expand Feed Forward Network (FFN), differing from the standard Transformer which expands and then reduces dimensions back to Dm. A block-wise scaling strategy is also implemented, allowing for shallow and narrow blocks near the input and wider, deeper blocks near the output, resulting in a deeper network with fewer parameters and operations.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.1 Adapting Transformer to Be Lightweight"}, "hypothetical_questions": ["How does the 'expand-and-reduce' DeLight transformation module contribute to learning wider representations?", "What is the purpose of the block-wise scaling strategy in DeLight?", "In what ways does the DeLight block differ from a standard Transformer block in terms of its structure and operation?"]}
b6117642-2ad3-4639-bd15-9d6553f87e03	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.029163457,0.017056383,0.006882994,0.04260447,-0.045736704,0.038377713,0.010083395,0.043977637,0.025931126,-0.00979943,-0.016889391,0.028352376,0.01839637,-0.012381462,-0.019824324,0.028021744,0.009539763,0.09318226,-0.008473387,-0.030340938,0.06886937,0.008019587,0.027034983,0.0015423482,0.0007854978,0.007527693,0.07037877,-0.004563158,-0.011642083,0.008160707,0.017250795,-0.05550595,0.043006975,0.008259101,0.0073491796,0.00701447,0.019451816,-0.047351275,0.009284863,-0.014251471,0.0062019224,0.022145396,-0.081197344,-0.012859904,0.02040844,-0.048776064,-0.07511994,-0.033553082,-0.019269068,-0.004041104,-0.026282778,0.038688146,-0.018630052,0.0112111885,-0.017887749,-0.01073845,-0.012605625,-0.027481282,-0.0077579906,-0.08618652,-0.015843185,-0.028885113,-0.0338915,0.0026101603,-0.0019334039,0.011210376,-0.012155482,0.057865385,0.060245905,0.14215453,0.041555252,-0.0044599473,-0.008485546,-0.064515375,0.11632424,0.05089285,-0.054796487,-0.013143355,-0.067294076,-0.0310382,-0.0392731,0.114357874,-0.043694805,-0.028809618,0.088100374,0.012575814,-0.034262657,-0.022156833,0.07392974,-0.03956386,0.0015721057,0.025521597,0.0071422523,0.053132556,-0.030797774,-0.06583112,-0.016650537,-0.03938134,0.011246004,-0.06749627,0.023521863,-0.007638375,0.09025435,0.09850117,0.02663095,-0.009490113,0.020651652,-0.03317615,-0.012387507,-0.00094364665,-0.016858796,0.019529391,-0.014144861,0.024787726,-0.011180428,-0.008262535,-0.05636067,0.010022177,0.03007646,-0.004169723,0.041554306,-0.021677747,-0.0028942532,0.041413132,0.065073796,0.01734044,0.03683399,-0.01533991,-0.015825666,0.013525199,0.04231011,0.0035743301,-0.0039057832,0.024589345,-0.04094561,0.024210196,0.09830266,0.03471222,-0.011642446,0.039722256,-0.0036678615,-0.05934682,-0.048972066,0.016242718,0.0096470555,0.016626792,-0.04899243,0.01767268,-0.044846773,0.06844571,0.04068834,-0.014972551,0.030373001,0.02044061,-0.02008335,0.010132314,-0.021620784,-0.044136215,-0.034444533,-0.021971796,-0.018693399,-0.045566946,-0.038741622,0.06093193,0.044868544,0.09202488,0.022904787,0.031331185,0.027767885,0.015140746,-0.025827734,-0.011728952,0.012031906,-0.009519449,-0.016629511,0.012971539,-0.025460763,-0.024550483,0.015046708,-0.0041685067,0.023396425,0.014944679,-0.0011989049,0.046098482,0.0026556836,0.010390748,-0.017877178,0.049304586,0.020830298,-0.018819984,0.014148205,-0.006460401,0.0030471885,0.036949717,0.02335704,0.061012562,0.048572835,-0.06391434,-0.0028845386,-0.043370303,-0.020567384,-0.026501628,-0.018727006,-0.030861618,-0.000910469,0.008368721,-3.680152e-05,0.0050026714,-0.009965671,0.0394053,0.008895956,-0.05334179,-0.020595375,0.09653439,0.008545711,0.007531732,0.008083749,0.04525972,0.020787848,-0.028871182,-0.04344745,0.009287371,-0.048934605,-0.012166356,-0.019775076,0.0031916865,-0.049414415,0.012391943,0.037568845,-0.024512088,0.0417184,-0.0018266978,-0.043224134,0.06293636,-0.00073034357,0.0487189,0.03787043,0.040972732,0.012874033,0.05674226,0.0632313,0.03126411,-0.009785742,0.08165654,0.054172974,-0.009958496,-0.0023936522,0.07986849,0.0031025277,0.047851697,-0.013738134,0.05080892,0.044792864,0.0068285107,-0.0044387747,-0.032353215,0.005564629,-0.016158154,0.05876181,0.023401985,-0.03680952,-0.023749616,0.025840621,0.00030955012,0.013973903,-0.0034264587,0.017136076,0.0020382514,-0.036139168,-0.022031523,0.0512115,0.03752333,-0.010211116,-0.027008882,-0.017580073,0.008329477,0.021916527,-0.0021586625,0.05695286,-0.011151382,-0.027704908,0.014701121,0.043391548,0.010155106,0.034095503,-0.0037882035,0.05244374,-0.04076319,-0.053141724,-0.00056955207,-0.006506529,-0.040747367,0.030213214,0.010426043,-0.025109341,-0.005656142,-0.016611634,0.004758111,-0.0075656907,-0.0012549687,0.123989075,-0.03476596,-0.002402477,0.021926811,-0.008133809,-0.0025082738,-0.067199424,0.046847634,0.030234946,0.011432556,0.019068625,-0.019090733,-0.052635107,-0.03464901,0.055028334,0.06708875,0.095279045,-0.0654856,0.0091132205,0.005224849,0.038682513,-0.0049688183,-0.04892286,-0.0018459557,0.04145215,6.1202125e-05,0.068707295,-0.03196476,-0.108727366,0.0557706,-0.039082404,0.068076506,0.07549009,0.017218404,-0.0027277071,-0.0073198583,0.013202773,0.039565697,-0.012975388,-0.002332215,0.0009236195,-0.04643223,-0.02312384,0.020461008,-0.0026316703,0.04346858,-0.020772077,0.03841035,0.013520586,0.011198807,0.019105798,0.01454152,0.008487274,-0.0007065999,-0.00622552,-0.016912144,0.011222188,0.03361877,-0.0074303714,-0.019307796,0.044505324,-0.06889509,0.0172885,-0.027960101,0.010717578,-0.015925964,-0.05646443,0.064694434,0.052452922,-0.019427046,-0.013055574,-0.0042189383,0.05339148,-0.043864056,-0.015620685,0.020625317,0.010599952,0.014061316,-0.0076997215,-0.00022486002,-0.01945132,0.009571822,-0.02086175,0.0059788725,0.009625837,0.016117685,-0.028013734,0.08944318,-0.0030704248,0.034856215,-0.023726303,0.018646223,0.029359374,0.0033162858,-0.010900978,-0.0116727045,0.032096747,0.038552996,0.029359773,-0.01901163,0.007274561,0.03777289,-0.068678625,0.01695584,0.049320046,0.022525292,-0.0794267,0.012170423,0.023301147,0.0046063666,0.002451788,-0.04126033,-0.029259568,0.012333968,-0.00084443664,-0.021406211,0.0024084642,0.033392493,-0.05393141,-0.0014075404,0.027342068,0.042759974,0.008494668,0.03746305,0.024918467,0.0038651319,0.006273401,0.029735344,0.004577802,-0.04173621,-0.01787611,-0.05922073,-0.0073743244,0.040506463,0.03417324,0.0061442675,-0.09604782,0.08460505,-0.026842607,0.0042112474,0.05132874,-0.027569296,0.022623964,0.00481989,0.018961944,-0.05142463,0.011050335,0.02872751,-0.024155816,0.0025460327,0.004279054,0.008970068,0.01911893,0.019566637,-0.0020152922,0.013292289,-0.025674969,-0.04578516,-0.06432662,0.00044791575,-0.010525227,0.0006572493,-0.01654028,-0.0029604477,0.056127213,-0.030905757,0.022571934,-0.06532753,0.0025437223,-0.015631624,-0.02613701,-0.0055241636,-0.029374544,0.06370777,-0.007847341,0.0010402193,0.018558418,0.046321873,0.008136839,0.009990139,-0.06498363,-0.020151751,-0.0047003767,0.031178633,-0.03800941,-0.037551206,-0.039223786,-0.0046621826,-0.068781085,-0.016861813,-0.001805585,0.059673704,0.0068197255,0.015442728,0.027755214,-0.015450863,0.014150106,-0.006842999,0.0061467965,-0.019679312,-0.016301835,-0.00021730832,0.07756814,0.041304436,-0.049720794,0.022060305,0.007107254,0.06583645,-0.041848376,0.051036224,0.01275727,-0.018473383,-0.0052148434,-0.015707746,-0.047851447,0.01612873,-0.012426363,-0.004608849,0.006636976,0.061914533,-0.01646672,0.045780573,0.012812251,0.016438296,-0.028959647,0.006529086,0.020304684,0.013364952,0.046556573,-0.09999704,0.050195497,-0.024996825,0.013398701,0.00057440763,-0.064672776,-0.076011434,0.006022022,-0.03778729,0.00096212607,0.016186379,0.016723674,-0.030890822,0.02474958,-0.014794864,0.05855645,-0.100704886,0.009553211,0.029278152,0.007136098,0.04828577,0.031101031,0.03438631,-0.008490663,0.009298359,-0.017511966,0.042563412,0.07153883,0.015533918,-0.008270893,0.06022603,-0.004172807,0.030756218,-0.0073519773,-0.052996222,-0.04107623,-0.029841648,-0.011051807,-0.01645887,-0.0020070882,0.021283902,-0.020719673,0.0065434207,-0.013373532,-0.006136515,-0.009688735,0.052861713,0.03245455,-0.0039694994,0.025281334,-0.023521775,0.043063335,0.009015647,0.041995432,0.028580496,-0.004580058,0.011385646,0.009414042,0.008631591,-0.0036321527,0.010274082,-0.01391233,-0.071604155,0.010430665,0.036492724,-0.008726134,-0.034109034,0.042679727,0.04761876,0.034497533,-0.03663406,-0.06736491,-0.055949595,-0.037016086,-0.028103271,-0.01518144,0.002476789,0.025679028,0.061052144,-0.08609301,0.053786892,-0.08775996,-0.032427836,-0.020500407,0.009362293,-0.018814906,-0.004127652,-0.04431502,0.00063994824,-0.008190121,-0.06799138,0.024819791,0.029306661,-0.02344369,0.012340023,0.031981934,-0.00913622,0.0044020982,-0.0023705529,0.0011266341,0.005227629,0.021996608,-0.014855185,0.077803046,-0.010682863,0.024644857,0.030157343,0.021280501,-0.07968666,-0.011020138,0.027526863,0.030869137,-0.016634027,0.020987531,0.03626988,0.039295033,-0.03876354,-0.012933217,-0.052728493,0.0051449253,-0.028637415,0.027157433,0.06841828,-0.015118578,-0.039316684,-0.02465613,0.02679593,-0.031264875,0.06757776,0.022989392,0.06064507,0.11294545,0.006157597,-0.02976262,-0.06437086,-0.010674226,-0.018227547,0.014285157,-0.034085553,0.055175617,-0.03344726,-0.026091138,-0.020523991,0.013119736,-0.0007893808,-0.021243159,-0.010704595,-0.01481177,0.0031473916,-0.041655775,-0.014410915,0.0013098458,-0.023535943,-0.03299635,0.029467534,0.012386705,-0.031752057,-0.069981,-0.052757867,-0.054159388,0.027631463,0.0063635004,-0.0051897997,-0.015877586,-0.013698066,0.0027809874,0.06927589,0.02096034,-0.040528275,-0.033641845,0.04781781,0.021928314,0.024085408,-0.04066699,-0.048134178,-0.074954495,0.014626206,-0.020444484,-0.03846339,0.0343012,0.01368731,-0.03388028,-0.017730882,0.0030994317,-0.031594228,0.030466408,0.020477686,0.015793214,0.028905222,-0.017985733,0.0035416388,-0.06796711,0.008060965,-0.01629864,0.022881027,-0.024810182,0.030035762,0.0064795786,-0.005193313,0.002753134,0.032251306,-0.0016535798,0.020425815,0.022339068,0.09454626,-0.00011449479,0.030295858,-0.022086939,0.00045347807,0.048927907,-0.0040782527,0.016855381,0.0102019245,0.013079264,0.037923854,0.005594583,-0.012259558,-0.04953174,-0.05974158,0.010108195,0.015554733,0.026335774,0.023065927,-0.0050756927,-0.027906623,-0.02728159,0.018122578,-0.03111856,0.020491533,0.06332311,0.01588802,-0.044711094,0.029358372,-0.023167137,0.07794544,-0.04574761,-0.0063911392,-0.019482685,-0.0035061913]	Keywords: cross-block connectivity, signal flow, Transformer blocks, attention distributions\nKey Objects: Transformer blocks, attention distributions, signal paths\nRefers to Images: None\nHypothetical Questions:\n- Why is creating more signal paths between Transformer blocks potentially beneficial?\n- How do Realformer and Predictive Attention Transformer achieve cross-block connectivity?\n- What kind of optimization issues might arise from the default architecture that these approaches aim to alleviate?\n---\nSummary:\nTo improve signal flow and potentially ease optimization in Transformer networks, some architectures explore creating additional pathways for information to traverse between blocks.\nOriginal Text:\n### 6.2 Strengthening Cross-Block Connectivity  \nIn vanilla Transformer, each block takes outputs from the previous block as inputs and outputs a sequence of hidden representations. One might be interested in creating more paths along which input signals can run through the networks. In Sec. 4.5.2, we introduced Realformer [51] and Predictive Attention Transformer [143] that reuses attention distributions from previous block to guide attention of current block. This can be seen as creating a forward path between adjacent Transformer blocks.\nContextualized Text:\nIn vanilla Transformer models, each block receives outputs from the preceding block and generates hidden representations. To enhance the flow of information and potentially address optimization challenges, some architectures, like Realformer and Predictive Attention Transformer, aim to create additional pathways for signals to traverse between adjacent Transformer blocks by reusing attention distributions.	{"tags": ["architecture", "connectivity", "Transformer"], "doc_id": "b6117642-2ad3-4639-bd15-9d6553f87e03", "summary": "To improve signal flow and potentially ease optimization in Transformer networks, some architectures explore creating additional pathways for information to traverse between blocks.", "doc_type": "text", "entities": ["Realformer", "Predictive Attention Transformer"], "keywords": ["cross-block connectivity", "signal flow", "Transformer blocks", "attention distributions"], "key_objects": ["Transformer blocks", "attention distributions", "signal paths"], "contextual_text": "In vanilla Transformer models, each block receives outputs from the preceding block and generates hidden representations. To enhance the flow of information and potentially address optimization challenges, some architectures, like Realformer and Predictive Attention Transformer, aim to create additional pathways for signals to traverse between adjacent Transformer blocks by reusing attention distributions.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.2 Strengthening Cross-Block Connectivity"}, "hypothetical_questions": ["Why is creating more signal paths between Transformer blocks potentially beneficial?", "How do Realformer and Predictive Attention Transformer achieve cross-block connectivity?", "What kind of optimization issues might arise from the default architecture that these approaches aim to alleviate?"]}
b48b308c-ee7a-4115-9d38-95a59e5a2ba9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.007222537,0.016634285,0.04260379,0.036453944,-0.035639383,0.06118234,0.009261279,0.045351792,0.02218199,-0.024484633,-0.018580433,0.009901852,0.019833028,0.0057660607,-0.041060537,0.05895337,0.003234861,0.08643938,-0.0001362067,-0.009707314,0.063555166,-0.0026500262,0.02104897,0.009854032,-0.021515086,0.012198564,0.044298727,0.01333168,-0.032130685,0.027093563,0.0135728475,-0.048203107,0.00035554578,0.037544165,-0.0018810094,0.015461859,-0.0006940127,-0.029716613,-0.0106762,-0.022861823,-0.016824758,0.05535161,-0.09361555,-0.012952136,0.009603555,-0.042397592,-0.089140415,-0.035239536,-0.0073792157,-0.008856371,-0.0098398365,-0.018675197,-0.0025454494,-0.011041188,-0.050748162,0.001838128,-0.0012798884,-0.065989,0.022440879,-0.08245706,0.015290532,0.01160879,-0.014793734,0.009685696,0.042512506,-0.0064681675,-0.015494039,0.067985654,0.02067933,0.12397809,-0.0012672675,-0.0006871129,0.023353653,-0.05897527,0.10213317,0.053038772,-0.06007179,-0.012932751,-0.042164046,-0.039964225,-0.02397109,0.06802322,0.013227562,-0.014546769,0.05197815,-0.008477991,-0.035685968,-0.0073360954,0.068646125,-0.035216212,-0.03150508,0.06929586,0.017085647,0.0613916,-0.0073758624,-0.065282606,-0.009643152,-0.039758313,0.04396719,-0.06285312,0.024445273,-0.004380912,0.110707946,0.110278994,-0.0006243021,0.010677202,-0.02196799,-0.02045326,-0.0035638413,0.03981153,-0.03076315,0.044487603,-0.06404172,0.032351103,-0.009987298,0.004739419,-0.033267878,0.010924313,-0.009897413,0.0077672885,0.010223288,-0.018242603,-0.03159225,0.009933998,0.08970108,0.028758626,0.027886443,0.0017484869,-0.03858387,0.022936452,0.061232515,-0.021194628,-0.044105224,0.0017721613,0.011547057,0.020165225,0.078490056,0.029551292,-0.0029745528,0.012271408,-0.005534904,-0.033036917,-0.04905331,0.011975654,-0.0405971,0.048695967,-0.046214454,0.056734312,-0.029634044,0.054071873,0.036507644,-0.011877809,0.04818149,0.02449903,-0.018224072,-0.004883438,-0.0009335111,-0.0453226,-0.0076181716,-0.010875312,-0.065153345,-0.0043307142,-0.009997036,0.039831743,0.038899545,0.09291454,-0.0052875155,0.018708207,0.037301794,0.017570795,-0.05990889,-0.012923566,-0.014360055,-0.0112912925,-0.008458418,0.044098172,0.008012304,-0.010839094,-0.0024083673,-0.0047341282,-0.0037171363,0.04014327,0.006023883,0.0271147,-0.049986955,0.01920697,0.018179106,0.04241818,-0.026468517,-0.04000612,0.02526515,0.00081632653,-0.006525869,-0.010381045,0.038164,0.038830984,0.02480819,-0.065733604,-0.034466654,-0.04685252,-0.0078117335,0.0019904564,0.0029244386,-0.040458344,0.02277292,-0.025211094,0.026579192,0.0006930053,-0.027426185,0.041539468,-0.014748141,-0.024139127,-0.003666204,0.059958804,0.039889082,0.0024668407,-0.004415625,-0.0074160406,0.00949841,-0.032401662,-0.055328727,0.021593522,-0.030864561,0.044989347,-0.021105738,0.055374652,-0.035408743,0.03113107,-0.0042244806,-0.021849893,0.030853866,0.041304115,-0.032782525,0.042087164,0.0040261853,0.008972899,0.010492203,0.021084677,-0.029589806,-0.0142060965,0.08632003,0.053298146,-0.012422168,0.08711502,0.024922192,0.025373809,0.0067283553,0.08039211,0.02337212,0.03058322,-0.024544032,0.056375895,0.043638255,-0.017693313,-0.00921477,-0.05864019,0.003101256,-0.0077157756,0.056316774,-0.012462904,-0.015204071,-0.04620865,-4.776434e-05,-0.020495351,-0.039208136,0.04102473,0.016572032,0.039457712,-0.040615678,-0.01853741,0.017132772,0.027195035,0.03853921,-0.017554415,-0.027297279,0.02736333,0.0036223156,0.021773709,0.038933363,-0.011480525,0.018366242,-0.012474054,0.029423676,-0.014709863,0.024161803,-0.030339299,0.008359482,-0.016346991,-0.0427556,-0.004719707,-0.01822215,-0.04159715,0.027245352,-0.01373141,-0.034556776,-0.01189801,0.01094947,-0.049621385,0.0004707807,0.018764209,0.101888545,-0.016012616,-0.020226538,0.036315497,-9.7796816e-05,0.015655104,-0.061348498,0.06397131,0.04018203,0.033160288,0.018029021,-0.027416939,-0.024753887,-0.03199281,0.033293515,0.06458835,0.08723216,-0.028364433,-0.0038419673,0.023386095,0.018600315,0.0010058492,-0.005461385,-0.0027305144,0.018399134,-0.021164052,-0.010777651,-0.043802604,-0.09330038,0.060799375,-0.04459939,0.0599209,0.045522552,0.01954024,0.013447168,0.008550663,-0.036865294,0.012031662,-0.018747425,-0.019853301,-0.013624576,-0.056976173,0.033611875,-0.037393715,-0.0069666863,0.0348985,0.01370525,0.03341429,0.0429937,0.04065543,0.024463058,0.024163105,0.021842038,-0.0069533675,0.0038024108,0.0014790295,0.014849068,0.043303944,0.009522287,-0.013334434,0.0746139,-0.06662689,0.015514535,-0.04307506,0.017839257,-0.010373955,-0.06267799,0.021537442,0.066768564,-0.02614642,-0.03553188,-0.03509092,0.021046715,-0.03300137,-0.048000123,0.04396046,0.039462376,0.029971251,-0.024070188,-0.025055332,-0.007470269,0.0129425125,-0.01371848,-0.039690405,0.005647152,0.014194238,0.012365108,0.10965748,-0.006248448,0.04392808,-0.027764235,-0.0044319825,0.014890024,0.0057233106,-0.0037274861,-0.009507025,0.03268077,0.037393313,0.038920417,-0.027047193,0.0025227808,0.02094964,-0.06534907,0.0261271,0.01988647,0.040123656,-0.08645771,0.045673363,0.022578198,0.006640115,0.030526957,-0.053420693,-0.03441133,0.014674633,-0.006023074,-0.006087027,-0.008117548,0.018040799,-0.041759882,-0.03711369,0.012358416,0.014499065,0.011568437,0.0614023,0.006344919,0.002190145,-0.012632187,0.04394094,-0.034138214,-0.042841863,0.017370626,-0.04647229,0.0018828346,0.062354863,0.013156994,0.0277949,-0.062493052,0.089858025,-0.046461128,0.012216146,0.051870704,-0.043599725,0.009870028,-0.0017683776,-0.025743457,-0.03153821,0.0035111909,0.0039744135,-0.014606504,0.0075475075,0.009004962,-0.013751084,0.012353207,0.0065883533,-0.0025365804,0.003468584,-0.005704993,-0.042635515,-0.0555309,0.002085981,0.006156311,0.045907956,0.03379874,-0.0063460395,0.021134477,0.005891694,-0.038477723,-0.046128355,-0.007995747,0.014324709,-0.003439932,0.019966984,-0.03202094,0.05002501,0.028422084,0.015743537,0.015912402,0.025799036,0.02607888,0.0110004125,-0.079298444,0.0062914114,0.019738207,0.03427611,-0.015503546,-0.011386781,-0.033317648,-0.02563706,-0.027818501,0.03115201,-0.0085842265,0.017106421,0.024132906,-0.015535089,0.022496125,-0.05072243,0.015819196,0.010362467,-0.016448814,-0.035917692,-0.051045056,0.0047309212,0.062102586,0.044810113,-0.015847353,-0.01178947,-0.0064449427,0.06293997,-0.0028811607,0.06113399,0.02434685,-0.002031169,0.0021051795,-0.0027897959,-0.0128560355,0.052129224,-0.05372871,-0.030597722,0.0139618395,0.034226745,-0.020209052,0.020050753,0.013461149,0.021189054,0.009017595,-0.014401088,0.02592027,0.054203805,0.04755551,-0.05893116,0.008900093,-0.014000442,-0.024229348,0.031774532,-0.07432035,-0.06997061,0.003673371,-0.054001614,0.025888497,-0.008738598,-0.02575143,-0.05692272,0.008335051,-0.025424257,0.09447295,-0.06744745,0.032241523,0.0408429,-0.047482833,0.10079287,-0.0031771087,-0.010156991,0.021528939,0.022337025,0.022245431,0.03372498,0.059595406,0.020789484,-0.011169024,0.08646067,-0.0066920742,-0.015575453,-0.028489592,-0.05494346,-0.056313306,0.0034537308,-0.0472226,-0.031456016,0.0017056011,0.03744745,-0.0066718366,0.03974962,-0.021877432,0.0383414,-0.01989346,0.061503366,0.049429655,0.009831455,0.0017971955,-0.0050212718,0.028016891,0.023354564,0.02758998,0.0031390751,-0.011188379,0.036239196,0.006352988,-0.007911769,-0.007287638,-0.00177536,-0.025100153,-0.030630121,0.025080515,0.018302519,0.016576955,-0.032031268,0.06845785,0.011690671,0.023386478,-0.018591091,-0.05997681,-0.029694548,-0.043663513,-0.044102564,-0.001864079,0.010638088,-0.050681185,0.030716475,-0.0034229038,0.056520514,-0.09968632,-0.033014454,-0.046224777,0.025255976,-0.0027256378,-0.012511939,-0.008900003,-0.015943639,-0.005697061,-0.022585241,0.03034125,-0.0005730237,-0.0035818983,0.03549624,0.0016542682,0.0025523005,-0.0063296845,0.022241196,-0.0046478426,-0.0011523515,0.017292222,-0.035692994,0.06267105,-0.04121993,0.003873664,0.050950035,-0.0010112843,-0.11091825,0.009603256,0.03661869,0.047853462,0.00012847544,0.013236673,0.017894596,0.043952547,-0.05369844,0.012197668,-0.08829931,0.023269074,0.019021582,0.0014983309,0.053809695,-0.011919783,-0.07323247,-0.01580814,0.02330445,-0.034390975,0.07263665,0.06760361,0.04067896,0.06630445,0.06729732,-0.03311525,-0.026513496,-0.02258461,-0.039937448,0.026120292,-0.058072038,0.04717187,-0.0023334955,0.006529119,-0.034020588,0.002272734,-0.0041993954,-0.0030041635,0.012439667,-0.046864893,-0.014734428,-0.022504138,0.011171312,0.023544604,0.017609973,-0.014442587,0.073966704,0.005267188,-0.03726346,-0.026083685,-0.04452305,-0.044678748,0.011165526,0.005335271,-0.018119257,-0.0153692635,0.0010346291,0.0018878902,0.06598355,0.027135618,-0.029015116,-0.035463624,0.026317108,0.038406502,0.043653477,-0.02304152,-0.02189831,-0.09800027,0.025608446,-0.026460009,-0.04962731,0.017128084,0.004596247,-0.016918533,-0.019337457,0.012000138,-0.013319143,-0.013994508,0.021883,0.02990279,0.01231163,-0.030897556,0.03646301,-0.064513594,-0.027780535,-0.04330064,-0.03854282,0.001201109,0.046755053,-0.013960883,0.006117011,0.0066211894,0.0024100346,0.005535754,0.015779462,-0.005539464,0.031264283,-0.026689518,-0.020094639,-0.023521634,0.018085327,0.09091347,0.004614003,-0.007213859,0.019167412,0.0060784873,0.011911286,0.034905292,-0.006915152,-0.09405646,-0.044491563,0.008390507,0.009128961,0.0027925256,0.03700328,0.008079828,-0.05211352,-0.0779178,-0.030323653,-0.0021888714,0.03646772,0.03982116,0.018854795,-0.0628692,0.033171237,-0.01832641,0.073998496,-0.014330784,-0.008331313,0.021798862,0.02230795]	Keywords: cross-attention modules, encoder representations, optimization issues, vanishing gradients, Transparent Attention\nKey Objects: cross-attention modules, encoder representations, error signal\nRefers to Images: None\nHypothetical Questions:\n- Why is it problematic for cross-attention modules to only use the final encoder outputs?\n- How does Transparent Attention improve optimization in deep Transformer models?\n- What are trainable parameters involved in Transparent Attention?\n---\nSummary:\nIn deep Transformer encoder-decoder models, relying solely on the final encoder outputs in cross-attention modules can lead to optimization challenges like vanishing gradients. Transparent Attention addresses this by using a weighted sum of encoder representations across all layers.\nOriginal Text:\nIn a deep Transformer encoder-decoder model, the cross-attention modules in the decoder only utilize the final outputs of the encoder, therefore the error signal will have to traverse along the depth of the encoder. This makes Transformer more susceptible to optimization issues (e.g., vanishing gradients). Transparent Attention [8] uses a weighted sum of encoder representations at all encoder layers (including the embedding layer) in each cross-attention module. For the j -th decoder block, the cross-attention module is modified to attend to  \n$$\\tilde { H } ( j ) = \\sum _ { i = 0 } ^ { N } \\exp ( w _ { i j } ) \\frac { H ( i ) } { \\exp ( w _ { k j } ) },$$  \nwhere each w$\\_{ij}$ is a trainable parameter. This effectively shortens the path from each layer in the encoder to the error signal and thus eases the optimization of deeper Transformer models.\nContextualized Text:\nDeep Transformer encoder-decoder models often face optimization problems, such as vanishing gradients, because cross-attention modules typically only use the final outputs of the encoder. To mitigate this, Transparent Attention introduces a modification: it utilizes a weighted sum of encoder representations from all encoder layers within each cross-attention module. This approach effectively shortens the path for the error signal and improves optimization, especially in deeper Transformer models.	{"tags": ["architecture", "optimization", "NLP", "transformer"], "doc_id": "b48b308c-ee7a-4115-9d38-95a59e5a2ba9", "summary": "In deep Transformer encoder-decoder models, relying solely on the final encoder outputs in cross-attention modules can lead to optimization challenges like vanishing gradients. Transparent Attention addresses this by using a weighted sum of encoder representations across all layers.", "doc_type": "text", "entities": ["Transparent Attention"], "keywords": ["cross-attention modules", "encoder representations", "optimization issues", "vanishing gradients", "Transparent Attention"], "key_objects": ["cross-attention modules", "encoder representations", "error signal"], "contextual_text": "Deep Transformer encoder-decoder models often face optimization problems, such as vanishing gradients, because cross-attention modules typically only use the final outputs of the encoder. To mitigate this, Transparent Attention introduces a modification: it utilizes a weighted sum of encoder representations from all encoder layers within each cross-attention module. This approach effectively shortens the path for the error signal and improves optimization, especially in deeper Transformer models.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.2 Strengthening Cross-Block Connectivity"}, "hypothetical_questions": ["Why is it problematic for cross-attention modules to only use the final encoder outputs?", "How does Transparent Attention improve optimization in deep Transformer models?", "What are trainable parameters involved in Transparent Attention?"]}
ae44345e-ce78-4c87-9055-08d2d846e017	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.026604353,0.03446733,0.029039908,0.048932727,-0.043875962,0.08470171,0.03587644,0.03761144,0.027592482,-0.02215139,-0.018845268,-0.0055388147,0.043571904,0.02721052,-0.03985536,0.027798438,0.011695883,0.07213941,0.00359835,0.0030454982,0.038830463,0.010053907,0.031882774,0.0053985678,0.006738964,0.026941601,0.05344122,0.0018151267,-0.010161169,0.0087092435,0.00463982,-0.077423394,0.015369884,0.016419202,0.0051903846,-6.293794e-05,-0.010611886,-0.035360765,0.004209035,-0.00080644176,-0.021868534,0.03676328,-0.0952571,0.004809493,-0.0022076012,-0.042453658,-0.08498341,-0.03475289,0.0136461565,-0.025301434,-0.030594535,-0.026661705,-0.034625437,-0.015474968,-0.022053698,0.0076526804,0.0035708917,-0.048338957,0.00798104,-0.08417142,-0.0044017104,-0.0043910723,-0.033488944,0.0015662315,0.01583199,0.012519068,0.016374975,0.056567818,0.015958957,0.1266942,-0.009392451,-0.015633835,0.015982136,-0.07539859,0.11848732,0.045324158,-0.06095366,0.012524912,-0.056202248,-0.024473457,-0.008064519,0.04320483,0.0014433841,-0.00539434,0.06276444,-0.035268124,-0.049955085,0.004633231,0.041597605,-0.032575235,-0.048563905,0.020869885,-0.0029930593,0.05375236,-0.014439571,-0.059688628,-0.022512827,-0.033018675,0.029998343,-0.054982122,0.014434136,-0.015386622,0.106766775,0.106384814,0.029481363,-0.017624326,-0.027391853,-0.024389938,0.005205894,-0.003745574,0.0072462144,0.023676885,-0.05328849,0.015896428,-0.029941123,0.012654954,-0.037102647,0.029557802,0.028160224,0.022251165,0.0086620385,-0.00062772405,-0.031012299,0.03136992,0.08035242,0.029738143,0.02485628,-0.023645014,-0.023092084,0.014203709,0.051351216,-0.009981689,-0.032646954,0.04319456,-0.033008747,0.0030258691,0.09234173,0.025577683,-0.0059607266,-0.021524264,-0.018217646,-0.02417023,-0.017591948,-0.0041090306,-0.0562385,0.025397351,-0.060752723,0.029662937,-0.034155436,0.063216135,0.036958646,-0.027365798,0.035456687,0.0044158995,-0.03803844,0.009978371,0.018372944,-0.046811648,-0.00776368,-0.013561392,-0.058152862,-0.027126841,0.0037193533,0.040889785,0.0522602,0.07726577,0.0014792107,0.014334293,0.00052644714,0.0026511364,-0.04537409,-0.021067772,0.021753224,-0.015768725,-0.015426686,0.028615208,0.00891646,-0.021528117,0.0035465453,-0.008752809,-0.006660785,0.025549224,-0.013360415,0.021137131,-0.06365057,0.009653736,0.046555907,0.04322461,-0.03379495,0.0017727561,0.021052495,-0.015461118,-0.0048353374,0.000925368,0.05021792,0.040980507,0.048372805,-0.07908246,-0.008633401,-0.04541359,-0.01724081,-0.02769139,-0.0047383243,-0.015716383,0.037966263,-0.02130852,0.029334491,-0.02734963,-0.012965497,0.044511132,-0.00727234,-0.03112139,-0.03114105,0.059417073,0.036752053,-0.019612052,0.0070801615,0.025850495,-0.008718691,-0.074230865,-0.04324887,0.041209705,-0.024433652,0.045906022,-0.015309455,0.05935837,-0.046149746,0.028640687,-0.0336016,-0.04383705,0.027926303,0.02756823,-0.02574584,0.048759677,-0.0048854095,0.023669466,-0.0064228587,0.02082598,-0.022589846,0.02058158,0.061226618,0.051694125,-0.0018159229,0.09138161,0.03232708,0.019903818,0.01298068,0.06888077,0.02566456,0.022757804,-0.007154241,0.03857727,0.0048194765,0.001477522,-0.00041658393,-0.029828098,0.0047118724,-0.004909859,0.049552847,0.01565659,0.010060021,-0.037372492,-0.0021365045,-0.0027678253,-0.022779286,-0.005157923,0.025355317,0.032681644,-0.041217774,-0.010852601,0.02260601,0.0077314954,0.0018797809,-0.027389089,-0.013581819,0.042668354,0.009100061,-0.012009338,0.032184523,-0.010581743,0.01963092,-0.013148573,0.048676487,0.008064997,0.026738875,-0.024241649,0.024988469,-0.023563169,-0.02948511,0.018510187,-0.014018051,-0.03801667,0.041998416,0.021267256,-0.041426,-0.009067462,0.008509448,-0.03386718,0.011863888,0.03056105,0.0967605,0.0005088465,-0.03097506,0.03319048,0.006672381,0.039275043,-0.04948565,0.03377621,0.028373647,0.013256982,0.02467624,-0.038116053,-0.04562573,-0.01931732,0.04043248,0.09043259,0.091396645,-0.028591089,-0.037304696,0.0121223815,0.03192263,0.014833421,0.0004971496,0.030112982,0.044356555,-0.008582866,-0.007930245,-0.0606265,-0.094165444,0.06057796,-0.060267195,0.06806093,0.06109766,0.034065496,0.007797119,-0.013829835,-0.030657282,0.027651703,-0.0075191767,0.001898656,-0.031267796,-0.04436172,0.020568445,-0.033176154,-0.014612552,0.039291278,-0.005901576,0.018971773,0.010086738,0.040102426,0.014048291,-0.006830631,0.017864877,0.0040560295,0.022253301,-0.0005454761,0.02271904,0.026396787,0.012372583,-0.018629696,0.08476328,-0.05823404,0.04080435,-0.033530723,0.016891895,-0.032854404,-0.07702344,0.03590233,0.06891418,-0.030395202,-0.016628606,-0.027158435,0.017267631,-0.04808753,-0.041361105,0.0385206,0.036793437,0.0064457837,-0.042646915,-0.016449677,-0.02302645,-0.018918743,-0.0028479144,-0.05756465,0.030719247,0.0021376389,0.016705373,0.107660405,-0.02353378,0.054983724,-0.03068356,0.013758857,0.004166996,0.013087008,-0.023017777,-0.012891791,0.02131918,0.04053911,0.006509333,-0.012401722,0.00916445,0.055311497,-0.04240577,0.00076079334,0.02706728,0.020568408,-0.084922664,0.019134874,0.011557237,0.00039053606,0.037244733,-0.030339153,-0.0366979,-0.008031864,0.014798544,-0.008746074,-0.055316836,0.007081004,-0.042570714,-0.048143853,0.0033150895,0.029056115,0.011708452,0.037108313,0.0038817064,-0.017061245,-0.022192027,0.053553842,-0.00992271,-0.04388857,0.012825282,-0.059514858,0.01718444,0.05609491,0.023772301,0.04950782,-0.08845239,0.07529573,-0.024559004,-0.016532818,0.05403052,-0.04949734,0.010973702,-0.012938727,-0.01769345,-0.05748749,0.030537479,-0.00092525495,-0.0026190295,-0.007492769,0.030482318,-0.020450395,-0.003955909,-0.01145278,0.030350732,-0.010176568,-0.013880725,-0.046196103,-0.06118798,0.008354782,-0.0020357163,0.010294967,0.031988803,0.0063065807,0.0044186586,-0.015540195,-0.05903518,-0.033645198,-0.021741798,0.0094555635,-0.018957987,0.0317708,-0.0461607,0.05033261,0.023207529,0.024592258,-0.0063612545,0.0057164337,0.03807724,0.00011890069,-0.087316625,-0.025481986,0.01866994,0.012767548,-0.0040184557,-0.031764083,0.0044543995,-0.017303862,-0.022769317,0.039173566,-0.0028037208,0.036086667,0.03257245,-0.004955846,0.03300575,-0.045976996,-0.01536644,0.004612472,-0.015306551,-0.032868847,-0.055113453,0.0010699518,0.043301933,0.038763355,0.0010777547,0.00780926,-0.01712637,0.03843618,-0.034067232,0.051699128,0.03786703,-0.035956774,-0.011894662,0.02326195,-0.037268087,0.03417422,-0.068480395,-0.015928125,-0.0067248805,0.03931845,-0.03382416,0.0031287945,-0.0065747164,-0.009509768,0.0030165701,0.011192345,0.019177344,0.07396233,0.008975685,-0.07559764,0.0140212355,-0.018583558,-0.007868587,0.02296398,-0.06135118,-0.062041238,-0.013404564,-0.063653335,0.034775086,-0.0077662733,-0.010869611,-0.055121418,0.006713998,-0.010732045,0.11213947,-0.09841305,0.0509181,0.033030152,-0.023139441,0.078948714,0.010881842,0.032860007,0.0036837547,0.019003855,-0.02347247,0.06885169,0.041974813,0.058194052,0.028362537,0.07030072,0.0013284036,-0.00333321,-0.0074005993,-0.056100592,-0.04045079,-0.014122384,-0.026601901,-0.033231776,-0.017753731,0.026868315,-0.0002489247,0.023974162,-0.009798164,0.027659627,-0.006502459,0.05142308,0.06256567,0.004364031,0.029232103,0.0031841772,0.0020073976,0.026583593,0.046481967,0.0039297403,-0.005201281,0.04581846,-0.0076504964,-0.006384359,0.01595909,0.005041126,-0.01332631,-0.054285333,0.024609562,0.009379758,0.0020542296,-0.0025216024,0.053312007,-0.013150465,0.028117582,-0.018110659,-0.07232787,-0.045338757,-0.06593947,-0.04251441,0.030266438,0.008551948,-0.019148473,0.029995155,0.0139609445,0.05826457,-0.086205564,-0.02987693,-0.037935443,0.051188312,-0.02018964,-0.020821583,-0.011310401,-0.024652215,0.012603591,-0.046765603,0.043111756,-0.0054485924,-0.019714465,0.006888162,0.0118855275,0.0061047794,-0.00456454,0.002391164,0.034124624,0.0015474232,0.001205165,-0.020319117,0.07530414,-0.03245867,0.023879716,0.07467479,0.009913253,-0.09982461,-0.036621705,0.025918258,0.045167204,0.019351492,-0.00040402447,-0.0044685034,0.007881953,-0.034689922,0.01705355,-0.038709156,0.0139486,0.017268987,-0.007827773,0.030491538,0.015560258,-0.046808213,-0.025690682,0.03078029,-0.026169926,0.067626715,0.03775782,0.048700176,0.08158005,0.040835924,-0.036292523,-0.041181497,-0.019939166,-0.030808946,0.042572547,-0.051574584,0.037228152,0.015831294,-0.002862357,-0.025133576,0.0031957028,-0.007622065,-0.009877187,0.023491938,-0.03198344,-0.01741866,-0.000412307,0.02455916,0.014683227,0.03901143,0.0004671924,0.0939687,0.023250677,-0.025889268,-0.03958463,-0.045416467,-0.0070023267,0.012890833,0.011302635,-0.025216425,-0.019545209,0.0006117016,0.011339105,0.07465986,0.035166886,-0.049801357,-0.036571134,0.015535561,0.03268368,0.07718861,-0.015728084,-0.026035568,-0.09114613,-0.002725872,-0.00925528,-0.06348645,0.018889148,0.009405642,-0.036252886,-0.018227799,0.01886428,-0.007600046,-0.0037547217,0.02960011,0.054178614,0.037101433,-0.021595309,0.026630616,-0.0577659,0.013342456,-0.04521367,-0.036926642,0.0062278127,0.055799108,-0.015346273,0.010335105,0.0020542513,-0.02122929,0.005209774,-0.031079467,-0.021879598,0.04091524,-0.03416097,-0.002707794,-0.015911194,0.011334435,0.08968817,-0.027217539,0.035745665,0.020579645,-0.009864889,0.015895562,0.004885613,0.0071654483,-0.07685117,-0.07652321,0.0028932677,0.022240484,0.0007523339,0.017297812,-0.00062809367,-0.040311545,-0.06463977,-0.040061336,0.0033774164,0.012823269,0.010211937,0.03530449,-0.05598753,0.0032499088,0.009951891,0.045792114,-0.011489093,0.017956678,-0.0067299143,0.006256835]	Keywords: cross-attention, encoder representations, error signal, optimization, deeper Transformer models\nKey Objects: encoder representations, error signal, optimization\nRefers to Images: None\nHypothetical Questions:\n- How does shortening the error signal path benefit the optimization of deep Transformer models?\n- What is the role of the trainable parameters (w_{ij}) in Transparent Attention?\n- Why is it problematic for error signals to have to traverse the entire depth of the encoder?\n---\nSummary:\nTransparent Attention utilizes trainable parameters (w_{ij}) to create a weighted sum of encoder representations at all layers, shortening the path for error signals and easing the optimization of deeper Transformer models.\nOriginal Text:\nwhere each w$\\_{ij}$ is a trainable parameter. This effectively shortens the path from each layer in the encoder to the error signal and thus eases the optimization of deeper Transformer models.  \nAnother issue associated with vanilla Transformer is that each position can only attend to history representations from lower layers. Feedback Transformer [34] proposes to add a feedback mechanism to Transformer decoder, where each position attends to a weighted sum of history representations from all layers  \n$$\\tilde { h } _ { i } = \\sum _ { i = 0 } ^ { N } \\frac { \\exp ( w _ { i j } ) } { \\sum _ { k = 0 } ^ { N } \\exp ( w _ { k } ) }.$$\nContextualized Text:\nTo address the issue of long error signal paths in deep Transformer models, Transparent Attention modifies the cross-attention module to attend to a weighted sum of encoder representations at all layers.  Each weight (w_{ij}) is a trainable parameter, effectively shortening the path from each layer in the encoder to the error signal and thus easing optimization.	{"tags": ["architecture", "optimization", "Transformer"], "doc_id": "ae44345e-ce78-4c87-9055-08d2d846e017", "summary": "Transparent Attention utilizes trainable parameters (w_{ij}) to create a weighted sum of encoder representations at all layers, shortening the path for error signals and easing the optimization of deeper Transformer models.", "doc_type": "text", "entities": ["Transparent Attention", "Transformer"], "keywords": ["cross-attention", "encoder representations", "error signal", "optimization", "deeper Transformer models"], "key_objects": ["encoder representations", "error signal", "optimization"], "contextual_text": "To address the issue of long error signal paths in deep Transformer models, Transparent Attention modifies the cross-attention module to attend to a weighted sum of encoder representations at all layers.  Each weight (w_{ij}) is a trainable parameter, effectively shortening the path from each layer in the encoder to the error signal and thus easing optimization.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.2 Strengthening Cross-Block Connectivity"}, "hypothetical_questions": ["How does shortening the error signal path benefit the optimization of deep Transformer models?", "What is the role of the trainable parameters (w_{ij}) in Transparent Attention?", "Why is it problematic for error signals to have to traverse the entire depth of the encoder?"]}
b697e9c4-715d-4c80-9457-d4304c547412	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017432181,-0.0070521845,0.03754953,0.043857694,-0.010554728,0.021854488,-0.0033484867,0.056245007,0.03253975,-0.008175482,-0.006705701,0.023504473,0.014128804,-0.012084197,-0.04257685,-0.010665574,0.04490198,0.08712775,-0.000559489,-0.013644919,0.027962836,0.012815463,0.01885597,-0.016492253,0.013720121,0.041532762,0.044939317,0.0064106733,-0.015873905,0.0001858616,0.030492611,-0.06756609,0.02071208,0.034768187,0.05386767,0.0009671937,0.015211988,-0.07739172,-0.03705109,-0.020324348,-0.021124454,0.042907916,-0.025339698,-0.03073297,-0.015210951,-0.04003199,-0.05230605,-0.049154237,0.034237396,-0.026339328,0.024682766,0.018666303,-0.04206849,0.0062351194,-0.021851614,0.0023786412,-0.026667308,-0.049391862,0.008776249,-0.10062492,-0.01442586,-0.00829172,-0.021118509,0.005680119,0.014068508,0.005036038,0.02214796,0.027428461,0.039133687,0.13580999,0.007052308,0.002592491,-0.0025974337,-0.049150497,0.11874678,0.069418624,-0.05773088,-0.05669296,-0.055129696,-0.038488932,-0.02492262,0.050942723,-0.06578581,-0.03271072,0.08836177,-0.017173352,-0.023574445,0.0071696527,0.071765035,-0.0025453463,-0.019108152,-0.013701666,0.026942777,0.015385302,-0.030139629,-0.058389444,-0.022562921,-0.013347379,0.055692848,-0.031523563,0.026721718,0.0028178792,0.050325584,0.07426122,0.023397483,-0.062582664,-0.022873698,-0.055650417,0.04611965,0.011846291,-0.0009650859,0.0076654344,-0.059098534,-0.00080535276,-0.029368911,-0.027269222,-0.017504882,0.015447016,0.031949323,0.022317471,0.059084073,-0.038125735,-0.01759091,0.011010832,0.056053758,0.022357889,0.024125088,-0.016999748,-0.057513345,0.030326635,0.0409403,-0.01775995,-0.016553953,-0.009121727,-0.019551903,0.00932802,0.0588723,0.021901127,0.008118757,0.015652921,-0.02223751,-0.033756856,-0.05106871,0.01578021,-0.00080589793,0.04854235,-0.066065185,0.0072140507,-0.005983097,0.06886436,0.034342326,0.0016278053,0.023515768,-0.043172203,-0.0061887437,-0.027710808,-0.009485051,-0.038590368,-0.02598722,-0.048976056,-0.048299816,-0.01840893,-0.0072797495,0.049812846,0.0594387,0.120774575,0.024449186,-0.027094275,0.024465919,-0.034743723,-0.030370336,-0.028464958,-0.01854144,-0.0416988,0.01582366,0.0045768376,0.017684659,-0.03012551,0.005456327,-0.034104668,0.013171199,0.059965897,-0.013859856,0.033589244,-0.028462537,0.0274386,-0.009248781,0.032195356,-0.025094343,-0.030894011,0.018465126,-0.0022933837,-0.0060886256,-0.033379618,0.04671267,0.028593736,0.0370956,-0.082472,-0.0360206,-0.028847627,-0.00026439002,-0.0030435128,0.005975003,-0.027355837,0.034107152,-0.01850699,0.010520581,-0.021516303,-0.012835731,0.017635612,-0.032335825,0.0008530244,-0.03584769,0.074869014,0.02628524,0.017855287,-0.02258532,0.042482536,0.031336214,-0.02045458,-0.028107842,-0.021614105,-0.018480541,0.04693719,-0.011399726,-0.005955963,-0.04181511,0.041540496,0.04293723,-0.043252796,0.0020634562,-0.016387492,-0.04397281,0.031382147,-0.007761929,0.021289624,0.037495244,0.009850176,0.017827462,0.040837765,0.04864185,0.055190217,0.0010351503,0.08173566,-0.0059091724,0.011797038,-0.005322739,0.03246555,0.029686272,0.028874472,-0.01471384,0.022684345,0.03211375,0.0034309297,0.010926607,-0.02539566,-0.0020664341,-0.001061666,0.034742586,-0.0019841355,0.021399423,0.024098538,0.018348172,0.062678,-0.002787627,-0.023799192,-0.0036825552,0.023553662,-0.0107972035,-0.0062537817,0.036940638,0.01299377,0.027550846,-0.03648773,-0.011622348,0.05302204,0.023362748,-0.01901097,0.03187868,-0.011503844,0.0017632102,0.0038512282,0.03291546,0.012685162,0.027853081,-0.026347242,0.009899805,-0.009016081,-0.04265115,-0.012649304,-0.010463989,-0.03735732,0.08717907,-0.01843057,-0.017370412,0.00442243,-0.014304816,-0.0280791,-0.014360008,0.045670215,0.12604149,-0.019342544,-0.06316769,0.010851938,0.010768296,0.017165994,-0.045674004,0.0340354,0.017280731,0.012996769,0.027905753,0.002617118,-0.03048193,-0.0015764876,0.024416093,0.06979096,0.07947003,-0.023565065,0.014827814,0.012153617,0.028950816,-0.048195437,0.009163031,0.04076356,0.021513693,-0.04184362,0.026054865,-0.0052286442,-0.082009286,0.06446914,-0.07356602,0.058218528,0.07167607,-0.021910178,-0.011154632,0.018693078,0.014259626,0.02087165,0.01244235,-0.01049021,-0.012441621,-0.06484942,-0.021790963,0.013141355,-0.029355062,0.056806795,-0.029532278,1.963807e-05,-0.007391794,0.016774993,-0.017074144,0.03048189,0.033620358,0.011156645,0.0020928748,-0.010634552,-0.004051815,-0.0092859985,0.024080127,-0.046726517,0.06256982,-0.049844097,0.03830012,-0.04008925,0.026950153,-0.00050646055,-0.045918744,0.019690633,0.019570503,0.00028815225,-0.031168874,-0.029953016,0.052459653,-0.022210771,0.034010388,0.030780321,0.048181314,0.014690981,-0.012743312,-0.021409081,-0.0057153157,-0.011699564,-0.03171776,-0.008418978,0.03438905,0.014828421,0.0016539698,0.08743186,-0.018951721,0.06846078,-0.0090712225,-0.0043912246,0.046963926,0.007951642,-0.02976583,-0.015551659,0.039009918,0.08304631,-0.0025255803,-0.0048189396,0.038329165,0.020977553,-0.049192492,0.03230132,0.03029105,-0.017711442,-0.07497554,0.03544795,0.05847541,0.008208587,0.033726986,-0.027592653,-0.035704482,-0.005967898,0.019324686,0.0004895953,-0.025756566,0.020743178,-0.02526549,-0.012829149,-0.014510235,0.027621781,0.013810516,0.017886896,0.05402447,0.00017857322,-0.045599997,0.039436027,0.016706351,-0.03284894,-0.000112084825,-0.082275994,-0.022323562,0.044442523,0.042361457,-0.032108825,-0.07984352,0.06190661,-0.05488114,0.044001915,0.046049237,-0.025378814,0.007458025,-0.00056193286,0.012730907,-0.008459603,0.020490188,0.03036,-3.328278e-05,-0.025988137,0.042659815,-0.019511728,0.00911429,-0.01087771,0.05229025,-0.026951607,0.027814737,-0.023883073,-0.062031608,0.01674611,-0.0064808815,0.016753273,0.009095866,-0.0058327815,0.0010316657,-0.024043653,-0.036332805,-0.008278871,-0.0046313233,-0.028733585,0.026170027,0.031001642,-0.0360331,0.03217807,-0.024975972,-0.015474115,-0.021386746,0.06254852,0.036560785,0.018463217,-0.08817027,-0.03705111,0.020702407,0.019160802,-0.020679511,-0.026512697,-0.02335173,-0.022221077,-0.019649552,0.020895787,-0.02529148,0.0153361745,0.005331355,0.031613443,0.020040683,-0.04070201,0.013780432,0.017860157,0.0011782526,-0.02334666,-0.043765593,-0.004326523,0.058400743,0.055083275,-0.0094818715,0.00916205,0.008186281,-0.000796214,-0.0021688705,0.0520924,-0.0042494074,-0.023495374,0.021004852,-0.0557079,-0.053192314,-0.003449387,-0.010950795,-0.020398548,0.015260587,0.05941243,0.028318964,0.0072345766,0.0277186,0.0014086919,0.031695694,0.017407425,0.054747924,0.039951395,0.049889494,-0.080373555,0.038108177,-0.023074128,-0.0014468106,0.0135271875,-0.07458891,-0.010439368,0.016063936,-0.044411805,0.030572755,-0.0047301087,-0.001610414,-0.06502109,0.021776643,-0.028340189,0.08375521,-0.08476297,0.007028652,0.007107298,-0.028391296,0.094621845,0.037225574,-0.0093301935,0.028478593,-0.007985201,-0.044163175,0.00822651,0.07612887,0.009503861,-0.0072354823,0.053712305,0.0017630426,-0.009967808,-0.056965936,-0.09967597,-0.018607402,-0.022983804,-0.025903529,-0.034177504,-0.014001098,0.021434749,0.01679544,-0.009968224,0.011847601,0.030294888,-0.021746729,0.078690216,0.007519963,-0.0050123497,0.029579418,0.020738043,0.05126975,0.038977113,0.029713502,-0.005580636,-0.0042811995,-0.0028094759,-0.015154223,0.014974851,0.013424718,-0.0060173026,-0.05663006,-0.014847739,-0.006254819,0.008976184,-0.026130283,-0.013006516,0.06915598,0.026520016,0.056944735,-0.012682505,-0.061079435,-0.04714265,-0.025243884,-0.0462207,0.03466757,-0.006728308,0.013611489,0.042433545,-0.03858768,0.04759741,-0.088038266,-0.028661903,-0.036418624,0.041491557,-0.009794643,-0.010647017,-0.022431018,-0.00255654,-0.008066123,-0.10632911,0.05360554,0.03991532,-0.019418146,-0.0052647693,0.00796693,-0.009131134,0.0026361635,0.005811794,-0.023748804,-0.0003503236,-0.044987746,-0.017895294,0.07858561,-0.005196163,0.0066969115,-0.010494153,-0.021385705,-0.085258655,-0.049247313,0.010649915,0.048106354,-0.014961721,0.011687954,0.034469515,0.002440029,-0.029516755,-0.013182329,-0.0935848,-0.0006599303,-0.018804507,0.028939327,0.066745386,0.00028050903,-0.027429696,0.028415484,0.0041366494,-0.050049707,0.038705513,0.016288478,0.039825626,0.05102615,0.046304807,-0.028509682,-0.08067684,-0.022857657,-0.039285406,0.019155903,-0.04032232,0.072129734,0.039559245,0.004205408,-0.005104458,-0.013438156,0.0011125288,-0.029664582,0.009017136,0.009267539,-0.054626074,-0.0008404204,0.003151105,0.021976562,-0.012618278,0.004583092,0.114280194,0.038534556,-0.027191548,-0.045728642,-0.04730262,-0.0012041399,0.024698664,0.02118563,0.009027043,0.021453131,0.009506335,-0.012579491,0.069008425,0.012653991,-0.047602728,-0.008936312,0.033095866,0.031315535,0.021388853,-0.001339986,0.00048878,-0.06934001,0.024300937,0.010092942,-0.026387049,0.029822154,0.012737552,-0.040478572,-0.048049383,0.02078078,-0.022783596,0.003135252,0.02223484,0.0107962,0.020332431,-0.02259219,0.0040483116,-0.043956004,-0.01523633,-0.0019121745,-0.0011651025,-0.023278657,0.04583789,-0.012775513,0.01980421,-0.022753913,0.007994489,0.0129026845,0.025427196,0.025364814,0.08532801,-0.04555658,0.0331383,0.0008692752,-0.027714917,0.08048466,-0.035005562,-0.0010293265,-0.01110356,0.04125056,-0.016917344,0.0052858307,-0.049755383,-0.076082036,-0.04724542,0.043709286,0.049296748,0.02561962,0.0354771,0.001358213,-0.041053545,-0.043281693,0.0009472947,0.0023769962,0.06727954,0.066396534,-0.02509663,-0.10526165,0.014640102,-0.012612769,0.05067328,-0.010266756,-0.005824177,-0.006727771,0.007251719]	Keywords: adaptive computation time, transformers, computation time, feature refinement, efficiency\nKey Objects: computation procedure, input data, representations\nRefers to Images: None\nHypothetical Questions:\n- How does ACT improve performance on challenging data?\n- What are the potential drawbacks of introducing adaptive computation time?\n- Could ACT be combined with other Transformer variants to further enhance performance?\n---\nSummary:\nAdaptive Computation Time (ACT) is a modification to Transformer models that conditions the computation time based on input data, potentially leading to better feature refinement for difficult examples and increased efficiency for simpler ones.\nOriginal Text:\n### 6.3 Adaptive Computation Time  \nVanilla Transformer, like most neural models, utilizes a fixed (learned) computation procedure to process each input. An intriguing and promising modification is to make computation time conditioned on the inputs, i.e., to introduce Adaptive Computation Time (ACT) [38] into Transformer models. Such modifications potentially give rise to the following advantages:  \n-  Feature refinement for hard examples. For data that are hard to process, a shallow representation might not be adequate to fulfill the task at hand. It would be more ideal to apply more computations to acquire a deeper and more refined representation.\n-  Efficiency for easy examples. When processing easy examples, a shallow representation might be enough for the task. In this case, it would be beneficial if the network can learn to extract features using reduced computation time.\nContextualized Text:\nUnlike standard Transformer models that use a fixed computation procedure, Adaptive Computation Time (ACT) adjusts the computation time based on the input. This allows for more refined feature representations for difficult examples and reduced computation for easier ones.	{"tags": ["architecture", "transformers", "optimization", "efficiency"], "doc_id": "b697e9c4-715d-4c80-9457-d4304c547412", "summary": "Adaptive Computation Time (ACT) is a modification to Transformer models that conditions the computation time based on input data, potentially leading to better feature refinement for difficult examples and increased efficiency for simpler ones.", "doc_type": "text", "entities": ["Transformer", "Adaptive Computation Time (ACT)"], "keywords": ["adaptive computation time", "transformers", "computation time", "feature refinement", "efficiency"], "key_objects": ["computation procedure", "input data", "representations"], "contextual_text": "Unlike standard Transformer models that use a fixed computation procedure, Adaptive Computation Time (ACT) adjusts the computation time based on the input. This allows for more refined feature representations for difficult examples and reduced computation for easier ones.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.3 Adaptive Computation Time"}, "hypothetical_questions": ["How does ACT improve performance on challenging data?", "What are the potential drawbacks of introducing adaptive computation time?", "Could ACT be combined with other Transformer variants to further enhance performance?"]}
1afda106-ab2d-42ec-9f0b-7ca1a5a37e8f	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.010779028,-0.0053380127,0.04054914,0.033480845,-0.029534912,0.04829698,0.0075327517,0.046211716,-0.0039695157,0.013152508,-0.0069950637,0.003222344,-0.03263645,0.019704655,-0.06326854,0.02809852,0.039434902,0.08762537,0.002008089,-0.02358617,0.048607606,-0.009081462,-0.001821678,-0.009273986,0.017817464,0.0544745,0.057394963,0.008677589,-0.037460558,0.009305035,0.004004849,-0.08068106,-0.004204669,0.04372366,0.050596587,-0.008359887,-0.028360875,-0.056901686,-0.005472273,-0.010606386,-0.02502784,0.0665895,-0.04529973,-0.00965081,-0.00052366697,-0.08710072,-0.0860658,-0.04692867,0.0068666316,-0.013020656,0.005554594,0.007851335,-0.02660124,-0.036773346,-0.0115959,-0.0031174084,-0.008188254,-0.031236453,0.0298469,-0.08424258,0.021478603,0.025502425,-0.018517045,0.024563642,0.003729495,0.02240963,0.028969107,-0.0117395,0.06060682,0.11345817,-0.023646854,0.020589717,-0.0029780874,-0.06852994,0.11199368,0.052179314,-0.036008883,-0.048959415,-0.055978965,-0.035774913,0.007130822,0.041428447,-0.043137547,-0.04849755,0.09375466,-0.026508283,-0.024021005,0.017581586,0.09346101,-0.014149538,-0.025768176,-0.015766827,0.029819597,0.02494175,-0.016066005,-0.056846432,-0.035723977,-0.016839147,0.025083335,-0.047937714,-0.0046143625,-0.011652858,0.063509226,0.09980948,0.051249433,-0.047646433,-0.027581647,-0.022553435,0.024768196,0.012549844,-0.010508362,0.024006147,-0.05997698,0.005063381,-0.026234495,-0.02597335,-0.02686322,0.015103316,0.010978424,-0.00042560106,0.045723088,-0.010591026,-0.043443743,0.02357774,0.056572583,0.043386646,0.021846155,-0.012710146,-0.042895306,0.039202258,0.028630368,0.019859014,-0.012527482,0.016866628,-0.039937545,-0.014127236,0.058717266,0.009655329,-0.014524025,0.027024176,-0.04013432,-0.010303455,-0.039704923,0.0036627874,0.0057440163,0.047757793,-0.064222656,0.03361681,-0.011903643,0.06883379,0.0034651023,-0.014382599,0.00401768,-0.025105942,-0.0029773246,0.008252393,0.008646537,-0.022685828,-0.04853198,-0.03472576,-0.029865252,-0.019200757,-0.0050531123,0.09231963,0.041064493,0.110882916,0.020192793,-0.007349448,0.04701552,-0.006517665,-0.026818931,-0.030173268,0.0011894191,-0.04300759,0.020756848,-0.004491606,0.053663906,-0.03849581,-0.018985787,0.00096875953,-0.020124808,0.05384948,0.01115694,0.051219027,-0.036315948,0.03337017,0.015598176,0.036250137,0.018969536,-0.04094301,0.01528279,-0.0149409445,-0.006176317,0.012215975,0.041629475,0.04882199,0.045988183,-0.090385094,-0.020038776,-0.06171062,0.004473212,-0.017683394,-0.020220215,0.00083723175,0.055928715,0.008315024,0.031860586,-0.024838328,-0.016169451,0.016946632,0.0038851055,0.03256015,-0.006356385,0.07012083,0.03355273,-0.0036607168,-0.03042795,0.014564551,0.048271276,-0.018310087,-0.009344323,-0.004811238,-0.031954397,0.029279651,-0.017320488,-0.0132517,-0.0066761062,0.038535047,0.0310108,-0.048579592,0.00041106262,0.04517844,-0.05072019,0.044449907,-0.013650236,0.03290383,-0.017876463,0.035082605,0.022260373,0.018192455,0.04650803,0.056796316,0.015096624,0.08186516,-0.011975701,0.0011152722,0.014281114,0.03270069,0.034149222,0.012421737,-0.040079944,0.021132067,0.040211875,-0.026167296,0.009349658,-0.019367408,-0.0049549253,0.020404926,0.0140659055,-0.012694071,-0.0002244967,-0.0505185,-0.026009498,0.015235255,-4.622985e-05,-0.0303258,-0.01627502,0.016810559,-0.025881939,0.0102283005,0.049126476,0.028438874,0.004519089,-0.023848798,-0.028325902,0.05600616,-0.0033227184,-0.0014658992,0.02666103,-0.007838752,0.0040144804,-0.015377219,0.013048412,0.004610929,0.038272087,-0.026716499,0.0064166514,-0.014582841,-0.0076461006,0.005130106,4.824871e-05,-0.016303558,0.08526685,-0.02860066,-0.014426964,-0.003757397,-0.004621809,-0.052978463,0.003998458,0.03968145,0.08999769,-0.011297195,-0.032801133,0.036978103,0.020526865,-0.00873038,-0.038930137,0.033091158,0.0076329475,0.003366419,0.025381722,-0.03655761,-0.010125519,-0.009532247,0.022247983,0.09856612,0.07792092,0.010826573,0.008298197,0.002706611,0.0010243075,-0.020755023,0.005849504,0.020903666,0.054220386,-0.028879302,0.023136083,-0.040038716,-0.09212174,0.077076785,-0.07767648,0.041789018,0.0573558,0.01698319,-0.008154888,0.03650302,0.030480765,0.008537114,0.0066887243,0.016151745,-0.04940057,-0.059178453,-0.02900694,0.0081604505,-0.015428107,0.031147119,-0.027384784,-0.017108165,0.010110961,-0.002104823,0.0051239687,0.024995355,-0.0022347134,0.001266143,0.04022361,-0.010544428,-0.02872555,0.017354066,0.006666083,-0.050590064,0.056177046,-0.042224865,0.06398442,-0.043421425,0.012352013,-0.009055327,-0.04782382,0.0020212983,0.007363182,-0.02001977,-0.025765868,-0.023465533,0.0319599,-0.060654998,0.017102357,0.038438164,0.05010674,0.011044686,-0.042530037,-0.0063697197,-0.03944701,-0.0033206383,-0.055635642,0.007380572,0.052132487,0.025526362,-0.0073296693,0.0856257,-0.016237473,0.04355588,-0.031218244,0.013739295,0.028361566,-0.029619446,-0.027359977,-0.047944475,0.0271486,0.084209226,-0.019044902,0.013290851,0.06764357,0.026865803,-0.04274752,0.032387946,0.030563565,0.012110001,-0.034712315,0.045350417,0.078532696,0.029594693,0.017617768,-0.012744909,-0.025944274,-0.023580886,0.035462104,-0.0071077794,-0.055222746,0.05496531,-0.022616014,-0.02315559,-0.008481906,0.05249677,0.024326034,0.01342591,0.07431055,-0.03645762,-0.03510025,0.06555618,0.010940785,-0.029116541,-0.0024993978,-0.07697894,0.0040523666,0.043784246,0.0041223895,-0.004776026,-0.094641,0.047804795,-0.07041813,-0.0022268367,0.05049783,-0.03971006,0.014999102,0.019293932,-0.0015643197,-0.024435958,0.038106035,0.009632547,0.0011684317,-0.014899384,0.036321394,0.03229225,-0.017932707,-0.009121676,0.042453807,-0.012005972,-0.0053089364,-0.033559237,-0.05751704,0.028378474,0.0020704935,0.016190981,-0.0020080756,0.018556586,0.020712644,-0.018807663,-0.031095477,0.01267846,-0.027824037,-0.045851525,-0.002652286,0.03197078,-0.07036304,0.07779164,-0.020061018,-0.016897773,-0.007686227,0.027352173,0.038749363,0.0037004368,-0.070207424,-0.051043283,-0.0006565574,0.03257733,-0.027314227,-0.0043546874,-0.021786768,-0.0063679437,-0.032147482,0.031058716,-0.02478391,0.016353091,0.0017957359,0.024948739,-0.002202001,-0.04312488,-0.03266999,0.01940178,0.0012286103,-0.0124435695,-0.04748423,0.0062828986,0.043045208,0.021153351,0.008594735,0.00570276,0.006464962,-0.0049252124,-0.029115723,0.057664584,0.010365831,-0.02866031,-0.00088430545,-0.03190722,-0.04943222,0.0034822845,0.011328503,-0.014324055,0.00839416,0.027077533,0.01885643,-0.0018563118,0.021498503,0.016628651,-0.0031779176,-0.015482953,0.028813489,0.02327255,0.034562465,-0.090684816,0.021685746,-0.045749348,-0.025735147,0.02609613,-0.034455355,-0.049362402,0.017902251,-0.06585679,0.04629745,0.007921114,-0.0031885495,-0.08562944,0.03605534,-0.032762527,0.052231465,-0.06188603,0.054711808,0.036645614,-0.017851777,0.06622805,0.027931342,-0.0045332303,-0.0052294787,0.027637294,-0.05830074,0.0047327746,0.040339522,0.0025694019,-0.0014569894,0.039633837,-0.0039369813,-3.377759e-05,-0.06412123,-0.08703362,0.0011670719,-0.022886762,-0.01656178,-0.022760954,-0.028365284,0.042336833,-0.00832741,0.003415391,-0.0022132646,0.053742927,-0.03359128,0.0780495,0.030738683,0.011766703,0.033707455,-0.017394926,0.046846226,0.026669754,0.04319016,0.010462429,0.01757911,0.0056320797,0.017992208,0.011185451,0.0010882801,0.002755701,-0.05247584,-0.028965512,-0.0018353685,-0.008284405,0.033318307,-0.029351763,0.06877456,0.031394444,0.02100403,-0.0019156586,-0.073855214,-0.100609176,-0.021702116,-0.05840244,0.05863065,0.027541166,0.029847939,0.008855646,-0.0040823747,0.037638016,-0.07130676,-0.013715638,-0.04790645,0.061993856,0.0041292747,-0.008823938,0.00969852,-0.019140778,0.011749782,-0.060122535,0.04085274,0.00894604,-0.04778997,0.0057526664,0.021493005,-0.0072132694,0.022780256,-0.025168445,-0.03448504,0.013898779,-0.009193949,-0.02214673,0.057858407,-0.012245227,0.016996486,0.041964486,0.0076094074,-0.059354536,-0.00890689,0.04151312,0.041203205,-0.02703405,-0.010446228,-0.002790584,-0.005333182,-0.061791066,0.0022241296,-0.076351374,0.0075872745,-0.021397753,0.039810847,0.047479767,-0.04068693,-0.040529273,0.017161863,-0.0064672064,-0.029559787,0.053273227,0.026144827,0.043205563,0.04147127,0.033244237,-0.010817643,-0.0763827,-0.02980584,-0.026680725,0.030411696,-0.03183793,0.08593696,0.009228527,-0.0060004485,-0.0030576903,-0.015378357,-0.028201053,-0.051746413,-0.0026684047,0.00072587345,0.0058327187,0.00788703,0.012022394,0.032590702,0.0062281033,-0.034287736,0.12410188,0.016488772,-0.029872965,-0.035241585,-0.024882944,0.0050097476,0.020796772,0.03197507,-0.0078003574,0.043865442,-0.028237289,0.020617891,0.079858564,-0.01547708,-0.04212793,-0.027614828,0.0014490498,0.05040207,0.01736365,-0.014421541,0.012359265,-0.07501513,0.011574436,0.00445513,-0.04357332,0.007965681,-0.01026195,-0.030535381,-0.045132726,0.03572566,0.005246326,0.010900461,0.014627864,0.018366933,0.016462145,0.01948785,0.023267843,-0.04776227,0.028281245,-0.025999784,-0.020208139,-0.019821003,0.021564277,0.038545594,0.01796162,-0.041306842,0.029119391,-0.006352317,-0.006566696,-0.026098259,0.07252521,-0.039285626,0.051117674,-0.005559514,-0.0124523705,0.064685605,0.007028718,0.030020587,-0.028034158,0.007339331,-0.015445237,-0.0061553014,-0.0030059216,-0.039620068,-0.056542926,0.019061092,0.009232358,-0.0007647376,0.050861023,0.023939047,-0.04481313,-0.019496443,-0.006324025,-0.028313637,0.07785678,0.048086237,-0.019199241,-0.07642821,0.007131332,-0.0371638,0.05970892,0.006467346,-0.009140283,0.002924115,0.0026468283]	Keywords: adaptive computation time, universal transformer, dynamic halting, recurrent neural networks, transformers\nKey Objects: representations, symbols, halting probability, module\nRefers to Images: None\nHypothetical Questions:\n- How does the dynamic halting mechanism contribute to efficiency in UT?\n- What is the purpose of using a module that is shared over depth in UT?\n- How does the halting probability influence the overall computation process in UT?\n---\nSummary:\nUniversal Transformer (UT) refines representations iteratively using a shared module and a dynamic halting mechanism, which stops recurrence when a symbol's halting probability exceeds a threshold or a maximum step is reached.\nOriginal Text:\nUniversal Transformer (UT) [26] incorporates a recurrence-over-depth mechanism that iteratively refines representations for all symbols using a module that is shared over depth, as illustrated in Fig. 12(a). It also adds a per-position dynamic halting mechanism that calculates a halting probability for each symbol at every time step. If a symbol's halting probability is greater than a predefined threshold, then the symbol's representation will remain unchanged for subsequent timesteps. The recurrence is stopped when all symbols halt or when a predefined maximum step is reached.  \nConditional Computation Transformer (CCT) [7] adds a gating module at each self-attention and feed-forward layer to decide whether to skip the current layer, as illustrated in Fig. 12(b). The authors also introduce an auxiliary loss that encourages the model to adjust the gating modules to match the practical computation cost to the available computation budget.  \nFig. 12. Three typical ACT paradigms.\nContextualized Text:\nAs an Adaptive Computation Time (ACT) approach, Universal Transformer (UT) iteratively refines representations for all symbols using a shared module and a dynamic halting mechanism. This mechanism calculates a halting probability for each symbol at every time step, halting the recurrence when the probability is above a threshold or a maximum step is reached.	{"tags": ["architecture", "NLP", "transformer", "ACT", "UT"], "doc_id": "1afda106-ab2d-42ec-9f0b-7ca1a5a37e8f", "summary": "Universal Transformer (UT) refines representations iteratively using a shared module and a dynamic halting mechanism, which stops recurrence when a symbol's halting probability exceeds a threshold or a maximum step is reached.", "doc_type": "text", "entities": ["Universal Transformer", "UT", "CCT", "Conditional Computation Transformer"], "keywords": ["adaptive computation time", "universal transformer", "dynamic halting", "recurrent neural networks", "transformers"], "key_objects": ["representations", "symbols", "halting probability", "module"], "contextual_text": "As an Adaptive Computation Time (ACT) approach, Universal Transformer (UT) iteratively refines representations for all symbols using a shared module and a dynamic halting mechanism. This mechanism calculates a halting probability for each symbol at every time step, halting the recurrence when the probability is above a threshold or a maximum step is reached.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.3 Adaptive Computation Time"}, "hypothetical_questions": ["How does the dynamic halting mechanism contribute to efficiency in UT?", "What is the purpose of using a module that is shared over depth in UT?", "How does the halting probability influence the overall computation process in UT?"]}
cf0fab84-1ff7-4552-9e5d-4225054b1b94	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.015040012,-0.01344545,0.04091863,0.020079838,-0.0015591008,0.011741772,0.016808787,0.020822339,0.031392775,-0.009684078,-0.038490843,0.014547486,0.006873824,-0.008847532,-0.050262585,0.0006722988,0.016877174,0.08267579,-0.009221299,-0.02145207,0.03481842,-0.008151951,0.01649084,-0.013738255,0.020381352,0.040382657,0.03795017,-0.01978395,-0.004831987,0.01741429,-0.00020202548,-0.08340509,0.026401749,0.04813232,0.03316947,0.024048518,0.006511424,-0.0728866,0.005884847,0.009034785,-0.043174367,0.053197548,-0.03014627,-0.02160699,-0.0047783074,-0.045873407,-0.07652295,-0.07542783,0.02837497,-0.0170679,-0.0034483927,0.0029475742,-0.023722848,0.001606464,0.0016581232,-0.0045837103,-0.035146717,-0.042908106,0.0023332448,-0.094693884,0.012364536,0.012321515,-0.035928633,0.009347603,-0.005827412,0.01575957,0.0072143697,0.005226848,0.037836075,0.11642223,-0.011590467,0.0010672384,0.008235154,-0.0005971156,0.13279556,0.08295719,-0.023322064,-0.029716995,-0.028068008,-0.020693054,-0.026241671,0.04633502,-0.06563268,-0.006517093,0.12646116,-0.010965414,-0.018098228,0.025624154,0.08152983,0.0075587984,-0.010788782,-0.017492667,0.020098764,0.011423157,-0.033612497,-0.048056778,-0.021256523,-0.029439555,0.04608332,-0.011975662,-0.0124966055,-0.02783174,0.043106265,0.078324914,0.05598532,-0.063797586,-0.014190382,-0.056253843,0.04564754,-0.01808838,0.0068558534,0.025874628,-0.029925222,0.0021457647,-0.009810624,-0.020567395,-0.022918893,0.021590676,0.010222946,0.015719965,0.060788546,-0.014101014,-0.040959645,0.015418021,0.05378222,0.029652635,-0.015283546,-0.026197128,-0.038044248,-0.0040971134,0.018378189,0.005506865,-0.005088681,0.022692354,-0.011207151,0.008688058,0.0575043,0.022025974,0.008973251,0.0056247525,-0.044826828,-0.024773398,-0.054733627,0.011820729,0.0041504093,0.035514764,-0.085564144,-0.010350536,-0.00030027647,0.06821102,0.027894272,-0.000908228,-0.02471872,-0.033562418,-0.0060845893,-0.028503258,0.0034353368,-0.04175627,-0.0417793,-0.053691674,-0.053165067,-0.0067514526,-0.013307914,0.026036829,0.05532945,0.12687916,0.0141712725,-0.004001631,0.0021655706,-0.036392994,-0.039036248,-0.02864028,0.006775771,-0.029497907,-0.019210462,0.0026124157,0.023786433,0.005753744,-0.021147091,0.0024885698,0.0034419047,0.0432133,-0.006686607,0.016297197,0.010225203,0.028055968,-0.0060226186,0.035774812,-0.014362231,0.0046298094,-0.0034531904,-0.024278212,0.037949454,-0.06228259,0.07201809,0.029091092,0.051657703,-0.07306416,-0.023058925,-0.046238232,-0.012450207,-0.020512762,-0.008215663,-0.014945843,0.060172487,0.015987448,0.009700028,-0.039395943,-0.016395489,0.024161097,-0.01841665,0.021877963,-0.017225984,0.072501756,0.019242994,0.021716526,-0.034771644,0.04002524,0.056079514,-0.013387623,-0.026066912,-0.022558304,0.0069795474,0.033212207,-0.029121164,-0.023176739,-0.046957105,0.023060314,0.041476987,-0.08275009,-0.014635729,0.012893352,-0.018489638,0.054386146,-0.011665929,0.011229759,0.02068835,0.056220975,0.047078203,0.03864364,0.025275638,0.03862665,0.017385567,0.08153137,0.0021721977,0.017412687,0.003626178,0.04254442,0.05839098,0.0036342614,-0.008936454,0.021270707,0.029227227,-0.037508722,-0.008109961,-0.05695714,-0.02809553,0.011007863,0.03178714,-0.02013892,0.027578691,0.01511188,-0.013113141,0.06011107,0.012799265,-0.028473454,0.018237945,0.03230055,0.023862185,0.030379143,0.039891273,0.017831778,0.016136058,-0.029964428,-0.006720109,0.049750626,0.010971521,0.014258482,0.045327,-0.00731876,-0.0008505443,0.01722992,0.046945866,0.033044204,0.02857357,-0.027006963,0.0032338672,-0.034787565,-0.06391567,-0.0031442414,0.03089379,-0.030491399,0.049598817,-0.01500028,-0.010270101,0.017336085,-0.009341818,-0.01888697,-0.03284579,0.059998393,0.07499757,-0.015400667,-0.027041437,0.014900922,-0.0074296766,-0.0077903606,-0.01594159,-0.008713686,-0.00034176186,-0.024819251,0.016046822,-0.012798775,-0.03414076,-0.014960462,0.033014014,0.08029895,0.07030986,-0.014755591,-0.007953533,-0.011201431,0.0067543387,-0.05301037,0.0077155936,0.02592865,0.020334234,-0.016460082,0.030889174,-0.026434837,-0.10330113,0.05480635,-0.07853087,0.05419528,0.0739531,0.003837094,-0.0070901,0.011708968,0.033081546,0.0068151886,0.0069176443,-0.03475873,-0.024709955,-0.060218215,-0.0040735556,0.024791615,-0.027562886,0.03836538,-0.03625679,-0.012289778,-0.023074105,0.0010088355,-0.02074203,0.02827055,0.026396723,0.028889205,0.042789992,-0.0144097535,-0.0019937637,-0.023893997,0.03846551,-0.026992695,0.022132821,-0.036153242,0.05136961,-0.044339333,0.006055709,0.00022274422,-0.027658204,-0.01853351,0.014183741,0.018917369,-0.054800022,-0.015318487,0.057471648,-0.035633437,0.036106456,0.024598429,0.046114296,-0.027571928,-0.033397954,-0.0077745374,-0.0066618505,-0.007749137,-0.013472203,0.017558811,0.045559943,0.033821512,-0.002907189,0.0771894,-0.024609044,0.060847964,-0.015988996,0.012821991,0.04230045,-0.019960374,-0.026628844,0.018283429,0.030420726,0.047594752,-0.00549401,0.009680195,0.026324036,0.029556217,-0.031181134,0.0694684,0.007584734,-0.058877647,-0.046925865,0.061510324,0.067724995,0.0007720179,0.041490883,-0.019044086,-0.012858934,-0.026588233,0.022772294,0.03159471,-0.03561508,0.02010866,-0.030054642,-0.035443123,-0.0470059,0.06498672,0.017704746,-0.017706346,0.06935715,-0.018439835,-0.040643554,0.050032288,-0.0070253257,-0.0038947903,0.00956026,-0.08410296,-0.013037827,0.027833324,0.0025229747,-0.037406918,-0.061194073,0.04675844,-0.048664294,0.007340423,0.050054673,-0.04652439,0.019816037,0.0056410753,0.008695971,0.008851032,0.029788217,0.034469273,-0.011496899,-0.022612393,0.04153809,0.015914468,-0.0022011697,-0.03799414,0.051499207,-0.02397554,0.009037298,-0.060841445,-0.043692272,0.037358135,-0.011716732,0.023205461,0.032842647,-0.028599158,0.029944995,-0.0060356944,-0.025420843,-0.011707965,-0.02505923,-0.019679328,-0.015515001,0.0493441,-0.059532326,0.022752598,-0.04120904,0.0028798804,-0.027557323,0.101133645,0.061921198,0.020286603,-0.082590535,-0.061019596,-0.008434006,-0.00048026317,-0.012220024,-0.036297034,0.011517898,-0.010051492,-0.017471526,0.035317864,-0.027512252,0.017629534,0.0009845224,0.045919787,0.0044998596,-0.060020663,0.009264379,0.032310773,-0.026204374,-0.02758585,-0.05865535,-0.002458836,0.026406767,0.03218103,0.00030431396,0.019125102,0.0029384056,-0.032589473,0.010414801,0.06975896,0.0013709553,-0.058008477,-0.006496383,-0.041349128,-0.03769316,-0.027651727,0.0037872149,0.005412099,0.04277148,0.051907852,0.04694226,0.0064227907,0.053264912,-0.03555858,0.023920234,-6.230436e-05,0.0151381735,0.028487258,0.040637765,-0.07425469,0.043000072,-0.019326609,0.0013511517,-0.017630136,-0.059000667,-0.050768085,0.008798269,-0.038597215,0.029459357,0.008580426,-0.012517456,-0.029689742,0.01735432,-0.04861178,0.07188855,-0.09790572,0.015364421,0.020007567,0.004008264,0.091544226,0.04852729,-0.01810969,0.04727375,0.0023580987,-0.068603165,0.02701642,0.05612717,0.011559569,-0.0064163916,0.040709984,-0.020557495,-0.022187244,-0.06025988,-0.103363834,-0.019144438,-0.022845728,0.02275829,-0.019969933,-0.030772397,0.0006628412,-0.002252683,-0.018425042,0.0062815305,0.06624695,-0.002548109,0.07396172,-0.0042411247,-0.004718037,0.015372216,-0.006026903,0.08109791,0.019940572,0.030973332,0.006248863,8.397186e-05,0.0075746193,-0.012549507,0.0017211353,0.023580795,-0.009046886,-0.043963462,0.0050776643,0.014889717,0.0005421437,-0.032740653,-0.009190688,0.03491593,0.009968824,0.03411852,-0.019016365,-0.049796406,-0.048773058,-0.028184934,-0.06013324,0.026121283,-0.026556209,0.03755202,0.010884591,-0.025099318,0.036270346,-0.06563056,-0.038928676,-0.026138324,0.047463566,-0.02707157,-0.030434558,-0.023372613,0.020638108,-0.049633376,-0.09757581,0.05470374,0.041625626,-0.025049623,0.0009209957,-0.0049697347,-0.039048564,0.009733942,0.018230857,-0.007133174,0.011619291,-0.010401426,-0.016235739,0.054540843,0.012691475,0.0323534,-0.017264528,-0.005334236,-0.054882113,-0.04667523,0.036368147,0.016451823,-0.01864297,-0.016355665,0.038501356,-0.032387625,-0.016596708,-0.000600475,-0.07400924,0.02439293,-0.032893654,0.0074336114,0.06369486,-0.034628145,-0.041470505,0.038767863,0.003120341,-0.041391373,0.009041131,0.009428753,0.041487906,0.02643308,0.036309373,-0.0073733754,-0.08793515,-0.023719976,-0.04938139,0.03660976,-0.0056005022,0.049243335,-0.01970483,0.0039004548,0.009746548,0.017958932,0.01028547,-0.04015068,-0.022865966,0.018243132,-0.046743553,-0.017725207,-0.001893459,0.02902337,-0.00031831,0.04113376,0.11807442,0.04527881,0.010292985,-0.03687362,-0.034722686,0.026821118,0.022819782,0.029646687,0.03276489,0.045757953,-0.037740123,-0.01874611,0.044050857,0.032376975,-0.039102796,0.029645534,0.02877592,0.025354965,-0.018285073,0.012447396,0.016254123,-0.07254129,0.019149706,0.013950486,-0.030945351,0.025320347,-0.016518023,-0.043304767,-0.044555817,0.04012832,-0.0016936816,0.0014327933,0.019207137,-0.011983853,0.006754901,0.020755928,0.009121624,-0.0015389761,0.013026588,0.006109378,0.01846437,-0.020045575,0.014329999,0.021305248,0.039477617,-0.013887156,0.030566752,-0.011782744,-0.016217252,0.033330657,0.0805142,-0.047625937,0.052582566,0.019625198,-0.024276545,0.08865955,-0.008359856,0.033934116,0.0012148358,0.039538957,-0.016779318,-0.005393128,-0.057456043,-0.05313065,-0.05512646,0.060055763,0.0056424113,0.016096435,0.015127764,-0.026370902,-0.03616655,-0.05657918,-0.0032672514,-0.030235397,0.0779764,0.010767291,-0.037605792,-0.055532176,0.016771033,-0.0011401952,0.060352575,2.4469082e-05,0.011875162,-0.009925832,0.017497512]	Keywords: Adaptive Computation Time, Transformer models, Universal Transformer, Conditional Computation Transformer, early exit mechanism\nKey Objects: Adaptive Computation Time, ACT paradigms, Fig. 12\nRefers to Images: ./images/a-survey-to-transformers/image_13.png\nHypothetical Questions:\n- What is the primary goal of introducing Adaptive Computation Time (ACT) into Transformer models?\n- How do Universal Transformers and Conditional Computation Transformers implement ACT differently?\n- What are some potential benefits of using ACT in terms of efficiency and accuracy?\n---\nSummary:\nFigure 12 illustrates three different approaches to Adaptive Computation Time (ACT) within Transformer models.\nOriginal Text:\nFig. 12. Three typical ACT paradigms.  \n\nContextualized Text:\nFigure 12 provides an overview of three common paradigms for Adaptive Computation Time (ACT) in Transformer models. These approaches, which aim to condition the computation time based on the input, include variations like those found in Universal Transformers and Conditional Computation Transformers.	{"tags": ["architecture", "transformers", "computation"], "doc_id": "cf0fab84-1ff7-4552-9e5d-4225054b1b94", "summary": "Figure 12 illustrates three different approaches to Adaptive Computation Time (ACT) within Transformer models.", "doc_type": "text", "entities": ["Transformer", "Universal Transformer", "Conditional Computation Transformer"], "keywords": ["Adaptive Computation Time", "Transformer models", "Universal Transformer", "Conditional Computation Transformer", "early exit mechanism"], "key_objects": ["Adaptive Computation Time", "ACT paradigms", "Fig. 12"], "contextual_text": "Figure 12 provides an overview of three common paradigms for Adaptive Computation Time (ACT) in Transformer models. These approaches, which aim to condition the computation time based on the input, include variations like those found in Universal Transformers and Conditional Computation Transformers.", "mentioned_images": ["./images/a-survey-to-transformers/image_13.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.3 Adaptive Computation Time"}, "hypothetical_questions": ["What is the primary goal of introducing Adaptive Computation Time (ACT) into Transformer models?", "How do Universal Transformers and Conditional Computation Transformers implement ACT differently?", "What are some potential benefits of using ACT in terms of efficiency and accuracy?"]}
7aff8ecf-de15-4753-868e-cd8ced0a3199	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.06989822,0.0106297815,0.0072964863,0.06808001,-0.06911245,0.0016711773,-0.07456543,0.029455557,0.030729268,0.021387521,-0.030600302,-0.059354648,-0.0026199203,0.012394573,-0.01423998,0.048471577,0.05464222,0.06811386,-0.010370824,-0.04234275,0.057033285,-0.003068501,-0.022700263,-4.6818946e-05,0.04927343,0.058919158,0.009878488,0.013797266,-0.043899983,0.011608694,0.035046544,-0.0045411726,0.041680455,0.043053415,0.036625426,-0.013913019,-0.0145739205,-0.03725175,0.011929651,-0.0060716546,0.0010480181,0.049397845,0.052287456,-0.007904755,-0.0059708254,-0.047673013,-0.098955974,-0.0056860843,0.0064962455,-0.044129115,0.012694008,0.019322013,-0.029594334,-0.0028887833,0.0063783866,-0.029953195,-0.05526488,-0.025469346,-0.052973695,-0.05800868,-0.021836048,0.008440892,-0.006046291,-0.025367085,0.0011985668,0.004533934,-0.0014715489,0.0131570455,0.02077938,0.143812,-0.021103607,0.011230256,0.0025480306,-0.07502617,0.12248949,0.04005931,0.009116274,0.023093443,-0.00044936044,0.012509033,0.059634384,0.04114401,-0.06386589,-0.06893156,0.05112414,0.020858537,-0.06805622,-0.02122414,0.09617399,-0.044184167,0.028231133,-0.033417474,0.006580824,0.05213343,-0.013110482,-0.06987096,-0.022622004,0.0009436037,-0.0564027,-0.03335733,-0.0005178791,-0.024563238,0.019058459,0.06342667,0.0326921,0.045322143,-0.006241916,-0.03413413,-0.01559678,0.0033097654,-0.009924727,-0.05051301,-0.0026595008,0.002471238,-0.02841392,-0.0146072265,-0.0059758807,-0.007933284,0.05044601,-0.026792197,0.06277656,0.013209946,-0.018195303,0.042826544,-0.0031808682,0.06222575,-0.0464994,0.007995832,-0.010334741,-0.05970296,0.029994791,0.03650669,-0.036735427,0.03824616,-0.046379067,0.015734803,0.028523939,0.0048336578,-0.027175883,-0.051643513,0.037673045,-0.06032794,-0.078296214,0.027261917,0.034171138,0.020229831,-0.07644696,-0.0072004595,-0.021856265,0.13025345,-0.005282158,0.019725641,0.03327391,-0.024504926,-0.014614455,0.035072517,-0.08184714,-0.018106515,-0.04474751,-0.079676144,-0.0858201,-0.022650015,-0.014315271,0.079203285,0.059317,0.07410554,0.006100746,0.07354374,-0.032695092,-0.010558021,-0.0012201533,-0.06243479,-0.015955556,-0.026904846,-0.0047317063,-0.01583251,0.03604935,-0.042261958,0.014457619,0.0047834045,-0.0267453,0.03652402,-0.0036988514,0.03275078,0.002071171,0.037168067,0.013315547,0.02743932,0.04311432,-0.013580942,0.03728635,-0.004058583,-0.0017097137,0.051293697,0.041353833,0.02165527,0.043134592,0.0065918867,0.022445157,0.004153619,-0.024681529,-0.03778055,-0.06278968,0.080087975,-0.0200718,-0.043842144,0.0024576252,0.02235328,-0.07004412,0.045260265,-0.009529363,0.0156084765,-0.019295186,0.032186862,0.024752693,0.034424797,-0.06657331,0.042095333,0.0014771984,-0.018231507,0.0051431,-0.018781431,-0.033013243,-0.018997481,-0.009123493,-0.08163134,-0.037377633,0.036047142,0.012670232,-0.0031818156,-0.0056503545,0.025547206,0.0074800393,-0.013769547,-0.007607776,0.022136759,0.017033862,0.033674423,0.047804132,0.0049674413,-0.00828002,0.020346826,-0.00082218053,0.07688322,-0.0009843244,-0.050953638,-0.007854726,0.0021978226,0.016322628,0.022603737,-0.05045432,0.026017567,0.0027503292,-0.046131995,-0.0044031995,0.0124250455,-0.036530256,0.0043958584,0.05351683,-0.04266343,0.037950788,-0.03936392,-0.037873168,0.07214691,0.031089388,-0.0072431606,0.034831136,0.038041774,-0.03584788,0.011278571,0.023891097,0.049891908,0.012816997,-0.040134553,-0.031086773,0.004288564,-0.038940284,-0.01103625,0.011635484,-0.039917286,-0.025786221,0.0064257374,0.044321097,0.014281713,0.016193423,-0.019040076,-0.0064660055,-0.054888263,-0.07219489,-0.024638865,0.006151707,0.01556616,0.045422826,-0.026614377,0.046278194,-0.0040947176,-0.02318622,0.019126706,-0.00028144298,0.050785676,0.07287473,-0.05501143,0.015324045,-0.031028232,-0.0032268667,-0.009852926,-0.01604105,0.015926171,0.0018801782,0.030908285,-0.020965455,-0.02100215,-0.0312304,0.021318352,0.041905355,0.06312843,0.05430697,-0.03326059,-0.034678157,0.07736589,0.047485705,-0.0050159525,-0.015977504,0.03892627,0.019476753,-0.015340031,0.065464795,-0.009947369,-0.073197365,0.048369877,-0.0033937595,0.00046322093,0.07059204,-0.0410806,0.013193151,0.030151922,0.04741525,0.02576676,0.0049982285,0.029531518,-0.04695227,-0.037084147,-0.03559937,0.0033898551,-0.030384693,0.0061517395,-0.053669702,-0.006273033,0.0007721096,-0.022841563,-0.00050557713,0.04436609,0.018995844,0.0032736503,-0.028850606,-0.014608978,-0.0038792177,0.06688459,0.042372294,-0.04648558,0.0025192902,-0.072560266,0.06294503,0.006972916,-0.0065281093,0.021432498,0.004476797,0.007851174,-0.045747567,0.013076176,0.0005890403,-0.04613829,0.0448694,-0.024831736,0.035646945,0.04699223,-0.0035461837,0.0061699934,-0.009807571,-0.02449047,-0.039246723,0.01925423,-0.0022089197,0.016871855,0.05584355,0.03765156,-0.031981483,0.042089753,-0.05128383,0.013011153,-0.014898263,0.053160187,0.01076402,0.0023238652,-0.014695423,-0.050306868,-0.026665868,0.03805756,0.036023974,-0.009101607,5.2419105e-05,0.08530488,-0.03414673,0.021749858,-0.0076025724,-0.02028832,-0.04910191,0.029207202,0.053800218,0.0037900978,0.05637569,-0.006765596,0.038389,-0.051986475,0.033810485,-0.031554285,0.018254403,0.004500455,-0.017315334,-0.029103845,-0.01924271,0.053863805,0.051518656,-0.02115271,0.0216636,0.008579051,0.015148954,0.04282739,0.010928578,-0.0636559,0.027592536,-0.015092734,-0.043456547,0.024541378,0.054981012,-0.018646514,-0.06410259,0.071576126,-0.062407855,-0.00028259444,-0.014815505,0.01545429,-0.011859771,-0.01579125,0.023636661,0.008243902,-0.037122123,0.03781487,0.00013933028,0.016483266,0.051387016,0.060027007,-0.014430006,-0.018502098,-0.00083306024,0.041707568,-0.022956787,-0.031202974,0.006976387,-0.011702823,-0.000332525,-0.02244213,-0.052466247,-0.023788411,0.011882592,-0.026796889,-0.04642645,0.008567669,-0.0013168548,-0.0023910725,-0.05857709,0.016216246,-0.0377443,0.022093166,-0.030773055,0.0070538493,-0.0047906004,-0.005219644,0.050075408,-0.031224921,-0.06441983,-0.03302516,0.0165905,-0.0467692,-0.07104193,-0.039942358,-0.036535665,-0.014957701,-0.022108482,0.017499378,-0.025123069,0.0018212325,-0.0036388962,-0.025997167,0.010364963,-0.029999295,-0.034314573,0.0051973867,-0.015487301,-0.0069852835,-0.0034373375,-0.0030100055,0.025327843,0.0046840045,-0.038390048,0.025468437,-0.011892874,-0.01511358,-0.03588688,-0.0027973424,-0.002443719,-0.026527379,-0.00010203994,0.021655997,-0.027088804,0.024767602,-0.029824369,-0.052675735,0.013253532,-0.011304229,0.05951849,0.014385352,0.016897798,0.018777784,-0.013906936,-0.009744142,-0.027431808,0.049004313,0.042764414,-0.047955174,0.004362928,0.0073895184,-0.010858842,0.0070811654,-0.033829093,-0.020391928,0.044661265,-0.0042813797,-0.022282729,0.0018535394,-0.010154171,-0.037456077,-0.009737044,-0.023990327,0.003964744,-0.0781326,0.008548322,-0.0070434846,0.039689727,-0.051600393,0.02964136,0.053148367,0.0003796464,0.06391085,-0.023012467,0.03616176,0.02792566,0.0015660258,0.050177515,0.016302822,0.012583275,-0.0148216905,0.022017902,-0.034436774,-0.021989694,-0.051206563,-0.023502545,-0.06097308,0.020401128,0.043725777,-0.032637496,-0.0060944376,0.008063867,0.04782778,-0.05758531,0.085633166,0.042470887,-0.011264492,-0.022116693,-0.013763683,0.017237782,-0.02440045,-0.021441115,0.03782345,0.06652723,0.0110494755,0.049493425,-0.014531356,-0.00011272317,0.052625082,-0.050281238,-0.005683465,0.014956377,-0.0052752653,0.01409002,-0.007874455,0.038213156,0.026209297,0.06296815,-0.020921823,-0.05348551,-0.042508375,-0.03189444,-0.015631996,0.03128342,0.014807753,-0.015525251,-0.026067054,-0.03733582,0.016349977,-0.021427305,-0.017052274,0.039927013,0.052356802,-0.055322606,0.0045170835,-0.0028561105,0.011109705,-0.0393392,-0.028903231,0.03893312,0.00698653,0.02237051,-0.0060897185,-0.0012530525,-0.03157153,0.04869728,-0.02568185,-0.051450454,0.0456599,0.059974153,0.004636849,0.088476904,-0.0012577045,0.014367216,0.013684426,0.058496512,-0.033774395,-0.034848798,0.019237371,0.05858205,0.047635265,-0.0210409,0.04109075,0.023339717,-0.060772505,-0.003303255,-0.05088151,-0.012850326,-0.06180005,0.019660601,-0.0083656395,-0.026770353,-0.028803498,0.0053434717,-0.03401778,-0.016643288,0.041094095,0.04920483,-0.01696338,0.0376001,0.025827115,0.0012946842,-0.063460425,-0.06017811,-0.010575889,-0.0020134274,-0.0014996044,-0.0038961659,-0.014426137,-0.047503658,0.023599034,0.004804015,-0.011933187,-0.0066372086,-0.0155037595,-0.061346352,0.0061930814,0.040873505,0.0033916847,0.06618772,-0.042123634,0.017583666,0.024877157,0.015238871,0.0012233723,-0.06931743,-0.0076786876,0.01865264,0.019777894,0.035061177,-0.022529704,0.05628465,-0.030715698,-0.00015884663,0.03271321,0.059451774,-0.046453465,-0.034891363,-0.013764038,0.031917784,-0.043048725,-0.001165022,-0.048697595,0.034512006,0.009543054,0.017267581,-0.0011088139,-0.020117031,-0.0356454,-0.051647805,0.04649519,0.0076817484,-0.034123924,0.08188634,0.018362395,-0.010917557,-0.05039629,-0.03643711,-0.008068134,-0.017865652,-0.026083894,0.03008865,0.018130686,0.051681414,0.025149776,0.04271024,0.041009482,-0.030516984,0.0397202,0.030097857,0.02007527,0.044435058,0.009036051,0.022258045,-0.011590867,-0.010249094,0.038185444,0.024970893,-0.037600473,0.0077664643,-0.06626982,-0.011731002,-0.024575897,0.01704598,0.0042842124,-0.052288376,-0.044590063,0.03184609,-0.04029865,0.008992284,-0.008651328,0.013171593,-0.014140549,-0.018563239,-0.08495007,-0.023025116,0.00082752533,0.041026287,-0.01743034,-0.07451192,-0.071240254,0.021731542,0.02726789,-0.022075012,0.042219166,-0.047108177,-0.015912313]	Image title: Three Control Flow Diagrams\nTags: control-flow, algorithm, program-structure, dynamic-halting, conditional-skip, early-exit\nKey objects: Decision, Process, Flow Lines, Dynamic Halting, Conditional Skip, Early Exit\n---\nSummary:\nThis image presents three distinct control flow diagrams illustrating different programming techniques: dynamic halting, conditional skip, and early exit. Each diagram uses basic control flow elements like decisions (diamonds) and processes (rectangles) to represent their respective workflows.\nFull description:\nThe first diagram (a) depicts 'dynamic halting'. It begins with a process and then transitions to a decision point labeled 'halt'. Depending on the outcome of this decision, the flow either terminates or loops back to the initial process. The second diagram (b) shows 'conditional skip'.  The flow starts with a process and then moves to a decision point labeled 'skip'. The decision determines whether the process continues or is bypassed. The third diagram (c) illustrates 'early exit'. The flow begins with a process and directly proceeds to a decision point labeled 'exit'.  Based on this decision, the flow either terminates the process or returns to a previous state, which is not depicted in the image.\nText found in image:\n- halt\n- skip\n- exit	{"tags": ["control-flow", "algorithm", "program-structure", "dynamic-halting", "conditional-skip", "early-exit"], "title": "Three Control Flow Diagrams", "doc_id": "7aff8ecf-de15-4753-868e-cd8ced0a3199", "source": "./images/a-survey-to-transformers/image_13.png", "summary": "This image presents three distinct control flow diagrams illustrating different programming techniques: dynamic halting, conditional skip, and early exit. Each diagram uses basic control flow elements like decisions (diamonds) and processes (rectangles) to represent their respective workflows.", "doc_type": "image", "key_objects": ["Decision", "Process", "Flow Lines", "Dynamic Halting", "Conditional Skip", "Early Exit"], "parent_doc_id": "cf0fab84-1ff7-4552-9e5d-4225054b1b94", "text_in_image": ["halt", "skip", "exit"], "contextual_description": "The first diagram (a) depicts 'dynamic halting'. It begins with a process and then transitions to a decision point labeled 'halt'. Depending on the outcome of this decision, the flow either terminates or loops back to the initial process. The second diagram (b) shows 'conditional skip'.  The flow starts with a process and then moves to a decision point labeled 'skip'. The decision determines whether the process continues or is bypassed. The third diagram (c) illustrates 'early exit'. The flow begins with a process and directly proceeds to a decision point labeled 'exit'.  Based on this decision, the flow either terminates the process or returns to a previous state, which is not depicted in the image."}
7014b697-c5e1-4db9-a5a6-c95cfac9cf68	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02060987,-0.027039384,0.01244337,0.05539409,-0.0679641,0.06988356,0.02326497,0.054083593,0.02549601,-0.033078168,-0.04035775,0.025659723,0.025904974,0.028567402,-0.06816891,0.012714599,0.010995244,0.09485467,0.038807243,-0.0047169416,0.06796337,0.0032104293,0.01970099,0.011288736,0.00061115675,0.08193795,0.037172157,-0.045128185,-0.027875258,-0.011870241,-0.013671296,-0.018303392,0.030665673,0.031835657,0.017744238,-0.015551466,0.011266723,-0.032939617,-0.0055369646,-0.026017308,-0.0059714643,0.044021327,-0.07916499,0.008554844,-0.054190885,-0.05639854,-0.07926984,-0.060308166,0.05805128,-0.026430562,0.044119675,0.023539698,-0.0506557,0.0038619332,-0.004087807,-0.00060241984,0.042109996,-0.028179597,-0.011225187,-0.06381664,-0.03167605,0.036895636,-0.0030930382,0.0073148026,0.06536058,-0.0140031455,0.005796695,-0.012085145,0.033590626,0.11509003,0.013620923,0.0026044913,0.006484072,-0.03599805,0.12737387,0.033503477,-0.065218516,-0.019921564,-0.06939122,-0.008525974,-0.012629124,0.053231895,-0.051852852,-0.06677223,0.07104594,-6.515773e-05,-0.0775792,0.044120617,0.059017338,-0.0005385296,-0.020087838,-0.008232878,-0.028714687,0.05931986,0.021141494,-0.0314531,0.0027635605,-0.019664379,0.0099872,-0.052330974,0.010596937,0.007339117,0.086806044,0.082471415,0.03330949,0.0057304827,-0.009324268,-0.007860914,0.008116983,0.012510968,-0.0060213944,0.027719615,-0.0160523,-0.01677232,-0.002234709,-0.027543534,-0.006251921,0.0041661635,-0.03767357,0.01597825,0.014271062,-0.006017385,-0.024474783,0.024972705,0.03044336,0.022117522,0.0051303585,-0.041984905,-0.03174753,0.022215486,0.05375685,-0.009434823,0.0019248757,0.029047508,-0.0318804,0.003863288,0.088096656,0.0005189783,-0.013435856,-0.0040328875,-0.02124777,-0.026737357,-0.04888381,0.013971634,-0.043769263,0.021716904,-0.08929184,0.005324723,-0.06294139,0.079541266,0.0069616027,0.013507187,0.05150516,-0.0043925433,-0.014473923,-0.019855876,0.003467153,-0.05678312,0.0038537707,-0.08366275,-0.07742345,-0.05626568,-0.034783937,0.04223477,0.080983035,0.09981816,0.008475945,0.02877933,0.03167457,-0.0031821,0.002126118,-0.013221285,-0.0019342838,-0.020206945,-0.007371701,0.000264699,0.039544877,-0.014814985,0.022208497,0.01961308,0.0076869023,-0.0051110326,0.0016745373,0.0061984537,-0.020926287,0.045282934,0.045479216,0.04570684,-0.07014858,-0.024949098,0.038175646,-0.0064102607,-0.041511707,0.021149421,0.054448564,0.040836163,0.056681283,-0.0067225457,-0.027906258,-0.043286443,-0.029449373,-0.023353329,0.018369587,0.048571788,0.041395403,-0.034191675,0.028557539,-0.018798487,-0.020444192,0.06290107,-0.041176844,-0.021045731,-0.05484073,0.027959445,0.0039656977,-0.025109392,-0.03477302,0.013563804,0.03651973,-0.023775367,-0.01933151,-0.006740979,-0.00053160125,0.017678995,-0.010107664,-0.007845656,-0.03827622,0.049706988,0.014575103,-0.004394375,0.03995427,0.028440904,-0.065405466,0.02831255,-0.041871816,0.023766581,-0.016114177,-0.011527531,-0.010733925,-0.012124153,0.02811002,0.04280994,-0.033017512,0.100360125,0.043722015,-0.04989858,0.0072947084,0.0396144,0.018252539,0.04746391,-0.018573543,0.055646818,0.027239548,0.03679425,-0.0026988809,-0.043554116,0.016873552,0.017498365,0.039964996,-0.0033593515,0.0131486105,-0.051413838,0.0140425665,0.0315385,0.006283122,-0.006287365,-0.013586637,0.013872401,-0.010841388,0.02391887,0.04508881,0.020815775,0.024536358,-0.048725262,-0.033677906,0.026746975,0.0054018744,-0.0003592588,0.0005323145,-0.050569043,-0.06376986,-0.023323357,-0.011783073,-0.0055317506,0.031184569,0.012954758,-0.012399407,-0.0069125625,-0.06305138,-0.003527838,0.004408388,-0.020611694,0.053861562,-0.012011164,0.022948647,0.0093916515,-0.004904086,-0.032715656,-0.043952707,0.046526752,0.1096663,0.001317563,-0.0016990504,-0.025721733,0.04181981,-0.0047420417,-0.116292104,0.022233518,0.008453441,-0.0030104609,0.037705556,0.0029971336,0.014140144,-0.0062467256,0.0001417859,0.041043963,0.071206234,-0.046012096,0.02210822,0.0302086,0.02140022,-0.023024663,-0.018862141,0.0031642755,0.0054797656,-0.02666183,-0.012345265,-0.018713271,-0.07987723,0.10514175,-0.086740606,0.012563849,0.06220492,0.0136863375,-0.027115535,0.018230388,-0.011285886,0.04785425,-0.02611626,-0.00063852477,0.008144572,-0.04350153,-0.024482617,0.013405726,-0.07170523,0.018104728,-0.008766973,-0.022151172,0.049633615,-0.031441018,0.019010011,0.006311364,0.03581613,0.04622398,0.034757145,-0.009101808,0.0013578641,0.028279476,0.045198463,-0.013714683,0.036048684,-0.06300921,0.039215043,-0.009822364,0.01298631,-0.015228305,-0.03441391,0.0007204146,0.01081296,-0.012889091,-0.0019440656,-0.041126244,0.046112128,-0.046666652,0.03633314,0.04504069,0.021519668,0.043783475,-0.029568389,-0.0006739096,0.00361645,0.010465866,-0.023230914,0.013485367,0.02906539,0.05684176,-0.00482691,0.054405745,-0.020045279,0.047630217,-0.039666273,0.06390323,0.04869971,-0.009625737,-0.043320194,-0.058889043,0.0065472643,-0.009984422,0.025975691,-0.030279834,0.015458882,-0.0028792028,-0.054148663,0.03809113,0.047424003,0.050000224,-0.09186015,0.05189318,0.072867155,0.042358194,0.035020806,-0.044583835,-0.049093485,-0.025295416,-0.007013387,0.0066283657,0.0072247707,0.053290125,-0.03153662,-0.018659659,-0.013223144,0.026290772,0.03586145,0.06674831,0.01787253,0.019649887,0.012662587,0.010507811,0.011029701,-0.009961771,0.057255086,-0.10108592,-0.021154782,0.046840977,-0.010640733,-0.0013607445,-0.029764194,0.04474035,-0.022468762,0.015552042,0.048498083,-0.032169584,-0.002596312,-0.0018564953,0.0064032306,-0.03207709,0.0074997325,0.0026741456,-0.024397442,-0.051719707,0.04381379,0.04964605,-0.0080916835,-0.0068899654,0.017565947,-0.02706956,-0.0036416943,0.007826125,-0.029569894,0.062977165,0.021897888,0.0036608714,-0.018535677,-0.025744257,0.027988976,-0.01931404,-0.010478439,-0.042780068,-0.038364682,0.008191921,0.03847782,-0.009121931,-0.05689336,0.042845037,-0.00077816314,-0.005780783,0.0028538322,0.004429443,0.042495478,-0.010919145,-0.042750888,-0.039568402,0.0038372527,0.019202672,-0.034193154,-0.027575435,-0.03442032,-0.01794383,-0.012471906,0.02528929,0.0018027674,0.044664014,0.034141447,0.06989025,0.016930187,-0.03175827,-0.009992208,0.010181292,-0.016272342,0.011800902,-0.013435265,-0.0054777884,0.02490304,-0.0018781407,0.011667415,-0.029500347,-0.0186648,-0.005418189,-0.036307875,0.041581582,0.042831898,0.028377222,-0.0067896545,-0.045584522,0.03192998,0.008571817,0.009738437,-0.033449456,-0.0069651883,0.017440524,-0.01760183,-0.004233779,0.033031393,0.04379693,0.01845352,0.019238323,0.023319304,0.0069399173,0.037697446,-0.01471616,0.0030893222,-0.053094465,-0.020586394,0.04371155,-0.040266905,-0.015133028,0.018502602,-0.031310625,-0.00850673,0.0024715203,-0.019323274,-0.024394728,0.03238071,0.032696784,0.05974801,-0.07196804,0.029230857,0.028317358,-0.0017875317,0.053742863,-0.0059537585,0.0008327016,0.0098304525,0.022165727,-0.07166316,-0.002595725,0.040492475,-0.016443327,0.00838839,0.05397381,0.015610624,-0.0051592286,-0.02761744,-0.03927402,-0.0019463266,-0.01694066,-0.0042702965,-0.030994238,-0.027288081,0.014845256,0.010592674,0.009869944,0.024177454,0.074945934,-0.024121754,0.019021634,0.020976976,-0.016310366,-0.010397341,-0.022261811,0.01956175,0.014075259,0.016515201,-0.024427896,0.0104787005,0.008123682,0.018904714,-0.010951526,-0.025139714,-0.022384493,-0.010189573,-0.041816298,0.03250349,0.0030717917,-0.030830543,-0.01676233,0.074713804,0.018289413,0.02075262,-0.0037702331,-0.06901338,-0.04829333,-0.02071608,-0.057976577,0.023975374,-0.009086395,0.047137298,0.010566361,-0.033906378,0.06450466,-0.02543996,-0.02298056,-0.029837854,0.060174186,-0.01354325,-0.052400604,-0.011685416,0.009272839,-0.0055418937,-0.09455404,0.048546866,0.034730323,-0.02989795,0.007903183,0.003279927,0.036799025,-0.00023922017,-0.03938879,-0.007389694,0.014782068,0.006008652,-0.009747292,0.09038491,0.017111719,-0.015049128,0.00023111532,0.032208726,-0.08323616,-0.0001653647,-0.035647374,0.012938291,-0.00037961875,-0.011577831,0.012965045,0.01838354,-0.03153745,-0.025056772,-0.056288783,-0.010714849,-0.041319177,-0.006616048,0.018389324,-0.043012273,-0.032681342,0.005989729,0.024494452,0.013484639,-0.01449211,0.009662848,0.051215664,0.081657976,0.02686644,-0.020982403,-0.08587703,-0.034249034,-0.02223561,0.01907681,-0.0013901085,0.08647576,0.03642379,0.03242279,0.026964886,0.044075966,-0.030453237,-0.05145344,-0.019237977,-0.06495548,-0.0055581755,-0.009799918,-0.011346406,0.023899596,0.016235154,-0.037975173,0.08131304,0.015589147,-0.027109249,-0.023061024,-0.056540314,-0.0025046482,0.026318222,-0.0147573,-0.011774401,0.03809181,0.018504089,0.044322282,0.030203257,0.033815943,0.02609479,-0.049724843,0.040096126,0.05041027,-0.021676334,-0.019740617,-0.038909834,-0.033862002,0.07068635,-0.0140734445,-0.021681728,-0.0039617047,-0.05393787,-0.040112715,0.0026711938,0.01583245,-0.06861545,-0.053008657,0.0061196378,0.0118198,0.004904317,-0.0014820442,0.060461476,-0.031755876,-0.04734454,-0.049859,-0.020996986,-0.030944543,0.08518574,-0.015418433,0.030283708,-0.007415078,0.025874931,-0.0034549092,-0.0016616697,-0.022525767,0.04245898,-0.03759907,0.011506109,0.010612118,-0.004395983,0.07495363,-0.0255675,0.027297655,-0.052747108,0.016712049,0.0044757244,0.020271556,-0.016731914,-0.043126017,-0.061633844,-0.0032652377,0.0025038507,0.008452173,0.007206968,0.019944139,-0.054678645,-0.07965171,-0.034406446,0.0014079188,0.04505148,0.03684732,-0.021323303,-0.072971046,0.008719027,-0.008346353,0.041537642,0.00027502375,0.023358285,0.03126092,-0.03261975]	Keywords: early exit, adaptive computation, speed-accuracy trade-off, internal classifier\nKey Objects: internal classifiers, early exit mechanisms, number of layers\nRefers to Images: None\nHypothetical Questions:\n- What are the potential benefits of using early exit mechanisms in Transformer models?\n- How does the criteria used to determine when to exit a layer impact the overall performance of the model?\n- What are some challenges associated with training and implementing early exit mechanisms?\n---\nSummary:\nEarly exit mechanisms adapt the number of layers used for each input to balance speed and accuracy, often employing internal classifiers at each layer and using criteria to determine when to exit.\nOriginal Text:\nSimilar to the dynamic halting mechanism used in UT, there is a line of work dedicated to adapting the number of layers to each input in order to achieve a good speed-accuracy trade-off, which is called early exit mechanism, as illustrated in Fig. 12(c). A commonly used technique is to add an internal classifier at each layer and jointly train all classifiers. The core of these methods is the criteria used to decide whether to exit at each layer. DeeBERT [150] uses the entropy of the output probability distribution of the current layer to determine whether to exit. PABEE [171] counts the number of times that the predictions remain unchanged to decide whether to exit. Li et al. [79] design a window-based uncertainty criterion to achieve token-level partial exiting for sequence labeling tasks. Sun et al. [129] introduces a voting-based exiting strategy that considers at each layer predictions of all the past internal classifiers to infer the correct label and to decide whether to exit.\nContextualized Text:\nTo improve efficiency in Transformer models, researchers have explored early exit mechanisms. These mechanisms adapt the number of layers used for each input to achieve a good speed-accuracy trade-off. A common approach involves adding internal classifiers at each layer and training them jointly. The decision to exit a layer is based on specific criteria, such as DeeBERT's use of entropy or PABEE's tracking of prediction stability.	{"tags": ["architecture", "optimization", "efficiency", "transformers"], "doc_id": "7014b697-c5e1-4db9-a5a6-c95cfac9cf68", "summary": "Early exit mechanisms adapt the number of layers used for each input to balance speed and accuracy, often employing internal classifiers at each layer and using criteria to determine when to exit.", "doc_type": "text", "entities": ["DeeBERT", "PABEE", "Li et al.", "Sun et al."], "keywords": ["early exit", "adaptive computation", "speed-accuracy trade-off", "internal classifier"], "key_objects": ["internal classifiers", "early exit mechanisms", "number of layers"], "contextual_text": "To improve efficiency in Transformer models, researchers have explored early exit mechanisms. These mechanisms adapt the number of layers used for each input to achieve a good speed-accuracy trade-off. A common approach involves adding internal classifiers at each layer and training them jointly. The decision to exit a layer is based on specific criteria, such as DeeBERT's use of entropy or PABEE's tracking of prediction stability.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.3 Adaptive Computation Time"}, "hypothetical_questions": ["What are the potential benefits of using early exit mechanisms in Transformer models?", "How does the criteria used to determine when to exit a layer impact the overall performance of the model?", "What are some challenges associated with training and implementing early exit mechanisms?"]}
84634acd-f65b-4131-8e99-c95fc7e095a3	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02928311,0.021854335,0.011081068,0.027798599,0.015464788,0.049638662,-0.0040288796,0.062671445,0.023720998,-0.046954144,-0.012820023,0.025481084,0.008027848,-0.009679171,-0.0009751792,0.019469706,0.049276166,0.065755494,-0.0020752242,-0.0091145355,0.044473395,0.020779416,0.007395874,-0.021982808,0.021156615,0.0623324,0.009585115,-0.037547044,-0.028671322,0.014786378,-0.0009362756,-0.047553826,0.029240808,0.022782369,0.024983795,-0.018430185,0.009452675,-0.039950695,0.00087316375,0.0011880236,-0.042200882,0.028042393,-0.055184424,-0.028273782,0.028388452,-0.043411195,-0.13164179,-0.017228916,0.0137148285,-0.054647196,-0.035179228,-0.01070388,-0.014740276,-0.031550497,-0.0029203533,-0.030368822,-0.02111855,-0.05087685,0.0064533274,-0.072718285,-0.029779607,-0.0063581537,-0.048472315,-0.011520528,0.01824817,-0.0080988845,-0.0007436113,0.021234699,0.04191799,0.12717178,-0.021920584,-0.0045241406,0.018365102,-0.058642242,0.11933233,0.05098464,-0.022659732,-0.020285778,-0.06707883,0.0212353,-0.029040363,0.071018875,-0.06736983,-0.045253232,0.07366355,-0.008478787,0.0027580813,0.017880185,0.049555015,-0.05413388,-0.006277043,0.024955768,-0.0020545353,0.021732131,-0.0305001,-0.08296346,-0.040182732,-0.0172623,-0.0026158537,-0.03127565,0.00027591808,-0.009425124,0.089087,0.11395944,0.014988901,-0.05158659,-0.015600189,-0.031093393,0.028159346,0.02012388,-0.022495521,0.007367717,-0.030270243,0.017397229,-0.01235533,0.007997371,-0.07559894,0.038937237,0.0030427803,-0.018712666,0.018579831,-0.012419336,-0.035467245,0.028007835,0.06211955,0.025170814,0.007785239,-0.041711047,-0.039129186,0.024501031,0.034986418,0.0067691756,-0.00551954,0.013510173,-0.031294413,0.046587225,0.086186126,0.009271893,-0.009878916,0.015904838,0.019348118,-0.03471017,-0.0410215,0.0048349835,0.017538937,0.020506078,-0.057593703,0.053149145,-0.008896585,0.048169196,0.007752539,-0.00139935,0.053619538,0.0040188953,0.014351847,0.0053271423,-0.017330943,-0.05865525,-0.021155993,-0.030375164,-0.03314726,-0.022540554,0.003010479,0.06523754,0.034396887,0.09956315,-0.0023977219,-0.0005781966,0.03297351,0.020185469,-0.062063377,-0.00027435832,-0.00783937,-0.07030053,-0.019283146,-0.020417366,0.0044126413,-0.045036133,0.030663386,-0.019636787,-0.0070028794,0.06602787,-0.013386032,0.020021768,-0.050828636,0.02167174,-0.0036822695,0.045796916,-0.038741652,-0.035693504,0.01439496,-0.03004702,0.019079136,0.01963512,0.07213409,0.041399382,0.045531537,-0.029670687,-0.043044996,-0.024583079,-0.012094334,-0.0041457773,-0.008985649,-0.012849159,0.035016447,0.013644573,-0.021907879,0.012727193,-0.04320687,0.019960174,-0.018258309,0.022858784,-0.02670815,0.0477338,0.023646664,0.009367763,-0.021520099,0.03228629,0.07122513,0.0046390705,-0.045491137,-0.0008544695,-0.004722853,0.012846949,-0.030817471,0.0023070255,-0.00084086374,0.012387722,0.0065756394,-0.01663995,0.032799654,0.032049812,-0.07541031,0.028251363,-0.03448618,0.035366043,0.016305406,0.018114693,-0.0096053,0.039353814,0.02686302,0.02510565,-0.011266517,0.09483293,0.026057988,-0.012222112,-0.005922606,0.048160262,0.007117513,-0.0072311745,-0.020454403,0.0594935,0.05227254,-0.015467867,-0.00078616734,-0.070870616,0.013044779,0.0064775497,0.02219235,-0.023832155,0.022904983,-0.00097527454,-0.000707151,0.03804368,-0.0019043657,-0.022337925,0.03092288,0.027975427,-0.061753638,0.0150051145,0.0340942,0.049637742,0.027876724,-0.006926417,0.02362847,-0.011661414,0.026629863,-0.016820943,0.054475795,-0.03427903,-0.019167716,-0.025508596,0.018907392,0.0066478676,0.017072396,-0.02454771,0.0130133685,-0.024524916,-0.061492063,-0.028689276,0.0023142989,-0.040800042,0.06414447,-0.024543654,-0.03416115,-0.018255152,-0.0019578957,-0.030256005,-0.0012966817,0.013323723,0.082699165,-0.028583638,-0.010513879,0.046115097,0.0014212963,0.037484065,-0.045319207,0.007714264,0.044559404,0.0034193345,-0.013321144,-0.023060022,-0.042928696,0.015184068,0.02568473,0.07767939,0.034132384,-0.01151266,-0.008900011,0.0018984615,0.04751473,3.1262014e-05,-0.015936306,0.0012739503,0.03924096,-0.052385155,0.013495736,-0.01542156,-0.10769156,0.057418462,-0.072707586,0.029837787,0.0611737,-0.008419643,-0.01099001,-0.0040296228,0.02115795,0.03743065,-0.036264222,0.027323006,-0.009260444,-0.055180274,0.002511434,0.0190407,-0.020736078,0.056270987,-0.0074828384,0.018733269,-0.006090149,-0.024069887,0.013900248,0.043320887,0.012822058,0.025095,0.023672447,-0.030196063,-0.0048287036,-0.008605325,0.025099613,0.004087901,0.089380585,-0.09116486,0.09488668,-0.016734827,-0.0012006179,-0.0017758214,-0.08056882,-0.009882452,0.042698763,-0.004185025,-0.025133898,-0.03683731,0.034988012,-0.07642849,-0.030714706,-0.0027060746,0.05835606,0.025730977,-0.02701121,0.007875861,0.017240461,0.021431087,0.0111610545,-0.05796974,0.009953818,0.015097846,0.015262315,0.08911699,-0.025366992,0.0718628,-0.06388791,0.032800075,0.013571186,0.031947054,-0.022427395,0.0044034417,0.0160085,0.025162404,0.008435342,0.004421536,-0.023061328,0.052974634,-0.049120106,0.01939433,0.0021549733,0.037527386,-0.06788873,0.03940088,0.051495966,0.013700558,0.02038546,-0.0010060548,-0.013400365,-0.012977535,0.019268077,-0.008962053,0.002231712,0.029678319,-0.04864359,-0.011359036,-0.0479714,0.03700049,0.025396137,0.042613108,0.042370927,-0.008224656,-0.0552625,0.021191439,0.0048520467,-0.03798621,0.0028874693,-0.026703127,-0.016264662,0.014332281,0.019778613,-0.018912204,-0.0610458,0.0313323,-0.057612356,0.038843326,0.03559664,-0.03745055,0.015883727,-0.0012732431,0.021446899,-0.0049174237,0.02035173,0.009723008,-0.03449038,-0.019545427,0.027973823,0.021610755,0.012529027,-0.010756328,0.029199505,-0.034121454,0.01957321,-0.027474612,-0.012428528,0.01623007,-0.011549478,0.031381223,0.04906028,-0.0056598634,0.010995044,-0.0058264965,-0.025645398,-0.010220946,-0.050733555,-0.05043526,0.009837866,0.040602732,-0.022056825,0.044351056,-0.026009755,-0.02441519,0.008610435,0.04265399,0.020557635,0.031392112,-0.04807207,-0.03536661,0.024626596,0.028984863,0.0140963355,-0.012702159,0.034954358,-0.00067834446,-0.043106187,0.005995337,-0.018866938,0.061019883,0.023015339,0.007937427,0.011443387,-0.027184965,-0.013948881,0.023520067,0.014417012,-0.044621535,-0.03252812,0.012364988,0.052557517,0.04646468,-0.06008496,-0.0004690158,0.0174359,-0.008419241,-0.027447809,0.06780141,0.0032041967,-0.015237907,0.010191544,0.010542535,-0.074723594,0.028386341,0.010233109,0.02745465,-0.020449022,0.048547134,0.014303766,0.022315353,0.025955636,0.013478206,0.018784719,0.02139212,0.0063670864,0.05163225,0.04926893,-0.07960001,0.026789498,-0.059108514,-0.021414941,-0.02091592,-0.067975685,-0.06069545,0.006509696,-0.04535203,0.0296525,0.03538918,-0.041180108,-0.022087514,0.039210938,-0.05451,0.08824222,-0.044184353,0.013152277,0.015106399,-0.012270352,0.07693718,0.012368707,0.027263623,-0.020654982,-0.021871952,-0.022402572,-0.009385936,0.059192512,0.045682997,-0.05223538,0.055285964,-0.03260902,0.004880921,-0.046674162,-0.050471194,-0.060235653,0.015889676,-0.0017047769,0.007366818,-0.038782395,-0.00013603237,-0.018708413,0.00066126994,-0.008834057,0.041320387,0.00038006433,0.07353301,0.02511865,0.029012306,0.016096268,-0.039769094,0.012746356,0.013071873,0.015268877,-0.0038267958,0.02781213,-0.014094341,0.017689869,-0.0012371856,0.02249905,0.024485886,-0.06169117,-0.027203355,-0.00586378,-0.008114959,-0.04111949,-0.038330633,0.0020957056,0.03654371,0.01919658,-0.027807686,-0.06985832,-0.048841618,-0.029031571,-0.032034513,-0.01581085,-0.0063276077,-0.011876116,0.03384563,-0.031316333,0.05939073,-0.10776179,-0.029084401,-0.05426662,0.049909867,0.011610029,0.026121972,-0.021023221,-0.0160755,-0.021525921,-0.06206015,0.06949148,0.017196352,-0.0012200297,0.020628305,-0.0043782336,0.0032140166,-0.012911969,-0.028666968,-0.032268945,0.011885485,0.031274807,-0.02475681,0.070085116,-0.0059288684,0.0140119335,0.016352847,0.033557773,-0.07317803,0.00732776,0.051799808,0.039615516,0.025002385,-0.013568269,0.023365099,0.01807715,-0.061904896,-0.019573102,-0.058698084,0.0058452194,-0.013100429,0.01599828,0.028096031,0.015169031,-0.028151248,-0.018877074,-0.01508104,-0.05359937,0.02644183,0.056993302,0.032887038,0.07702323,0.018630499,-0.03993115,-0.07464203,-0.00646998,-0.038186055,0.010207889,-0.0072502727,0.045614507,-0.015048697,-0.012885711,-0.016687639,0.04207921,0.010404107,-0.001246724,0.045856517,0.028778566,-0.038219877,-0.040568493,0.021144997,0.03726517,-0.042413656,-0.008619096,0.050746292,0.033101343,-0.031905547,-0.05507282,-0.077461794,-0.05757596,0.014916051,-0.023272043,0.0023391915,0.031096058,0.0009837062,-0.036575932,0.04135675,0.040217273,-0.053671956,-0.012545159,0.01636129,0.010889912,0.03443746,0.019185938,0.013503756,-0.119495496,-0.0029294812,-0.00616713,-0.03263518,0.03496114,0.022411829,-0.022288775,-0.0032685166,0.005761577,-0.008682485,-0.0002643785,0.0055342973,0.01772042,0.016096989,0.0036234325,0.0013757035,-0.07504985,-0.05932302,0.0038169832,-0.013794423,-0.013257916,0.027789427,0.06431341,0.019524649,0.025051095,-0.016426591,0.010524356,-0.006908939,-0.010160434,0.076824315,-0.07091101,0.05686647,0.00011615738,-0.0245986,0.099527776,-0.0046245013,0.0052033304,-0.008584157,0.0047710105,-0.02667926,0.014889038,0.020098658,-0.070975065,-0.034016617,0.050100163,0.029453747,-0.00018155258,0.032061465,0.0013353992,-0.03504319,-0.068352744,-0.009129392,-0.0048823785,-0.00031476494,0.014608391,0.024014741,-0.056555603,0.055911172,-0.052037317,0.023778982,0.03242114,0.021495815,0.014914299,-0.0043144976]	Keywords: quadratic complexity, self-attention, divide-and-conquer, long sequences, Transformer\nKey Objects: sequences, segments, Transformer models\nRefers to Images: ./images/a-survey-to-transformers/image_14.png\nHypothetical Questions:\n- Why does self-attention have quadratic complexity?\n- What are the advantages of using a divide-and-conquer approach compared to standard Transformer architectures?\n- How does decomposing sequences into segments help overcome limitations associated with long-range dependencies?\n---\nSummary:\nTo address the quadratic complexity of self-attention and its impact on tasks requiring long-range context, a divide-and-conquer strategy can be employed, which involves decomposing long sequences into smaller segments that are processed by Transformer models.\nOriginal Text:\n### 6.4 Transformers with Divide-and-Conquer Strategies  \nThe quadratic complexity of self-attention on sequences length can significantly limit the performance of some downstream tasks. For example, language modeling usually needs long-range context. Apart from the techniques introduced in Sec. 4, another effective way of dealing with long sequences is to use divide-and-conquer strategy, i.e., to decompose an input sequence into finer segments that can be efficiently processed by Transformer or Transformer modules. We identify two representative class of methods, recurrent and hierarchical Transformers, as illustrated in Fig. 13. These techniques can be understood as a wrapper for the Transformer model in which Transformer acts as an elementary component that is reused to process different input segments.  \nFig. 13. Illustrations of recurrent and hierarchical Transformers.  \n\nContextualized Text:\nDue to the quadratic complexity of self-attention with increasing sequence length, techniques like divide-and-conquer strategies are useful.  These strategies break down input sequences into smaller segments that can be efficiently processed by Transformer models, allowing for better handling of long-range context required in tasks like language modeling.	{"tags": ["architecture", "NLP", "transformer", "efficiency"], "doc_id": "84634acd-f65b-4131-8e99-c95fc7e095a3", "summary": "To address the quadratic complexity of self-attention and its impact on tasks requiring long-range context, a divide-and-conquer strategy can be employed, which involves decomposing long sequences into smaller segments that are processed by Transformer models.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["quadratic complexity", "self-attention", "divide-and-conquer", "long sequences", "Transformer"], "key_objects": ["sequences", "segments", "Transformer models"], "contextual_text": "Due to the quadratic complexity of self-attention with increasing sequence length, techniques like divide-and-conquer strategies are useful.  These strategies break down input sequences into smaller segments that can be efficiently processed by Transformer models, allowing for better handling of long-range context required in tasks like language modeling.", "mentioned_images": ["./images/a-survey-to-transformers/image_14.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.4 Transformers with Divide-and-Conquer Strategies"}, "hypothetical_questions": ["Why does self-attention have quadratic complexity?", "What are the advantages of using a divide-and-conquer approach compared to standard Transformer architectures?", "How does decomposing sequences into segments help overcome limitations associated with long-range dependencies?"]}
918f46ef-d2f8-45c5-b305-10a2d500a995	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.047723044,0.024262866,0.018055305,0.08527813,-0.043700583,-0.005679233,-0.006137022,0.04424413,0.06826134,-0.01061288,-0.014786117,0.028451467,0.034993578,-0.030204859,-0.022077922,0.02259085,0.059167746,0.09830394,0.025286665,-0.052024707,0.050343074,0.010234037,0.021129282,-0.04263639,0.008712049,0.023958556,0.056693263,-0.028811289,-0.019073127,0.012531978,-0.019216364,-0.027430898,0.03971414,0.008585173,0.020949436,-0.019626945,0.028471477,-0.023141898,-0.019236501,-0.012157214,-0.0073178736,0.028127018,-0.08042925,0.0137124555,1.7277089e-05,-0.059160616,-0.12545177,0.01837078,0.0151508,-0.057965495,-0.02958948,0.017010793,-0.010396724,0.0103946775,-0.022588905,-0.07106479,-0.042585254,-0.07248802,-0.015428621,-0.085384995,-0.024064064,0.039906878,-0.005929058,-0.05033078,-0.022641588,-0.006462415,0.035053946,0.021950817,0.03472524,0.12615126,0.0101390565,-0.017155655,0.018254105,-0.03331197,0.10696219,0.043249827,-0.043776195,-0.0065751695,-0.052469768,-0.013892294,0.008558425,0.080073126,-0.03503927,-0.033923056,0.08651325,-0.019240754,-0.055794038,0.005072919,0.091321826,-0.045194533,0.004495469,-0.021790814,0.046440024,0.017124366,-0.008250873,-0.08462755,-0.03904589,-0.05818683,-0.005580541,-0.024963131,-0.022504961,-0.01883875,0.08775654,0.08120785,0.026434321,-0.03237689,-0.005015722,0.0012189366,0.02089002,0.026825588,-0.028126113,0.037481997,-0.0076562343,0.06310378,0.002282454,0.018898731,-0.033441927,0.003782612,0.0109358365,-0.012760561,0.021174306,-0.0076445416,-0.03743129,0.039297093,0.047510825,0.044517223,-0.0014236934,-0.035449687,-0.011645841,-0.03476198,0.02724119,0.044369344,0.02555488,0.05085234,-0.02638784,0.02418603,0.05221086,0.040693205,-0.011912009,0.0144415,0.027005397,-0.06403102,-0.042175446,0.03075392,-0.014040089,0.054294582,-0.054868124,0.0028211982,-0.01250201,0.06196864,0.020031365,-0.007965515,0.016736856,0.027404038,-0.037023332,0.0027063815,-0.04422621,-0.049223352,0.003466929,-0.06177924,-0.049254913,-0.070327066,-0.0067893583,0.061484676,0.025069455,0.062217947,0.022620894,0.0022111041,0.016078725,0.008368,-0.043964103,-0.010961624,-0.0031759287,-0.03638781,0.040589403,0.004600143,0.009376256,-0.0146908555,0.0008163628,-0.02041859,-0.016985077,0.037716664,0.018113028,0.045793112,0.013470585,0.037613932,-0.015959926,0.04714225,0.030784102,-0.026368447,0.022007411,-0.04355073,0.0010802404,0.019840017,0.077534445,0.025261058,0.035864122,-0.032528244,-0.046712894,-0.0063845506,0.020118508,-0.00528606,0.05226744,-0.0014025598,0.045648612,0.009017888,0.022832612,0.000703295,-0.024031399,0.017483765,0.041152455,-0.028262027,0.00888363,0.0028816606,0.04354991,0.027850065,-0.0026906286,0.046911854,0.038494222,-0.025304986,-0.029325534,-0.010683422,-0.013392233,0.0009180095,-0.044677943,-0.02845301,-0.059874147,0.00886793,0.025052844,-0.013934804,0.038288917,0.033292238,-0.028986419,0.023103496,0.0093984185,0.06796972,0.030309897,0.009168114,0.01812802,0.047675405,0.023007141,0.0004888378,-0.03786953,0.05745408,0.023517888,-0.019702453,0.020791478,0.04102731,0.004252591,0.029895011,0.013189144,0.05459643,0.03588109,0.00011572496,-0.030681616,-0.037282094,-0.040646937,0.0056809355,-0.0010041074,0.0025525226,-0.046089984,0.011864262,0.030355796,0.04531924,0.062262043,0.011117954,0.013050063,0.040469803,-0.010585391,-0.021450013,0.004317904,0.03371869,0.01049459,-0.021068675,0.026697902,0.008029021,0.019616123,0.0015258295,0.041942447,-0.0076366104,-0.011777902,-0.021478485,0.017722553,0.019933207,0.029661026,-0.013789201,-0.026176179,-0.05933494,-0.07537399,-0.029720323,0.0014528529,-0.03292156,0.065765016,-0.0045039924,-0.031220363,0.003528828,-0.008401353,-0.026631001,-0.03602242,0.028437646,0.114440866,-0.058472976,-0.003618073,-0.02790425,-0.024404336,-0.01432499,-0.065404,0.020105422,0.05516188,0.03336138,0.057212193,-0.02056823,0.009781464,0.034310553,0.053872593,0.08229402,0.050650205,-0.057706825,0.018433211,0.041690152,0.036100883,0.008634759,-0.046404406,0.036152598,0.025451532,-0.030689582,0.038186684,-0.042264305,-0.07242955,0.06780319,-0.05351949,0.031952605,0.07164696,-0.006985377,-0.024945656,0.0039919093,0.039033063,0.048313826,-0.005053407,0.017337583,-0.0057728086,-0.04286101,0.005307967,0.03160074,-0.02933955,0.016371543,-0.013094377,0.0120220585,0.0027412202,-0.033851705,-0.01616165,0.010226824,0.019185008,0.023774674,-0.009333172,-0.03325799,0.004609671,0.037453886,-0.008004592,-0.0235944,0.039191607,-0.057397403,0.07737159,-0.00033485913,-0.017047433,-0.03869705,-0.07159025,0.015777962,-0.018398236,0.001116271,-0.047262143,-0.03667361,0.06387887,-0.012433895,-0.021065462,-0.008209398,0.027670935,0.039606884,-0.02450189,-0.035206173,-0.018467963,0.0076101404,-0.025937032,-0.006511978,-0.014157718,-0.0036298616,-0.04337314,0.05037841,-0.0075836545,0.03706432,-0.042817585,0.017921424,-0.0037917509,0.026955217,-0.008097382,-0.030024989,-0.00037845175,0.03211213,0.008378576,-0.0024421944,0.013631179,0.07015167,-0.043337665,0.015858142,0.034724224,-0.0004948929,-0.038984884,-0.0100954985,0.07219333,-0.012564171,0.04424874,0.006065177,-0.008049038,0.006010203,0.021561734,-0.015507278,-0.035859674,0.059773643,-0.015688464,-0.0065041613,-0.031640895,0.01169167,0.021675246,0.008965407,0.01079805,-0.025205212,-0.0375268,0.019965362,0.018259263,-0.015085579,-0.00020093858,-0.045662344,0.012581341,0.043899454,0.097383715,-0.0019755405,-0.09195947,0.04056125,-0.07858308,0.03407995,0.034513526,-0.001840488,0.025582446,0.04069271,0.03849219,-0.027328039,0.02255584,0.009121873,-0.012261099,-0.029602136,0.015006695,0.013902699,0.035219412,-0.019168813,0.02227256,-0.02617936,-0.033795208,-0.052276783,-0.04812728,-0.0346602,-0.04645232,0.0022068599,0.0011161312,-0.038073875,-0.003082345,-0.005607485,-0.013229133,0.0014923381,-0.032778814,-0.023254776,-0.018025203,0.029904347,-0.03795755,0.03056893,-0.01622016,0.0045206835,-0.016623706,0.03175225,0.039100114,0.015438101,-0.03449456,-0.023909561,0.017906478,0.018941702,-0.010810771,-0.020109605,-0.01042812,0.0017977238,-0.05358178,0.019265555,0.03008606,0.025348576,0.02076502,0.025940185,0.0051936153,-0.017481407,-0.015809009,0.028625917,-0.006729844,-0.02035047,0.0105512785,-0.019851845,0.06424307,0.0355756,-0.058203757,0.027105121,0.012001433,-0.016910767,-0.041701414,0.048245862,0.03675875,-0.011898169,-0.030064223,-0.009547839,-0.04582438,0.01709142,-0.010228599,-0.022040706,-0.0010578632,0.026046438,-0.048124544,0.030099079,0.02382532,0.023517888,-0.006258706,0.0056385878,0.019141365,0.022152977,0.102829374,-0.07514359,0.056317396,-0.04679378,-0.03568929,-0.023225864,-0.051160436,-0.051907536,-0.012161717,-0.019590769,0.025437845,0.012972097,0.0066108145,-0.0037171107,-0.00096694584,-0.03150974,0.073525086,-0.060463503,0.014973769,-0.02052997,0.000113266586,0.038556606,-0.007972475,0.016483424,-0.023068506,-0.0063702497,0.025081035,0.056360357,0.042838644,0.03674918,-0.0054030856,0.032247998,-0.0034645507,0.005152764,-0.035678655,-0.04112494,-0.013258308,-0.02381767,0.013091155,-0.040998705,-0.012085479,0.059869252,-0.036109716,0.021940814,-0.0054346435,-0.015506978,-0.048167482,0.0364238,0.059466723,-0.009962938,0.048777547,-0.071182996,0.029876469,0.07566655,-0.020369643,0.007289819,0.031145144,-0.0049050176,0.0048961397,0.011082324,0.0055607627,0.01071649,-0.014001708,-0.037668545,-0.009246679,-0.0047121383,-0.017286448,0.005898889,0.006031911,0.04085784,0.041438434,-0.007058749,-0.08306577,-0.045544516,-0.0043499167,-0.010200276,-0.01824126,0.018078167,-0.016303409,-0.0044496693,-0.060460974,0.07073862,-0.07262799,-0.0049044406,-0.05408031,0.061544448,-0.008396112,-0.013206489,-0.003987372,-0.0059762765,-0.029281277,-0.04991582,0.046090256,0.052179888,-0.028238839,0.0073523545,-0.015122972,-0.00368358,0.014190627,-0.034958042,-0.086179405,0.0314836,0.021092946,-0.02649399,0.064434685,0.033877745,0.030864099,0.019557089,-0.048751336,-0.060604755,-0.010088163,0.02443742,0.02057927,0.079924546,-0.0039956463,0.015207618,0.03447553,-0.08767025,-0.0070443787,-0.0641822,-0.045274656,-0.015379069,0.018136324,0.03710344,-0.023320623,-0.058969248,-0.0057941508,-0.020898335,-0.031894173,0.022936527,0.010674577,0.031124534,0.058054533,0.046833616,-0.023278013,-0.11549749,-0.033154204,-0.037726197,0.0041577145,-0.038472373,0.039765205,-0.0066322037,-0.011604184,-0.037773225,0.010902831,-0.003695967,-0.0313419,0.023725176,0.052241363,0.0013070905,-0.01988836,0.04749795,0.0879016,-0.06645491,0.05283855,0.031646136,0.05479913,-0.035109315,-0.04741727,-0.06499818,-0.005600758,0.030939246,0.0128186215,0.018768158,-0.007956184,-0.0017477456,-0.045140713,0.023776876,0.016351597,-0.027133608,-0.027401922,0.0014248166,-0.019760784,0.031581804,-0.004853285,-0.023415133,-0.093082875,0.0315385,-0.0053212177,-0.0018024765,0.057997182,0.0017479447,-0.032165695,-0.0014043682,0.021970093,-0.03258271,0.044016387,0.008267314,0.0057660774,0.0012797202,0.0061793714,0.0020069624,-0.047981244,0.002900292,-0.003918546,0.0035371908,0.0031716165,-0.015294808,0.0015226494,0.024756638,0.054053206,-0.011029331,0.013560444,0.0140283415,0.016265638,0.03577242,-0.044052552,0.057212874,-0.0031000427,0.021430975,0.057564013,-0.015888842,0.022697274,-0.04191156,0.021303171,-0.007181481,0.05840988,0.02338918,-0.063428365,-0.041001845,0.005992989,-0.038837444,0.029237233,0.002179528,0.026276939,-0.043573797,-0.07063972,-0.0024179944,-0.028247455,0.0377445,0.025339255,0.04959272,-0.0045546233,0.0138236955,-0.05072766,0.028294915,0.018686727,-0.009052526,0.019641485,-0.018491047]	Image title: Recurrent and Hierarchical Transformer Architectures\nTags: transformer, architecture, recurrent, hierarchical, neural-network\nKey objects: Recurrent Transformer, Hierarchical Transformer, Transformer Block, Level 1, Level 1 Output, Level 1 to Level 2, Level 2, Level 2 Output\n---\nSummary:\nThis diagram illustrates two transformer architectures: recurrent and hierarchical. The recurrent transformer (left) shows data flowing sequentially through multiple transformer blocks. The hierarchical transformer (right) depicts a hierarchical structure where transformer blocks are organized in multiple levels, creating a layered processing approach.\nFull description:\nThe diagram presents two different approaches to transformer architecture. The recurrent transformer (left) shows a sequential data flow. Data enters the system and passes through multiple 'Transformer' blocks in a chain. The hierarchical transformer (right) presents a more complex, multi-layered structure. Data first goes through a level of 'Transformer' blocks labeled 'Level 1'. The output from 'Level 1' is then fed into a second level of 'Transformer' blocks, labelled 'Level 2'. The output from 'Level 2' represents the final processed information.\nText found in image:\n- (a) Recurrent Transformer\n- (b) Hierarchical Transformer\n- Level 1\n- Level 1 Output\n- Level 1 to Level 2\n- Level 2\n- Level 2 Output	{"tags": ["transformer", "architecture", "recurrent", "hierarchical", "neural-network"], "title": "Recurrent and Hierarchical Transformer Architectures", "doc_id": "918f46ef-d2f8-45c5-b305-10a2d500a995", "source": "./images/a-survey-to-transformers/image_14.png", "summary": "This diagram illustrates two transformer architectures: recurrent and hierarchical. The recurrent transformer (left) shows data flowing sequentially through multiple transformer blocks. The hierarchical transformer (right) depicts a hierarchical structure where transformer blocks are organized in multiple levels, creating a layered processing approach.", "doc_type": "image", "key_objects": ["Recurrent Transformer", "Hierarchical Transformer", "Transformer Block", "Level 1", "Level 1 Output", "Level 1 to Level 2", "Level 2", "Level 2 Output"], "parent_doc_id": "84634acd-f65b-4131-8e99-c95fc7e095a3", "text_in_image": ["(a) Recurrent Transformer", "(b) Hierarchical Transformer", "Level 1", "Level 1 Output", "Level 1 to Level 2", "Level 2", "Level 2 Output"], "contextual_description": "The diagram presents two different approaches to transformer architecture. The recurrent transformer (left) shows a sequential data flow. Data enters the system and passes through multiple 'Transformer' blocks in a chain. The hierarchical transformer (right) presents a more complex, multi-layered structure. Data first goes through a level of 'Transformer' blocks labeled 'Level 1'. The output from 'Level 1' is then fed into a second level of 'Transformer' blocks, labelled 'Level 2'. The output from 'Level 2' represents the final processed information."}
b5a36ce7-2c6c-4292-9eaf-134b8f3f10d8	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02880241,-0.018787507,0.008280272,0.025640434,-0.011431956,0.0147655085,0.01509506,0.054102335,0.04884199,-0.013432275,0.02615401,0.012629123,-0.008352816,0.020250395,-0.052717675,0.04328088,0.035012435,0.07056481,0.0063795703,-0.037954144,0.028828198,0.02618392,-0.018666321,-0.04825681,-0.004682799,0.03340171,0.041666117,0.030608404,-0.015456668,0.023882266,-0.0044551375,-0.050951943,0.041913386,0.017793113,0.052522372,-0.04241414,-0.011449073,-0.01655495,-0.018492332,0.005152781,-0.039729327,0.014521363,-0.08278353,7.256201e-05,0.014210484,-0.04079041,-0.095581636,0.0028421849,0.05729655,-0.03024526,-0.007049224,0.009475541,-0.06621254,-0.021796906,-0.032735817,0.0028602618,-0.0035615729,-0.054744147,0.017475104,-0.06915771,-0.025958735,0.01804451,-0.015068426,-0.052219536,0.0037006226,-0.019477291,0.0063875266,0.024870083,0.0113638835,0.12912795,-0.009776184,-0.031122627,0.008702658,-0.08089377,0.1064899,0.061308008,-0.02700093,-0.046417516,-0.07122597,-0.000965736,-0.028629052,0.09791287,-0.03245416,-0.010844593,0.08280981,-0.013280039,-0.062574804,0.012812553,0.05663118,-0.039210226,0.017613748,0.002590208,0.016446754,0.030179484,-0.0087998705,-0.055785146,-0.0038896839,-0.008508769,0.00213215,-0.04418568,0.009536149,-0.032301012,0.070971794,0.10126486,-0.012088638,-0.035680976,-0.018734131,-0.039672434,0.051717103,0.05467346,-0.045856968,0.017383974,-0.045959648,0.033539176,-0.049299575,0.019261055,-0.033317216,0.020685978,0.0049111773,-0.013153364,0.025642127,0.0029175386,-0.003339493,0.030441117,0.02384125,0.032546457,0.02292375,-0.0652925,-0.04072972,0.03305246,0.038159084,0.052750263,-0.038359832,0.014298105,-0.016140947,0.018997442,0.058733348,0.01659853,-0.039927356,0.0053521865,0.0061051217,-0.006603932,-0.029804422,0.0037628894,0.0034969477,0.021336365,-0.07971957,0.02597659,-0.028208578,0.056780443,0.03172174,0.01561935,0.006172206,0.0032443558,-0.0011801469,-0.00082696957,-0.032618426,-0.017267894,-0.02276709,-0.01530342,-0.01686042,-0.045224316,-0.00300977,0.09559656,0.015742922,0.099408165,0.0021569442,0.0085473275,0.053070065,0.0233896,-0.0542253,-0.03426774,-0.008291121,-0.06229371,-0.009316912,0.013231457,0.0019177898,-0.039591778,0.001580752,-0.015394403,-0.012678489,0.02434268,-0.01690938,0.076921,-0.02814358,0.06252046,-0.033233818,0.0704533,-0.025509872,-0.06149293,-0.004905104,0.010002959,0.001066582,0.022226328,0.065214224,0.019709993,0.038241774,-0.033710185,-0.024489611,-0.037206113,0.035984695,-0.01071498,0.01648992,-0.05574963,0.051493026,0.028790351,-0.016352339,-0.014212603,-0.021614216,0.002696101,0.0018229579,-0.007408239,-0.035303958,0.009902981,0.017726129,0.029007914,-0.039320912,0.03192695,0.074723996,0.006805044,-0.0120942285,-0.0019720295,-0.0058797086,0.033818115,-0.036204524,0.008798507,0.009520542,0.014600683,0.046780135,-0.018634178,0.01618947,-0.013136268,-0.07767084,0.045807227,-0.039235704,0.059499353,-0.00061736326,0.0026739403,0.056025825,0.04976812,0.048594132,0.041290447,-0.05739879,0.06318429,-0.0140999835,-0.008118951,-0.012945871,0.02555436,0.0036660465,0.018167445,0.025988,0.051674347,0.07321192,-0.028439678,-0.006778936,-0.03813776,0.0028298267,0.02353113,-0.035009038,0.013046208,0.004141533,-0.014363539,-0.00910991,0.050552383,0.020810489,-0.008082056,0.004572094,0.012239523,-0.032547526,-0.003817142,0.03390692,0.05004027,0.014750404,-0.01890518,0.016998017,0.010448764,0.0313824,-0.023681043,0.05758165,-0.039242305,-0.034485124,-0.06053754,-0.0021011028,0.01481679,0.015547229,-0.04000875,-0.029483441,-0.016722044,-0.033177502,-0.011024221,-0.021409785,-0.040759556,0.05107319,-0.004297918,-0.050182845,-0.045856897,-0.008355224,-0.021119092,-0.01801664,0.00092775107,0.10596716,-0.03898019,-0.009460558,0.0006671374,-0.0051568537,0.0073145195,-0.068962194,0.0120057,0.025640437,0.022106374,0.019693889,-0.031751532,-0.013597199,-0.018499976,0.030929843,0.0740383,0.042723157,-0.029148512,-0.009275979,0.016033538,0.0020811835,0.012665194,-0.020479506,0.014649161,0.037467834,-0.052779105,0.014804612,-0.031320594,-0.107778504,0.072259575,-0.090220354,0.05487893,0.07589279,-0.0027223723,-0.04301402,0.023734242,0.019205049,0.048905034,-0.024141235,0.04635325,-0.02712529,-0.05673781,-0.0018757873,0.01524673,-0.011880596,0.032775477,-0.00029834593,0.0019730001,-0.0020702234,0.0036409977,0.021201247,0.030374324,-0.006202089,0.008010258,0.030639693,-0.01142927,0.016773762,0.0160509,-0.0020879998,-0.0028715194,0.05886039,-0.05705691,0.05346804,-0.036655698,-0.010159653,-0.017798003,-0.055549353,-0.01744585,0.0026294738,-0.026232839,-0.012658939,-0.033011496,0.014097172,-0.05290857,0.0070906924,-0.010833185,0.05812526,0.04940981,-0.024882795,-0.021446394,-0.01665963,0.00048308336,-0.015005523,-0.035217024,-0.0017877388,0.003027096,-0.0069555156,0.062933974,-0.027961895,0.060385242,-0.0091145,0.04475287,0.013664214,0.005803521,-0.0032899906,-0.007702328,0.012370246,0.030949252,0.0134433545,0.021138243,0.006945859,0.041627865,-0.045456734,0.019333014,0.008201398,0.03679942,-0.051425613,0.0076752705,0.08525181,0.032134105,0.07293222,-0.012323754,-0.0005965974,0.0017632003,0.050378468,0.006751021,0.01851236,0.031492986,-0.029612966,-0.0028417432,-0.035094205,0.028882409,0.007077051,0.044006795,0.033868976,-0.015171968,-0.048069227,0.008411196,0.029446,-0.031567194,0.0028196578,-0.03468928,0.029466726,0.026861815,0.052210063,-0.01107323,-0.04400657,0.05913059,-0.051880226,0.034709897,0.039623316,-0.031933423,-0.0020850669,-0.03794672,0.02719647,-0.012016209,0.044886265,0.031430032,-0.03041894,-0.027251773,0.01149187,0.039633233,0.0030671218,-0.042831972,0.039046742,-0.016947899,0.0036510157,-0.059572227,0.001087914,-0.026017861,0.0016369346,0.0054511186,0.0061480496,0.0054859775,-0.016534233,0.011255269,-0.018332293,-0.036990736,-0.04348995,-0.03932277,-0.015986145,0.0137225045,-0.0098735355,0.037870623,0.0039943894,-0.03885981,-0.010880198,0.03534639,0.0088781575,0.006140757,-0.062496744,-0.03001841,0.041193582,0.042528875,0.011740689,-0.014285798,-0.008047866,-0.0076080845,-0.04023801,0.010147197,-0.010627287,0.058679618,0.015558095,-0.01144829,0.03339117,-0.0006403601,-0.002196001,-0.008039825,0.011969083,-0.004690557,0.013752953,-0.03346029,0.06481424,0.07505588,-0.012601625,0.039485645,-0.0033688955,-0.011385452,-0.02698658,0.050745353,0.029709408,-0.0069987536,-0.0066964864,-0.021830501,-0.047819078,0.01817673,0.017171055,0.042753853,-0.03912296,0.017244786,0.016336758,0.022699479,-0.00037375145,-0.014558157,0.013984456,-0.00912303,0.017880138,0.018010959,0.06955258,-0.08554428,0.029723788,-0.04035783,-0.027863488,0.012759271,-0.050969895,-0.082054,0.007338647,-0.04492599,0.047120776,0.005915458,-0.0040053907,-0.036920805,0.029796863,-0.028322252,0.094678685,-0.06377931,0.005334095,-0.020979226,-0.004015278,0.06940803,-0.011313882,0.0040974463,-0.03086771,-0.017995581,-0.002028723,0.011748288,0.053610187,0.008291419,-0.006425017,0.04851418,-0.03715328,0.030576248,-0.005725131,-0.041475955,-0.027864682,0.012687497,0.0058470936,-0.01919173,0.00035815043,0.032286778,-0.014363622,0.028809223,-0.03150711,0.025279539,-0.031814665,0.03388975,0.065413974,0.017796809,0.015765378,-0.024837999,-0.012856603,-0.004338463,0.04598429,-0.008607615,0.03905238,-0.02371734,-0.015683064,0.02724081,0.021484653,0.032024957,-0.016440233,-0.05354215,-0.0093050795,-0.0131464545,0.021237187,-0.042361233,0.023371944,0.032871675,0.03515003,-0.047928616,-0.05619972,-0.011280998,-0.040160704,-0.03996239,-0.03433075,0.016888117,0.008172847,-0.0024185362,-0.05252412,0.08610441,-0.08293449,-0.015094162,-0.055758256,0.043688525,0.004768265,-0.0229646,-0.035691872,-0.041386914,-0.0112899,-0.07089296,0.084924534,0.061651524,-0.017779306,-0.0019426969,-0.021336872,0.024192985,0.065066904,-0.043606445,-0.026192581,0.008215289,0.02244584,-0.009740559,0.056276646,0.013568548,-0.00019620497,0.011333645,0.011916141,-0.07889786,0.0009741456,0.009169685,0.022347176,0.0353654,0.02117306,0.019636728,0.025410442,-0.070590034,-0.053955115,-0.051104948,-0.017291399,8.373603e-05,0.019875536,0.045291986,-0.006464406,-0.027755179,0.008582205,0.008587333,-0.028542655,0.027002037,0.045489527,0.0010124331,0.065898284,0.030167887,-0.05025575,-0.08294387,-0.041709576,-0.011533443,-0.009991411,-0.023775075,0.059946373,0.003278393,-0.041401472,-0.014298848,0.0073044687,-0.009331805,-0.012402859,-0.008670485,-0.0054087015,0.013292874,-0.006966703,-0.015600913,0.016215578,-0.025370482,0.054789476,0.05578998,0.014582006,-0.02373264,-0.044901185,-0.057601556,-0.027917465,-0.003161802,-0.047909483,0.021169787,0.029576408,-0.02660332,0.0005877616,0.011836141,0.02208489,-0.060894907,-0.041236147,0.038817618,0.045748614,0.033302948,0.038817253,-0.00027479563,-0.15005772,0.026005419,0.0010348539,0.012172298,0.093082346,0.024915226,-0.026871653,-0.014608438,0.02309479,0.0124950325,0.05811321,-0.0081342645,0.02303062,0.0033090487,0.014180677,0.008293279,-0.050153054,-0.045415774,-0.0037296554,0.00870214,-0.030664666,0.05260035,0.02411069,0.016007256,0.002625622,-0.050941065,0.046775404,0.025550034,-0.019578898,0.05806729,-0.028986165,0.046656575,-0.0040753856,0.034135137,0.08382566,-0.011469269,-0.01397004,1.0038072e-05,0.025607526,-0.03139348,0.0054307296,-0.0031049978,-0.047240295,-0.02203596,0.021228345,0.025804864,0.00037888045,0.0044337856,0.028051842,-0.02763925,-0.07584489,0.018321538,-0.014455204,0.04794701,0.04554646,0.00088490616,-0.055276766,0.026058942,-0.03657343,0.077169254,0.043403354,-0.005484586,-0.023906553,-0.02850661]	Keywords: recurrent transformers, cache memory, history information, long sequences, context window\nKey Objects: cache memory, text segment, history information\nRefers to Images: ./images/a-survey-to-transformers/image_14.png\nHypothetical Questions:\n- How does the cache memory improve the handling of long sequences?\n- What are the different ways that a recurrent Transformer can write to the memory?\n- How does Transformer-XL leverage this cache memory mechanism?\n---\nSummary:\nRecurrent Transformers address limitations in handling long sequences by maintaining a cache memory that incorporates historical information. This cache allows the network to read from and write to memory, extending the context window for processing text segments.\nOriginal Text:\nFig. 13. Illustrations of recurrent and hierarchical Transformers.  \n  \n6.4.1 Recurrent Transformers. In recurrent Transformers, a cache memory is maintained to incorporate the history information. While processing a segment of text, the network reads from the cache as an additional input. After the processing is done, the network writes to the memory by simply copying hidden states or using more complex mechanisms. The abstract process is illustrated in Fig. 13(a).  \nTransformer-XL [24] address the limitation of a fixed length context by caching representations from the previous segment and reuse it as an extended context when the model processes the current segment. For the l -th layer and the (  + 1)-th segment, the input representation H$\\_{(}$$\\_{l}$$\\_{-}$$\\_{1}$$\\_{)}$ is concatenated with the representation H ( l - 1 - 1 ) from previous segment to produce the keys and values\nContextualized Text:\nTo handle long sequences, recurrent Transformers employ a cache memory to incorporate historical information. While processing a segment of text, the network reads from this cache as an additional input, allowing it to maintain context across segments. The abstract process is illustrated in Fig. 13(a).	{"tags": ["architecture", "NLP", "transformers", "recurrent"], "doc_id": "b5a36ce7-2c6c-4292-9eaf-134b8f3f10d8", "summary": "Recurrent Transformers address limitations in handling long sequences by maintaining a cache memory that incorporates historical information. This cache allows the network to read from and write to memory, extending the context window for processing text segments.", "doc_type": "text", "entities": ["Transformer-XL"], "keywords": ["recurrent transformers", "cache memory", "history information", "long sequences", "context window"], "key_objects": ["cache memory", "text segment", "history information"], "contextual_text": "To handle long sequences, recurrent Transformers employ a cache memory to incorporate historical information. While processing a segment of text, the network reads from this cache as an additional input, allowing it to maintain context across segments. The abstract process is illustrated in Fig. 13(a).", "mentioned_images": ["./images/a-survey-to-transformers/image_14.png"], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.4 Transformers with Divide-and-Conquer Strategies"}, "hypothetical_questions": ["How does the cache memory improve the handling of long sequences?", "What are the different ways that a recurrent Transformer can write to the memory?", "How does Transformer-XL leverage this cache memory mechanism?"]}
1ac84e0b-d5a7-4ed1-8019-368db32cdd4a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.036093436,0.006061609,-0.0030610315,0.045831922,-0.022678595,0.009808152,0.019055685,0.061118167,0.06052502,-0.012825727,-0.0033914994,0.0304193,0.021895718,-0.025568206,-0.042198144,0.027011586,0.055915833,0.09089631,0.024416778,-0.06122155,0.045226097,0.007270766,0.027115978,-0.049557537,0.015318108,0.037991453,0.046726696,-0.028012147,-0.015531658,0.022446403,-0.017810134,-0.02800451,0.03772183,0.01054325,0.03444653,-0.030725619,0.017740052,-0.028505152,-0.017909098,-0.00091284787,-0.010046882,0.038366422,-0.0829129,0.019944575,0.013852412,-0.06172625,-0.13254602,0.008349462,0.024166895,-0.071592286,-0.007310553,0.017420711,-0.033622216,0.0065405956,-0.026805524,-0.044835497,-0.035090417,-0.085984625,-0.003932776,-0.07472776,-0.019084394,0.030009015,-0.00036639726,-0.041039307,-0.008069669,-0.0005371912,0.036590282,0.018812208,0.043935783,0.123816036,-0.0017841327,-0.006282356,0.020579377,-0.06543136,0.11461113,0.058885697,-0.044432387,0.0035585912,-0.044244748,-0.011194862,0.008971071,0.06295593,-0.022735974,-0.03889599,0.084475614,0.0040330393,-0.04550266,-0.0053158356,0.07951176,-0.054851416,0.014529,-0.03124064,0.030599335,0.014332266,-0.014590281,-0.056630667,-0.044096272,-0.044550825,0.0011278224,-0.027047431,-0.003233606,-0.0337278,0.10873372,0.08546368,0.021047631,-0.018949855,-0.030864973,-0.008157761,0.0082180975,0.04015961,-0.019244876,0.02873719,-0.023042426,0.050745238,0.0040516746,0.020883763,-0.032207876,0.016813466,0.0058945664,0.0017582861,0.012993729,0.0032423649,-0.030547997,0.031112036,0.06268439,0.04798289,0.018989725,-0.043194123,-0.013519875,-0.03314892,0.021259267,0.023935767,0.016825916,0.04513561,-0.009642832,0.01990137,0.05725705,0.036190078,0.008389914,-0.007200161,0.02767674,-0.058392,-0.03465185,0.024528058,-0.011391768,0.049457464,-0.05212304,0.0067689223,-0.0048290966,0.058340043,0.0185792,0.013888366,0.028169895,0.02607664,-0.04923999,0.00019146038,-0.029771065,-0.038010098,0.017956581,-0.057734646,-0.031503517,-0.06407372,-0.010260623,0.055802535,0.0007421167,0.06315934,0.0022611585,-0.0015830959,0.026706193,0.011337306,-0.05375062,0.0054293266,0.0009853862,-0.046347972,0.04503358,0.007173084,0.032614604,-0.012707331,0.005975854,-0.017758314,-0.007466929,0.03707336,0.033979055,0.03144806,0.0043101986,0.044384405,-0.021581717,0.045387182,0.01771686,-0.02438672,0.026652196,-0.024779856,-0.0027866566,0.02811104,0.079118915,0.039266236,0.03641304,-0.032408703,-0.05455149,-0.007157293,0.024517236,0.0189395,0.030205559,-0.0020785257,0.041029096,0.020133367,0.023314761,-0.0051078703,-0.019025333,0.020996565,0.04425281,-0.01676542,-0.005747188,-0.008129698,0.050938644,0.028127657,-0.00018240708,0.037742913,0.043807108,-0.02429509,-0.029215604,0.005833127,-0.002508403,-0.002477738,-0.03893301,-0.018355919,-0.05201426,0.0062607024,0.030906763,-0.020170512,0.045980997,0.03617558,-0.029935023,0.015651366,-0.02158173,0.06335928,0.022075118,0.01975441,0.0019045933,0.03601319,0.02716021,0.02443872,-0.035231072,0.06876185,0.032948367,-0.019708594,0.018020183,0.032841474,-0.0057514776,0.046742104,0.009653238,0.04913756,0.024435846,0.010496135,-0.01832818,-0.06589717,-0.037339978,0.009365405,0.0021014663,0.020286536,-0.04182358,-0.0022562626,0.02537194,0.050047696,0.050627295,0.00914834,0.0012570771,0.037454545,-0.022044012,-0.023077456,-0.010583156,0.04859108,0.006304563,-0.016820438,0.025491925,0.007830512,0.022800257,0.0047924984,0.06182266,-0.025205662,-0.008331861,-0.021142075,0.01583648,0.00979799,0.035919983,-0.013956939,-0.022967078,-0.062063213,-0.063529804,-0.0283337,0.001845976,-0.021890907,0.04655001,-0.0046746177,-0.04266838,0.014529198,-0.012063305,-0.01663536,-0.031271372,0.04162242,0.09473296,-0.06213721,0.00051605626,-0.011846799,-0.023610847,-0.024905391,-0.06831804,0.011852753,0.07388862,0.019862311,0.034847215,-0.01001977,-0.013849936,0.049547315,0.037572153,0.08294975,0.07285604,-0.051916532,0.025043886,0.036575846,0.043232203,0.008611619,-0.034395404,0.038160708,0.021207199,-0.033672627,0.030668234,-0.055922743,-0.07770289,0.08063405,-0.055719674,0.028173165,0.060728014,-0.025597744,-0.022302514,0.00090240024,0.01500213,0.05351999,-0.020637928,-0.0009203804,-0.0057212203,-0.040892836,0.012769374,0.045354486,-0.024598172,0.026640832,-0.021818446,0.007222297,0.0022751677,-0.04101316,-0.036252744,0.018386176,0.023173308,0.030162985,-0.011635114,-0.028788978,-0.0008418099,0.025046773,-0.017151386,-0.017238822,0.038304795,-0.06613323,0.07994135,-0.0023239297,0.0020244252,-0.010870048,-0.07908848,0.015722357,-0.02341824,-0.014685348,-0.029633408,-0.047896866,0.04541115,-0.010276638,-0.0030204619,-0.0048165414,0.027965754,0.021584205,-0.03791754,-0.055114318,-0.021988953,-0.007140559,-0.028773366,-0.014977968,-0.01724558,-0.0071853274,-0.034905303,0.058938175,-0.0027662509,0.024811085,-0.04291138,0.027160656,-0.010144917,0.025930122,-0.0033119263,-0.024520589,0.0051747514,0.019583408,0.004550662,-0.003366136,0.021094358,0.07795658,-0.048582613,0.013705025,0.023117328,-0.015143235,-0.048434213,0.00039767372,0.07616491,0.00057969044,0.056079097,0.0058684577,0.003357619,0.013460549,0.03169504,-0.019299729,-0.027289396,0.034022518,-0.025296416,0.0044414364,-0.029198064,0.010056596,0.032243606,0.021473384,0.0036257834,-0.024103442,-0.04405383,0.008865381,0.022476923,-0.011333742,0.00033719526,-0.049669296,0.008834346,0.040708873,0.09973773,-0.0072114603,-0.084848925,0.057780284,-0.09521636,0.042613715,0.040868107,-0.01956449,0.016313406,0.026785417,0.02764071,-0.034215346,0.028787073,0.03426712,-0.019858606,-0.014206382,0.0059813405,0.01886814,0.03494458,-0.010919661,0.031138694,-0.01974926,-0.019216312,-0.053772934,-0.058566324,-0.028458616,-0.03652522,0.027141076,0.007338061,-0.029268786,-0.017931422,-0.020847628,-0.011887963,-0.010064885,-0.025152123,-0.024375658,-0.0176558,0.045067325,-0.050265532,0.033100005,-0.010248682,0.01821205,-0.019625464,0.029984873,0.01744117,0.011805415,-0.03287606,-0.040145636,0.025235433,0.010695774,-0.018671649,-0.007947709,-0.024979409,0.003168532,-0.054669134,0.025602728,0.022502495,0.031688854,0.022004537,0.014139898,-0.013372636,-0.05448973,-0.0372069,0.029822497,0.018026555,-0.025546161,0.010251362,-0.02738659,0.05017677,0.048614927,-0.053154625,0.020474339,0.016437778,-0.026818136,-0.02548889,0.06034493,0.038498744,-0.010220568,-0.027953472,0.00029879026,-0.05261237,0.007797535,-0.00738217,-0.0058500334,0.007887733,0.005097954,-0.04809836,0.02849703,0.0055522122,0.030254196,-0.007656259,0.012413346,0.013879747,0.0081141675,0.10294077,-0.06836886,0.06786995,-0.0686067,-0.04378508,-0.021647934,-0.05320485,-0.061365932,-0.01889961,-0.036838144,0.028082976,0.029632403,0.009310578,-0.019957807,-0.00051468925,-0.016648462,0.06989486,-0.07790477,0.023854768,-0.023683453,0.0053717615,0.04155129,-0.011790919,0.014457612,-0.025246346,-0.0044015893,0.022988176,0.05059858,0.043071546,0.018072695,-0.0009572423,0.026169844,-0.007848235,0.0049770046,-0.039567143,-0.054468513,-0.025528515,-0.017285157,-0.0013086846,-0.029012082,-0.037284084,0.06040179,-0.033989288,0.021557132,0.011650735,-0.003568507,-0.049335044,0.048262298,0.055469967,-0.0032324658,0.034603458,-0.059064258,0.031710595,0.07137439,-0.013802459,0.007257819,0.038294647,-0.0031186766,0.011733728,-0.0021496448,0.0040124743,0.017767444,-0.031929523,-0.038032603,-0.0033999025,-0.025250735,-0.014678048,-0.007150304,0.0020698986,0.039101847,0.03398903,-0.018668778,-0.07896494,-0.036687497,-0.007185722,-0.0068640774,-0.021105723,0.013177055,-0.019481836,0.010005333,-0.061554056,0.06949223,-0.079162635,0.009981,-0.063083746,0.07056953,0.005491599,-0.017670633,-0.005135674,-0.024569517,-0.017512688,-0.04623066,0.058374815,0.049633086,-0.031594787,0.014083612,-0.01768504,-0.023426458,0.016086632,-0.032014813,-0.07724146,0.023943298,0.029805379,-0.021692641,0.07100319,0.030103581,0.024735862,0.024439165,-0.025098072,-0.05043015,-0.0021044633,0.051689424,0.023358965,0.06784304,-0.005000411,0.022722328,0.047239155,-0.08433809,-0.008117474,-0.05383696,-0.045388877,-0.0039899806,0.03395053,0.044186283,-0.009929108,-0.054068267,-0.016337426,-0.014790568,-0.041401166,0.018247783,0.01436438,0.045779686,0.07322691,0.053965505,-0.0089384625,-0.10281497,-0.030600332,-0.022700679,-0.0060153157,-0.027919244,0.0385471,0.0050643813,-0.010102684,-0.032633584,0.017006388,-0.008761299,-0.015657231,0.008278284,0.030671969,0.007710598,-0.02720048,0.019345978,0.08004764,-0.045704436,0.034446552,0.028901353,0.06072435,-0.023535091,-0.037889097,-0.07166545,-0.009231166,0.03086261,0.019568853,0.029063812,-0.0037357714,-0.017233884,-0.042524025,0.02582326,0.012123653,-0.016333293,-0.035201702,0.003895225,-0.015852472,0.05049397,0.0062288735,-0.013178832,-0.10109991,0.025958138,0.00056762696,0.004746886,0.060447153,0.011690745,-0.027885363,0.0055926526,0.019996384,-0.011862068,0.045630883,-0.0060193674,-0.0010050477,0.004235105,0.021470616,-0.0023958557,-0.05309118,-0.005531115,-0.014961733,-0.0005501159,0.007360166,0.0018752144,-0.00019302666,0.031399027,0.054046493,-0.013805706,0.02415915,0.00687001,0.0044786287,0.041459545,-0.06000288,0.045611653,-0.006086703,0.024767078,0.045739982,-0.023109358,0.013150862,-0.05765143,0.028921155,-0.02183089,0.06619113,0.02175989,-0.04162905,-0.030983536,-0.009312199,-0.029052265,0.0073476415,0.0058000707,0.010853425,-0.04671023,-0.07122776,-0.007321904,-0.052703008,0.027973425,0.0117961615,0.04366625,-0.018073222,0.02332397,-0.035746325,0.045962166,0.00082966546,-0.0055841072,0.026776042,-0.0072584013]	Image title: Recurrent vs. Hierarchical Transformer Architectures\nTags: transformer, neural-network, architecture, recurrent, hierarchical, sequence\nKey objects: Recurrent Transformer, Hierarchical Transformer, Input Sequence, Hidden State, Transformer Block, Lower-Level Transformer, Higher-Level Transformer\n---\nSummary:\nThis diagram illustrates two different approaches to transformer architectures: a recurrent transformer and a hierarchical transformer. The recurrent transformer processes a sequence iteratively, passing information between steps. The hierarchical transformer, on the other hand, processes the input sequence in a hierarchical manner, with lower-level transformers generating representations that are then fed into higher-level transformers.\nFull description:\nThe diagram presents two transformer architectures. (a) shows a recurrent transformer. An 'Input Sequence' is fed into the first 'Transformer Block'. The output of the first block then becomes the input to the next 'Transformer Block'. The previous output is also passed along to the next block which can be seen as a 'Hidden State'. (b) illustrates a hierarchical transformer. The input sequence is processed in a hierarchical manner.  'Lower-Level Transformers' process portions of the input, and their outputs are then passed to a 'Higher-Level Transformer'. This creates a hierarchy of representation, where the higher-level transformer operates on representations generated by the lower-level transformers.\nText found in image:\n- (a) Recurrent Transformer\n- (b) Hierarchical Transformer\n- Input Sequence\n- Hidden State\n- Transformer Block	{"tags": ["transformer", "neural-network", "architecture", "recurrent", "hierarchical", "sequence"], "title": "Recurrent vs. Hierarchical Transformer Architectures", "doc_id": "1ac84e0b-d5a7-4ed1-8019-368db32cdd4a", "source": "./images/a-survey-to-transformers/image_14.png", "summary": "This diagram illustrates two different approaches to transformer architectures: a recurrent transformer and a hierarchical transformer. The recurrent transformer processes a sequence iteratively, passing information between steps. The hierarchical transformer, on the other hand, processes the input sequence in a hierarchical manner, with lower-level transformers generating representations that are then fed into higher-level transformers.", "doc_type": "image", "key_objects": ["Recurrent Transformer", "Hierarchical Transformer", "Input Sequence", "Hidden State", "Transformer Block", "Lower-Level Transformer", "Higher-Level Transformer"], "parent_doc_id": "b5a36ce7-2c6c-4292-9eaf-134b8f3f10d8", "text_in_image": ["(a) Recurrent Transformer", "(b) Hierarchical Transformer", "Input Sequence", "Hidden State", "Transformer Block"], "contextual_description": "The diagram presents two transformer architectures. (a) shows a recurrent transformer. An 'Input Sequence' is fed into the first 'Transformer Block'. The output of the first block then becomes the input to the next 'Transformer Block'. The previous output is also passed along to the next block which can be seen as a 'Hidden State'. (b) illustrates a hierarchical transformer. The input sequence is processed in a hierarchical manner.  'Lower-Level Transformers' process portions of the input, and their outputs are then passed to a 'Higher-Level Transformer'. This creates a hierarchy of representation, where the higher-level transformer operates on representations generated by the lower-level transformers."}
8dbe6a89-4f77-46c1-af37-b106806644a4	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.037757933,0.003103499,0.012370983,0.058159236,-0.0010148173,0.04203933,-0.008665544,0.055570643,0.037634216,-0.008164903,-0.029523322,0.00831429,0.039801825,0.043320414,-0.051896602,0.04117027,0.021903213,0.0506413,0.029316455,-0.045226038,0.055330355,0.01163299,-0.0033454674,-0.055829246,0.025136592,0.051715538,0.018349314,0.019688047,-0.021670815,0.008405516,0.02420978,-0.069651775,0.054844663,0.016587134,0.04595245,-0.016455151,0.0043670684,-0.0328887,-0.016054366,-0.02450615,-0.04116205,0.04259178,-0.06689656,0.009897382,0.017719928,-0.052117515,-0.08417287,-0.01860993,0.01837517,-0.014890816,-0.00833832,0.012040176,-0.04307378,-0.0721368,-0.009813473,-0.004632526,0.032618117,-0.036312446,0.008432424,-0.06617065,0.005066969,-0.019593218,0.0057308213,-0.026457332,0.02668938,-0.008938681,0.017674027,0.004132694,0.03667144,0.13521205,0.002886219,-0.012423843,0.007315871,-0.03986526,0.09270478,0.046626456,-0.026671695,-0.05596771,-0.073870614,-0.01717125,-0.008659591,0.07899014,-0.060377132,-0.019392833,0.0692433,-0.00481758,-0.051213305,-0.003225057,0.033111684,-0.065993324,0.016879627,0.0004114735,0.008071658,0.03780327,-0.013263573,-0.06606749,-0.041635342,-0.040707976,0.043055393,-0.022145132,0.013677164,-0.00087298,0.0597244,0.09842112,-0.002978366,-0.028559808,-0.023783319,-0.030658524,0.053128604,0.024428383,-0.051413726,0.04506155,-0.069408305,-0.006823825,-0.03565071,-0.018438708,-0.033853475,0.014322144,0.010492739,-0.004711339,0.019796977,-0.008766936,0.0011837055,0.021070873,0.0279862,0.04998004,0.014295729,-0.05883323,-0.03548942,-0.0030198363,0.037739705,0.03116337,-0.0047267815,0.017043397,-0.010442774,0.014215218,0.06933111,0.0025296218,-0.005844707,-0.0048658247,0.00511281,-0.0028070954,-0.014689023,0.010221948,0.013168,0.027956929,-0.05459959,0.035980012,-0.011851801,0.06483893,0.042842332,-0.01816962,0.022267684,0.0039834687,-0.02828507,-0.019393675,-0.011751534,-0.008258065,-0.008130685,-0.049967248,-0.041836083,-0.02911303,-0.012056894,0.055414636,0.03158394,0.09228372,-0.017408887,0.022130713,0.017758805,0.015704697,-0.06671784,-0.006786099,0.024896199,-0.04384433,0.036606133,-0.021255463,0.00928029,-0.058018632,0.014249122,-0.008557455,-0.028023494,0.045297507,-0.0031784629,0.045095433,-0.020874914,0.050246947,0.010751738,0.0299628,-0.05028313,-0.040401254,0.029598605,0.0059696343,0.01959853,0.014532626,0.09133635,0.054851197,0.04831142,-0.032246046,-0.034768216,-0.026325997,0.034425575,-0.03606323,0.016074765,-0.012040805,0.043085758,0.019177575,0.010231507,-0.010955749,-0.0024310104,0.022448953,0.0004945807,0.0019228592,-0.034099516,0.02015029,0.04424186,0.008916135,-0.0436226,0.021972815,0.070755005,-0.009010655,-0.028815297,0.042866003,0.01214417,0.050387226,-0.00034369153,-0.007838735,0.006110836,0.026883604,0.03375246,-0.035978112,0.009991203,0.03019777,-0.04456944,0.056491222,-0.015898215,0.022869948,-0.031977404,0.0034457182,0.00093706715,0.041763645,0.04956118,0.02160369,-0.05429563,0.05926981,0.0046441676,-0.006607866,0.0037325926,0.04324025,0.015779575,0.0419524,0.0056733475,0.030341335,0.07470698,-0.030954234,0.015521874,-0.029059952,0.016973604,0.044542387,-0.0052994196,0.017143486,0.0009042689,-0.018625626,-0.0122206565,0.049251188,0.012109463,-0.030420408,0.022442892,0.04851057,-0.052424,-0.004730693,0.060859084,0.013733578,0.015615935,-0.024739372,-0.014996748,0.0014891134,0.0058174315,-0.015812997,0.044986274,-0.021153577,-0.051762436,-0.041109815,0.026897013,-0.002597972,0.059925806,0.0017045541,0.0052078096,-0.035183627,-0.03255188,-0.025513731,-0.034763012,-0.04058071,0.0269637,0.006955949,-0.035565272,-0.019999571,-0.017478677,-0.041956373,-0.024928257,0.03105439,0.09473743,-0.019587785,-0.016730348,0.014804988,0.017315315,0.0335866,-0.032231603,0.0149072595,0.018773263,0.006001113,-0.0030372727,-0.033709798,-0.040937208,0.033842955,0.0017921699,0.08394497,0.048651595,-0.060512323,0.013842834,0.016049832,0.007685982,0.010190036,-0.017168827,0.0570763,0.024773188,-0.062299963,0.036959164,-0.022402467,-0.06390228,0.05851324,-0.052849162,0.051308718,0.054379817,0.02534411,-0.029090125,0.017790314,0.04449499,0.04465157,0.014434348,0.040089574,-0.008431383,-0.05371692,0.02686407,0.015674097,-0.024183987,0.047630753,-0.0019367334,0.0034735948,0.015036862,-0.012523638,0.0115681,0.006329573,0.004479841,0.02158184,0.050114214,-0.037387446,0.022025434,0.009095212,0.004990248,0.016386727,0.069322824,-0.058318388,0.038235623,-0.007862817,0.0031710302,-0.023011742,-0.064135104,-0.017593702,0.005722577,-0.03521811,-0.00380435,-0.030367022,0.045529768,-0.053955674,0.020495715,-0.004463798,0.05963194,0.049710114,-0.028477821,-0.03414902,-0.037629213,0.014010567,-0.041040022,-0.002841158,0.02547194,-0.014137412,0.019158695,0.06769157,-0.010577877,0.057998307,-0.05874943,0.03468777,0.038353898,0.019450279,0.0095453,-0.019721666,0.036857396,0.04491032,0.0061369836,-0.008400027,0.0028688847,0.07030451,-0.04954793,0.009067952,0.017272828,0.020691078,-0.064281546,0.007962075,0.091488644,0.021074474,0.043537233,-0.004035695,-0.044531964,-0.0013884673,0.029364817,0.015468368,-0.024240244,-0.0016220617,0.001444437,-0.0101258,-0.016349146,0.03357951,0.043139137,0.07055973,0.028739186,-0.039250482,-0.071967535,-0.024730993,0.04152313,-0.07172646,0.009171871,-0.048614815,0.0058451,0.0605566,0.08643609,-0.017603166,-0.05950024,0.026527422,-0.08402927,0.05862535,0.041662447,-0.028550634,0.006284504,-0.026667804,0.013904248,-0.05105509,0.053231027,0.02139817,-0.03589496,-0.03455354,0.009793839,0.025557794,0.012686115,-0.038847513,0.034438897,-0.00022318565,-0.004000637,-0.03192213,-0.016529053,0.023000168,-0.0014906646,0.0015125896,-0.019450486,0.0062534986,-0.052188095,-0.00868189,-0.011681088,-0.02586414,-0.022100702,0.011692382,-0.0017923958,0.009834571,-0.04088348,0.06280431,-0.007948979,-0.00012768361,-0.008322404,0.010294187,0.02130703,-0.023513874,-0.07665136,-0.021133134,0.019048033,0.08376736,0.0068369494,-0.005458135,-0.017509349,-0.028735595,-0.044262808,0.03355075,-0.0031644397,0.08084652,0.0060552107,-0.0072219456,0.02460636,-0.05484377,-0.038804825,0.016505186,-0.0147438515,-0.026973508,0.01168301,-0.046675455,0.032628097,0.0267454,-0.020569187,0.03115807,-0.007181917,-0.013408641,-0.03426249,0.049111083,0.0038861546,-0.005407093,-0.03813709,-0.021861732,-0.056157008,0.01997078,-0.0065328972,0.0214521,0.0025542395,0.025537018,0.011768809,0.010719968,-0.014233412,-0.0142309675,0.0149489725,-0.00766427,0.04284174,0.023525396,0.044312596,-0.05915042,0.06546988,-0.029123684,-0.033715237,0.0023738178,-0.061348993,-0.064854704,-0.0026604475,-0.058568224,0.04296079,0.009920045,-0.0002485202,-0.024729688,0.021067696,-0.017496152,0.06568277,-0.05773126,-0.007361018,-0.030343322,-0.0206077,0.09150894,-0.009350799,0.011964763,-0.035819795,-0.026791608,0.026668616,0.016669326,0.066613704,0.044921674,-0.0074086725,0.049221147,-0.04105424,0.027635194,0.00074270077,-0.03164563,-0.022335848,-0.023656048,-0.00919605,-0.050258867,-0.034497462,0.029254163,-0.0262115,0.021642942,-0.023308521,0.011119019,-0.00080447714,0.048702143,0.047485415,-0.0073361387,0.03219098,0.00086967356,0.019327678,0.0125587415,0.022410877,0.028776845,0.031567346,0.0035823595,-0.01245619,-0.023510879,0.02014439,-0.011672019,-0.044727854,-0.02366022,0.004109371,-0.014137768,-0.00020388553,-0.029246422,0.013591975,0.06460308,0.033685204,-0.036423273,-0.10159283,-0.061718836,-0.03993954,-0.03898699,-0.0026089442,0.000999532,-0.023081593,0.039921626,-0.05511188,0.07080149,-0.09392943,0.0028769963,-0.079211935,0.045118626,0.0046870727,0.028179945,-0.0011529432,-0.032670707,-0.039602198,-0.04484645,0.062413894,0.027826268,-0.044713024,-0.014955826,0.015256813,0.023637477,0.04876367,-0.035349973,0.0058139698,0.018068923,-0.04793915,-0.01618187,0.06780825,0.0017363649,0.013098734,0.0449453,0.021987291,-0.050620485,-0.0004054279,0.052329823,0.0393344,0.034613717,0.019714044,0.020697046,0.045044053,-0.055395305,-0.0038192458,-0.013040628,-0.006573237,0.024609663,0.013016943,0.03923133,-0.005006743,-0.034145255,-0.033450466,0.0027239611,-0.015315166,0.028224653,0.023275528,0.0070497068,0.048207685,0.0458271,-0.08541852,-0.12399565,-0.05525871,-0.028797522,-0.003913492,-0.04026688,0.09970875,0.0040510283,-0.0023185078,-0.002735904,0.003802647,0.004982614,-0.0014938302,0.004539248,-0.020411752,-0.015990866,-0.017869527,0.015344639,0.032158,-0.018356727,-0.013568922,0.04441733,0.00085563347,-0.014143856,-0.041341983,-0.023843031,0.000412701,0.0105343135,-0.031114103,-0.004069404,0.033051107,-0.01545354,0.022694856,0.022664111,0.022207487,-0.05294565,-0.047138393,0.003903171,0.024858128,0.023194054,0.028829861,0.013320404,-0.09587256,0.040101487,-0.010001308,-0.038905628,0.054090872,-0.024472823,-0.011141014,-0.032975025,0.007832667,0.002676306,0.01111651,-0.008857335,0.010339893,-0.009268643,0.026397537,0.0028136293,-0.06087894,-0.02134025,0.014093788,0.030700821,-0.03560899,0.06372187,0.008393742,-0.0035527241,0.013056319,-0.023194624,0.00095560175,0.0063235434,-0.03988837,0.047368996,-0.0120285805,0.04406302,0.017530188,0.0063047307,0.11290008,-0.04114625,0.0034113936,0.0138919875,0.015963906,-0.021215059,0.023242364,-0.0010705539,-0.05198124,-0.03857556,-0.021793554,0.014218852,0.014525823,0.0037263646,0.031334575,-0.04933516,-0.033842567,-0.012697057,-0.011912774,0.028525589,0.043912854,-0.004957378,-0.096353956,0.02799755,-0.045240764,0.066801414,0.03783055,0.0010199337,0.010213157,-0.050290562]	Keywords: recurrence mechanism, history representations, context length, Transformer-XL, ERNIE-Doc\nKey Objects: history representations, context length\nRefers to Images: None\nHypothetical Questions:\n- How does the incorporation of history representations from the l-th layer impact the model's ability to capture long-range dependencies?\n- What is the difference between using representations from the (l-1)-th layer versus the l-th layer in the recurrence mechanism?\n- How does this approach address the limitations of a fixed-length context in traditional Transformers?\n---\nSummary:\nTransformer-XL and ERNIE-Doc enhance the recurrence mechanism by incorporating history representations from the l-th layer, resulting in a larger effective context length.\nOriginal Text:\n$$\\tilde { H } _ { ( l ) } ^ { + 1 } = [ S G H ^ { ( l - 1 ) } ] \\circ H ^ { ( l - 1 ) } _ { ( r - 1 ) },$$  \nERNIE-Doc [30] proposes an enhanced recurrence mechanism based on the recurrence mechanism used in Transformer-XL, by replacing the memory with the history representations from the l -th layer.  \n$$\\tilde { H } _ { r + 1 } ^ { ( l ) } = [ S G ( H _ { r } ^ { ( l ) } ) \\circ H _ { r + 1 } ^ { ( l - 1 ) } ],$$  \nas opposed to using representations from the ( l - 1)-th layer in Eq. (48). This modification essentially leads to a larger effective context length.\nContextualized Text:\nTo address limitations in context length, Transformer-XL and ERNIE-Doc enhance Transformer models using a recurrence mechanism. These models incorporate history representations from a previous layer of the Transformer, leading to a larger effective context length and improving the model's ability to process long sequences.	{"tags": ["transformers", "architecture", "NLP", "recurrent"], "doc_id": "8dbe6a89-4f77-46c1-af37-b106806644a4", "summary": "Transformer-XL and ERNIE-Doc enhance the recurrence mechanism by incorporating history representations from the l-th layer, resulting in a larger effective context length.", "doc_type": "text", "entities": ["Transformer-XL", "ERNIE-Doc"], "keywords": ["recurrence mechanism", "history representations", "context length", "Transformer-XL", "ERNIE-Doc"], "key_objects": ["history representations", "context length"], "contextual_text": "To address limitations in context length, Transformer-XL and ERNIE-Doc enhance Transformer models using a recurrence mechanism. These models incorporate history representations from a previous layer of the Transformer, leading to a larger effective context length and improving the model's ability to process long sequences.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.4 Transformers with Divide-and-Conquer Strategies"}, "hypothetical_questions": ["How does the incorporation of history representations from the l-th layer impact the model's ability to capture long-range dependencies?", "What is the difference between using representations from the (l-1)-th layer versus the l-th layer in the recurrence mechanism?", "How does this approach address the limitations of a fixed-length context in traditional Transformers?"]}
82852eae-3380-451c-8050-6105cedab116	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.024524659,0.007939152,0.030865658,0.048013054,-0.023606943,0.033611685,0.020675702,0.076743074,0.03679866,-0.005111889,-0.024763675,-0.0098511735,0.02533354,0.051822,-0.048774797,0.00508857,0.027329458,0.0692098,-0.0019471667,-0.03111282,0.057035618,0.013002176,0.009380657,-0.022761097,0.026684875,0.027201496,0.0134540815,-0.005421031,-0.027212402,0.0012293747,0.00044911713,-0.05316685,0.057161685,-0.013440488,0.026230922,-0.0294369,0.00709378,-0.025959715,-0.017075656,-0.012493465,-0.041027404,0.023398211,-0.077857114,-0.010322952,0.029090034,-0.04452061,-0.09031985,-0.03349499,0.049029257,-0.017781358,-0.014838213,0.006156179,-0.03281981,-0.045215387,-0.017186873,-0.027129756,0.025453191,-0.03610968,0.025976656,-0.081653446,-0.030106425,-2.9044772e-06,0.025893133,0.00593206,0.067491114,-0.013228732,0.024456857,-0.01120861,0.042355783,0.13124932,0.0055433735,-0.036471996,0.01739737,-0.00959149,0.10284971,0.034235626,-0.044797774,-0.03937138,-0.0651328,-0.019371117,-0.023280673,0.093693085,-0.043576684,-0.054247253,0.07537631,-0.0052228915,-0.047114816,-0.013926476,0.03892228,-0.08405746,0.003939578,-0.009230236,0.025706442,0.03982791,-0.004131828,-0.0772593,-0.045291394,-0.047288198,0.036677696,-0.004600513,-0.011148679,-0.009014463,0.053880043,0.104642294,-0.020617327,-0.010688952,-0.030699402,-0.034783933,0.06628947,0.033563934,-0.050461866,0.007925013,-0.033169854,-0.010930269,-0.023436498,-0.005313693,-0.019488914,0.036888693,0.03283324,-0.008797304,0.01263798,-0.0046002027,0.018064495,0.0046578404,0.022266181,0.057684686,-0.0036629008,-0.03711643,-0.010066781,-0.0007717994,0.046595614,0.026028646,0.002347044,0.001973251,0.0001219194,0.00296418,0.059327196,0.00092297915,-0.01896521,-0.01091513,-0.014799414,-0.012617122,0.016031757,0.018778795,0.015733454,0.019161155,-0.04901593,0.014269259,0.010325045,0.06577081,0.015087375,-0.05376843,-0.002885114,-0.009566984,-0.02795604,-0.007338355,0.009786904,-0.011105753,-0.008332689,-0.03980427,-0.05864837,-0.051754482,0.0034091426,0.056727573,0.015951473,0.07408356,-0.00941927,-0.0033380038,0.02145096,-0.0061357864,-0.04264506,-0.0056089126,0.010254555,-0.019558387,0.022274539,-0.014800086,0.010396744,-0.057425965,-0.00021132806,-0.01689353,-0.019176967,0.057023436,-0.017688226,-0.006882784,-0.054962415,0.049898818,-0.021112233,0.03361932,-0.0556065,0.0031612136,0.044918567,0.013346949,0.010877193,0.022622313,0.047136474,0.063518204,0.049161505,-0.04460348,-0.030466655,-0.009765951,0.023524199,-0.04418885,0.03458857,-0.00062587945,0.015438098,-0.0016177574,0.030528188,0.021219056,0.013700793,0.035591587,0.004035784,-0.0030474188,0.0054395045,-0.015531497,0.04220006,0.023415923,-0.06839815,-0.012021457,0.06577399,0.00023125882,-0.020647539,0.026634153,0.0032365648,0.047895927,-0.024939544,0.0074213403,-0.005589746,-0.009123732,0.04587663,-0.018659735,0.037620813,0.048609093,-0.01609425,0.047948707,-0.034143854,0.029570295,-0.027911747,-0.0034980373,-0.025635779,0.056667976,0.061831392,0.008623899,-0.051739745,0.06882481,0.01985649,-0.017561533,0.011707744,0.034030948,-0.004583954,0.042564724,0.0038080066,0.050672557,0.09164288,-0.0031036655,0.0047050966,-0.03491073,0.029297391,0.06173555,-0.008502265,0.026752168,-0.03691058,-0.028942978,-0.0030844046,0.022206869,0.006145642,-0.026255807,0.009942871,0.05587665,-0.039849304,-0.03513003,0.06079499,0.0030965917,0.054295067,-0.051145237,-0.0100232605,-0.0045834323,-0.008966078,-0.04651913,0.025170937,-0.020438954,-0.0447351,-0.033925306,-0.002554528,-0.0031547311,0.069488935,-0.028742746,0.016068922,-0.02106381,-0.05492036,-0.038750578,-0.016231427,-0.036868997,0.02491535,0.021806723,0.0072234767,-0.010247057,-0.01656704,-0.078390084,-0.050355706,0.030340815,0.10439016,-0.04067666,-0.016487401,0.02762016,0.0027635307,0.018922219,-0.055897683,0.041738667,0.020199388,0.0050565717,0.017466761,0.0022633416,-0.02791734,0.04150286,0.001971857,0.069058076,0.050791852,-0.043464527,0.008468447,0.013836052,0.023864515,0.011198789,-0.0154698435,0.022452882,0.012654713,-0.0541858,0.027001463,-0.019451542,-0.050943244,0.062145367,-0.09598217,0.010553377,0.056171954,0.024287611,-0.042531602,0.054575514,0.07255163,0.030721182,0.01358434,0.035090934,-0.023561954,-0.07425598,0.019130345,0.01987089,-0.029224833,0.028906027,-0.0070092822,0.020818735,0.018818224,-0.036809467,0.017161734,-0.0029123481,0.0137258135,0.035324775,0.034456998,-0.0493853,-0.0024638658,0.0075718993,0.017987283,0.013747191,0.058276527,-0.06436606,0.043271836,-0.0063073416,0.026220048,-0.02362224,-0.049912143,-0.03242764,0.01040772,-0.031069817,0.007699677,-0.041363113,0.06412802,-0.04865551,0.021846103,0.041553088,0.050191842,0.044791535,-0.031956438,-0.022258373,-0.03733671,0.002142591,-0.043654732,-0.022683034,0.010510984,0.01016748,0.017492907,0.07699133,-0.030964453,0.019780485,-0.04897244,0.010721424,0.039936047,0.024811003,0.0067994283,0.0034426667,0.022455081,0.029513622,0.015999887,-0.00902444,0.027633583,0.0844474,-0.05854659,0.03264451,0.060655348,0.014864724,-0.05120683,0.01650823,0.076217934,0.016264247,0.04250265,0.00013125599,-0.045241572,0.02231535,0.007629568,0.022479178,-0.009413525,0.012192709,-0.0024245263,0.033100687,-0.020227378,0.033095915,0.03150198,0.03626188,0.028194664,-0.015285106,-0.045950375,-0.004993768,0.04620941,-0.07567738,-0.00016305187,-0.074026436,0.016827878,0.059666947,0.08816399,-0.0056139855,-0.08096594,0.04101488,-0.08934534,0.065029345,0.0498969,-0.040875733,0.008503964,-0.0026116099,-0.0071783196,-0.03674082,0.05212549,0.0065140347,-0.040000293,-0.040899906,-0.00022927995,-0.00037392677,0.01732979,-0.038815804,0.03415067,-0.027351419,0.019699076,-0.027861772,-0.020693285,0.012885157,0.021632826,-0.004088009,-0.009577383,-0.018389981,-0.026121281,-0.0064067314,-0.03892052,-0.008254741,-0.031050531,0.0063394587,0.032438837,0.041057803,-0.014432281,0.081618056,-0.0041812505,-0.029947653,0.006948225,0.0104357805,0.01722521,-0.010087322,-0.0555442,-0.023090698,0.002185351,0.054955438,-0.01048543,-0.018052455,-0.018408392,0.0057845153,-0.061622724,0.041497007,0.0065925885,0.095886774,0.023072505,0.015922328,0.01831844,-0.057112012,-0.01975365,0.003404514,0.0006285244,0.0038974308,0.020580791,-0.028079052,0.036537167,0.007104153,-0.027242215,7.83383e-05,0.04119422,0.0014218827,-0.01780601,0.061022732,0.0019370053,-0.030059261,-0.032298595,-0.014374607,-0.050976735,0.011009993,-0.007141194,0.020387132,0.011560001,0.0317629,-0.0051623704,-0.023478717,0.016407127,0.008086627,0.015111067,0.0053143622,0.034851763,0.014948493,0.045611534,-0.01945486,0.07594321,-0.047535792,-0.05102926,0.00402464,-0.043969385,-0.046702433,-0.0022884293,-0.05512547,0.035793044,0.022503259,0.003726098,-0.02998543,0.024480801,-0.008864923,0.0789974,-0.06637071,0.021050273,-0.04115927,0.021074297,0.096638165,0.0051141684,0.03482142,0.003166632,-0.028534748,0.0068289195,0.015524255,0.033891913,0.041655798,-0.025192916,0.044543967,-0.009448386,0.0012912826,-0.040238705,-0.0338645,-0.01820239,-0.0546337,0.009206362,-0.027088083,-0.041456647,0.025672635,-0.011248805,0.04106994,-0.0071296515,0.0013155378,-0.024538536,0.04050706,0.01759195,0.036453325,0.05203936,-0.0027652446,0.03125649,0.04182,0.03221826,0.0095339,0.013521766,0.014326447,-0.03128447,-0.027360931,0.021675939,0.012532632,-0.03818976,-0.030777745,0.01265702,0.0035695692,-0.018165825,-0.00020023169,0.01573951,0.041153718,0.00023138645,-0.039020065,-0.09897484,-0.03975131,-0.033318352,-0.053279605,0.0029952412,0.0049459646,0.008669917,0.013695501,-0.085245475,0.06293747,-0.066361256,0.0012325097,-0.06901258,0.0501357,-0.00856339,0.015561515,0.02425959,-0.013552533,-0.010509725,-0.05732443,0.026315045,0.008385754,-0.01857138,0.0039822655,0.0117342835,0.010986188,0.019851377,-0.018725784,0.0012569929,0.01775561,-0.027182057,-0.016459672,0.07623466,-0.03203009,-0.016808517,0.022743514,-0.0004068392,-0.073053546,0.0059834486,0.029604204,0.05909359,0.028876213,0.026318582,0.026022177,0.028475251,-0.035627943,-0.011509116,-0.0025697544,-0.026262753,0.018676318,0.003243709,0.044014104,-0.0139088705,-0.042854484,-0.014648645,0.00020918886,-0.007852383,0.034617115,0.00021424488,-0.004768019,0.033246342,0.042930473,-0.06822259,-0.11684373,-0.027894974,-0.017609613,-0.015463746,-0.020962827,0.08853201,0.009273803,0.0010585774,0.008995666,0.037388317,0.021956548,-0.015907003,0.02533005,-0.019153759,0.015635421,-0.015926076,0.018987898,0.051770102,-0.035816573,-0.014839105,0.024030995,0.028434588,-0.03333818,-0.020522172,-0.048688766,0.009900398,0.022612803,-0.06924763,0.0036145158,0.006253818,-0.015002153,0.0053440873,0.011147792,0.0011578123,-0.036540296,-0.034565587,0.04068027,0.021065384,0.019677512,0.008410264,-0.03138034,-0.05501122,-0.0028118873,-0.01668253,-0.026396645,0.027947186,-0.037355155,-0.025165398,-0.022603784,-0.017875144,-0.014494041,-0.0068212273,0.0011989742,0.0039604316,-0.0028869035,0.009417156,-0.00021485731,-0.037256368,-0.031344756,-0.0023437105,0.0040818206,-0.03677391,0.05092168,-0.006918306,-2.6500673e-05,-0.005678072,-0.030749226,0.007220805,-0.02832792,-0.0020614024,0.067451596,-0.008911707,0.06411526,0.034663096,-0.0053184787,0.13158454,-0.0039133052,0.024356974,0.012061936,0.051978428,-0.0127452025,0.008835098,-0.013788783,-0.05970871,-0.05812855,-0.020162135,0.0060695168,0.007471199,0.0024529276,0.056300815,-0.04828367,-0.065240785,-0.040194057,-0.013175797,0.037670746,0.012446616,0.0070198383,-0.07920923,0.039012887,-0.06668968,0.06897672,0.0046668085,0.006815816,0.010435673,-0.046637636]	Keywords: recurrence mechanism, effective context length, Transformer-XL, ERNIE-Doc\nKey Objects: representations, context length\nRefers to Images: None\nHypothetical Questions:\n- How does using representations from the l-th layer instead of (l-1)-th layer increase context length?\n- What is the purpose of using representations from a previous segment?\n- How does ERNIE-Doc differ from the original Transformer-XL architecture?\n---\nSummary:\nERNIE-Doc enhances Transformer-XL's recurrence mechanism by utilizing representations from the l-th layer instead of the (l-1)-th layer, which effectively increases the model's context length.\nOriginal Text:\nas opposed to using representations from the ( l - 1)-th layer in Eq. (48). This modification essentially leads to a larger effective context length.  \n6.4.2 Hierarchical Transformers. Hierarchical Transformer decomposes inputs hierarchically into elements of finer granularity. Low-level features are first fed to a Transformer encoder, producing output representations that are then aggregated (using pooling or other operations) to form a high-level feature, which is then processed by a high-level Transformer. This class of methods can be understood as a process of hierarchical abstraction. The overview of this approach is depicted in Fig. 13(b). The advantages of this approach are twofold: (1) Hierarchical modeling allows the model to handle long inputs with limited resources; (2) It has the potential to generate richer representations that are beneficial to tasks.\nContextualized Text:\nERNIE-Doc builds upon Transformer-XL's recurrence mechanism, which caches representations from previous segments. A key improvement in ERNIE-Doc is that it uses representations from the l-th layer, rather than the (l-1)-th layer, to achieve a larger effective context length.	{"tags": ["transformers", "NLP", "architecture"], "doc_id": "82852eae-3380-451c-8050-6105cedab116", "summary": "ERNIE-Doc enhances Transformer-XL's recurrence mechanism by utilizing representations from the l-th layer instead of the (l-1)-th layer, which effectively increases the model's context length.", "doc_type": "text", "entities": ["Transformer-XL", "ERNIE-Doc"], "keywords": ["recurrence mechanism", "effective context length", "Transformer-XL", "ERNIE-Doc"], "key_objects": ["representations", "context length"], "contextual_text": "ERNIE-Doc builds upon Transformer-XL's recurrence mechanism, which caches representations from previous segments. A key improvement in ERNIE-Doc is that it uses representations from the l-th layer, rather than the (l-1)-th layer, to achieve a larger effective context length.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.4 Transformers with Divide-and-Conquer Strategies"}, "hypothetical_questions": ["How does using representations from the l-th layer instead of (l-1)-th layer increase context length?", "What is the purpose of using representations from a previous segment?", "How does ERNIE-Doc differ from the original Transformer-XL architecture?"]}
ae2eb4e7-7618-41f2-8465-29ef4bf7897d	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0045617223,0.026118625,0.052328024,0.064049445,-0.037782874,0.02334554,0.0050119986,0.050144117,0.03830484,0.008095145,0.0040632915,0.010440512,0.0024301019,-0.0023341891,-0.049032375,0.04662732,0.021652663,0.09425213,0.019971963,-0.024621533,0.029326292,0.019895408,-0.0037352028,-0.025781048,0.021849452,0.02415275,0.023782823,0.012718415,-0.03197565,0.0016003322,0.021594966,-0.03730967,0.043735664,6.0541526e-05,0.05547994,-0.031468768,-0.0019574119,-0.029199695,-0.012367755,0.0016434378,-0.006581357,0.05382319,-0.09203043,-0.017620258,-0.015157109,-0.077097654,-0.10932001,0.01085446,0.047633115,-0.002120918,-0.015260864,-0.009087172,-0.047098566,-0.027093004,-0.00207424,-0.005503338,-0.0029265531,-0.04998999,-0.006387986,-0.09573847,-0.048013628,0.025133597,-0.0006729861,-0.009764207,0.033926558,-0.007870802,0.004820829,-0.040026903,0.010294827,0.114323296,-0.020677803,-0.008481694,0.005039647,-0.047700852,0.105130665,0.034978222,-0.04741054,-0.019986315,-0.06729642,-0.026416207,0.010633863,0.10482676,-0.03648509,-0.01956213,0.0756054,-0.01815781,-0.038657375,0.031093651,0.017244238,-0.0565886,-0.021129204,-0.027337257,0.019491564,0.060466915,-0.052028686,-0.038083777,-0.019236373,-0.021774039,0.017997524,-0.05978663,0.018232377,-0.021121243,0.09393969,0.11091267,0.0005893707,-0.07102087,-0.01866321,-0.018937778,-0.00073153275,0.060029134,-0.039145857,0.03990539,-0.066045016,0.027817968,-0.01232473,0.032186095,-0.049465746,0.0049666693,0.012521274,-0.014170841,0.016672423,-0.030932102,-0.015120483,0.006568184,0.047429107,0.039913394,0.015297941,-0.026873592,-0.031244924,0.029571796,0.027776523,-0.0059651984,-0.0060585598,0.023874428,-0.032535914,0.01455129,0.078798674,0.020220555,-0.018566772,-0.015539189,-0.0034200647,0.00782564,-0.0073611266,0.053619646,0.025885344,0.028218986,-0.038834073,0.052814245,-0.004432095,0.05059362,0.04504789,-0.023877332,0.05820874,-0.0048223934,-0.011657722,0.011672301,-0.02641001,-0.043180082,-0.014086425,-0.03347975,-0.045433573,-0.027211796,0.017348062,0.041151654,0.016499575,0.09370928,-0.006172222,0.0108194295,0.04215347,-0.007143896,-0.05628912,0.013581859,0.0013306245,-0.034256183,-0.007069897,0.007655498,0.023418523,-0.07503768,-0.013473852,-0.0039736954,0.006411955,0.046189416,0.022960676,0.021158377,-0.07367049,0.0380455,-0.0116254855,0.0569956,-0.030594103,-0.05390415,0.017691491,0.0037030329,0.0181176,0.010540409,0.0120362425,0.058510214,0.03454639,-0.07671316,-0.022592613,-0.016955614,-0.005781016,-0.029713048,-0.010806673,-0.025396757,-0.006034815,-0.012308448,0.017747296,-0.037237603,0.007854414,0.012740018,-0.016688095,0.02102721,-0.011005053,0.008357285,0.051134083,0.0032320637,-0.042808484,0.016293284,0.05146024,-0.021014202,-0.07710771,0.010706503,-0.011547462,0.03538903,0.008296075,0.0001969532,0.012500368,0.01748328,0.010377815,-0.03334224,0.050652888,0.032161575,-0.046262935,0.028561713,-0.022504095,0.050286394,-0.0015082546,0.0015413319,-0.029085135,0.035629828,0.04165163,0.044112712,-0.016173512,0.07719269,-0.0061477297,-0.018015532,0.014438052,0.04099823,0.0026225124,0.04302813,-0.010278838,0.07187243,0.052818097,-0.02726726,-0.01577058,-0.056121707,0.0101654325,0.010000254,0.011134958,0.02150528,0.006326557,-0.015897762,0.019234706,0.046263438,0.016858125,-0.0010284818,0.00088723836,0.03133748,-0.043735962,-0.018634679,0.033410165,0.041952077,0.029703256,-0.029988887,-0.05164745,-0.0066559324,0.012536797,0.00048722193,0.041714557,-0.038265686,-0.019211419,0.0068053515,0.031503092,-0.015160047,0.04440589,-0.03036018,0.0071309046,4.6078036e-05,-0.045006957,-0.014960916,-0.017870758,-0.042204164,0.03089409,-0.0006883159,-0.02689986,-0.02671094,0.0022152276,-0.019794192,-0.021795496,-0.0029280751,0.05775138,-0.048132893,0.018857284,0.035013083,0.015971247,0.042544104,-0.04236508,0.030190216,0.01250151,0.033923004,0.008203116,-0.007424957,-0.035588015,0.0072405287,-0.0068171755,0.08568993,0.082867384,0.017194413,-0.012105417,0.01918854,0.020562656,-0.009215896,0.0103963865,0.022336809,0.017730651,-0.013952297,0.025674125,-0.022238536,-0.07150519,0.095658086,-0.108521625,0.015752088,0.032047514,-0.009734336,-0.012917373,0.008936512,0.031983197,0.039424825,0.0033296233,0.006132802,-0.019969448,-0.04202831,0.018390872,-0.020577407,-0.022335714,-0.009590838,-0.006342351,0.0022404396,0.014616272,-0.034969926,0.008803791,0.003926719,-0.0058998507,0.0040324843,0.031891163,-0.08706929,-0.029031498,0.027025566,0.017528428,0.036949486,0.053315748,-0.07366536,0.046740156,-0.032074515,0.061354455,0.0062228874,-0.065581016,-0.0004719373,0.045610253,0.00037025227,0.002360066,-0.03257509,0.052791644,-0.058761746,0.014408745,0.033837296,0.06900826,0.0638107,-0.06756968,-0.02216295,0.007460545,0.00031299825,-0.035056546,-0.058575757,0.013653088,0.004990158,0.0062920805,0.04798737,0.000581622,0.023809273,-0.044638194,0.014155658,0.021296086,0.013741652,0.0018864863,0.013151788,0.026719224,0.04030367,-0.014913482,0.01186233,0.013504034,0.020756245,-0.042470925,0.008143944,0.062982716,0.012862304,-0.058786403,0.024116421,0.061553024,0.042207453,0.026096625,-0.005952215,-0.0066271196,-0.005702001,0.03822512,-0.020418398,-0.018609544,0.005543076,-0.0059426543,-0.014872001,-0.016875062,0.06704013,0.051172495,0.063740104,0.052688718,-0.018978575,-0.04505861,0.056530207,0.002950369,-0.042206414,-0.015637765,-0.06001032,-0.016391048,0.05380352,0.0333422,0.0053221243,-0.06686427,0.04520552,-0.092735074,0.06568825,0.041132044,-0.046453662,-0.012312155,-0.023337563,-0.013510228,-0.018603267,0.06340704,0.021201823,-0.064922966,-0.012095405,0.0068501397,0.007661864,0.04795736,-0.0216919,0.03813002,-0.018645098,0.0047786827,-0.035374753,-0.043729443,0.0012279453,0.002113942,-0.016934616,0.005082877,0.009791751,-0.0055406117,-0.013290638,-0.037180047,-0.01988102,-0.024373755,0.015356401,-0.011713219,0.020786228,-0.028915592,0.0793114,0.0059230183,-0.029873366,-0.0040010903,0.032365073,0.03611957,-0.01139646,-0.039785083,-0.017596724,-0.003940133,0.026122063,0.003500733,-0.026451528,-0.023943765,-0.029518697,-0.03721602,0.069807425,-0.0049804724,0.07247843,0.023988025,0.013914915,0.019323792,-0.021265514,-0.028009063,0.018824039,-0.0038260582,-0.016155401,-0.008550118,-0.005042745,0.05328597,0.026514689,-0.014138707,0.020318797,-0.018749127,-0.015531895,-0.05368037,0.059366707,-0.009458508,-0.021395301,-0.0129804015,-0.003857524,-0.024785962,0.036126867,-0.025407996,0.01912938,0.027926635,0.04087703,0.029402625,0.015099235,0.019441444,0.008204872,-0.0023828566,0.0065049217,0.026477717,0.03438531,0.031367574,-0.039370622,-0.0042271484,-0.019008854,-0.059297875,0.008614554,-0.07911643,-0.034159925,-0.022082623,-0.09561576,0.01791173,-0.0034712693,0.028500577,-0.031300366,0.007858694,0.001450148,0.06720431,-0.079802506,0.00081366574,-0.006893055,-0.004039524,0.061777584,0.033039067,-7.211232e-05,-0.03324763,-0.0015160283,0.0010735127,0.0452468,0.053846188,0.017669162,0.0070929676,0.051403932,-0.011081842,-0.009496564,-0.033646613,-0.022794798,-0.024449896,-0.033429567,-0.033526566,0.0030479976,-0.017361509,0.049937386,0.011181189,0.030562427,0.011833301,0.018200034,-0.026343752,0.055000912,0.046190158,0.005200578,0.061584815,-0.04791333,0.016687619,0.0278638,0.05465983,0.005727088,0.057863504,0.018238092,0.0068139136,0.0061517237,-0.0032583731,0.015350567,-0.046642072,-0.00076040084,0.007842761,0.0018556983,0.027459001,-0.028202806,0.025752584,-0.006921489,0.023786267,-0.06556876,-0.10020494,-0.016230024,-0.02400597,-0.03006531,0.023887018,-0.0021330523,0.007081528,0.02810646,-0.01193421,0.055263262,-0.09902315,-0.028094208,-0.028263947,0.040074214,-0.0036156515,-0.00022114751,-0.017745042,-0.028270774,-0.034157608,-0.06357238,0.068415344,0.035528183,-0.038657635,-0.017237417,0.0016393423,0.032277163,0.028517066,-0.027584802,-0.025885485,0.01422449,-0.034006175,-0.026133338,0.06910601,-0.018519083,-0.008928691,0.027341228,-0.014202174,-0.06565557,-0.022641385,-0.0015818677,0.05293459,0.013184531,0.01797409,0.030141508,0.03755528,-0.075553946,-0.013846724,-0.012709616,-0.020108111,0.012321607,0.008138498,0.034327805,-0.010216978,-0.06965585,0.011728105,0.0013913682,-0.011282587,0.020747798,0.040191364,0.0010555227,0.092706956,0.057314094,-0.05329985,-0.087951906,-0.038040742,-0.044007227,-0.030920666,-0.0072496664,0.06078369,-0.034396015,-0.010205096,-0.0068568387,0.016180363,0.0044913143,0.0036859063,0.05594144,-0.022135194,-0.030083785,-0.022616751,-0.010583113,0.051545568,-0.048699807,0.02167974,0.054766342,0.044780254,-0.051750116,-0.014479331,-0.058530115,-0.039122153,0.00054317794,-0.015205173,0.018419305,0.0032808,-0.037162665,0.03232828,0.048169468,0.0274068,-0.026654568,-0.005018009,0.036887765,0.034503285,0.04126851,0.0055586514,-0.025762545,-0.117533356,0.0009611285,-0.032805964,-0.052878316,0.043213077,-0.006262208,0.009463381,-0.017644795,0.002578006,0.031812932,-0.013087599,-0.010109191,0.06431526,0.0030806393,0.018764894,-0.022375634,-0.06874503,-0.010573483,0.0039465977,-0.011861363,-0.031449027,0.05860315,0.008039258,-0.00027241427,-0.013032653,0.00014986258,-0.02945426,0.0076447716,-0.037505515,0.07513484,-0.0056038657,0.02249263,0.014178389,0.011629509,0.070961356,-0.0016283173,0.0034459461,0.016950585,0.024219012,-0.03755807,0.0075399945,0.0011897106,-0.051678494,-0.027015233,0.024072481,0.03215441,-0.0008579083,0.029693538,0.01524428,-0.049684394,-0.06697953,-0.055219457,-0.0129173845,0.0071293735,0.01733664,-0.0032884502,-0.04927656,0.020910358,-0.060055286,0.04409741,0.023245748,0.04565704,0.0071495106,-0.02875433]	Keywords: long sequence inputs, long-range dependencies, document-level machine translation, document summarization, hierarchical Transformers\nKey Objects: sentence representations, document representations, attention mechanism\nRefers to Images: None\nHypothetical Questions:\n- How do hierarchical Transformers handle the increased complexity of processing very long sequences?\n- What is the role of the attention mechanism in aggregating low-level information in hierarchical Transformers?\n- Can you describe a scenario where using hierarchical Transformers would be particularly beneficial compared to a standard Transformer?\n---\nSummary:\nHierarchical Transformers are employed to effectively model long-range dependencies in tasks involving inherently long input sequences, such as document-level machine translation and summarization.\nOriginal Text:\n6.5.2.1 Hierarchical for long sequence inputs. For tasks with inherently long input length, one can use hierarchical Transformers for effective modeling of long-range dependencies. For documentlevel machine translation tasks, Miculich et al. [92] introduce dependencies on the previous sentences from both the source and target sides when translating a sentence. They use an attention mechanism as the aggregation operation to summarize low-level information. For document summarization, HIBERT [166] encodes a document of text by first learn sentence representations for all sentences and then use these sentence representations to encode document-level representations that are then used to generate the summary. The model uses the last hidden representation (corresponding to the EOS token) as the representation for each sentence. Liu and Lapata [86] propose a similar hierarchical Transformer for multi-document summarization where the extracted low-level representations are aggregated using an attention layer with a\nContextualized Text:\nTo effectively model long-range dependencies in tasks involving long input sequences, such as document-level machine translation and summarization, hierarchical Transformers are used. These models first learn sentence representations for all sentences and then use these to encode document-level representations, often leveraging attention mechanisms to summarize low-level information.	{"tags": ["NLP", "Transformers", "architecture", "long-range dependencies"], "doc_id": "ae2eb4e7-7618-41f2-8465-29ef4bf7897d", "summary": "Hierarchical Transformers are employed to effectively model long-range dependencies in tasks involving inherently long input sequences, such as document-level machine translation and summarization.", "doc_type": "text", "entities": ["HIBERT", "Liu and Lapata"], "keywords": ["long sequence inputs", "long-range dependencies", "document-level machine translation", "document summarization", "hierarchical Transformers"], "key_objects": ["sentence representations", "document representations", "attention mechanism"], "contextual_text": "To effectively model long-range dependencies in tasks involving long input sequences, such as document-level machine translation and summarization, hierarchical Transformers are used. These models first learn sentence representations for all sentences and then use these to encode document-level representations, often leveraging attention mechanisms to summarize low-level information.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.4 Transformers with Divide-and-Conquer Strategies"}, "hypothetical_questions": ["How do hierarchical Transformers handle the increased complexity of processing very long sequences?", "What is the role of the attention mechanism in aggregating low-level information in hierarchical Transformers?", "Can you describe a scenario where using hierarchical Transformers would be particularly beneficial compared to a standard Transformer?"]}
2a9b98ad-83d4-483a-9881-9f9b74b08987	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.030870073,0.024781553,0.031019099,0.02969189,0.0028720102,0.055074677,0.0066899657,0.043320876,0.032782417,-0.02327115,-0.033105984,-0.02275022,0.009996138,0.008825676,-0.033329535,0.0013243736,0.019175477,0.09582131,-0.01872543,-0.013822873,0.05460466,0.013562412,0.0391168,-0.040657893,-0.026165156,0.03269483,0.023381654,-0.00018440503,-0.018628063,-0.008834022,0.00135549,-0.05119628,0.050233483,0.0051324926,0.06255811,-0.0065041427,0.02234953,-0.030394658,-0.018751098,0.0022957432,-0.022727322,0.041814104,-0.059100978,-0.018782182,0.015709609,-0.060435995,-0.107082434,-0.017277777,0.018120147,-0.03275127,0.01253707,-0.007867888,-0.04711639,-0.0020072383,0.018456766,-0.012228382,0.017044118,-0.04941205,0.009304294,-0.10537678,-0.04530597,0.0043521393,-0.0029330743,-0.02486991,0.054037865,-0.010862446,0.0033118983,-0.036524944,0.024276707,0.12916894,-0.021605823,0.024088094,0.009154105,-0.06424596,0.11483493,0.035691984,-0.035711844,-0.017200962,-0.025553282,-0.033062954,-0.02293664,0.08289136,-0.038577914,-0.027181836,0.085949466,-0.036658555,-0.045429025,0.008025918,0.0664414,-0.024799367,0.004989717,-0.038960498,-0.045781493,0.048080303,-0.043579604,-0.032383,0.010818041,-0.0017760231,0.016365645,-0.057612073,0.0028154152,-0.033947933,0.11684512,0.091085896,-0.00011088079,-0.04327349,-0.008318051,0.011470437,0.01033435,0.03522799,-0.010222067,0.05057888,-0.05159748,0.022264207,-0.03794956,0.031035373,-0.04703784,0.00063855236,0.0025968014,-0.0075593404,-0.015557883,-0.04139087,0.009279864,0.0033850824,0.07897245,0.07988833,0.027192976,-0.058432266,-0.010324178,0.0155113535,0.046315096,0.005860623,0.020632595,0.027156604,-0.0058566052,-0.0074975346,0.09872595,0.030864615,0.004223558,-0.00082516694,-0.009305352,-0.05282748,-0.03377641,0.05669601,0.016167585,0.012430932,-0.07334532,0.024076823,-0.014336621,0.04864039,0.015088456,-0.014731073,0.030900039,0.023696963,-0.02973628,0.017786536,-0.0172319,-0.05879683,-0.023428766,-0.038589474,-0.06921114,-0.04527431,0.00013735941,0.012342979,0.014901168,0.08295372,-0.018018996,-0.010837472,0.025113164,0.0402313,-0.03668173,-0.010662332,0.002702652,-0.03914241,-0.033578746,0.033586856,0.0052979235,-0.03812948,-0.01309306,0.011973735,0.026164567,0.033594515,-0.01545914,0.011645989,-0.08221916,0.028945258,0.017900389,0.035569746,-0.041365374,-0.018165566,0.031074338,0.007897419,0.016057989,0.022906603,0.037039794,0.023436442,0.054876707,-0.07970689,-0.024254892,-0.03993786,0.0018072563,-0.015589406,0.013274982,-0.013823502,0.008912135,-0.020463813,-3.471716e-05,-0.03229821,-0.018294878,0.009751967,-0.025780853,0.0102692675,-0.010424843,0.0003035119,0.058043353,0.04672618,-0.029481234,-0.0013075314,0.025736226,-0.060665086,-0.053880874,-0.0027319468,-0.02417375,0.025970517,-0.014654429,-0.0032072393,-0.018885607,0.014401673,0.018310023,-0.026830448,0.05674475,0.03822902,-0.031890947,0.050099224,-0.021046717,0.043742646,-0.004678123,-0.0068050288,-0.016289184,0.015954982,0.03906367,0.038400486,-0.012877434,0.069190934,0.03365831,-0.024000853,0.064511314,0.015297471,-0.011786653,-0.010299361,-0.013416657,0.06502119,0.05644581,0.014888303,-0.03078305,-0.06587156,-0.028642375,0.024956293,0.042179205,0.025180338,0.007956955,0.004554142,0.022655407,-0.001355973,-5.6462413e-05,0.004810587,0.036381435,0.042474758,-0.0378647,0.017648013,0.027635287,0.029437922,0.010410038,-0.014474484,-0.018702589,0.019355394,0.022409521,-0.010535667,0.0492724,-0.014320945,0.0036111753,-0.0057771076,0.04728262,0.013487515,0.03732005,-0.025150746,0.013567377,-0.006580859,-0.033371344,-0.026183084,0.0053587905,-0.020099394,0.016715048,-0.014954095,0.0061600003,-0.012645003,-0.009982274,-0.023404676,-0.050622962,0.0039202645,0.06706513,-0.061099645,0.015271462,0.02807582,0.043450594,0.013082442,-0.06451207,0.036087792,0.040477507,0.0056335065,0.05431424,0.0028450934,-0.029433927,0.018772349,0.034543585,0.0669652,0.0608931,0.0016764362,0.007833466,0.009735897,0.0005940001,-0.02478124,-0.029701313,0.00532436,0.03695904,0.0058340207,0.031970583,-0.043876637,-0.062320422,0.064555086,-0.038974468,0.010908017,0.05838519,-0.019903323,-0.044711236,0.010536909,0.0235765,0.043653913,0.013613698,0.035301693,-0.014638827,-0.06840127,-0.0112213865,-0.012319241,-0.0073701264,0.049175374,-0.04787228,-0.0086047,0.00835718,-0.039835833,-0.0020096586,0.03654012,0.025136456,0.012163435,0.013854151,-0.03766137,-0.03822168,0.009842317,-0.007924493,0.002256743,0.049040932,-0.072616495,0.04456213,-0.01397285,0.027687974,0.014406006,-0.06696239,0.009177318,0.052878104,0.007813759,0.00015231341,-0.029151227,0.06941881,-0.04202449,-0.018498397,0.0034746565,0.09517712,0.017741416,-0.045765422,-0.010704488,-0.019409584,0.012264645,-0.048700027,-0.04554077,0.024136202,0.00043474798,0.056202993,0.04845113,0.027619055,0.07226251,-0.03894991,0.021389915,0.028030768,-0.0006986725,-0.024170527,0.014851546,-0.009156499,0.014158474,0.004238777,-0.0010650993,0.048858874,0.015602802,-0.036249902,0.011075898,0.04732257,0.030638745,-0.06737928,0.03741408,0.034372743,0.03599104,0.045936137,-0.0077910735,-0.037460003,0.01168423,0.024068503,-0.023033233,-0.043660305,-3.01613e-05,-0.033194326,-0.0141824465,-0.08326301,0.022542724,0.03432387,0.046550196,0.04635737,-0.011493566,-0.0542298,0.042529967,0.0068839537,-0.037352104,-0.01874103,-0.0778753,-0.004000809,0.066142455,0.039224654,0.00334645,-0.07662759,0.061585125,-0.049935445,0.03962323,0.059145205,-0.03601962,0.003840033,-0.010964696,-0.016149841,-0.06403089,0.053749375,0.04362789,-0.03821975,-0.030637959,-0.020238755,0.0014643068,0.06526256,-0.02096015,0.058609597,-0.024978522,0.009064225,-0.018252408,-0.061745383,0.007903087,0.00972063,0.051664613,0.015030217,-0.012804943,-0.008444317,0.02519518,-0.07345155,-0.028674634,-0.022831263,0.0049598506,0.0050253673,0.06785995,-0.006233535,0.05243006,-0.014129132,0.012396093,-0.01003075,0.07879271,0.062986754,0.016147166,-0.037763547,-0.01972662,-0.01454735,0.027348889,0.017346546,0.011483442,-0.03884149,-0.038179003,-0.014018287,0.017121013,-0.011920368,0.058851324,0.022151656,0.04605947,0.03279622,-0.039432555,-0.015182278,0.022046084,0.00073737977,-0.021396363,-0.020282382,0.0265415,0.05458516,0.03818697,-0.012989659,-0.0010675727,0.009981754,-0.051820744,-0.010437922,0.03688575,-0.018127223,-0.027482975,-0.0008009038,-0.03579697,-0.025870683,0.0430225,-0.0026370767,0.017405942,0.024023052,0.08610748,-0.018775592,-0.014148252,0.03423164,-0.00021647816,-0.0014644852,0.0065692216,0.05413028,0.03874168,0.072940215,-0.0336485,0.039891984,-0.039383966,-0.058840837,-0.00164945,-0.08351776,-0.037608754,0.02342399,-0.08200223,-0.009466946,-0.00075233885,-0.029474745,-0.00387146,0.00590968,-0.030078273,0.055237845,-0.039834615,0.006331371,-0.0019488462,0.01623094,0.054198593,0.006811603,-0.0189309,-0.011684357,-0.05215735,-0.0074474714,0.023852512,0.038947545,0.033140484,-0.030703135,0.035427842,0.005710176,0.03855162,-0.042365637,-0.036344822,-0.017576933,0.0015124396,-0.01895967,0.003251854,-0.019594405,0.031310663,-0.0042497376,0.001723957,0.013280416,-0.0028294877,-0.012782692,0.0459762,0.013632561,0.0063877683,0.038190752,-0.049580228,0.040246785,0.04883892,0.048089717,0.0023579905,0.020935426,0.0054650404,-0.014437996,-0.0020707154,0.0134300105,-0.0129215615,-0.025512356,-0.012282975,0.029340042,0.0025503987,0.019779334,-0.0029649218,0.0029232746,0.003940627,-0.004050857,-0.071575284,-0.069449775,-0.06068659,-0.030999245,-0.024842037,-0.0056598037,0.030392902,-0.0036648654,0.0045103403,-0.013407894,0.079692245,-0.08318753,-0.013019352,-0.038402967,0.04457594,-0.025032554,-0.026646528,-0.004424088,-0.0010637634,-0.016519608,-0.033299923,0.061124027,0.0018497729,-0.012654245,-0.011778666,-0.012543871,-0.0065386454,-0.0065674107,-0.02956278,-0.014399473,0.03254899,-0.03328389,-0.016249688,0.09886179,-0.023700677,-0.0066315993,0.016679592,-0.027528735,-0.12260789,-0.047200084,-0.011192274,0.011676159,0.01730611,-0.0036872334,0.013762055,0.009504456,-0.06382166,-0.0022386997,-0.030343521,-0.01386929,-0.013279727,0.02652199,0.06394141,0.015982455,-0.044751868,0.029841788,-0.044244044,-0.025880009,0.012774793,0.04495829,0.0026713083,0.078930296,0.03664341,-0.023296919,-0.109167926,-0.03592332,-0.053457975,-0.033753365,-0.0045946646,0.03693394,-0.025062673,0.007938321,-0.021786429,-0.0017502762,0.0068016765,-0.0031190033,0.040416565,-0.014520598,-0.0144197745,-0.05482292,0.013968639,0.056207903,-0.021017844,-0.005929739,0.089144334,0.029974237,-0.043423537,-0.02944596,-0.017190734,-0.04577658,0.039660115,-0.020564487,0.016832301,0.010917647,-0.021679617,-0.022117374,0.04631258,0.013975014,-0.02614997,0.011675243,0.019791188,0.008551645,0.04603422,0.029385932,-0.011817155,-0.07004557,0.0057126377,-0.0026160574,-0.045653895,0.05402669,-0.030203419,-0.03111913,0.0025465363,-0.0112430565,0.0022136923,0.014667445,0.006458647,0.044200968,0.013913208,0.0029704124,-0.023589121,-0.07794221,-0.012847244,0.019537749,0.031277295,-0.036205966,0.041425217,0.022703784,0.016420718,-0.010278189,-0.02229024,0.00021496293,-0.0112808375,-0.04492277,0.062134463,-0.025079282,0.03476729,0.022432862,0.02780658,0.033060905,-0.010273443,0.016592959,0.001846478,0.042595673,-0.019420246,0.007855248,-0.0029001976,-0.06695883,-0.04557288,-0.0027331896,0.056282062,0.011536328,0.021370538,-0.016268263,-0.028447019,-0.056483805,-0.0464877,-0.026029704,0.007894928,-0.008958025,0.032744076,-0.06268348,0.015024556,-0.07523131,0.0027298653,0.027036235,0.007957548,-0.00436821,0.00018971527]	Keywords: hierarchical transformers, multi-document summarization, sentence transformer, document transformer, attention layer\nKey Objects: attention layer, sentence representations, document representations\nRefers to Images: None\nHypothetical Questions:\n- How do Liu and Lapata's approach and Hi-Transformer's approach to hierarchical summarization differ?\n- What are the advantages of using sentence and document Transformers in Hi-Transformer?\n- How does the global trainable query node in Liu and Lapata's model contribute to the summarization process?\n---\nSummary:\nLiu and Lapata [86] and Hi-Transformer [145] both utilize hierarchical Transformers to improve summarization, with Liu and Lapata using an attention layer and Hi-Transformer employing sentence and document Transformers to learn context-aware representations.\nOriginal Text:\nsentence. Liu and Lapata [86] propose a similar hierarchical Transformer for multi-document summarization where the extracted low-level representations are aggregated using an attention layer with a global trainable query node and low-level representations as the source of key-value pairs. Hi-Transformer [145] first utilizes a sentence Transformer and a document Transformer to hierarchically learn document context-aware sentence representations. The document context-aware sentence representations are then fed to another sentence Transformer to further improve the sentence context modeling.\nContextualized Text:\nFor multi-document summarization, Liu and Lapata [86] designed a hierarchical Transformer that aggregates extracted low-level representations using an attention layer with a global trainable query node. Similarly, Hi-Transformer [145] utilizes a combination of sentence and document Transformers to hierarchically learn document context-aware sentence representations and further refine sentence context modeling.	{"tags": ["NLP", "summarization", "architecture", "transformers"], "doc_id": "2a9b98ad-83d4-483a-9881-9f9b74b08987", "summary": "Liu and Lapata [86] and Hi-Transformer [145] both utilize hierarchical Transformers to improve summarization, with Liu and Lapata using an attention layer and Hi-Transformer employing sentence and document Transformers to learn context-aware representations.", "doc_type": "text", "entities": ["Liu and Lapata", "Hi-Transformer"], "keywords": ["hierarchical transformers", "multi-document summarization", "sentence transformer", "document transformer", "attention layer"], "key_objects": ["attention layer", "sentence representations", "document representations"], "contextual_text": "For multi-document summarization, Liu and Lapata [86] designed a hierarchical Transformer that aggregates extracted low-level representations using an attention layer with a global trainable query node. Similarly, Hi-Transformer [145] utilizes a combination of sentence and document Transformers to hierarchically learn document context-aware sentence representations and further refine sentence context modeling.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.4 Transformers with Divide-and-Conquer Strategies"}, "hypothetical_questions": ["How do Liu and Lapata's approach and Hi-Transformer's approach to hierarchical summarization differ?", "What are the advantages of using sentence and document Transformers in Hi-Transformer?", "How does the global trainable query node in Liu and Lapata's model contribute to the summarization process?"]}
140648cd-2507-4b00-9b29-ab5c691b6ec7	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017603042,0.01929691,0.0117680635,0.029765572,-0.02485337,0.032878518,0.0013903101,0.08625763,0.03012917,-0.012430527,0.024786321,-0.008748585,-0.007565069,-0.050334126,-0.03614144,0.012090961,0.033393696,0.098777935,0.012246588,-0.003460665,0.03569569,0.015112716,0.012492403,-0.033834502,0.002925558,0.03288111,0.040810578,-0.028352307,-0.007774688,0.008580244,0.0023078488,-0.008579698,0.0586171,-0.017074544,0.048614774,0.0022775207,0.0023135387,-0.033024292,-0.003011239,-0.008460917,-0.03116324,0.02295378,-0.079545334,-0.005434838,0.020108132,-0.0675577,-0.12346742,-0.0072000916,0.0051228506,-0.009797895,0.017547015,0.022863789,-0.009997349,0.038507473,-0.015756087,-0.0143147195,-0.01905412,-0.050449006,-0.0029218714,-0.10000627,-0.046785947,0.036553796,0.0040983674,0.028111579,0.03191886,-0.035228454,0.015617915,-0.006795646,0.043697823,0.13532645,-0.011700183,-0.031361863,0.0071594287,-0.07291485,0.11948325,0.058968224,-0.04137079,-0.009762388,-0.05840076,-0.01662792,0.009881377,0.07453161,-0.022961799,-0.07255814,0.07578385,0.012506698,-0.04673172,0.021405157,0.059318732,-0.03980182,0.005066578,-0.0022361183,0.0036558085,0.08043776,-0.056872632,-0.05207034,-0.011678218,-0.027532173,0.051168427,-0.020502036,0.0077715013,-0.0519532,0.10892362,0.10596616,0.015808718,-0.002173411,-0.04149142,0.0060165077,0.006032156,0.050378375,0.010295473,0.032862894,-0.049041856,0.036913846,-0.03966133,0.009691048,-0.059245717,0.0019275402,0.032608885,0.013977828,0.0021394915,-0.016333094,0.0034459871,0.0057643466,0.04591631,0.06519371,0.013039966,-0.052881528,-0.0061991448,0.010376848,0.048570827,-0.02722945,-0.018856144,0.015750779,0.004805397,0.0315975,0.074672654,0.0039972994,-0.008321187,-0.016859535,0.0067176665,-0.05402467,-0.019429147,0.06482242,-0.01632744,0.04580288,-0.045619145,0.022274446,0.00954613,0.05107424,0.03585088,-0.017798642,0.04441844,-0.0002986362,-0.034135643,-0.0034353242,-0.0060413545,-0.014044907,0.0006910126,-0.009916578,-0.034297727,-0.06261101,-0.014733748,0.08734398,0.035041045,0.10075501,-0.015430949,0.010331427,0.04477005,-0.010317705,-0.050335065,0.021253021,0.014793709,-0.04954853,-0.013285162,0.045870714,0.005508528,-0.054942194,0.0022374694,-0.0011726192,0.005999965,0.024589622,0.008641234,0.014151663,-0.05262425,0.03633976,-0.021518696,0.02789285,-0.020848574,-0.04076524,0.011831116,-0.0016726054,-0.015062157,0.026847873,0.019166483,0.06103159,0.038469825,-0.038613606,-0.04240821,-0.015917605,0.025464114,0.011563093,0.021127064,-0.031892683,-0.0077749714,0.00019196523,0.01657561,-0.027336316,-0.02044258,0.021732416,0.009519532,-0.0020517046,-0.0025342035,0.04211421,0.046925783,0.0018348928,-0.044456895,0.016934296,0.0502418,-0.035245817,-0.040993154,-0.007802887,-0.019013403,0.027100392,-0.043842927,0.009608266,-0.017768214,0.034229916,0.00507291,-0.0013443987,0.064744405,0.037609674,-0.020334091,0.04504613,-0.053142153,0.05607408,0.026842019,-0.005958605,-0.008751865,0.038954508,0.066951685,0.06788829,-0.017684573,0.0715354,0.013296513,-0.010898662,0.04511621,0.03013733,-0.014846865,0.047618397,-0.009209127,0.07221678,0.0656608,-0.010295242,-0.0077906963,-0.042162385,-0.007385825,0.004060308,0.04077705,0.010904631,0.023316134,0.01896236,0.024023881,0.039583378,-0.00017689461,0.009917745,0.0056534,-0.010413909,-0.008182858,-0.011304227,0.030305136,0.027799243,0.00820391,-0.030781444,-0.060122155,0.012694383,0.025475059,-0.0002512531,0.043632966,-0.00995568,-0.021214476,-0.001169135,-0.0056263837,-0.0009778666,0.03925486,-0.010524918,0.028310703,0.0010778519,-0.043750975,0.002434239,-0.01593797,-0.031595755,0.02979726,-0.030706648,-0.010303868,0.009681487,-0.013095635,-0.025838375,-0.05637558,0.016737066,0.08801457,-0.04194987,-0.0013863328,0.015721675,0.034006797,-0.011501651,-0.05830096,0.006865787,0.020346139,0.015297825,0.017560665,0.009456026,-0.025754213,0.0037713244,0.049857397,0.06884238,0.07468822,-0.020313678,0.0023906352,0.011311529,0.011562671,-0.014286542,0.012711707,-0.0024974276,0.023018071,-0.012410318,0.04270629,-0.025258765,-0.09229484,0.06148182,-0.06057443,0.017310819,0.047469396,-0.0011641772,-0.04458155,0.048803702,0.01644766,0.05692381,0.016608953,0.0031148382,-0.015051596,-0.032058455,0.010671328,-0.003787776,-0.016611315,0.06556141,0.01375761,-0.020062648,0.018932058,-0.004160843,0.0059967237,0.03834322,0.015509695,0.029099882,0.019887175,-0.04272575,-0.011640562,0.024020158,0.008062338,-0.013455529,0.02946643,-0.07999356,0.059748802,-0.006184409,0.035377443,-0.023489933,-0.07222673,-0.007083694,0.043645024,0.010302782,-0.018598968,0.004644531,0.034067176,-0.018846594,-0.020462524,0.009215404,0.07024893,0.038843952,-0.05765175,-0.044028804,-0.050529025,-0.0035870385,-0.047460314,-0.038243603,0.00057100476,0.0021345257,-0.006133703,0.06924451,-0.017236896,0.03131452,-0.047412682,-0.003854187,0.018721202,0.033421386,0.009464983,0.027160008,0.011737683,0.013179951,-0.00215556,-0.019052433,0.029310215,-0.01626143,-0.063533805,0.048515946,0.052028213,0.01663,-0.029753283,0.030583778,0.057886165,0.01624953,0.063832015,-0.010739991,-0.012110662,0.023768108,0.012864903,-0.0075533646,-0.021139434,0.041244857,-0.034804814,-0.017582327,-0.047438778,0.032992348,0.025980586,0.05779722,0.05329383,-0.013163128,-0.057327025,0.018495439,0.020239439,-0.075907394,-0.0064351857,-0.064743064,0.01660417,0.068569295,0.011175935,-0.0048319544,-0.04670855,0.07932442,-0.064443335,0.07768654,0.070107214,-0.03351636,-0.013861035,0.0011223449,-0.023379661,-0.030809136,0.032023214,0.0008386555,0.0045520677,-0.029180942,0.014033586,0.021659836,0.02335911,-0.025027113,0.045024157,-0.014654366,0.015017072,-0.015081003,-0.06984233,-0.011426301,0.009754468,0.029729111,-0.015113672,0.0034138456,-0.0018939751,-0.036942404,-0.02825566,-0.023098627,-0.024829678,-0.019313464,4.0107963e-07,0.014841798,-0.032296363,0.05167567,0.001878653,-0.00022394829,0.0020253002,0.04462268,0.027606895,0.03184422,-0.030722627,-0.051277183,-0.00020734906,0.0137678245,-0.026660055,-0.0023699857,-0.041481756,-0.02082585,-0.051437795,0.026294312,-0.0075064623,0.05306026,0.010140037,0.019467028,0.018325726,-0.024402115,0.0005330594,0.041504443,0.029269312,-0.009285085,-0.025973624,-0.022320457,0.049676444,0.04781588,-0.026532102,0.01999723,0.0017429929,0.0020607032,-0.031262502,0.010011095,-0.008109019,-0.007159668,0.019646266,-0.0009406857,-3.5110676e-05,0.01593205,-0.016081167,-0.0038589837,0.030584365,0.044188354,-0.0016942627,-0.010910039,0.008982717,0.02537869,0.0072444123,0.01340412,-0.002897466,0.04302029,0.07471721,-0.055998866,0.038774904,-0.04870008,-0.032870933,0.001377429,-0.09413378,-0.016235417,-0.028530875,-0.07663873,0.0066035767,0.032666005,-0.019308379,-0.03650703,0.01417028,-0.016222646,0.07804527,-0.07752137,0.020121755,0.00621451,0.008043875,0.081415735,-0.0022149142,0.016935451,-0.02226723,-0.02508396,-0.019762028,0.029762693,0.06881716,0.030666962,-0.0075987712,0.05888258,0.00142922,0.0034696285,-0.013529857,-0.0457465,-0.055770658,-0.013781216,0.014918129,-0.007605099,-0.021551028,0.058671575,0.0030112837,0.009607174,0.015127389,0.0048475256,-0.01571059,0.015505075,0.014561629,0.025136014,0.02662196,-0.04179257,0.011005607,0.04004371,0.036871236,-0.020765772,-0.010906565,0.045615047,0.0058221067,0.016927622,-0.0028841577,0.0044322894,-0.025262699,-0.050177857,0.02836887,0.04072504,-0.0079810815,-0.007480085,0.032886185,0.006577777,0.0050381366,-0.057117686,-0.08736135,-0.019687437,-0.015630657,-0.05128754,0.028688433,-0.0074940575,0.006433538,0.05184733,-0.024277773,0.03831132,-0.07683196,-0.0014275974,-0.06614465,0.036096156,0.007432105,-0.024277352,-0.0023045954,-0.025300063,-0.013178724,-0.039514307,0.051218923,0.029619306,-0.036162898,0.0042992495,-0.023736928,0.017636305,-0.03003639,-0.026245397,-0.023880001,-0.00078943005,-0.0019334733,-0.019334456,0.088621415,-0.056248605,-0.011798606,0.020959746,-0.032880515,-0.10204371,-0.013713035,0.06989968,0.061628506,0.034943745,-0.0015906637,0.050944902,0.02504053,-0.08404121,-0.019668719,-0.037717838,2.405465e-05,0.0063490747,0.010200231,0.02565629,0.020680685,-0.052278474,0.0069676554,-0.0069712484,-0.04770476,0.052821,0.036896553,0.0019116205,0.075089954,0.042536452,-0.025503602,-0.08920007,-0.008303771,-0.042941466,-0.01090029,-0.032129556,0.046378706,-0.015384124,-0.03046363,-0.013800835,0.05290709,0.010514674,0.023840731,0.035726476,-0.016945284,-0.012544582,-0.029477745,-0.00484612,0.034590267,-0.037596103,0.010633471,0.063875206,0.04147883,-0.020027686,0.0064501367,-0.022353113,-0.005415114,0.045481697,-0.0027357896,0.035811175,-0.016725544,-0.017228266,0.02457179,0.05238605,0.048407715,-0.017119728,-0.01445823,0.041009653,0.017471828,0.016429685,0.01659345,-0.0683665,-0.07558603,0.029850801,-0.01403186,-0.028063275,0.05856807,0.023341004,-0.015857972,-0.0093870405,-0.0153657235,0.0020689815,-0.0029761284,-0.006364492,-0.0057880864,0.013394829,0.009776884,-0.020549191,-0.08778013,-0.020102374,-0.0366461,0.017471716,-0.009336492,0.025307251,-0.016964473,-0.007851875,-0.010047686,-0.015646355,0.011938445,-0.0015230118,-0.007091558,0.081370145,-0.018585272,0.03811245,0.0019865434,0.027843062,0.06251421,0.023236679,0.0073626414,-0.017306989,0.019903595,-6.441582e-06,-0.019193662,-0.0356401,-0.032710608,-0.025520977,-0.010500743,0.038177457,0.02300087,0.023473352,-0.014606462,-0.044808105,-0.08633318,-0.06722808,-0.02999316,0.061965384,0.024160936,0.008805821,-0.05134439,0.032551702,-0.07033395,0.04389966,0.020667665,0.004805774,0.017528217,-0.0144157335]	Keywords: hierarchical transformers, richer representations, character features, word embeddings, Vision Transformer, pixel representations\nKey Objects: character features, word embeddings, pixel representations, hierarchical models\nRefers to Images: None\nHypothetical Questions:\n- How do richer representations generated by hierarchical models lead to improved task performance?\n- What are the potential drawbacks of using hierarchical models for representation learning?\n- Can you describe a specific scenario where incorporating character features would be particularly beneficial?\n---\nSummary:\nHierarchical Transformers can also be utilized to create richer representations for improved task performance, such as incorporating character features alongside word embeddings using models like TENER or addressing pixel-level information loss in Vision Transformers using Transformer in Transformer (TNT).\nOriginal Text:\n6.5.2.2 Hierarchical for richer representations. One might also be interested in using hierarchical models to acquire richer representations that are beneficial to the tasks at hand. For example, TENER [154] uses a low-level Transformer encoder to encode character features, which is then concatenated with word embeddings as the inputs to the high-level Transformer encoder. This incorporates more features and alleviates the problems of data sparsity and out-of-vocabulary (OV). Recently emerging Vision Transformer [33] divides an input image into several patches that serve as the basic input elements of Transformer, which potentially loses intrinsic pixellevel information within patches. To address this issue, Transformer in Transformer (TNT) [48] uses at each layer an inner Transformer block that transforms pixel representations and an outer Transformer block that takes fused vectors of patch representations and pixel representations as input.\nContextualized Text:\nTo enhance task performance, hierarchical Transformers can be employed to acquire richer representations. For example, TENER [154] uses a low-level Transformer encoder to encode character features, which are then concatenated with word embeddings to feed into a high-level Transformer encoder. This approach incorporates more data and helps solve issues like data sparsity and out-of-vocabulary words, a common challenge in many NLP tasks.	{"tags": ["architecture", "NLP", "vision", "representation learning"], "doc_id": "140648cd-2507-4b00-9b29-ab5c691b6ec7", "summary": "Hierarchical Transformers can also be utilized to create richer representations for improved task performance, such as incorporating character features alongside word embeddings using models like TENER or addressing pixel-level information loss in Vision Transformers using Transformer in Transformer (TNT).", "doc_type": "text", "entities": ["TENER", "Vision Transformer", "TNT"], "keywords": ["hierarchical transformers", "richer representations", "character features", "word embeddings", "Vision Transformer", "pixel representations"], "key_objects": ["character features", "word embeddings", "pixel representations", "hierarchical models"], "contextual_text": "To enhance task performance, hierarchical Transformers can be employed to acquire richer representations. For example, TENER [154] uses a low-level Transformer encoder to encode character features, which are then concatenated with word embeddings to feed into a high-level Transformer encoder. This approach incorporates more data and helps solve issues like data sparsity and out-of-vocabulary words, a common challenge in many NLP tasks.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.4 Transformers with Divide-and-Conquer Strategies"}, "hypothetical_questions": ["How do richer representations generated by hierarchical models lead to improved task performance?", "What are the potential drawbacks of using hierarchical models for representation learning?", "Can you describe a specific scenario where incorporating character features would be particularly beneficial?"]}
050554e4-b52b-4d92-9375-8a5962d472cb	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0066187857,0.02942685,0.0029806972,0.026099065,0.018665304,0.051537544,0.009725953,0.014498977,0.02587408,-0.028471798,-0.020429818,0.07163545,0.0030564372,-0.0023131452,-0.028087448,0.014581115,0.029428886,0.07056905,0.03665547,-0.06355038,0.04443389,0.010077164,0.032394312,-0.047995593,-0.0065170457,0.012132018,0.05592996,-0.0025971627,-0.009222976,0.010175045,-0.0043047103,-0.042151395,0.06918168,0.027111016,0.023110276,0.00150917,0.0018026811,-0.028469056,0.0023955614,0.007448108,-0.007934647,0.043017972,-0.1065482,-0.00229844,-0.044797678,-0.047902126,-0.06964599,-0.0351217,0.0016080202,-0.017189052,-0.03268932,0.03065595,-0.033544384,-0.016771983,-0.0058282856,-0.020914214,-0.042964336,-0.003641897,-0.019252965,-0.062446825,-0.010070081,-0.015648475,-0.013450313,0.000117097225,0.021483548,-0.002061611,0.025211118,0.03339131,0.04919114,0.1275625,-0.01798563,0.030055905,0.008165804,-0.028066644,0.12452304,0.03645151,-0.07571209,-0.03542421,-0.0599305,-0.004345052,-0.023087958,0.066261806,-0.06314909,-0.017043535,0.073438086,0.02730362,-0.051238947,-0.015042981,0.07889759,-0.019318894,0.008344297,0.008491257,-0.0040205442,0.049893092,-0.02979586,-0.07825712,-0.0009946972,-0.053486444,0.029137023,-0.054184143,0.02511427,-0.027657883,0.07469792,0.089907415,0.0373498,-0.0361473,-0.0149919195,-0.07579667,0.032877523,0.013041694,-0.014722032,0.01028214,-0.012109506,0.020919634,0.0011724344,0.0097146155,-0.032501187,0.026313724,0.0062721847,-0.021768237,0.028313436,-0.038169682,-0.022115607,-0.009353031,0.056244012,0.05420916,0.020523015,-0.0017025659,-0.010340099,-0.00793259,-0.00078900665,0.0148178255,0.042501416,0.01284611,-0.030484617,-0.019567026,0.09385499,0.0056746243,0.019569026,0.0009001627,-0.0032270087,-0.052877497,-0.044709057,0.03846933,-0.0121576125,0.02542197,-0.042586453,0.006289733,-0.007031051,0.045196187,0.020832093,-0.017393118,0.050647844,0.021331552,-0.041047405,-0.0026668974,0.0033693754,-0.03504933,-0.0094076935,-0.05744849,-0.02823979,-0.06630942,-0.043921553,0.018536126,0.008781748,0.07775754,-0.015130487,0.014208386,0.029580755,0.023842858,-0.038683772,0.008860059,0.00017230771,-0.0010963706,0.012465689,0.00093968457,-0.006107688,0.02255463,0.0035257824,-0.025058994,-0.0034908645,0.02915364,0.0015052449,0.034480076,0.0040346645,0.030539172,-0.016934184,0.04051532,-0.026982935,0.0037656804,0.050520763,-0.045319684,-0.015918693,0.006589161,0.067763954,0.059490707,0.03718093,-0.067859374,-0.025021918,-0.048260733,0.01863863,-0.01883479,0.0017818026,-0.018093705,0.0016623358,-0.032520454,0.006814463,-0.026686234,-0.01869721,0.05300876,-0.02329887,-0.017309355,-0.012500833,0.039921235,0.06641866,-0.014262822,0.033518072,-0.0014811239,0.039706666,-0.018126434,-0.054138582,0.03626525,-0.011732836,-0.0018825411,-0.03551052,0.0037404539,-0.062975034,0.003383601,0.034083117,-0.04721076,0.04236535,0.04340968,-0.022507785,0.055790313,0.0039517805,0.018795967,0.012230159,0.030021107,0.019648736,0.07274821,0.07603643,0.023902958,-0.03370613,0.079787135,0.010906581,-0.0044331737,0.019240467,0.07339967,0.0046743704,0.0074300463,0.0068838685,0.0447197,0.05245425,-0.0025597424,-0.008227161,-0.05664702,0.029561108,0.022739567,0.045818552,-0.028178133,-0.009768022,-0.00017817636,-0.0027669033,0.02438337,-0.009688897,0.0043831966,0.028011862,0.012884736,-0.021084221,-0.0020244478,0.029498495,0.020852165,0.027056571,-0.051030993,-0.020702828,0.047602326,0.006875935,0.029185113,0.06930484,-0.016603166,-0.01444761,-0.041878317,0.01248257,-0.009144048,0.0059967963,-0.020329485,0.011456189,-0.025162626,-0.08450512,-0.02787413,0.014501513,-0.049696565,0.0479512,-0.015022201,-0.008391636,0.019502696,-0.004854325,-0.022831345,-0.038974166,0.02635246,0.08373495,-0.014479803,-0.044422507,-0.0025600088,0.0006386014,0.0029535878,-0.039884623,-0.0030594368,0.043105405,-0.005080412,0.02599001,-0.015569233,-0.03088626,-0.016432874,0.026446003,0.09166926,0.082898706,-0.073256485,-0.027508406,0.033165347,0.010658967,-0.01686867,-0.060753264,0.033385865,0.03027199,0.008592971,0.0037176898,-0.043875538,-0.08107439,0.05708739,-0.08291183,0.0376347,0.049496684,0.018872181,0.014676355,0.0059874305,-0.03461705,0.036626063,0.025058368,-0.017772691,0.0029877878,-0.060178548,-0.03628081,0.034679983,0.011599797,0.028426329,-0.011113419,0.045110676,0.015943581,0.00043285242,0.020775978,0.054439466,0.008408765,0.009432292,0.0010655569,-0.08479604,0.0032813253,0.01902019,0.00955621,0.006964765,0.059601586,-0.044520136,0.038124297,-0.011479812,0.01078173,-0.0017644744,-0.062306706,0.042032894,0.027403317,-0.019279284,-0.027154583,-0.018789709,0.08128426,-0.01968752,-0.016004538,-0.001408579,0.02910251,0.043357726,-0.028425142,-0.023951927,-0.023682687,-0.017136555,-0.047414154,-0.004716899,-0.01153436,-0.0022967586,0.0019568398,0.054745745,-0.016515834,0.043554023,-0.05281333,-0.032482326,0.042323288,0.027264273,-0.05345853,0.030795943,0.025765112,0.020688357,-0.009959262,-0.018133238,0.015309768,0.0019077687,-0.051302,0.043992452,0.01908575,-0.021425586,-0.05733098,-0.00034016167,0.0503489,0.047448862,0.02874417,-0.030773712,-0.050095227,0.020414712,-0.0007663798,-0.0028117932,-0.00035176307,0.06615851,-0.032192815,-0.0023684571,-0.03985547,0.025169343,-0.0012005015,0.020837456,0.03776537,0.02138951,0.0005464171,0.03686454,-0.03548491,0.006031307,0.039579585,-0.056946184,-0.038504653,0.06756091,0.021960562,-0.009210014,-0.0893032,0.065281324,-0.06080127,0.029722145,0.056970675,-0.019699756,0.048360225,-0.016136065,0.024752727,-0.0472662,0.00029298154,0.029503092,-0.015876347,-0.008095586,0.036556106,0.0009635552,0.044681534,0.0046739657,0.019732043,0.016785739,0.0349963,-0.018422967,-0.043523207,-0.011286423,0.001922931,0.012761938,-0.0066443537,0.0012792703,0.018213166,-0.021625338,-0.009791565,-0.03684799,-0.030784497,0.01166606,0.027605604,0.025829226,-0.029270226,0.024058836,0.015578584,0.00046688918,-0.00070437574,0.053192817,0.05161475,0.012777593,-0.09165283,-0.028375506,0.009405973,0.02912598,-0.0565385,0.014046004,-0.04193965,-0.04172952,-0.05030605,0.047163624,0.015282085,0.084350534,0.040772222,0.03454948,0.06702642,-0.06198691,-0.036036864,0.014973164,0.022083294,-0.016942127,-0.028612124,-0.004150525,0.06876961,0.018077824,-0.043259762,-0.020469368,0.00804327,0.02045789,-0.004984143,0.0672918,0.05472123,-0.028075881,-0.03853955,-0.02054867,-0.0052326224,0.002027004,0.01860583,-0.061554562,0.03757289,0.033155765,0.0026462777,0.054095607,0.04388047,-0.015318776,-0.03591803,0.019460848,0.036524605,-0.0004431447,0.045523655,-0.067144796,0.06823045,-0.031134352,0.010259319,-0.0012483507,-0.039369207,-0.06290455,0.069640845,-0.019331498,-0.01684303,0.049719434,0.020866646,-0.039084814,-0.024316678,-0.027307265,0.06553789,-0.0649984,0.006505715,-0.021877771,0.0060396814,0.059306487,-0.004203878,0.01876715,-0.002017106,0.012065741,-0.026538907,-0.0007377048,0.056381963,0.012310429,-0.05365393,0.07676949,0.008617114,-0.012100552,-0.02012678,-0.07256655,-0.0169605,-0.049388498,0.034035776,-0.007501724,-0.023488462,0.06254518,-0.026266674,-0.028076597,0.0016933096,0.030673066,-0.03994876,0.041510463,0.068178475,-0.029984113,0.043101057,0.013388981,0.011245864,0.048811253,0.011956622,-0.0032016616,-0.0050392556,0.03061833,0.03062446,-0.008315465,0.014337244,-0.046407662,6.9056645e-05,-0.041791238,0.012344871,-0.018819394,-0.042808536,-0.022284197,0.039979152,0.04562399,0.039931998,-0.027545473,-0.055214107,-0.02862692,-0.043485306,-0.062257417,0.016472451,-0.033645693,0.0019898461,-0.013742652,-0.041562617,0.070387766,-0.09196837,0.004727236,-0.039760258,0.023714853,-0.010664794,-0.007513199,0.007845588,-0.009376661,-0.011660517,-0.04346482,0.047531433,0.062385686,-0.005910461,0.019728383,-0.0023103962,-0.012681299,-0.044188775,0.03072353,-0.032967553,-0.005510484,-0.007835731,-0.025857432,0.04430277,-0.032399673,0.03813532,0.0190526,-0.022872943,-0.06943272,-0.013051917,0.036732007,0.04440969,0.038084578,0.0117436405,0.008388874,0.059568953,-0.049108844,-0.016272442,-0.05133946,0.017690718,-0.016124882,0.010421825,0.06337542,0.001632796,-0.06438222,-0.009309625,-0.0036317843,-0.0517649,0.029339053,0.04112962,0.036138944,0.049286146,0.010781214,-0.026918143,-0.06939567,-0.008913509,-8.859777e-05,0.049846284,-0.007308095,0.040349014,0.008640259,0.010949826,-0.017675657,0.007128711,-0.019109657,-0.032582115,-0.026460098,0.033685897,-0.06449144,-0.031966943,0.020873014,-0.009026348,-0.02031999,-0.025229601,0.047248866,0.022344539,-0.07821157,-0.034889136,-0.044795703,0.017785633,0.017289618,0.019594777,0.011875309,0.01976782,-0.0036485088,-0.028883355,0.0434395,0.0076256497,-0.031134002,-0.020995278,-0.0025561976,-0.0033329902,0.028519062,0.017628314,-0.047301687,-0.104237765,0.02398684,-0.016333597,-0.051648155,0.025682464,-0.009008479,-0.0033466725,-0.054843247,-0.029264858,-0.018902577,-0.019753907,0.033375278,0.04040456,0.028255386,0.030903786,-0.009746792,-0.034034092,0.012938564,-0.029284026,-0.004598554,-0.004580664,0.034372117,0.005802832,-0.018658997,0.03145295,0.010431916,0.023085807,0.008357992,0.035587426,0.037936322,-0.024137285,0.028146528,0.010974833,-0.01141228,0.05054367,-0.027317822,0.029785965,-0.062412128,0.062403485,0.0007596639,0.021633338,-0.016795624,-0.029189516,-0.07831209,0.023641953,0.020945972,0.00011654004,0.009009067,0.06219041,-0.039773755,-0.02463547,0.001012976,0.016373815,0.024563093,-0.0008432486,0.015117335,-0.041811932,0.018959459,-0.05884414,0.04207951,0.003368153,-0.023290772,-0.014360783,-0.052978903]	Keywords: alternative architectures, Transformer, FFN, ODE solver\nKey Objects: Transformer architecture, FFN, Attention Modules\nRefers to Images: None\nHypothetical Questions:\n- What motivates the exploration of alternative Transformer architectures?\n- How does the Macaron Transformer modify the standard Transformer block?\n- What advantages does the Sandwich Transformer offer over the original architecture?\n---\nSummary:\nResearchers have investigated alternative Transformer architectures to explore whether the standard architecture represents the optimal design.\nOriginal Text:\n### 6.5 Exploring Alternative Architecture  \nDespite the success of Transformer architecture, one might question whether the current Transformer architecture is optimal. Interestingly, several studies have explored alternative architectures for Transformer.  \nLu et al. [89] interpret Transformer as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system and design Macaron Transformer, which replaces each Transformer block with a FFN-attention-FFN variant.  \nSandwich Transformer [99] explores reorganizing attention modules and FFN modules such that attention modules are mainly located in lower layers and FFN modules in upper layers. The induced model improves perplexity on multiple language modeling benchmarks, without increasing parameters, memory or training time.\nContextualized Text:\nGiven the success of the Transformer architecture, researchers have questioned whether it is the most efficient design. Several studies have since explored alternative architectures, such as the Macaron Transformer, which replaces each Transformer block with an FFN-attention-FFN variant, and the Sandwich Transformer, which reorganizes attention and FFN modules to improve perplexity.	{"tags": ["architecture", "NLP", "transformer", "research"], "doc_id": "050554e4-b52b-4d92-9375-8a5962d472cb", "summary": "Researchers have investigated alternative Transformer architectures to explore whether the standard architecture represents the optimal design.", "doc_type": "text", "entities": ["Lu et al.", "Macaron Transformer", "Sandwich Transformer"], "keywords": ["alternative architectures", "Transformer", "FFN", "ODE solver"], "key_objects": ["Transformer architecture", "FFN", "Attention Modules"], "contextual_text": "Given the success of the Transformer architecture, researchers have questioned whether it is the most efficient design. Several studies have since explored alternative architectures, such as the Macaron Transformer, which replaces each Transformer block with an FFN-attention-FFN variant, and the Sandwich Transformer, which reorganizes attention and FFN modules to improve perplexity.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.5 Exploring Alternative Architecture"}, "hypothetical_questions": ["What motivates the exploration of alternative Transformer architectures?", "How does the Macaron Transformer modify the standard Transformer block?", "What advantages does the Sandwich Transformer offer over the original architecture?"]}
12238e00-172a-41c6-b99d-41412e8cfc23	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.030487878,0.015443374,0.056551233,0.04089236,0.017938396,0.04056496,-0.000605451,0.041948084,0.0075327647,-0.027717937,0.00085192436,0.034390103,0.005929208,-0.016142722,-0.017776458,-0.0081522325,0.043133065,0.06099699,-0.028391799,-0.043242846,0.05496934,0.021719482,0.003199769,0.00882937,0.0008487764,0.06039678,0.031002922,0.02057897,-0.020091,-0.005508902,0.021847231,-0.045029633,0.05147266,0.013672603,0.040137243,-0.02483634,0.041923016,-0.051681604,-0.01751058,0.01668416,-0.005325858,0.06232045,-0.027913917,0.0027421345,-0.007421463,-0.07402108,-0.081396006,-0.012155142,0.018772379,0.008440712,-0.0066802907,0.0017678416,-0.01401857,-0.011629439,-0.020789279,-0.0071241884,0.0018254523,-0.031188466,0.00676971,-0.11086267,-0.022164356,-0.0031410507,0.010474035,0.0075836773,0.020410303,-0.0389922,-0.012688487,-0.00062190235,0.010707688,0.15617962,-0.02759409,0.031336475,0.0012333,-0.055780318,0.093941964,0.047929898,0.00295752,-0.002116367,-0.05209099,-0.033513367,0.015557694,0.047307808,-0.040478375,-0.05335154,0.0898566,-0.022814708,-0.047261752,0.00038032688,0.052514076,-0.049673647,-0.0029500714,-0.0044661947,0.013334225,0.08045119,-0.026978489,-0.08061007,0.006535256,-0.03509885,0.040635426,-0.0733028,-0.0072944537,0.010370674,0.08404705,0.1011941,0.045718435,-0.051729716,-0.015771719,-0.04784623,-0.013428955,-0.0027030406,-0.022741389,0.0065332567,-0.0373887,-0.007599606,-0.017484551,0.032503787,-0.03902853,0.027760677,0.00891179,-0.016089063,0.044593986,-0.045087297,-0.012501392,0.023432264,0.07192133,0.063456476,0.030167187,0.008375671,-0.030700611,0.0026985635,0.07848222,0.028467108,-0.024103414,0.013289344,-0.016298441,-0.023127442,0.08649079,0.0128860455,-0.003059297,0.037441973,0.03449983,-0.031364582,-0.033955898,0.0036239566,0.000871821,0.0063505126,-0.07286485,-0.004007165,-0.017177623,0.0078120553,0.041646503,-0.002566218,0.035216365,-0.02645693,-0.02614308,0.022859778,-0.025088388,-0.02541468,0.021052463,-0.042759042,-0.029432341,-0.029041959,-0.010576675,0.033293158,0.03712341,0.104740255,0.010369204,0.0028650272,0.033711556,-0.0044565764,-0.019078638,-0.0026964894,-0.0141437845,-0.032671813,0.0068243295,-0.0045000636,0.009688948,-0.025212144,0.012222067,-0.008751762,0.010770874,0.04148787,0.002833735,0.021457125,-0.035470057,0.013227778,-0.03986462,0.04642679,-0.032124892,-0.03524464,0.008809501,-0.030889416,0.01855173,0.008302161,0.0024286008,0.055043306,0.041420545,-0.07433945,-0.030074446,-0.022826025,0.014113323,0.0008501557,-0.0019447863,-0.013358181,0.0044824234,-0.01827998,0.010591253,-0.0027423336,-0.00952692,0.030702397,-0.0050001554,0.0049022753,-0.03564142,0.05136517,0.019293588,-0.008362988,-0.043439597,0.014294857,0.04750482,-0.031259067,-0.067016415,-0.008261668,-0.0028095727,0.07042446,-0.023708384,0.029096825,-0.043350037,-0.004505933,0.004402634,-0.053852007,0.008853162,0.01865836,-0.017163198,0.058001146,-0.032532252,0.0155034065,-0.042680122,0.031279765,-0.014701932,0.018056951,0.07638784,0.054059543,0.0155238565,0.0485937,-0.009459456,0.030879289,0.018501084,0.05555493,0.028166689,0.024210554,-0.021688078,0.05961601,0.054446574,-0.025182055,-0.00061420107,-0.06652943,0.0019007415,0.054494295,0.006946303,0.02258583,0.011986894,-0.018713335,0.04900361,0.010897132,-0.014101013,-0.007002178,0.011559107,0.053278767,-0.06410231,0.0028612076,0.052103892,0.009660285,0.00012879662,-0.05109855,-0.019863911,0.016079435,0.0049232375,-0.025830926,0.007928826,-0.025125787,-0.012201775,-0.0099717425,0.0541531,-0.00046090686,0.019875642,0.01334433,0.023400856,-0.014486354,-0.029592598,-0.018945122,-0.046612576,-0.024532702,0.040458363,-0.034613784,0.004774314,-0.0127878785,-0.006712987,-0.04904483,-0.0020989513,0.013681307,0.08667134,-0.07596569,-0.02225063,0.034896065,0.024662994,0.02035852,-0.028108953,0.02892256,0.032653634,0.031881843,0.022172494,-0.029020045,-0.06341439,-0.015375756,0.026570778,0.07807322,0.0853663,-0.0017869545,-0.0010409685,-0.024038797,0.019526435,-0.0025966857,0.005518009,0.051910155,0.05852921,-0.022388108,0.027045157,-0.019956497,-0.07896495,0.05436001,-0.08551187,0.02790823,0.05170433,-0.0042401296,0.0029059814,-0.0022778301,-0.013613315,0.06021084,0.04020752,0.018090561,-0.022498708,-0.049674567,0.0013047977,0.013374174,-0.026679853,0.047517806,-0.0050867186,-0.013146694,0.039537583,0.0045039994,0.03123116,0.045001954,0.022148468,0.02616505,-0.0047907415,-0.03484483,-0.013427228,0.004758212,-0.016913751,-0.009371221,0.055484027,-0.08128357,0.055763498,-0.04254549,0.008909428,-0.012211116,-0.053074248,0.04707403,0.0046987208,-0.023867229,0.015429489,-0.043154564,0.034300517,-0.033011563,-0.057962466,0.031781185,0.054374218,0.032829545,-0.027383078,-0.017415784,-0.04231828,-0.004719845,-0.036029123,-0.007939826,0.028175699,0.005912843,0.010942936,0.06492129,-0.018324379,0.027907515,-0.052333638,0.019702397,0.011530107,-0.011829906,-0.051484704,0.020913443,0.036572203,0.084410466,0.012272958,-0.035861414,0.03956119,0.037687562,-0.041545715,0.03105437,0.06384749,0.0126157915,-0.07271207,0.0067267064,0.0529657,0.05426167,0.042190075,-0.033806324,-0.038263112,0.010738954,0.027342372,-0.028253298,-0.02403227,0.022913156,-0.04278948,0.0060267234,-0.009842525,0.03528561,0.008839096,0.054975115,0.06276356,0.0001890303,-0.045796722,0.043425705,0.00079840404,-0.051958296,-0.0030015374,-0.037459128,-0.0067036273,0.06136742,-0.014133566,-0.015814278,-0.04369859,0.09148413,-0.060951374,0.030676967,0.05796761,-0.046420105,0.0118307145,0.0012149102,0.009326199,-0.06272689,0.030674074,0.029347403,-0.020952698,-0.011639436,0.027103895,0.028823517,0.03967884,0.008679177,0.046572614,-0.024556385,-0.0029044831,-0.06108452,-0.03244723,0.01146156,0.022165675,0.03568616,-0.014608002,-0.0049779536,0.0150529705,-0.0048124106,-0.013929847,-0.049883947,0.002922166,0.004532781,0.01536057,0.025084032,-0.017501434,0.07244548,-0.022080744,-0.030411772,-0.0053905514,0.021787044,0.044933565,0.0057585933,-0.05876721,-0.03362019,-0.0037993214,0.035154063,0.010575504,0.0014050865,-0.04872731,-0.028707955,-0.049622107,0.01996349,-0.0020977925,0.03744265,0.00036820304,-0.0032287168,0.027706424,-0.041389737,0.016568102,0.009016088,-0.0016211354,-0.0331063,-0.046209864,0.017927757,0.06512004,0.018889774,-0.04403862,0.028514931,-0.023373485,-0.014664342,-0.022760471,0.064428344,-0.0014130323,-0.026705066,-0.0061016046,-0.008457321,-0.042267248,0.016364807,0.0259808,0.00083832664,-0.013809287,0.05162347,-0.006333052,0.016085466,0.03821753,0.0065166093,-0.0075925607,0.0138132,0.043966357,0.048961304,0.036925364,-0.08482681,0.021062488,-0.01322171,0.015235584,-0.00080973224,-0.07026604,-0.062069803,0.0076650046,-0.07176263,-0.022421202,0.0081782425,-0.032346085,-0.068638645,-0.013046082,-0.021701459,0.03711078,-0.069200344,0.01981121,0.009816944,-0.017105937,0.11400547,0.017605416,-0.022739526,-0.024960138,0.0052339933,-0.028366096,0.011470806,0.066761695,0.008131171,-0.017256105,0.05099035,0.007500617,-0.023686899,-0.04066591,-0.03997222,-0.027194683,-0.0068695485,-0.03099174,-0.011254972,-0.014541302,0.027992127,0.04236372,-0.014307825,-0.015379128,0.02779212,0.004381462,0.05892776,0.019595845,-0.02090585,0.0025607136,-0.0152308475,0.030801266,0.035290234,0.03788309,0.03895466,-0.013668148,0.022732714,0.006281043,0.002810166,-0.013519421,0.011197754,-0.028087562,-0.042618573,0.01591084,0.0085592745,0.019069005,-0.0011172856,0.05002742,0.0028452394,0.016807219,-0.009658853,-0.10798793,-0.027730882,-0.058758106,-0.07149917,-0.013465853,-0.007862917,-0.0014595968,0.015955597,-0.035264168,0.044737473,-0.08171524,-0.020796338,-0.032606523,0.04255142,0.016790017,-0.017081099,-0.02310188,0.020664271,-0.012553074,-0.015121884,0.066393085,0.03414735,-0.03781281,-0.019988745,0.0028740005,0.006259882,-0.0061392765,0.014582098,-0.015262051,-0.0005326161,0.005318181,-0.023456443,0.07285518,-0.048439465,0.02545052,0.012660301,0.0071729477,-0.09001866,-0.033490174,0.053214356,0.028909614,-0.00902036,0.012347668,0.004168575,0.034114618,-0.05518024,0.009981788,-0.057645142,0.022990504,0.014328165,0.010211799,0.029119015,-0.016330726,-0.05064776,0.0040602707,0.012168604,-0.041406542,0.06708903,0.033835433,0.02781655,0.066594474,0.055729523,0.008625453,-0.063212864,-0.026988266,-0.03687623,0.0017527051,-0.03363886,0.03252742,-0.008945559,-0.01994451,-0.013911374,-0.01246687,0.01618972,-0.0054821414,0.038511135,0.0005987985,-0.081805356,-0.054244284,0.006494371,0.035730477,0.02689094,-0.02790829,0.065244816,0.00902847,-0.036995057,-0.011683294,-0.067990206,-0.015834345,0.026954109,0.005266474,0.0057290858,0.029573012,-0.010895872,-0.008022305,0.085319966,-0.012430956,-0.020306913,-0.051411875,0.04188676,0.037212107,0.06355739,-0.017883303,0.01827737,-0.104703605,0.015313323,-0.023495236,-0.030866781,0.0032632067,-0.026767509,-0.02946171,-0.042062696,-0.0064533385,0.035622295,0.025871515,0.0079803495,0.017121924,0.015755987,-0.03928045,0.0013396101,-0.0719737,-0.031685133,-0.013348969,0.001939045,-0.014762627,0.049422044,0.014960798,0.004517422,-0.003297252,-0.013914794,-0.0053757736,0.012914684,-0.031574134,0.029604344,-0.05167372,-0.0076367618,0.010443297,-0.0058439085,0.039145835,-0.008186084,0.0083222035,-0.024192786,0.05193909,0.014076304,0.024176676,-0.0556168,-0.039844047,-0.050712544,0.007989385,0.038022824,0.009561979,0.010886659,-0.026045367,-0.042804074,-0.04090082,-0.053078067,-0.033076033,0.060932077,0.0277089,0.012711541,-0.08630501,0.0068599298,-0.038003255,0.05574943,0.013883555,0.007127873,-0.022137288,0.027242616]	Keywords: dynamic mask attention, self-attention, machine translation, abstractive summarization, text data\nKey Objects: Dynamic Mask Attention Module, Self-attention Module, Token Representations\nRefers to Images: None\nHypothetical Questions:\n- How does the dynamic mask attention module in MAN contribute to modeling locality in text data?\n- What are the conditions that influence the dynamic mask in MAN?\n- What are the benefits of using a dynamic mask versus a static mask in the attention mechanism?\n---\nSummary:\nMask Attention Network (MAN) enhances Transformer blocks by adding a dynamic mask attention module, which improves performance in machine translation and abstractive summarization.\nOriginal Text:\nMask Attention Network (MAN) [35] prepends a dynamic mask attention module to the selfattention module in each Transformer block. The mask is conditioned on token representations, the relative distance between tokens and head indices. The proposed dynamic mask attention is shown to effectively model locality in text data and the induced model consistently outperforms the baseline model in machine translation and abstractive summarization.\nContextualized Text:\nTo explore alternatives to the standard Transformer architecture, researchers have proposed modifications like the Mask Attention Network (MAN). MAN prepends a dynamic mask attention module to the self-attention module within each Transformer block. This module's mask is conditioned on token representations, the distance between tokens, and head indices. The resulting architecture demonstrates effective modeling of locality in text data and consistently outperforms baseline Transformer models in machine translation and abstractive summarization.	{"tags": ["architecture", "NLP", "transformer", "attention"], "doc_id": "12238e00-172a-41c6-b99d-41412e8cfc23", "summary": "Mask Attention Network (MAN) enhances Transformer blocks by adding a dynamic mask attention module, which improves performance in machine translation and abstractive summarization.", "doc_type": "text", "entities": ["Transformer", "MAN"], "keywords": ["dynamic mask attention", "self-attention", "machine translation", "abstractive summarization", "text data"], "key_objects": ["Dynamic Mask Attention Module", "Self-attention Module", "Token Representations"], "contextual_text": "To explore alternatives to the standard Transformer architecture, researchers have proposed modifications like the Mask Attention Network (MAN). MAN prepends a dynamic mask attention module to the self-attention module within each Transformer block. This module's mask is conditioned on token representations, the distance between tokens, and head indices. The resulting architecture demonstrates effective modeling of locality in text data and consistently outperforms baseline Transformer models in machine translation and abstractive summarization.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.5 Exploring Alternative Architecture"}, "hypothetical_questions": ["How does the dynamic mask attention module in MAN contribute to modeling locality in text data?", "What are the conditions that influence the dynamic mask in MAN?", "What are the benefits of using a dynamic mask versus a static mask in the attention mechanism?"]}
1ba04fea-dd9a-4968-a900-b423aea7514d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.028602669,0.00770033,0.038317747,0.02437539,0.040108908,0.031018982,-0.06470825,0.012836809,0.04255514,-0.02064686,0.014859742,0.0020050234,-0.036769103,0.00929045,-0.015025837,-0.03144886,-0.00016867481,0.048457526,0.02589449,-0.053530104,0.029910484,0.016032,0.016510312,-0.026497927,0.011640361,0.039706945,0.053320542,-0.032332767,0.004547109,-0.029447049,-0.00722127,-0.0011377076,0.02000706,-0.037265945,0.030305058,-0.0040522274,-0.0015357174,-0.024575805,0.027524078,-0.023473142,-0.035032723,0.002326362,-0.039614387,-0.015605759,-0.021996906,-0.04817761,-0.055476215,-0.031713866,-0.0024698344,-0.021229934,-0.012710433,0.03653593,-0.0216486,0.028738819,-0.04973736,-0.054148212,0.008382764,-0.029876899,0.02248508,-0.06763413,-0.0669109,0.000345483,-0.024855802,-0.012352024,0.08109294,-0.046012886,-0.00649132,0.0030625872,0.060283203,0.13515559,0.0029986135,0.01742908,-0.03607758,-0.050276246,0.09860563,0.04223279,-0.026318716,0.009649407,-0.036620595,0.02726104,0.008621213,0.0936654,-0.048146054,-0.076041825,0.088335045,-0.018108372,-0.050207574,-0.04597891,0.021059547,-0.0072328555,0.024663225,-0.0097710015,-0.0003371905,0.077290356,-0.068905145,-0.062337846,0.017657297,-0.027482215,0.028668491,-0.06685322,-0.0126545895,-0.035970505,0.004627366,0.083773874,0.024987493,-0.015737534,-0.042129442,-0.04577386,0.03125439,0.008279418,-0.05240537,0.058198936,-0.011551872,0.01715976,-0.021443067,-0.015114202,-0.04985824,0.009493809,0.011557625,-0.024576897,0.0483022,-0.023391984,-0.014629639,0.021252155,0.01441508,0.052835993,0.04872679,-0.018941406,0.023429854,-0.014920616,0.016949475,-0.026036808,0.01119957,-0.010099098,-0.007879693,0.0030945106,0.023343923,-0.031327017,0.0073884386,0.009874226,0.025636738,-0.011003579,-0.027183989,0.0247368,-0.04188222,0.010668984,-0.052950114,0.045159336,-0.035309326,0.05134531,0.010231493,-0.023769872,0.03753241,-0.025946977,-0.0049768267,-0.048783284,0.03421469,-0.013036035,-0.0031302914,-0.012176707,-0.018932609,-0.013918189,-0.0036657515,0.028587194,0.038248785,0.13272245,-0.0015781347,-0.0012033201,0.03429586,0.029319275,-0.030199558,0.006441513,0.048827857,-0.033871453,0.015391288,0.00020737316,0.044177283,-0.003446507,0.00022438822,0.0062978677,-0.03426285,0.049729586,-0.015586723,0.037104063,-0.028341565,0.03148454,-0.0011414664,0.0427663,-0.0060633663,-0.01517701,0.048452314,-0.030402362,-0.024088895,-0.0010208983,0.045885626,0.030003855,0.05101253,-0.08467062,-0.041777562,-0.040734645,0.048626162,-0.0021486615,-0.006615224,-0.021173073,-0.010692086,-0.024219511,-0.012616543,0.0088991625,0.01147958,0.026705798,-0.01161597,-0.0120728435,-0.010109245,0.041017126,0.055750433,0.022321278,0.007134304,-0.004983422,-0.019417092,-0.016861089,-0.059265804,0.0135061825,0.014018803,0.020521289,-0.0487082,0.0060899896,-0.035102155,-0.03218896,0.043663774,-0.018412588,0.05218402,0.008480512,-0.027400287,0.028679186,-0.008235075,0.023183964,0.010707242,0.053018562,0.036455214,0.015877092,0.049067684,0.025199387,-0.0041999584,0.067180835,0.043787338,0.007314148,0.046973813,0.0662648,0.011763871,0.013054164,0.0039838804,0.05140607,0.07108421,0.016621176,-0.020861907,-0.03962815,0.017698038,0.018675834,0.013325023,-0.011566673,-0.011257509,-0.014468427,0.013550008,0.020231036,-0.007651865,-0.033145007,0.02028896,0.015159374,0.0070965528,-0.0060514533,0.0070704124,0.015758451,0.017478341,-0.008211266,0.0015069863,0.015731156,-0.012716397,0.008729658,0.027292415,0.014707807,-0.0024558653,-0.007080391,-0.0008352431,-0.0029069923,0.030740395,0.00042174387,0.035361465,0.0029892998,-0.052757006,-0.0146722,0.024778834,-0.028342169,0.023725811,0.006905274,0.03981579,0.018875638,-0.013484925,-0.02105393,-0.008995427,0.02285754,0.09926064,-0.061444223,-0.015329994,-0.01084171,0.023324912,-0.012360751,-0.053030334,0.005715743,0.0061995797,0.00069779163,0.045099147,0.040031973,-0.027626842,0.0010795161,0.0076705585,0.084116094,0.086811975,-0.016108166,-0.0033899997,0.019909335,0.010087034,-0.009235872,-0.010304838,0.04353619,0.031599082,-0.028030133,-0.0058563994,-0.034237117,-0.06590122,0.026406793,-0.083046064,-0.0033453298,0.04432607,0.0009352755,0.0143139465,0.014525144,-0.03562751,0.037708383,0.09912325,0.030538198,-0.005317798,-0.0585535,-0.0058963452,-0.025241949,0.016013838,0.022819048,-0.0008266858,-0.009225628,0.03303555,0.0002964233,0.015925555,0.08046102,0.03242945,0.013437471,-0.018596983,-0.06896227,0.014766349,-0.0047551403,0.0064231353,0.0023698725,0.06430882,-0.053284228,0.078260705,0.002694612,-0.006680404,0.0062981346,-0.07375445,-0.000974475,0.008720083,-0.005067295,-0.012223726,-0.034664605,0.054849885,-0.009835016,-0.004420788,0.012492542,0.011106571,0.040145665,-0.0012983192,-0.015341608,-0.0037116804,-0.018120583,-0.016238,0.02142126,0.006260848,-0.014077621,-0.04415772,0.06758341,0.005605693,0.017355533,-0.059972133,0.026403928,0.046475854,0.044074386,-0.02928032,-0.038825992,0.046989556,0.048399765,0.029594954,-0.03075841,0.04703273,0.04619727,-0.03410551,0.05929411,0.045183107,-0.00085027405,-0.04166686,0.040056907,0.045156896,0.04314746,0.032677293,-0.022864725,-0.03630666,0.0038730982,-0.036361847,-0.03224879,0.02196961,0.020376561,-0.057808384,0.057430126,-0.027616698,0.031862542,0.011110103,0.04671721,-0.004335559,0.014007826,-0.037518684,0.012167822,-0.023796061,0.011245767,-0.00762462,-0.017780483,9.1753674e-05,0.07918895,0.06582203,-0.029582499,-0.08821546,0.050424453,-0.05628339,0.04537081,0.06266832,0.0058554644,0.027385801,-0.02305376,0.040691458,-0.02323692,-0.0038076732,0.014580517,0.0058596022,-0.020018851,0.023958897,-0.039698146,0.029534677,-0.010428822,0.06383277,-0.033978842,-0.007898425,-0.008215819,-0.018057548,0.04848857,0.060751364,0.06708056,-0.03995374,0.0147640295,-0.0038030732,-0.0031592718,-0.0024696365,-0.053425774,-0.029125962,-0.032215904,0.00033639005,0.019904401,-0.024308233,-0.0046407543,-0.050830647,-0.033942007,-0.010453261,0.01693656,0.03905035,-0.0037570249,-0.05330873,-0.0043820064,-0.008030326,0.053381458,-0.012723944,-0.03971607,-0.06537295,-0.04313852,-0.07826261,0.037612565,0.0027986031,0.094564885,-0.0008621094,0.005926573,0.027333777,-0.045409165,0.04108606,-0.0034355726,0.015231922,-0.02931128,-0.025448129,-0.018406598,0.06190235,0.058875,-0.04374083,-0.002123648,0.041341897,0.031881947,-0.011432388,0.030530805,0.06876766,-0.006290256,0.033548996,-0.014634633,-0.03238282,0.020656886,0.014485364,-0.025861047,-0.0013534907,0.02154243,-0.03515228,0.01916821,0.0028272143,-0.05157401,0.042431537,-0.015811816,0.033870146,0.04653566,0.057344344,-0.06341073,0.07105391,-0.01719807,-0.024757763,-0.013293912,-0.028401684,-0.034672037,0.018837107,-0.043741748,0.0031201905,0.060662813,-0.029948099,-0.0499885,-0.030417407,-0.0067184754,0.050879717,-0.06569709,0.0025740287,-0.04109937,0.075446926,0.06398442,0.009457286,-0.0047244956,0.004505433,0.0044641355,0.008234355,0.03469656,0.029210445,0.016736321,-0.011022221,0.07637989,0.01594943,-0.0036383527,-0.027576705,-0.10427678,-0.020595223,-0.029034235,0.010014604,-0.025375599,0.0017551532,-0.0026709854,0.0027653,-0.032104354,0.056115493,0.014471585,-0.010031821,0.03806677,0.030661369,-0.018499719,0.018445581,-0.03656683,0.023083579,0.08415139,0.020752158,-0.015202966,0.010319249,0.009816921,0.036856756,-0.016483009,0.04577371,-0.017989246,-0.016408525,-0.02636466,0.02710094,0.003978955,-0.049156096,0.006765315,-0.0025542183,0.019315409,0.010022633,0.026615525,-0.11329983,-0.012293744,0.0058886134,-0.07163808,-0.021619175,0.0021978596,0.015539249,0.030310338,-0.052823752,0.061791953,-0.08075891,-0.03636861,-0.040622372,0.05027662,-0.033923227,-0.0061469674,-0.013808749,-0.00025749003,-0.04666454,-0.037061263,0.04524093,0.03725026,-0.009952159,0.04600728,-0.02952006,0.007677151,-0.07398066,-0.004861091,-0.0315252,-0.0059210807,0.008023262,-0.012497621,0.07475963,-0.055472977,0.018673563,0.001994751,-0.011500188,-0.09555224,-0.042548284,0.025774177,0.021884825,0.016487869,-0.04797995,0.042109117,0.025223386,-0.054304387,-0.059127826,-0.04334583,0.044669036,0.012268789,0.051474784,0.044237968,0.038625307,-0.042265654,-0.044947524,0.030639783,-0.035431173,0.029910745,0.0327107,0.02087819,0.035477318,-0.0112183085,-0.030655032,-0.083728254,-0.005000789,0.03892715,0.020598058,-0.04787418,0.07038208,-0.013013231,0.008251293,-0.02433102,-0.03542821,-0.0015988878,-0.0051873545,-0.009490828,0.010099536,-0.05039228,0.003031719,0.006793917,0.03339626,-0.000985058,0.012031368,0.05124097,0.011640008,-0.053315163,-0.033519365,-0.07113119,0.009838912,0.03172347,0.04355887,0.014127898,0.028878178,-0.010455145,0.00577664,-0.006409223,-0.01630558,0.0227269,-0.010081421,0.01597244,0.03015196,0.012496206,0.015797652,0.009239025,-0.08678963,0.03936131,-0.02314506,-0.048456367,0.02897355,0.03240507,-0.01386067,-0.030635664,0.0004973625,-0.0049485583,0.040476546,0.060445413,-0.0011263336,0.014283572,0.036857627,0.00861577,-0.02404013,0.0020377282,-0.047123913,0.0017919485,-0.01954617,0.042430695,-0.020279888,0.036635682,0.02859238,0.01620264,0.018449238,0.02984151,-1.0158646e-05,0.019727359,-0.11231676,0.06785058,0.0027574878,0.002912846,0.039508052,-0.030635707,-0.035838563,-0.0606803,0.027038822,0.032296658,-0.0017223792,-0.033994876,-0.007345125,-0.07684869,0.004743415,-0.026551956,-0.012331004,-0.001927314,0.0261284,-0.02416128,-0.014542117,-0.026365653,-0.048418712,0.046629876,-0.021275584,-0.023439802,-0.067713,0.037039626,-0.020503612,0.0477352,-0.006547817,-0.028073678,-0.012741338,-0.05021132]	Keywords: Neural Architecture Search, Transformer architecture, Evolved Transformer, DARTsformer, architecture search\nKey Objects: Transformer architectures, Neural Architecture Search\nRefers to Images: None\nHypothetical Questions:\n- How does Neural Architecture Search (NAS) differ from manually designing Transformer architectures?\n- What are the advantages of using evolution-based search (as in ET) versus differentiable search (as in DARTsformer)?\n- Why is reducing the search cost important when exploring alternative Transformer architectures?\n---\nSummary:\nResearchers are exploring Neural Architecture Search (NAS) techniques to discover and develop alternative Transformer architectures that improve performance and efficiency.\nOriginal Text:\nNotably, there's a line of work that uses Neural Architecture Search (NAS) to search for alternative Transformer architectures. The Evolved Transformer (ET) [123] employs evolution-based architecture search with the standard Transformer architecture seeding the initial population. The searched model demonstrates consistent improvement over Transformer on several language tasks. As another representative work, DARTsformer [167] applies differentiable architecture search (DARTS) [82], combined with a multi-split reversible network and a backpropagation-withreconstruction algorithm for memory efficiency. The resulting model consistently outperforms standard Transformer and compares favorably to larger ET models, with a significantly reduced search cost.\nContextualized Text:\nSeveral studies are investigating methods to automatically search for improved Transformer architectures.  Specifically, Neural Architecture Search (NAS) techniques are being used to find alternatives. The Evolved Transformer (ET) utilizes evolution-based search, while DARTsformer employs differentiable architecture search (DARTS) to create models that consistently outperform standard Transformers with reduced search costs.	{"tags": ["NAS", "architecture", "optimization", "deep-learning"], "doc_id": "1ba04fea-dd9a-4968-a900-b423aea7514d", "summary": "Researchers are exploring Neural Architecture Search (NAS) techniques to discover and develop alternative Transformer architectures that improve performance and efficiency.", "doc_type": "text", "entities": ["Evolved Transformer", "DARTsformer"], "keywords": ["Neural Architecture Search", "Transformer architecture", "Evolved Transformer", "DARTsformer", "architecture search"], "key_objects": ["Transformer architectures", "Neural Architecture Search"], "contextual_text": "Several studies are investigating methods to automatically search for improved Transformer architectures.  Specifically, Neural Architecture Search (NAS) techniques are being used to find alternatives. The Evolved Transformer (ET) utilizes evolution-based search, while DARTsformer employs differentiable architecture search (DARTS) to create models that consistently outperform standard Transformers with reduced search costs.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "6 ARCHITECTURE-LEVEL VARIANTS", "h3": "6.5 Exploring Alternative Architecture"}, "hypothetical_questions": ["How does Neural Architecture Search (NAS) differ from manually designing Transformer architectures?", "What are the advantages of using evolution-based search (as in ET) versus differentiable search (as in DARTsformer)?", "Why is reducing the search cost important when exploring alternative Transformer architectures?"]}
e204831a-9a90-4fb9-905d-6ea2d3d9707a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.024084292,0.0053222156,0.0402028,0.029467907,-0.008638721,0.04741043,0.01252026,0.06504025,0.017592916,-0.02590872,-0.03385128,0.021577744,0.05297643,0.007126153,-0.0014407323,0.02426923,-0.0149122635,0.10617293,0.019443147,-0.0075060395,0.06795811,0.025592415,0.024266213,-0.04063268,0.035873003,0.033562697,0.06409895,0.0019947877,0.0031457832,0.023231793,0.013322688,-0.011695406,0.038350694,0.038023856,0.018583663,-0.0021134217,0.026285756,-0.053028632,-0.033240154,-0.026168702,-0.02296661,0.06787045,-0.058446508,-8.4435764e-05,-0.008924855,-0.040134247,-0.14063692,-0.024343027,-0.01733156,-0.04829713,0.0005616801,-0.008733987,-0.024685057,-0.0058999886,-0.038080614,-0.017783301,0.00044659092,0.004568047,0.038620297,-0.07493073,-0.026510509,0.016812723,-0.024738256,-0.010966944,0.0073964493,-0.020366767,0.02546587,0.04030939,0.037000123,0.12965502,-0.021293756,0.009165392,-0.020869436,-0.042308666,0.12410304,0.016243022,-0.08033565,-0.050186574,-0.043605503,0.0040718173,-0.045060508,0.08121443,-0.009684772,0.0013031121,0.04376787,0.0063802386,-0.048506454,-0.016635276,0.097313195,-0.02148709,-0.0073005855,0.011293251,0.025380427,0.03254293,-0.010493877,-0.104853444,-0.007779634,-0.054568794,0.013337059,0.00766061,0.010150298,-0.028141502,0.09855703,0.08685041,0.073139,-0.031133937,-0.010590184,-0.061458107,0.015065518,0.031962853,-0.05223282,0.025117764,-0.037542235,0.042272262,-0.043286916,0.00066636543,-0.0420671,-6.408457e-05,0.0021980186,-0.0059153815,0.002672117,-0.0027581393,-0.022131216,0.058968186,0.05119303,0.042096246,-0.025473488,-0.022512037,0.026619717,-0.00066398166,0.04808498,-0.016396783,-0.014364904,0.009270154,0.012950961,0.0043927897,0.062519066,0.03855842,0.013280017,0.01701303,0.021372594,-0.06928308,-0.04611228,0.054493707,-0.011828765,0.042398248,-0.050475042,0.030370042,-0.015279468,0.048066366,0.06191518,0.004626794,0.03922231,0.021514563,0.004765095,-0.017062,-0.023712886,-0.02273283,0.012158593,-0.0770405,-0.05406802,-0.060460508,-0.027733818,0.031587888,0.019053921,0.09816786,0.028679006,-0.025538817,0.026278798,0.047084685,-0.04033626,-0.0016865745,-0.0044903727,-0.09024009,-0.0010965207,-0.021803,0.033034943,0.0041957954,0.032618456,0.0072982684,0.011959212,0.026049223,0.020918787,0.058673296,-0.029813655,0.007981868,-0.0028372556,0.063899696,-0.024973292,-0.013548175,0.030896848,-0.026400547,0.0073574153,0.011074386,0.06970209,0.02555264,0.03287739,-0.07773522,-0.034604482,-0.010990116,-4.4904617e-05,0.007407418,0.027671034,-0.03657429,0.066178344,0.007896574,-0.018912226,0.008615354,-0.039450802,0.029233418,0.017288715,-0.021451991,-0.03757924,0.0049204025,0.059817404,-0.026269479,0.009550836,0.050133962,0.0520799,0.016822727,-0.024876235,0.004879971,-0.006969466,-0.005153743,-0.018406399,0.03102832,-0.03415983,-0.002776723,-0.0035093985,0.013750319,0.037176155,0.019762427,-0.044490054,0.0775847,-0.01880761,0.047855765,-0.010437497,0.012863046,0.022831893,0.0023147634,0.056665182,-0.001079434,-0.054610424,0.0775673,0.0029217445,-0.04140667,0.008346532,0.02629525,0.025902556,0.028634094,-0.0014636475,0.043081097,0.016008344,0.00044127382,0.0018251971,-0.042718675,-0.005191452,0.021461207,0.004777021,-0.015083724,-0.0149482945,0.011336861,2.3412338e-05,-0.011493684,-0.0002200902,-0.049421236,-0.010659144,0.0007896177,-0.022656523,-0.049193505,0.028447233,0.027155962,0.010281581,-0.03165249,0.036676534,0.0359227,0.041415945,-0.018006116,0.056071956,-0.032877073,0.01672463,0.02503299,-0.014582987,0.008011865,0.0077691227,-0.0067082494,-0.02492093,-0.054540813,-0.08373788,-0.008408432,-0.02643047,-0.03973411,-0.010170116,-0.034723084,-0.070609465,-0.00684075,-0.017442506,0.02011421,-0.01202128,0.023692489,0.078761294,-0.011238189,0.0024638071,-0.04506746,-0.00491411,-0.0021380538,-0.09049045,0.004480184,0.039340645,0.0028571112,0.056569178,-0.009517319,-0.033401474,-0.0063907187,0.050895955,0.07495357,0.04436034,-0.045836072,-0.01645227,0.005121259,0.02921607,-0.029461447,-0.018549273,0.05296209,0.020911653,-0.025800908,0.0033469407,0.0002655169,-0.10812011,0.045169562,-0.041157585,0.046271197,0.066895515,-0.015755678,0.00023877504,-0.0014691679,0.027298465,0.057296034,-0.0011552279,-0.0033678312,0.028177818,-0.090337574,-0.015461394,0.006091199,0.0026151591,0.010289119,-0.021371244,0.016482295,0.0022239645,-0.008140506,-0.026060184,0.046123624,0.04995371,0.036509354,0.0049973223,-0.0099693285,0.006340279,0.019780716,0.018225728,-0.028512422,0.057702206,-0.009078623,0.020114774,-0.01851884,0.010850078,0.018192615,-0.10556877,-0.019435115,-0.0025034249,-0.03186612,0.0064749517,-0.04403984,0.06351835,-0.029654771,-0.009159515,0.0042482577,0.014066794,0.021035073,-0.03634474,-0.08058431,-0.005892966,0.03352178,-0.0063373754,-0.021688795,0.0013295725,0.017844638,-0.012128434,0.07588053,-0.0073402748,0.067693025,-0.043541916,0.043346856,-0.002470691,-0.022349285,0.001152177,0.00023082878,0.020472933,0.029334676,-0.028855942,-0.006465846,0.017585829,0.009382134,-0.051922444,0.05732237,0.039554425,-0.009009451,-0.063740686,-0.036223594,0.066297136,0.004969594,0.021499896,-0.0072690737,-0.053820834,0.008054643,-0.0013390557,-0.019363705,-0.013348806,0.002340211,-0.053161312,0.013589549,-0.035030767,0.03047427,0.0060775406,0.04005626,0.011727737,0.020847276,-0.05630348,0.007243592,-0.020551503,-0.029054655,0.00032995007,-0.086482175,-0.025620753,0.0513515,0.07493949,0.00842808,-0.046487655,0.05785433,-0.036145613,0.04315031,0.07469915,-0.024542604,0.027468015,0.019819366,0.04803675,-0.0032633292,-0.013439279,-0.004243157,-0.011523601,-0.020257834,0.014191736,-0.010823466,-0.009077031,-0.0029804707,-0.001794668,-0.015297233,0.00085823325,-0.001326673,-0.016652336,-0.042290233,-0.028009064,0.053211294,-0.0074815997,0.00491937,0.0059986883,0.007276997,-0.01423657,-0.0053762775,-0.01515825,-0.0037351605,-0.0022823217,0.016881315,-0.02379522,0.016309835,-0.027433183,0.019145334,-0.018214792,0.06747804,-0.008657464,0.0014414819,-0.047198016,-0.012019905,0.011719508,0.03377641,0.008987693,-0.0077384175,-0.011376189,-0.003966873,-0.08069341,-0.012432845,0.022209436,0.015510108,0.0049388837,0.017310869,0.053921394,-0.023375312,-0.018148117,0.0387419,0.03612802,-0.029534409,-0.055458717,-0.015971242,0.07119793,0.06617457,-0.04821389,0.030484954,0.010617554,-0.00023111385,-0.011183084,0.049783703,0.00850736,-0.014395572,0.013326978,-0.04186001,-0.009415311,0.0049963742,0.010463454,-0.0030535744,0.025005424,0.061183754,-0.019897101,0.015568859,0.033406932,-0.015729813,-0.043306213,-0.0021915708,0.03996212,0.0376225,0.08019856,-0.087168574,0.0373232,-0.02979179,-0.017778054,-0.008251824,-0.055621173,-0.021055656,-0.0013377637,-0.00888217,0.011573755,0.04202053,-0.004746159,-0.0041147415,0.022786068,-0.014615316,0.057474796,-0.05574612,0.030158103,0.01855152,-0.021353923,0.07793838,0.035023957,-0.01714305,0.008691255,-0.018753266,-0.018041892,0.005647032,0.07170863,-0.0022474837,-0.006869279,0.056782845,-0.0012133648,-0.002803571,-0.018834053,-0.046492945,-0.016180372,-0.011432529,-0.021295352,-0.008824806,-0.018344417,0.04323112,-0.011155609,-0.01139429,0.0060608895,0.017589182,-0.031051442,0.030732904,0.032240592,-0.000102426995,0.05971552,-0.008894312,0.03823621,0.026872905,-0.0069136396,0.011039298,0.010808158,0.038364712,0.025480507,-0.011595399,0.007482784,0.005918778,0.0072612423,-0.015439073,0.03460906,8.20075e-05,-0.06479847,0.0027773795,0.0054990454,0.017561482,0.031355843,-0.008241899,-0.092641294,-0.025427552,-0.034268994,-0.03602022,0.0026166472,-0.010368934,-0.01049621,0.04841291,-0.06331177,0.067094594,-0.0670386,-0.016997173,-0.042506736,0.030061003,-0.011480425,-0.0028493174,0.0050758705,-0.040461633,-0.026889516,-0.07507866,-0.005421147,0.046024036,-0.03144387,0.0092761135,-0.024003161,0.0081499275,0.0048706285,-0.0027172384,0.0012137548,0.017136093,0.016237292,-0.03688412,0.099043824,0.017118393,0.014409266,0.036347583,-0.0121127805,-0.07058332,-0.019037526,0.0020046232,0.0288049,0.028391005,0.022438692,0.009655338,0.03768283,-0.026393112,-0.009042231,-0.059306312,-0.038044285,-0.05579236,0.029546453,0.056322746,-0.019583145,-0.0327163,0.02363052,-0.0069127968,-0.053360578,0.017619377,0.033949587,0.09604077,0.06011206,0.024152314,-0.027042603,-0.070575975,-0.0113452235,-0.0059156744,0.04209521,-0.008818593,0.030536512,0.024594396,-0.010503603,-0.015292376,0.013048855,0.00053154124,0.00041269368,0.002752276,-0.0042686956,-0.027517622,-0.07893156,0.020175185,0.043635037,-0.040222775,0.0023674835,0.04393205,0.010809815,-0.027668688,-0.034805488,-0.046310555,-0.03894904,0.010967584,0.0040200357,0.022678014,-0.027295854,-0.015905052,-0.068613075,0.018366566,0.0012955129,-0.0123305945,-0.06609836,0.03528175,0.003764727,0.035372306,0.0075714886,0.017717423,-0.061304662,0.02935621,0.009273478,-0.06636153,0.052385118,0.036390588,-0.020232381,-0.026211405,0.01753437,-0.02423149,0.0012583218,-0.01091665,0.0013357992,0.04058021,0.018254826,-0.041799627,-0.03942198,-0.020928377,-0.016628718,-0.0033195985,-0.023551917,0.06508995,-0.0015801914,0.0053630644,0.039886165,-0.02049793,0.008728905,0.016655406,-0.001953874,0.08133526,-0.051301304,0.057262678,0.012247623,0.030657547,0.09388129,-0.038985625,0.02665714,-0.02278286,0.034030486,-0.015065501,0.014989037,-0.059311528,-0.033274073,-0.027296152,-0.0048787184,0.02340334,0.014433155,-0.03002186,-0.015121502,-0.03645945,-0.0559349,-0.005132519,-0.02323231,0.08433096,0.008213008,0.04428808,-0.01686171,0.0053097634,-0.02613938,0.01606383,0.005576102,-0.015855595,0.0052483436,-0.00060712313]	Keywords: inductive bias, overfitting, Transformer architecture, data structure\nKey Objects: Transformer architecture, data structure, inductive bias\nRefers to Images: None\nHypothetical Questions:\n- What is meant by 'inductive bias of locality'?\n- How does the lack of inductive bias in Transformers affect their ability to generalize?\n- What are some potential methods for introducing inductive bias into Transformer models?\n---\nSummary:\nUnlike convolutional and recurrent networks, Transformers do not assume any structure in the data, making them highly versatile but also prone to overfitting when data is limited.\nOriginal Text:\n## 7 PRE-TRAINED TRANSFORMERS  \nAs a key difference from convolutional networks and recurrent networks that inherently incorporates the inductive bias of locality, Transformer does not make any assumption about how the data is structured. On the one hand, this effectively makes Transformer a very universal architecture that has the potential of capturing dependencies of different ranges. On the other hand, this makes Transformer prone to overfitting when the data is limited. One way to alleviate this issue is to introduce inductive bias into the model.\nContextualized Text:\nA key difference between Transformers and convolutional or recurrent networks is that Transformers do not incorporate an inherent inductive bias of locality and make no assumptions about how the data is structured. This versatility allows Transformers to capture dependencies across various ranges, but also makes them susceptible to overfitting when training data is limited.	{"tags": ["architecture", "deep-learning", "overfitting", "NLP"], "doc_id": "e204831a-9a90-4fb9-905d-6ea2d3d9707a", "summary": "Unlike convolutional and recurrent networks, Transformers do not assume any structure in the data, making them highly versatile but also prone to overfitting when data is limited.", "doc_type": "text", "entities": ["Transformer", "convolutional networks", "recurrent networks"], "keywords": ["inductive bias", "overfitting", "Transformer architecture", "data structure"], "key_objects": ["Transformer architecture", "data structure", "inductive bias"], "contextual_text": "A key difference between Transformers and convolutional or recurrent networks is that Transformers do not incorporate an inherent inductive bias of locality and make no assumptions about how the data is structured. This versatility allows Transformers to capture dependencies across various ranges, but also makes them susceptible to overfitting when training data is limited.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "7 PRE-TRAINED TRANSFORMERS"}, "hypothetical_questions": ["What is meant by 'inductive bias of locality'?", "How does the lack of inductive bias in Transformers affect their ability to generalize?", "What are some potential methods for introducing inductive bias into Transformer models?"]}
b43a15bb-ab1a-4770-8b7d-74b0af4316b5	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.023636121,-0.009079218,0.030346349,-0.0033589678,0.015022269,0.060975563,0.0189167,0.058726158,0.025062708,-0.02164531,0.01669966,0.029273516,0.037199527,0.001990465,-0.061942607,0.029563544,0.011546759,0.09556115,0.008845417,-0.004368096,0.042080965,0.016160637,0.010333407,-0.009226853,0.017234748,0.0019140777,0.066147014,-0.059718985,0.0044156844,0.016934963,-0.012396028,-0.0023426614,0.007409177,-0.0031669675,0.025850622,-0.00041769358,0.022941457,-0.01928755,-0.008878914,-0.032102175,-0.02398293,0.0634169,-0.06412378,-0.0015834676,-0.018074846,-0.077057496,-0.113497764,-0.027221648,0.02153873,-0.034740426,-0.011374635,0.00441118,-0.054565437,-0.01551011,0.022111632,-0.007799171,-0.016170332,-0.01997744,0.027381917,-0.07282693,-0.073213845,0.035471234,-0.019837081,0.040714774,0.02521157,-0.03360798,0.02831609,-0.02319351,0.024630213,0.104902916,-0.04322534,0.015588109,-0.010616788,-0.036634378,0.1197205,0.030339656,-0.014072757,-0.031784244,-0.050822057,0.021464985,-4.5583142e-06,0.07395634,-0.020822491,-0.066260345,0.08802672,-0.032546982,-0.07323534,0.028804123,0.0370638,0.021367095,-0.035845563,-0.009023764,-0.004067421,0.08298245,-0.03832015,-0.073708236,-0.002440313,-0.025585614,0.037559226,-0.02678201,0.014443097,-0.020284716,0.11190357,0.059922293,0.02552631,-0.02480538,-0.032480113,-0.0019038062,0.024335403,0.0116815185,-0.0036303739,-0.014371453,-0.012029585,-0.039998196,-0.025392774,0.02768041,-0.035336226,0.004053658,0.026290422,0.018779093,-0.025266841,-0.024479201,0.047643095,-0.041042786,0.045354664,0.07249629,-0.0037714764,-0.038727507,-0.010095839,0.049213067,0.049017947,-0.03549632,-0.01788775,0.045952156,0.022217065,0.044331674,0.05308395,-0.03227488,-0.0008484083,0.013975613,0.029059788,-0.07970793,-0.028435359,0.0136971865,-0.037464842,0.013179132,-0.07540261,-0.009845094,-0.033580437,0.04584636,0.024396619,-0.010873077,0.008613739,-0.010954447,-0.0049702106,-0.009566935,-0.010603277,-0.061359823,0.013128571,-0.009002136,-0.056294244,-0.044005334,-0.04027212,0.05166772,0.034723863,0.09996323,0.03709397,-0.0018278083,0.06494594,0.01570857,0.013576553,0.027832465,-0.004880278,-0.03532448,-0.013682532,0.049414523,0.008773502,-0.031534854,-0.010226829,-0.010557945,0.0068606376,-0.0013070349,0.013218559,0.032086547,-0.063743025,0.025646336,-0.011863982,0.03726804,-0.024571104,-0.02596482,0.037414648,0.0066942186,-0.018725904,0.018825058,-0.017623961,0.031151421,0.040693574,-0.053119387,-0.075610906,-0.019721564,0.02729072,-0.016393268,0.025505116,-0.021752879,0.020744262,0.0007608152,0.0151454015,-0.03454167,0.0036622614,0.007574481,-0.0098255165,-0.009566517,-0.03146052,0.04034697,0.032919236,-0.011569741,-0.036787465,0.03458831,0.04073279,-0.0006334868,-0.05877572,-0.015665539,0.008120178,0.0039391597,-0.041014686,-0.0012325089,-0.04394449,0.013524133,-0.024253078,-0.0011723926,0.014767084,-0.00729387,-0.00043686337,0.046446905,-0.027282113,0.05779975,-0.019149901,0.007031141,-0.023617594,0.039361082,0.05188275,0.032193277,-0.0150871305,0.07242485,-0.0006705233,-0.0010112802,0.05207397,0.025871025,0.018893886,0.061148774,0.0047688987,0.022893757,0.066238634,-0.003456207,0.003934582,-0.060336016,0.0030086476,0.013460303,0.051577684,0.015489969,0.0021167353,0.02409959,0.033412103,0.00086172624,-0.035384353,-0.0069655795,0.04267358,0.041778907,-0.00018786702,-0.035701133,0.04961997,0.031244706,-0.035983052,-0.03607255,-0.046491507,0.030101648,0.015154545,-0.012251322,0.033675894,-0.061094664,0.00014597681,0.0128311245,0.04982744,-0.01776986,-0.0010209327,-0.042131633,0.0056673293,-0.024845256,-0.050286554,0.005369719,0.007971542,-0.07612927,0.031580728,0.003947075,-0.035445005,0.0077163517,-0.009932522,0.0069451276,-0.024930326,0.024769735,0.09783529,-0.07697368,0.02323496,-0.011677763,0.00779243,0.044156104,-0.0921544,-0.020062082,0.026876437,0.00587825,0.008450354,0.012058749,-0.016080212,-0.023600798,0.04532164,0.060389455,0.067264944,-0.012514915,0.013631971,0.0053697717,0.02692245,-0.0076392842,0.02368683,0.03564248,0.01431426,-0.04461257,-0.008298531,-0.021921592,-0.07768523,0.053033788,-0.046175208,0.057743274,0.04101416,-0.020238332,-0.029488979,0.026350848,0.001907315,0.08011928,-0.0038541933,0.049397703,-0.007839025,-0.085808076,0.007728482,0.0142294485,-0.009599438,0.028845748,-0.02195609,0.013438693,0.005137386,0.015167062,-0.01553782,0.008544784,0.016108084,0.004960879,-0.011815956,-0.04690479,-0.024279313,-0.013000671,-0.021981241,0.0021534893,0.046235528,-0.027277652,0.06634842,-0.0077603417,0.030265179,-0.031197434,-0.06327506,-0.025005575,0.010974193,-0.02551406,-0.010478909,-0.008551307,0.03805825,-0.028327327,0.004157125,0.013768996,0.038256362,0.01962673,-0.014109915,-0.013661839,-0.021113044,-0.0010248467,-0.059792902,-0.040372264,-0.013150709,0.025749914,0.0008091269,0.061873496,0.017380627,0.03922848,-0.041337732,-0.004165107,0.028296022,0.0018107548,-0.0034625502,-0.024011733,0.026865287,0.026777329,-0.024018236,-0.032562513,0.00032939628,0.012620924,-0.045016818,0.07597785,0.061858848,0.049100522,-0.04140037,0.048952434,0.027359653,0.04956872,0.03885201,-0.0375764,-0.057709787,0.01557047,-0.040678177,0.026122428,-0.028543241,0.010507068,-0.044792995,-0.002588022,-0.014033207,0.01628588,0.023423677,0.06663955,4.420178e-05,-0.016423978,-0.029004704,-0.004725252,-0.024035862,-0.044730075,-0.009194276,-0.074714795,-0.007206927,0.079179764,0.029379988,-0.002296798,-0.035521485,0.057307106,-0.06689169,0.027067462,0.05905021,-0.02196475,-0.012274699,0.0151692955,0.02853557,-0.0066732997,0.029230665,-0.0117175765,-0.025808103,0.0018026719,0.0224532,0.011222434,-0.0050242157,-0.003788565,0.0262825,-0.037461832,0.005477035,-0.013979989,-0.056320693,-0.028530266,-0.008031043,0.03391806,-0.026760867,0.013528311,0.005909142,0.0021257754,-0.021406485,-0.048674073,0.0051856902,0.01826736,-0.003768811,0.02682299,-0.049081918,0.0027006345,-0.0142793935,-0.028457051,0.014744374,0.0348722,0.002796505,-0.003626581,-0.027068686,-0.07304004,-0.01639664,0.04032332,-0.022328403,-0.0056692353,0.0015392946,-0.031083666,-0.040626835,0.0003093026,-0.016495634,0.05105097,0.011241404,0.030312339,0.028194161,0.00021512776,-0.029834788,0.029879417,0.029181337,-0.028149175,-0.039170876,-0.007692925,0.08706094,0.046904583,-0.03091667,0.027614553,-0.010433171,0.014042988,-0.01338573,0.011724414,-0.025519257,-0.02010445,-0.0073011806,-0.008358198,0.0035347606,0.02306559,0.01924258,-0.0059095616,-0.019085519,0.04329907,-0.04905251,0.002231557,-0.03016019,-0.01914679,0.015608787,-0.01752866,0.02033064,0.039044093,0.03783167,-0.071468666,0.02945222,-0.0071757156,-0.059886407,0.009610818,-0.028571408,0.011295764,0.04122798,-0.043311846,-0.012569849,0.029122913,-0.010898324,-0.03039074,0.04098617,-0.014843731,0.07310328,-0.07307206,0.0173875,-0.0124079045,0.009393656,0.08163628,0.0039250995,-0.041878402,-0.026368644,-0.042267706,-0.053728756,0.031549253,0.054334745,0.022919614,-0.019949326,0.074643694,0.012754255,0.046818636,-0.043005783,-0.03510449,-0.009576856,-0.0043144813,-0.023253674,0.0035149725,0.016408047,0.00082627777,0.03668243,-0.021583093,0.016177354,0.018736148,-0.012650467,0.016638305,0.011927653,0.008619603,0.042224813,-0.019140378,0.021033755,0.043653034,0.010801998,0.027092483,-0.035125427,-0.0041380324,0.0012248664,-0.05573741,-0.03204097,-0.01236439,-0.024984566,-0.022374582,0.01417208,0.041140936,-0.021321133,0.0038781564,0.04276689,0.013497978,-0.027420081,-0.021855077,-0.071581274,-0.034114532,-0.086743765,-0.096681416,0.04509096,-0.012650881,0.017348219,0.042248208,-0.047308024,0.10750385,-0.07215396,-0.0027847306,-0.052378017,0.05256981,-0.0010259701,-0.0418871,0.027478943,-0.026472485,-0.03030538,-0.043642815,0.040158894,0.056243226,-0.018973796,0.01506156,0.009553882,0.026430193,-0.02542484,-0.009903277,-0.02944892,-0.0121298665,0.011942591,-0.028016303,0.07488601,0.0016134181,-0.008718731,0.048775185,-0.017650846,-0.06595625,-0.0019859932,0.052857943,0.011808012,0.047523733,0.021848887,0.037392043,0.058150403,-0.06504502,-0.03421933,-0.040032044,-0.028594324,-0.010646516,0.03991217,0.018790444,-0.023946887,-0.049353004,0.057781264,-0.0046326555,-0.07040326,0.0033579215,0.019386068,0.04396395,0.058792405,0.040351853,-0.023056094,-0.071693465,-0.026387736,-0.047161814,0.0017311418,-0.032901954,0.023565046,-0.021771545,0.0024780123,-0.01365454,0.033961598,0.010765669,0.018135305,0.0040872605,-0.008223866,-0.05310407,-0.0282831,-0.005730916,0.04137802,-0.009016872,-0.010114629,0.089356475,0.038848553,-0.057094973,-0.04908755,-0.037581798,-0.019274129,0.0036883822,-0.053248737,0.042148788,0.06697699,0.021983815,0.012411529,0.054341443,0.010242698,-0.002145066,-0.06039134,0.039832406,0.016061824,0.030011998,0.018984746,-0.020218724,-0.11883917,0.020103108,-0.004732915,-0.07172878,0.036288682,0.018091565,-0.0074886563,-0.020770688,0.034402907,-0.032367863,-0.030186724,0.005534351,-0.019869193,0.029259048,0.0050819134,-0.010533846,-0.04319851,-0.056289546,-0.018263116,-0.032368265,-0.032849293,0.061545823,-0.039659195,0.00883308,0.008594189,0.034072027,0.0040775947,0.014779864,-0.026284412,0.0681917,-0.050156802,0.044606328,-0.008216082,-0.00919419,0.046334963,0.04532053,0.0486287,-0.049085513,0.02052048,-0.021311143,0.019999208,-0.0008773899,-0.04709909,-0.026910502,0.024152268,0.0075314282,-0.02001436,-0.013971555,-0.016804324,-0.044339508,-0.047414117,-0.014672205,-0.007487332,0.06568763,0.023811532,-0.0038383994,-0.0625493,0.033775907,-0.03233638,0.03560171,-0.0023344476,-0.01098851,0.007349171,0.021930983]	Keywords: pre-trained models, language representations, downstream tasks, fine-tuning\nKey Objects: Transformer Models, Language Representations, Self-supervised objectives\nRefers to Images: None\nHypothetical Questions:\n- What are some specific examples of self-supervised objectives used in pre-training Transformer models?\n- How does fine-tuning a pre-trained model compare to training a model from scratch in terms of time and resources?\n- Why is it advantageous for Transformer models to learn universal language representations?\n---\nSummary:\nTransformer models pre-trained on large datasets can learn general language representations that improve performance on specific downstream tasks, allowing for fine-tuning instead of training from scratch.\nOriginal Text:\nRecent studies suggest that Transformer models that are pre-trained on large corpora can learn universal language representations that are beneficial for downstream tasks [100]. The models are pre-trained using various self-supervised objectives, e.g., predicting a masked word given its context. After pre-training a model, one can simply fine-tune it on downstream datasets, instead of training a model from scratch. To illustrate typical ways of using Transformers in pre-training, we identify some of the pre-trained Transformers and categorize them as follows.  \n-  Encoder only. A line of work uses the Transformer encoder as its backbone architecture. BERT [28] is a representative PTM that is typically used for natural language understanding tasks. It utilizes Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) as the self-supervised training objective. RoBERTa [87] further adapts the training of BERT and removes the NSP objective as it is found to hurt performance on downstream tasks.\nContextualized Text:\nTransformer models, known for their flexibility, can be pre-trained on large datasets. These models learn general language representations that are beneficial for downstream tasks, allowing for fine-tuning instead of training a model from scratch. For instance, BERT and RoBERTa exemplify this approach, utilizing techniques like Masked Language Modeling and Next Sentence Prediction.	{"tags": ["NLP", "pre-training", "transformers", "deep-learning"], "doc_id": "b43a15bb-ab1a-4770-8b7d-74b0af4316b5", "summary": "Transformer models pre-trained on large datasets can learn general language representations that improve performance on specific downstream tasks, allowing for fine-tuning instead of training from scratch.", "doc_type": "text", "entities": ["BERT", "RoBERTa"], "keywords": ["pre-trained models", "language representations", "downstream tasks", "fine-tuning"], "key_objects": ["Transformer Models", "Language Representations", "Self-supervised objectives"], "contextual_text": "Transformer models, known for their flexibility, can be pre-trained on large datasets. These models learn general language representations that are beneficial for downstream tasks, allowing for fine-tuning instead of training a model from scratch. For instance, BERT and RoBERTa exemplify this approach, utilizing techniques like Masked Language Modeling and Next Sentence Prediction.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "7 PRE-TRAINED TRANSFORMERS"}, "hypothetical_questions": ["What are some specific examples of self-supervised objectives used in pre-training Transformer models?", "How does fine-tuning a pre-trained model compare to training a model from scratch in terms of time and resources?", "Why is it advantageous for Transformer models to learn universal language representations?"]}
b8f2edaa-0ee0-41ab-8107-b87d57384133	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.010746427,-0.008033965,0.055507928,0.021785893,0.020463653,0.0708693,-0.032289874,0.070729434,0.046642676,-0.039818116,0.0064867223,0.041166093,0.038858548,-0.009967204,-0.045092832,0.052097503,0.0020621195,0.09500517,0.006151991,-0.025374688,0.031219438,-0.0014294675,-0.0061721117,-0.015831776,0.018101541,0.02769137,0.044192187,-0.044679765,-0.0029966938,-0.009172339,-0.0042737313,-0.0061630774,0.0048431703,-0.009389044,0.02593255,0.017404284,-0.010881894,0.008207065,0.0016335981,-0.017714765,0.0042766975,0.090539604,-0.06149469,0.0027958825,-0.012956204,-0.0785723,-0.0977778,-0.044420343,0.0072963554,-0.035444085,-0.012565861,-0.014661406,-0.046658363,-0.015086114,0.043158974,0.013061652,0.008460752,-0.022926288,0.00036244583,-0.074711375,-0.047786236,0.006200745,-0.021131543,0.021162635,0.03252705,-0.04000447,0.008238312,-0.009544746,-0.013473911,0.11396658,-0.041893005,0.02683653,-0.01382408,-0.049044613,0.107391596,0.055043567,-0.016581034,-0.048092358,-0.06547154,-0.029104618,-0.0006849546,0.07647801,-0.03312552,-0.065965414,0.06978561,-0.017796112,-0.07092793,0.0115420045,0.05823089,-0.052711777,-0.014544766,0.0022268659,0.00037207728,0.09726723,-0.053770382,-0.08348304,0.00078559044,-0.030938972,0.041715864,-0.03179258,0.01312984,-0.060223944,0.08166612,0.0802775,0.009270334,2.4721996e-05,-0.009093738,-0.028540824,0.023284886,0.039248966,-0.014947241,-0.018648507,-0.062439386,-0.045622822,-0.033050753,0.00029392896,-0.022432966,0.0013801676,0.0081537925,0.014936407,-0.024775503,-0.02241222,0.015741024,-0.040570017,0.075110935,0.062254246,0.007621215,-0.010800924,-0.017207978,0.016588492,0.0255644,-0.018613234,-0.010111161,0.052814063,0.011327421,0.042240765,0.035929304,-0.012032403,0.017844602,0.025692172,-0.0018044107,-0.031886198,-0.040780853,0.010379621,-0.020511294,0.020179128,-0.049571637,-0.033232983,-0.029425194,0.06940389,0.011559015,-0.02352737,0.02939346,0.016742222,0.0016265722,-0.025475763,-0.011193674,-0.03653773,-0.01663488,-0.0013025381,-0.0780605,-0.0382984,-0.04143176,0.03142106,0.033897407,0.09884286,0.027232096,0.012604543,0.048853844,-0.0071317293,-0.019713849,0.0184151,0.0018960572,-0.038420018,0.0003734295,0.0016389021,0.028101321,-0.04001078,-0.034049097,0.008661262,-0.016844925,0.039246906,0.040249422,0.048176125,-0.043344427,0.027941456,0.035033554,0.06525683,-0.04150021,-0.024722982,0.06498554,0.017816735,-0.0029034265,0.014274891,0.022943871,0.03200406,0.022618016,-0.060858514,-0.044590265,-0.013002907,0.010544214,-0.015894424,0.014299148,-0.024068952,0.003187962,0.004547292,0.008733426,-0.019027675,0.0033307576,0.021063237,-0.016611347,-0.0017196303,-0.029947907,0.03984065,0.024016347,0.0062178965,-0.037759587,0.012079086,0.046524566,-0.017482791,-0.0665723,-0.004167204,0.02549721,-0.012521404,-0.01321079,0.000114359165,-0.013462367,0.031661503,-0.030483684,-0.024849411,0.0254699,-0.0019747363,-0.021296408,0.048422582,-0.013228326,0.06929344,-0.008156753,0.00894638,-0.033726417,0.049573723,0.077539995,0.02799052,-0.017682074,0.060898203,-0.0009319976,-0.015667282,0.057171382,0.03140425,0.0298185,0.039309558,0.009357373,0.06276329,0.051394697,0.010957443,0.010747318,-0.03660103,-0.008760362,0.011451931,0.04006221,0.009073622,-0.028142028,-0.0051252735,0.0037712348,0.0043133674,-0.045486383,0.01670432,0.049285572,0.038342863,-0.014303633,-0.037490226,0.047419894,0.04076418,0.009134118,-0.072202764,-0.06283582,0.028721176,-0.014130558,-0.0090763485,0.03223031,-0.026268898,0.0057288194,-0.011505774,0.037431598,-0.017781269,0.008398639,-0.0059607774,-0.01873542,0.0021566495,-0.0524708,0.016699813,0.040144317,-0.03757883,0.0487625,0.02369244,-0.012369751,0.023256628,-0.0036083846,-0.049429957,0.009216821,0.01296979,0.08011361,-0.07596196,-0.00019095158,0.014325124,-0.0005781653,0.028604146,-0.03968442,0.013247317,0.020028718,0.022834312,0.0065098125,-0.0033029893,-0.001500804,-0.010783033,0.017145151,0.06853555,0.08368886,-0.023589583,0.018188749,0.00393367,0.007885532,-0.019733382,-0.01708815,0.023822473,0.018663011,-0.051213108,-0.009664274,-0.008744576,-0.114431,0.06353638,-0.054386865,0.02117932,0.023742884,0.009413696,-0.004043584,0.0042968565,0.032160368,0.04013701,-0.006795687,0.047879845,-0.025713822,-0.0575447,0.032075163,0.002309108,-0.023425996,0.00727638,-0.016541854,-0.0024238238,0.035930585,0.02377929,-0.036781617,0.023689697,0.016432771,0.017870026,0.012755263,-0.01784164,-0.008988927,0.007400084,-0.024176642,-0.00054462254,0.043479852,-0.05052828,0.033833895,-0.020937134,0.020683538,-0.01138196,-0.044249322,-0.014718614,0.033104125,-0.034248043,-0.01241364,0.007334269,0.037307255,-0.037465017,-0.00056709425,-0.005955928,0.03678591,0.01935079,-0.039199766,-0.018118132,-0.0026775983,-0.00990342,-0.050241143,-0.032436796,-0.029908687,0.028172052,0.019362116,0.07803884,-0.008930086,0.0462891,-0.045666374,0.00574856,0.045287244,0.017284032,-0.011065768,-0.015437993,0.021275913,0.028980847,-0.023844648,-0.015876764,0.029842837,0.0058952644,-0.0539524,0.057144698,0.04667134,0.02679519,-0.04228332,0.0468948,0.042273026,0.058374915,0.039985117,-0.022434082,-0.040926337,0.012116216,-0.0009868789,-0.0045022736,-0.012521735,0.051465254,-0.038372748,-0.013748991,-0.01803781,0.016237825,0.023542656,0.058994826,-0.0020808266,-0.029624423,-0.020773577,0.0027618688,-0.041446004,-0.054276224,0.0034286308,-0.059608,0.01863537,0.057477914,0.016961437,-0.0059367022,-0.04899893,0.067307435,-0.11216035,0.03239421,0.036178842,-0.019925814,-0.02106601,-0.0064583225,0.020512844,0.047399998,0.0021806178,0.005979302,-0.032726903,-0.009238591,0.03503701,0.0044694757,-0.006043782,0.016144099,0.026499974,-0.025512913,0.0046614353,-0.022765387,-0.10403713,-0.00088829227,-0.02386665,0.020545727,-0.035319075,0.008467901,0.031755615,-0.017389966,-0.01237707,-0.02458594,-0.0014986047,0.011830344,-0.013803193,0.023244575,-0.039398067,0.019497242,-0.0241902,-0.030430533,-0.024458187,0.04653279,0.021171324,-0.008436484,-0.04682468,-0.06472186,-0.0328916,0.020877043,-0.042273253,-0.032234028,-0.030617636,-0.03482155,-0.026261391,0.043109454,-0.028032608,0.031155476,0.027864834,-0.007883091,0.01579987,-0.036798034,-0.0114501035,0.01873704,0.0047530113,-0.055222467,-0.022665363,-0.004664168,0.100936,0.05378424,-0.015277921,-0.0073559163,0.016218802,0.012873944,0.014500506,0.010005643,0.036074404,-0.006276987,-0.010464081,-0.025509879,-0.015380057,0.030200915,0.00017627829,-0.014028096,0.00013115416,0.05600078,-0.017407788,0.031042961,-0.006050591,-0.008251011,0.025416119,-0.00518026,0.02140511,0.04423402,0.0803146,-0.07060464,0.021276506,-0.022597821,0.0021196736,0.017253907,-0.038299803,-0.025653388,0.021955425,-0.056586783,-0.017321162,0.028178385,0.010882451,-0.028217703,0.028655363,-0.01386087,0.06709759,-0.07105497,0.021165539,-0.024018314,-0.0012159521,0.09341862,0.034365904,-0.024395954,-0.005033566,-0.03847077,-0.012676044,0.019678306,0.057664715,0.0035355058,-0.013777023,0.07940568,-0.0094322795,0.030373273,-0.054670043,-0.011278981,-0.024899699,-0.038831294,-0.027705722,0.0002324156,0.03878835,0.024262687,0.029561136,-0.0068372176,0.018356033,0.0634906,-0.017409122,0.032998245,0.016448272,-0.0008758778,0.012817675,-0.013108123,0.069315635,0.03103722,-0.0029849107,0.008121372,-0.006827864,0.0033804434,0.03310062,-0.034371633,0.0068744263,-0.011779101,0.0022886402,-0.005548242,0.026451088,0.0055476227,-0.030871836,-0.013395594,0.037594803,0.031618793,-0.013597505,-0.019830588,-0.077810094,-0.020327548,-0.07403035,-0.074919514,0.028482923,-0.01957168,0.008619519,0.049078822,-0.050742667,0.08125459,-0.11810044,0.009027689,-0.04272328,0.03082411,-0.0031163488,-0.033614535,-0.0008630635,-0.0046336735,-0.03144664,-0.05839096,0.03520001,0.019002553,-0.009075437,0.011177471,0.008932949,-0.0010216342,-0.014681503,0.005402965,-0.016240958,-0.0017089371,0.010357048,-0.02318105,0.089412056,-0.008281996,-0.01841771,0.051872134,0.00025606062,-0.08767085,0.022976842,0.024699258,0.001571495,0.042612612,0.008773758,0.04750287,0.07435972,-0.051291376,-0.032165308,-0.06273777,0.01823938,-0.006040708,0.04712036,0.03851774,-0.033065103,-0.085191205,0.0019980832,0.024734557,-0.041555636,-0.0019185619,0.026129572,0.041272454,0.022947105,0.047235392,-0.060262583,-0.09051454,-0.041475732,-0.07804276,-0.013696705,-0.025350258,0.010873665,-0.016494412,-0.027689645,-0.008548553,0.028404979,-0.002622338,-0.008111273,0.010503592,-0.021792384,-0.033384327,-0.032761134,-0.023244575,0.03719386,0.00043830916,-0.010272184,0.08674482,0.0449572,-0.04902925,-0.06367658,-0.020188842,0.006653247,-0.0048912056,-0.031211471,0.01795544,0.023790471,-0.012321057,0.030020278,0.06003609,0.010788249,-0.015415332,-0.04071961,0.022418525,0.0202826,0.015581193,0.031563267,-0.048416495,-0.09835446,0.023448657,-0.027152918,-0.08485047,0.015550972,0.035418328,0.011878079,-0.024969025,-0.031050114,-0.04682091,-0.021521619,0.008561668,0.0089861415,-0.0109730465,0.029526427,0.001506387,-0.029740177,-0.015552266,-0.049154468,-0.024650123,-0.014009796,0.06901693,-0.015012088,0.009692556,0.0020894313,0.011898626,-0.015823254,-0.0030537168,-0.027658612,0.076121345,-0.06490053,0.040855665,0.003186208,-0.012605104,0.061111916,0.030729648,0.023826232,-0.03465678,0.012472837,-0.0048369654,-0.002457027,0.020076053,-0.047186173,-0.040014453,0.023831664,-0.018132443,-0.0123477075,0.010016076,-0.02919304,-0.049512204,-0.06069743,-0.043907754,-0.0092614675,0.07494788,0.020765426,-0.0026846684,-0.06492133,0.014192636,-0.02027076,0.029015172,0.014081409,0.002052274,0.0059346333,0.012617408]	Keywords: Transformer decoders, language modeling, GPT series, few-shot learning, prompts\nKey Objects: Transformer decoders, language modeling\nRefers to Images: None\nHypothetical Questions:\n- What advantages does pre-training Transformer decoders on language modeling offer?\n- How do prompts enable few-shot performance in models like GPT-3?\n- Why is scaling pre-trained Transformer decoders a key factor in achieving impressive results?\n---\nSummary:\nSeveral studies have focused on pre-training Transformer decoders on language modeling, with the Generative Pre-trained Transformer (GPT) series, including GPT, GPT-2, and GPT-3, demonstrating impressive few-shot performance through scaling pre-trained decoders and using prompts.\nOriginal Text:\n-  Decoder only. Several studies focus on pre-training Transformer decoders on language modeling. For example, the Generative Pre-trained Transformer (GPT) series (i.e., GPT [101], GPT-2 [102], and GPT-3 [12]) is dedicated to scaling pre-trained Transformer decoders and has recently illustrated that a large-scale PTM can achieve impressive few-shot performance with the task and examples fed to the model as constructed prompts [12].  \n-  Encoder-Decoder. There are also PTMs that adopt Transformer encoder-decoder as the overall architecture. BART [72] extends the denoising objective of BERT to encoder-decoder architecture. The benefit of using an encoder-decoder architecture is that the inducing model is equipped with the ability to perform both natural language understanding and generation. T5 [104] adopts similar architecture and was one of the earliest studies that use task-specific text prefix in downstream tasks.\nContextualized Text:\nResearchers have explored pre-training Transformer decoders using language modeling techniques. The Generative Pre-trained Transformer (GPT) series, including GPT, GPT-2, and GPT-3, has become notable for scaling these decoders and demonstrating strong performance in few-shot learning scenarios, where the model utilizes prompts to guide its responses.	{"tags": ["NLP", "pre-training", "transformers", "architecture"], "doc_id": "b8f2edaa-0ee0-41ab-8107-b87d57384133", "summary": "Several studies have focused on pre-training Transformer decoders on language modeling, with the Generative Pre-trained Transformer (GPT) series, including GPT, GPT-2, and GPT-3, demonstrating impressive few-shot performance through scaling pre-trained decoders and using prompts.", "doc_type": "text", "entities": ["GPT", "GPT-2", "GPT-3", "BART", "T5"], "keywords": ["Transformer decoders", "language modeling", "GPT series", "few-shot learning", "prompts"], "key_objects": ["Transformer decoders", "language modeling"], "contextual_text": "Researchers have explored pre-training Transformer decoders using language modeling techniques. The Generative Pre-trained Transformer (GPT) series, including GPT, GPT-2, and GPT-3, has become notable for scaling these decoders and demonstrating strong performance in few-shot learning scenarios, where the model utilizes prompts to guide its responses.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "7 PRE-TRAINED TRANSFORMERS"}, "hypothetical_questions": ["What advantages does pre-training Transformer decoders on language modeling offer?", "How do prompts enable few-shot performance in models like GPT-3?", "Why is scaling pre-trained Transformer decoders a key factor in achieving impressive results?"]}
86e47b9e-e89c-4113-af11-057f6ab68af0	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.006764621,0.00046308653,0.026799181,0.029706512,-0.0013546591,0.059579734,-0.0075624627,0.042385094,0.03958039,-0.011207127,0.016177421,0.024276637,0.0013924201,-0.012500733,-0.0357924,0.038219582,0.030040514,0.03953796,0.032517385,-0.03801791,0.040617507,0.015713353,0.014720494,0.019140938,-0.0036442212,0.061612047,0.0048314743,-0.017248599,0.010140278,-0.005801214,0.05043417,-0.06862733,0.030628774,0.0048766634,0.02895356,-0.008617393,-0.009141817,-0.05344164,0.00030789754,-0.012259741,0.015788913,0.068714224,-0.06061994,-0.02522917,-0.010849598,-0.07562508,-0.097578995,-0.04781318,0.02565553,0.0038156651,-0.016520616,0.034535028,-0.01842167,-0.013523045,-0.012023778,0.00017249378,0.019427922,-0.037908886,0.024928248,-0.10409486,-0.04198826,0.019608336,-0.015156207,0.032467455,0.0011159193,-0.0016400613,0.008866721,0.027893672,0.013720888,0.12410096,-0.015590709,0.005991456,-0.034089487,-0.05625324,0.10354408,0.084677644,-0.036111023,-0.0041528377,-0.063908644,-0.043306395,-0.013399884,0.06086879,-0.025815878,-0.05304778,0.064811334,-0.012230825,-0.027963443,0.016431285,0.05115589,-0.042188793,-0.010075799,-0.003228,0.015133509,0.08163696,-0.045367207,-0.05615968,-0.00960428,-0.053668458,0.04665251,-0.05648478,0.007869243,-0.022343254,0.08661332,0.11843686,0.028657015,-0.025846964,0.015743628,-0.03533232,-0.0005036524,-0.011174015,-0.030562222,0.030512583,-0.035700064,-0.009465452,-0.026099741,0.008229461,-0.032340102,0.014561023,-0.01475432,-0.0009008605,0.008173116,-0.022242483,0.002979321,0.0344337,0.06036338,0.03242655,0.052331444,-0.020189755,-0.013368156,0.018649718,0.06072317,0.002222383,-0.04801706,0.029100295,-0.016550783,0.018458927,0.07460915,-0.030360881,-0.0025801533,0.013174193,-0.0010029045,-0.020217089,-0.03850773,-0.016269373,-0.031299815,0.02400925,-0.041075576,0.009321203,-0.019249177,0.044321112,0.03685228,-0.021604989,0.047631614,-0.016863726,-0.022804525,-0.03945923,0.0021984202,-0.050315533,0.003438295,-0.024107302,-0.063727826,-0.055742327,0.0038737745,0.05157922,0.04436617,0.10984727,0.028049031,-0.0037882943,-0.0024027543,0.008449601,-0.0546265,0.008744548,-0.007072328,-0.039292462,0.01546662,0.03132904,0.027275557,-0.03199941,-0.0066432334,-0.010927729,-0.0011296892,0.051661048,-0.0064600036,0.016861113,-0.04111329,0.02438468,-0.0010440694,0.06363296,-0.016649948,-0.040754687,0.019241108,-0.03313569,-0.016282126,0.019768,0.047144677,0.071728654,0.031784467,-0.0469292,-0.04998126,-0.03812624,-0.009379132,-0.0018405011,-0.0052310135,-0.018139794,0.03725212,-0.038488496,0.032120045,-0.056104254,-0.010044129,0.046439733,-0.016573507,-0.0020545088,-0.03263259,0.06600329,0.06538676,0.0150275575,-0.03161201,0.03886188,0.026513604,-0.022605088,-0.05540258,-0.007870238,-0.019428987,0.03619076,-0.024713296,0.028028158,-0.031954937,0.016345115,-0.018161343,-0.040083297,0.010398881,7.86726e-05,-0.043317493,0.033410594,-0.016655883,0.041710578,0.022427367,0.024946902,-0.024867112,0.023400774,0.06479399,0.07003647,-0.027302865,0.065187566,0.0057814093,0.021979721,0.009635968,0.037465442,0.058033653,0.034884997,-0.032523282,0.06439415,0.03888331,-0.019805022,0.007224818,-0.038397655,0.02241712,0.0037003935,0.014977549,-0.006481588,0.00013531931,-0.012466403,0.017734976,0.036131516,-0.01642342,-0.0006473578,0.020496901,0.024365304,-0.023038981,-0.00027515425,0.027450068,0.039124403,0.02931374,-0.019263424,-0.04204986,0.045292284,-0.011079994,0.004076345,0.014952238,-0.02430895,-0.0017897815,-0.014422199,0.032294285,-0.0061264685,0.03346881,0.014085128,0.02819149,-0.014057711,-0.05256572,0.026493466,-0.002274318,-0.06797072,0.027192177,-0.0111465845,-0.014286399,-0.028600605,0.0051820017,-0.04844174,0.005624201,0.016680062,0.11184838,-0.036385756,-0.014832458,0.007930355,0.00580178,0.024466362,-0.046037603,0.021456106,0.02669843,0.0036775821,0.007484585,-0.009239261,0.002237496,0.001098016,0.013952655,0.07095516,0.08157661,0.001228858,0.007112135,0.006963963,0.020751525,-0.010965581,-0.011549336,0.015410731,0.047528975,-0.029426843,0.035267085,-0.060931735,-0.08898372,0.07497816,-0.07632578,0.043977324,0.037605777,-0.020219143,0.0066593876,-0.0049356115,0.009838187,0.03656263,0.035768673,0.030482993,-0.023879338,-0.05787332,0.027714113,-0.010813276,-0.039230004,0.049764168,-0.012306593,-0.016566034,0.051598143,0.014873621,-0.007177683,0.04679444,0.015737455,0.019216828,0.012569712,-0.016041992,0.011919607,-0.0031118419,0.0048510246,-0.012370414,0.05519909,-0.06834536,0.08488694,-0.052270662,0.00625392,-0.033473145,-0.06092645,0.019625315,0.053742945,-0.025441997,-0.0009270699,-0.0060738735,0.056395132,-0.018719101,-0.015055878,0.033684533,0.04621724,0.04187179,-0.036814,-0.030074237,-0.010464757,-0.0054073,-0.06339566,-0.030043725,-0.0025414217,0.0115225455,-0.00016247392,0.0939399,-0.010037627,0.05247228,-0.043960504,0.036957614,0.021275096,0.015580085,-0.03214211,-0.012841286,0.035313636,0.06338169,0.0030447824,0.010158442,0.064461045,0.04079723,-0.058043428,0.01824758,0.0373487,0.0056333398,-0.07846264,0.034971688,0.027311554,0.027032671,0.05150789,-0.048440404,-0.026860736,-0.0014323926,0.015496944,-0.010696968,-0.014909121,0.03985452,-0.037150122,-0.029459035,-0.015087468,0.04400367,0.020410532,0.0328571,0.032709397,-0.0073406235,-0.04064795,0.025551142,-0.006213332,-0.05235253,0.021788543,-0.032156173,-0.026383096,0.04991096,-0.005735588,-0.0036036922,-0.042112034,0.0855989,-0.05440584,-0.025984302,0.062289733,-0.05776513,-0.0024291596,0.008872598,0.006884195,-0.019550513,-0.01317202,0.024434235,-0.01818598,-0.008988707,0.059593476,0.034757085,0.01356443,-0.0076384023,0.05119567,-0.012336651,-0.0044927765,-0.020078454,-0.065203466,0.012095572,0.021064563,0.052896246,0.0047505903,-0.00051034056,0.0013495856,0.0019206747,-0.022069048,-0.024893299,-0.013484933,-0.010778162,0.008244365,0.029479366,-0.015976747,0.06653231,-0.005220752,-0.022727016,-0.021276396,0.053213216,0.04295189,0.0076291044,-0.03803268,-0.03704454,0.0013284368,0.04010036,-0.003319104,-0.0017285949,-0.005846169,-0.013301818,-0.015598715,0.023970727,-0.008539109,0.048736684,0.019847404,-0.008585862,-0.02309422,-0.063740894,0.016210956,0.010058992,0.013004649,-0.032632302,-0.01605343,-0.020370508,0.05641354,0.042870574,-0.01804096,-0.008201791,-0.0030799527,0.021450503,0.003948189,0.06335701,0.031744126,-0.020027513,-0.006289308,-0.019734642,-0.03686589,0.02883737,-0.016111005,-0.013859615,0.019429475,0.047908053,-0.00039894492,0.012209661,0.020193836,0.0062702014,-0.00859967,0.0030397533,0.0062242136,0.034961693,0.069317624,-0.09122275,0.021247886,-0.07976633,-0.005147768,0.009342419,-0.07893152,-0.06381679,0.014476442,-0.036955398,0.024373744,0.01761753,-0.009111136,-0.053771175,-0.0026960208,-0.02637495,0.072629236,-0.079943165,0.03978383,0.0050963275,-0.008437283,0.10560932,0.016768191,-0.010683285,-0.020732386,-0.009709166,0.004352996,0.027564246,0.068708785,0.01058011,-0.023513403,0.058425132,0.0168663,0.008664605,-0.056577817,-0.04265174,-0.04477595,-0.020516194,-0.0012507144,-0.02240199,-0.012831341,0.023095524,0.0067014527,-0.015790273,0.0066549093,0.033276543,0.0077096755,0.050567646,0.059885766,-0.026462985,0.037448324,0.0012901555,0.071052074,0.037265535,0.023413138,0.030877398,-0.006571456,0.019107085,0.020358244,-0.024747603,0.0025426564,-0.014993508,-0.027208172,-0.0315682,0.012107412,0.00044252665,-0.008047339,-0.024847386,0.05058209,0.017186506,0.017793313,-0.022215704,-0.0609353,-0.007923684,-0.030051485,-0.050773412,0.026852673,-0.014939464,0.014454403,0.043223407,-0.022298912,0.061371215,-0.1183163,-0.036422938,-0.043870546,0.025276853,-0.011024863,-0.0349055,-0.012396209,-0.026995454,0.009553859,-0.033795726,0.05879419,0.031424396,-0.030708976,0.033018924,0.0064108344,-0.034063052,-0.0014082015,0.016632384,-0.013955065,-0.018838735,0.007413489,-0.021204343,0.0895856,-0.05527176,-0.0038136658,0.022274306,0.020030808,-0.06965118,-0.004021468,0.018422479,0.04352378,0.005263713,0.017565165,0.021089302,0.026524998,-0.045191646,-0.014548689,-0.055756003,-6.922923e-06,-0.004970122,0.039044563,0.05028051,-0.034985606,-0.0661586,-0.03319178,0.01609948,-0.033045135,0.036763784,0.02982264,0.05136671,0.06920989,0.056238804,-0.024965221,-0.023392687,-0.031455632,-0.04514794,0.009024821,-0.040448494,0.02627855,0.0065041035,-0.0018778417,0.009898985,0.024901345,-0.0020943237,-0.0051266765,-0.02153965,-0.014775749,-0.04853732,-0.026254972,0.0037657865,0.012300356,0.045466855,-0.011573934,0.10211562,0.02906814,-0.056548666,-0.03844544,-0.055386856,-0.039803598,0.009560317,-0.00043621764,-0.007875351,0.01055823,-0.006556545,0.040788867,0.070931956,0.0012742976,-0.043768905,-0.03544302,0.051230825,0.024541141,0.033206332,-0.0062802383,-0.0096703395,-0.071931936,0.035821363,-0.04737554,-0.051107742,0.034080815,-0.021382352,-0.008649563,-0.012775555,-0.0047326214,-0.009076235,-0.025935788,-0.008049347,0.00830133,0.0040064617,-0.04464742,0.009228016,-0.062064145,0.009250206,-0.04345564,-0.018650247,0.0036114173,0.03637043,0.01573712,0.001865655,-0.013828363,0.013408918,0.00824449,0.044814907,7.951482e-06,0.061364055,-0.021789923,0.055367302,-0.022352135,0.0065613044,0.07431523,-0.012678066,0.0072476617,-0.011707073,0.044109307,-0.0014855287,0.024372347,-0.0113503225,-0.03677559,-0.06054078,0.029945863,0.025865927,-0.0013990779,0.032241814,0.0009738752,-0.058412526,-0.06318295,-0.0014405254,-0.016202472,0.081612185,0.03685571,0.014478653,-0.03171416,0.037751526,-0.049787745,0.055109065,-0.00060766906,0.01669305,-0.0049934923,0.024449816]	Keywords: sparse attention, mixture-of-experts, BigBird, GPT-3, SwitchTransformer\nKey Objects: Sparse Attention, Mixture-of-Experts, FFN Layers\nRefers to Images: None\nHypothetical Questions:\n- What are the advantages of using sparse attention mechanisms in Transformer models?\n- How does SwitchTransformer maintain efficiency while increasing the number of parameters?\n- In what ways do these architecture variants contribute to the overall capabilities of PTMs?\n---\nSummary:\nSeveral Transformer architecture variants, such as BigBird, GPT-3, and SwitchTransformer, have been adapted for use in pre-trained Transformer models (PTMs) to enhance their capabilities.\nOriginal Text:\nSome of the Transformer architecture variants can also be applied to Transformer-based PTMs. For instance, BigBird [163] introduced in Sec. 4.1 is a encoder-based PTM that uses compound position-based sparse attention to enable long sequence inputs. GPT-3 [12] uses alternating dense and locally banded sparse attention (which was also introduced in Sec. 4.1) in self-attention modules. SwitchTransformer [36] is an encoder-based PTM that replaces FFN layers with mixture-of-experts layers and can increase parameter count while keeping the FLOPs per example constant.\nContextualized Text:\nTo improve performance, several Transformer architecture variants are being incorporated into pre-trained Transformer models (PTMs). For example, BigBird uses compound position-based sparse attention for long sequence inputs, GPT-3 utilizes alternating dense and locally banded sparse attention within its self-attention modules, and SwitchTransformer replaces standard feedforward network (FFN) layers with mixture-of-experts layers to increase model size without a corresponding increase in computational cost.	{"tags": ["architecture", "transformers", "PTM", "sparse attention"], "doc_id": "86e47b9e-e89c-4113-af11-057f6ab68af0", "summary": "Several Transformer architecture variants, such as BigBird, GPT-3, and SwitchTransformer, have been adapted for use in pre-trained Transformer models (PTMs) to enhance their capabilities.", "doc_type": "text", "entities": ["BigBird", "GPT-3", "SwitchTransformer"], "keywords": ["sparse attention", "mixture-of-experts", "BigBird", "GPT-3", "SwitchTransformer"], "key_objects": ["Sparse Attention", "Mixture-of-Experts", "FFN Layers"], "contextual_text": "To improve performance, several Transformer architecture variants are being incorporated into pre-trained Transformer models (PTMs). For example, BigBird uses compound position-based sparse attention for long sequence inputs, GPT-3 utilizes alternating dense and locally banded sparse attention within its self-attention modules, and SwitchTransformer replaces standard feedforward network (FFN) layers with mixture-of-experts layers to increase model size without a corresponding increase in computational cost.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "7 PRE-TRAINED TRANSFORMERS"}, "hypothetical_questions": ["What are the advantages of using sparse attention mechanisms in Transformer models?", "How does SwitchTransformer maintain efficiency while increasing the number of parameters?", "In what ways do these architecture variants contribute to the overall capabilities of PTMs?"]}
4e6cdc5a-5214-4356-9e2c-d2f6c55e4e77	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.020643607,0.0018580334,0.047305044,0.03975513,-7.955223e-05,0.058763396,-0.01035686,0.058744963,0.04415725,-0.029782511,-0.0065475507,-0.0021826825,-0.0039875237,-0.032860402,-0.008380935,0.038258053,-0.0038881644,0.08191858,0.034105167,-0.029958546,0.034841437,0.014521448,0.012517697,-0.040462293,0.02485492,0.032251466,0.044271223,-0.0042519425,0.009546258,0.04985547,0.021584768,-0.027746871,0.023774121,0.011310256,0.012810924,0.041761406,0.048588455,0.010608365,-0.0033242903,-0.0231967,-0.021052565,0.052829985,-0.052897237,0.021445781,-0.009970042,0.004568993,-0.07593194,-0.02883524,0.020772662,0.016505467,-0.05509737,-0.034539245,0.0040354184,0.0026945218,-0.024695164,-0.027461145,-0.0082718,-0.037689634,0.038629316,-0.068094276,-0.014820107,0.027430601,-0.04249413,0.011738155,0.04044201,-0.04804063,-0.01750777,-0.006261991,0.01754921,0.13089974,0.012417854,-0.0060494314,0.015057287,-0.080399185,0.05240995,0.056916475,-0.05572468,-0.027459132,-0.02000798,-0.020082645,-0.014634763,0.07895891,-0.033910614,-0.03604785,0.06848012,0.0062454618,-0.032192584,-0.0007604565,0.06845489,0.00023021105,-0.0022603383,0.038276818,-0.0022582384,0.048604235,-0.009828189,-0.08806697,0.03168545,-0.044281404,0.034401905,-0.080201715,0.0010101467,-0.01571126,0.09560038,0.083415516,0.042826824,0.00072849373,-0.01892687,-0.048448764,0.0047987257,0.024731286,-0.023001185,0.022272853,-0.049017195,0.020234173,-0.008978558,0.01367667,-0.06672548,-0.00016640317,0.032370485,-0.017576331,0.006914218,-0.03482263,0.0073561813,0.017314937,0.08571436,0.0484687,0.02561089,-0.007457706,0.0030756644,-0.020151068,0.018462447,-0.044169646,0.008320546,-0.01735976,0.012427522,-0.006647036,0.059837937,-0.0009146995,0.02813508,0.036395624,0.010708366,-0.04693202,-0.042092107,0.10635843,0.0007396093,0.0429069,-0.014939474,0.044374727,-0.011218286,0.035600178,0.021487752,0.020507354,0.03618941,0.0071770526,0.011789929,-0.020797756,-0.02662089,-0.040883414,-0.015610314,-0.005025525,-0.050925042,-0.06437343,-0.040889144,0.08246929,0.02733529,0.09750179,-0.0025493219,0.019042624,0.026169365,-0.033307232,-0.07090179,-0.0018317729,0.025463488,-0.06919464,0.012252784,-0.0095454035,0.005120865,-0.025789903,-0.03620508,-0.018529598,0.0069745546,-0.0025348219,0.0041549136,0.07921615,-0.04778488,0.009831044,0.0021068794,0.06434398,-0.021702033,-0.04832093,0.047429238,-0.006135122,-0.032598864,-0.0068028215,0.02975562,0.009159565,0.02644441,-0.07887761,-0.03167692,0.022305224,-0.018578818,-0.037887722,0.018727373,0.004034672,0.010977568,-0.03910278,0.0009271367,-0.039766725,-0.04034495,0.00018279899,0.015796741,-0.040825404,0.0013178608,0.10803482,0.06307259,-0.021559035,0.0107477475,0.000635168,0.023147242,-0.03869746,-0.045519315,-0.021389736,-0.03554879,0.035794772,-0.034098104,0.026126156,-0.05314319,0.06282051,0.018306958,-0.04053638,0.050501,0.041864883,-0.047637843,0.0024547505,0.015518785,0.027106661,-0.009305112,0.0049743312,0.016814666,0.0124180205,0.055941537,0.021992208,-0.009073185,0.028396975,-0.0019018814,0.017377038,0.021248,0.046444673,0.014688985,0.037471663,0.034375522,0.04937774,0.037507292,0.0058402764,-0.0067974124,-0.041960977,-0.014934361,0.027368067,0.039333016,0.0019516596,-0.00401031,-0.011404416,0.007061848,0.007903086,0.01812148,0.018985426,0.0075659254,0.01737373,0.010477769,0.0115918685,0.018348036,0.03327651,-0.021466695,-0.013756525,-0.036991566,0.014235856,-0.0040571643,0.01659584,0.064745136,-0.028049802,0.0028706852,0.02240791,0.051163815,-0.016705528,0.031152073,0.018148385,0.03550955,0.0028741031,-0.03833635,0.0092899855,-0.01320526,-0.037872564,0.039525356,-0.059353422,0.0159614,0.060852673,0.008150127,-0.04761477,-0.028410284,0.00838954,0.093532585,-0.046925,0.011735434,0.010117067,0.015631618,0.009845143,-0.04178111,-0.009420886,0.03934589,0.00013009629,0.014078699,-0.019060904,-0.025707394,-0.002419156,0.032048397,0.09162042,0.04615097,-0.028128823,0.0073429937,0.002367581,-0.0047108573,-0.013865228,0.002248119,0.044907123,0.004274482,-0.028581211,0.023879886,-0.02871648,-0.10671135,0.011670251,-0.07633413,0.049768798,0.041538727,0.0035547435,0.02191182,-0.018778924,0.036186427,0.011437776,0.0066758986,0.0035644046,-0.0038984993,-0.049206965,-0.028534135,-0.031053316,-0.0014825964,0.008556373,0.019183882,0.010362681,0.05697687,-0.007338159,-0.031329885,0.05114782,0.0414806,-0.007996777,-0.004722266,-0.024805006,0.0065390705,0.00021452145,-0.0151641555,-0.011800595,0.031553082,-0.012145413,0.050561782,-0.02270793,-0.017287835,0.024128556,-0.07517706,-0.015528263,0.049375698,-0.025857536,-0.022872804,-0.0148670105,0.045379993,-0.015386314,-0.0059529496,0.007514124,0.04446663,0.051569834,-0.04411002,0.0034750854,-0.017218003,0.035652187,-0.04060375,-0.004420807,-0.025451554,0.0060910624,-0.0039430186,0.035866156,-0.0036872046,0.04327101,-0.019188728,-0.00081552967,0.026725158,0.014680576,-0.0013487746,-0.0013782827,0.008609971,0.028676126,-0.0041377777,-0.036119554,0.020637397,-0.008232624,-0.04363487,0.05685056,0.041307166,0.036119215,-0.08152947,0.0006060739,0.073401935,0.06364452,0.020699859,-0.01668561,-0.0100076385,0.03979049,-0.004120644,-0.011317257,-0.009829771,0.028365586,-0.050161898,0.017495995,-0.029451218,0.026509987,0.011002184,0.057564035,0.065864496,-0.016952952,-0.01551962,0.013068245,-0.03760421,-0.033550013,0.054439854,-0.076256186,-0.038331967,0.06520919,0.059516363,0.013134947,-0.07604011,0.07727209,-0.086125165,0.040309004,0.06964591,0.005866,0.026867688,-0.035459362,0.0015689798,-0.0031446475,-0.028170493,0.022268921,-0.020830393,-0.038475055,0.032372195,-0.007434923,0.036438487,0.016438691,0.0073340605,-0.028024297,0.0011351105,-0.0025586484,-0.018637102,-0.0034840265,0.01287716,0.06977928,0.014271695,0.031931784,0.014759038,-0.028910885,-0.024071438,-0.013422112,-0.0022247217,-0.014536841,0.009026531,0.017020846,-0.0422331,0.006819852,-0.049007095,-0.03960043,-0.006619605,0.003116818,0.030363118,0.025063138,-0.0795312,-0.033045024,-0.0005182567,0.045103297,-0.038742986,-0.026820654,-0.037982576,-0.043983668,-0.045303285,-0.0006836777,0.020295873,0.02403682,0.016638005,0.047888465,0.0051571163,-0.046125963,0.00955693,-0.0036920968,0.00975564,-0.057976466,-0.055761024,-0.015852898,0.057429794,0.019099059,0.021252736,-0.01201681,0.027356554,-0.00064536714,0.004222068,0.041510355,0.011325169,0.024033805,-0.013033411,-0.00067673594,-0.028021265,0.00481694,-0.020173239,-0.034070794,0.01699309,0.04025069,-0.005480548,-0.004738311,0.034902085,0.0108215315,0.037227903,-0.028698137,0.048169587,0.05342493,0.071227446,-0.03819643,0.062418703,-0.025219742,-0.031052658,0.008267123,-0.036789812,-0.0076080346,0.029178245,-0.046934105,0.03088932,0.0030558049,0.0028382405,-0.024291849,0.036581483,-0.031515785,0.06897884,-0.05657478,0.0064868154,-0.0077508,-0.03144455,0.097888514,0.0080328295,-0.025947563,-0.020295834,-0.035997402,-0.014374219,0.05881221,0.0736196,0.003220039,0.01229688,0.09452862,0.021369763,0.04779626,-0.008041561,-0.03749751,-0.048507836,0.007744319,-0.017908549,0.01603708,-0.0021237717,0.025843665,0.022351114,0.022770915,0.0210186,0.008648552,-0.041886028,0.06253954,0.024519576,-0.025133438,0.019431831,-0.022376932,0.00027577873,0.036700316,0.03148665,0.025004234,0.021202145,0.026207224,-0.006841739,-0.025667572,0.028409742,-0.006368533,-0.018212859,-0.0529321,0.03514348,0.050653964,-0.020479284,-0.0047492515,0.033335276,-0.00077015854,0.025341392,-0.014705153,-0.071422525,-0.04711614,-0.050017137,-0.06472247,0.010721018,0.0008285088,-0.0039714477,0.050094884,-0.0334035,0.10627023,-0.0918154,-0.029469397,-0.041895915,0.007366265,-0.0041018724,0.0026696771,0.0052464646,0.014608763,-0.0009501901,-0.034387436,0.065878056,0.026390396,-0.010045889,-0.008974239,0.0110853035,-0.024115548,0.016732542,-0.012543311,-0.030086393,-0.006289334,-0.0036099916,-0.031010408,0.046551373,-0.03554012,-0.0265698,0.02618553,-0.018787835,-0.098093875,-0.0027704488,0.042714965,0.073120974,0.021439463,-0.0073211673,0.0032169195,0.053586785,-0.07332209,0.021702183,-0.040712144,0.022576256,0.0035304762,0.0010792238,0.077259056,0.017134892,-0.07520543,0.01807416,0.0016206314,-0.028818332,0.009190823,0.026317615,0.051594004,0.06661727,0.06387385,-0.04421658,-0.09301294,-0.014193996,-0.031624727,0.0070244917,-0.021890568,0.049709648,0.014465104,-0.010393738,-0.015323514,-0.03725038,0.006329906,0.011755029,0.019389994,-0.0030065314,-0.055506084,-0.06794127,-0.0041922196,0.031072728,-0.021265304,0.015557031,0.0531703,0.041880276,-0.032745462,0.007547129,-0.07193348,-0.022628278,0.05397232,0.0017986669,0.0014336848,-0.015570601,-0.012360458,-0.0022803633,0.026770309,-0.019153928,-0.027004031,-0.0069748,0.034321144,0.011377219,0.04197103,-0.022753503,-0.014619085,-0.05310076,0.042604905,-0.045162264,-0.05306395,0.025898341,0.012423425,0.004338722,-0.030460196,-0.014108687,0.017705074,-0.023383204,0.020252796,0.01437903,0.012165346,0.015825817,-0.04089573,-0.056679904,0.0011972787,-0.06162749,-0.005563026,0.007430236,0.036697496,-0.004321273,-0.031140493,-0.01307622,0.0018147447,0.008372097,-0.00077861897,-0.0026185883,0.056778718,-0.024407757,0.0063772746,0.00065145595,0.020402785,0.061332855,-0.021392643,0.026193246,-0.0049989303,0.036747724,0.014076252,-0.001871504,-0.015967669,-0.048467968,-0.09636679,0.039299343,0.05373763,0.0031227316,0.03383267,-0.029552551,-0.03315217,-0.060218304,-0.02371952,-0.0020881167,0.063499674,0.05899956,0.0051134643,-0.065522954,0.04434384,-0.049462948,0.06126101,0.005738647,-0.04107857,0.03171765,0.0067370124]	Keywords: computer vision, image classification, object detection, video processing\nKey Objects: Vision Tasks, Computer Vision\nRefers to Images: None\nHypothetical Questions:\n- Why is the flexible architecture of the Transformer so suitable for applications beyond NLP?\n- What are some of the specific challenges of adapting Transformers for computer vision tasks?\n- How do the techniques used in visual Transformers differ from those used in NLP Transformers?\n---\nSummary:\nTransformer models have been adapted for various computer vision tasks, including image classification, object detection, image generation, and video processing.\nOriginal Text:\n- (2) Computer Vision. Transformer have also been adapted for various vision tasks, e.g., image classification [14, 33, 88], object detection [13, 88, 168, 172], image generation [61, 94] and video processing [3, 115]. Han et al. [47] and Khan et al. [64] provide reviews on existing work of visual Transformers. We encourage readers to refer to these surveys for further understand the current research progress on Transformers in CV.\n- (3) Audio Applications. Transformer can also be extended for audio-related applications, e.g., speech recognition [15, 31, 41, 97], speech synthesis [57, 76, 169], speech enhancement [65, 162] and music generation [56].\n- (4) Multimodal Applications. Owing to its flexible architecture, Transformer has also been applied in various multimodal scenarios, e.g., visual question answering [55, 75, 77, 125], visual commonsense reasoning [75, 125], caption generation [22, 81, 128], speech-to-text translation [46] and text-to-image generation [29, 81, 107].\nContextualized Text:\nOriginally designed for machine translation, the Transformer architecture's flexibility has enabled its adoption across diverse fields. In computer vision, Transformers are being used for tasks such as image classification, object detection, image generation, and video processing. Researchers interested in the current progress of visual Transformers are encouraged to consult the reviews by Han et al. and Khan et al.	{"tags": ["CV", "deep-learning", "architecture", "applications"], "doc_id": "4e6cdc5a-5214-4356-9e2c-d2f6c55e4e77", "summary": "Transformer models have been adapted for various computer vision tasks, including image classification, object detection, image generation, and video processing.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["computer vision", "image classification", "object detection", "video processing"], "key_objects": ["Vision Tasks", "Computer Vision"], "contextual_text": "Originally designed for machine translation, the Transformer architecture's flexibility has enabled its adoption across diverse fields. In computer vision, Transformers are being used for tasks such as image classification, object detection, image generation, and video processing. Researchers interested in the current progress of visual Transformers are encouraged to consult the reviews by Han et al. and Khan et al.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "8 APPLICATIONS OF TRANSFORMER"}, "hypothetical_questions": ["Why is the flexible architecture of the Transformer so suitable for applications beyond NLP?", "What are some of the specific challenges of adapting Transformers for computer vision tasks?", "How do the techniques used in visual Transformers differ from those used in NLP Transformers?"]}
2926c6ab-48d5-4699-a82b-c62718bacd96	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03506046,0.024016777,0.020299392,0.031295232,-0.0075398292,0.028191032,0.015819177,0.032239042,0.004337348,0.03334265,0.019051382,0.026424767,0.020772299,-0.0017512273,-0.03822479,0.032824673,0.011428654,0.073251285,-0.008039531,0.004207548,0.06250775,-0.0031926115,0.019007748,-0.014943274,0.026549164,0.0041576885,0.035377443,-0.005734134,0.037661232,-0.0136113865,-0.010169277,-0.050556615,0.048648465,0.024866523,0.0047719083,0.017715223,0.021626296,-0.03268058,-0.017215246,0.00033625364,-0.04445053,0.03834384,-0.069514,0.02118801,-0.072850324,-0.03303005,-0.09110698,-0.037350256,0.03088936,-0.014338864,-0.02212471,-0.009618779,-0.05722726,-0.008073615,0.026813788,-0.010748334,-0.03880842,-0.04450836,-0.009716784,-0.04206635,-0.035163045,0.032646723,-0.0348589,-0.0516456,0.06405165,-0.023980606,0.0036007983,0.032908875,0.010683526,0.124627985,0.021600451,-0.05916862,-0.013687003,0.031673826,0.1231575,0.021944346,-0.025052512,0.009736567,-0.031769335,-0.0015514189,-0.019419935,0.106433496,-0.039286222,-0.033662733,0.09731592,2.8905073e-05,-0.053837158,-0.05663701,0.08363383,-0.035905655,0.000935326,0.000699511,-0.034913257,0.074847266,0.0054980954,-0.13893536,-0.00035388232,-0.0657544,0.021968678,-0.050434574,-0.0098263305,-0.011732538,0.04246314,0.05454233,0.016011536,0.010268697,0.006614705,-0.018836703,0.031780507,-0.0010281674,-0.04173317,0.022760348,-0.015643593,0.0024910078,-0.03257467,-0.031807035,-0.04275639,-0.019181892,0.019558454,0.0243691,-0.007107819,-0.018371673,-0.025692554,-0.021345643,0.037296332,0.038875543,0.006297952,0.0028326756,-0.009409659,-0.04035906,0.024819946,-0.032703273,0.014064842,0.006964536,-0.03403394,0.0044302936,0.1097234,-0.009533445,0.0074012573,-0.023054486,0.013730903,-0.044792783,-0.030664267,-0.0072427006,0.010268658,-0.010118123,-0.020871254,0.007590199,0.013312737,0.052786488,0.04119383,-0.025373017,0.06328831,0.012818467,-0.03285319,0.023757126,-0.030408788,-0.057176802,-0.0006322057,-0.018102026,-0.05353916,-0.021279931,-0.031986684,-0.010765405,0.005385623,0.073863566,0.04655371,0.019932691,0.010099518,0.024917008,-0.03469108,-0.005201899,-0.010466832,-0.02844922,0.010307947,0.0028716205,-0.010539579,-0.049703937,-0.016570533,-0.0021212916,-0.008070074,0.00050411763,0.004362597,0.073141105,0.034264743,0.038532577,-0.011324565,0.032818392,-0.030366529,-0.00032582658,0.023590136,-0.029840369,-0.0067499564,0.024601124,0.033800926,0.036544945,0.041956943,-0.05599218,0.0056181718,0.004862967,0.023226632,-0.0015566257,0.004514832,-0.024605658,-0.030667664,0.0024585414,-0.01733333,-0.017788842,-0.0077084363,0.023586916,-0.0077656303,-0.01042643,-0.027708616,0.108074255,0.02050577,0.0295023,-0.018348744,-0.003032565,0.06846363,-0.019330127,-0.03990205,0.017078826,-0.026950821,-0.007544245,0.018384174,-0.010234642,-0.035633743,0.028218634,0.004378397,-0.031003503,0.0649275,0.05126842,-0.014935409,-0.01382222,-0.007147022,0.014491458,-0.017293485,0.00549593,0.041866075,0.0067974194,0.015717246,0.030141002,-0.04312715,0.07401288,-0.0035845921,-0.0129713705,-0.023573827,0.011774776,0.011740873,0.034026913,-0.00955047,0.06096288,0.06651288,0.0055679185,-0.009544823,-0.019636163,0.04530211,-0.016305676,0.06455159,0.009496322,-0.02122372,-0.028072711,-0.018737838,0.038574863,-0.014590189,-0.019017104,-0.007816881,0.019705987,0.058237087,-0.02798238,0.031062681,0.025055574,0.024418302,-0.03917533,-0.037893534,0.014484352,-0.0016613209,0.01741,0.03281844,-0.05013627,-0.0052532037,-0.037850797,0.06369915,0.0012734259,0.012173329,-0.037655365,0.0047682994,-0.082439125,-0.08964979,0.00027104534,-0.03440535,-0.024006588,0.011434664,-0.007683779,0.013842716,0.02986659,-0.013231349,-0.030881025,-0.028484471,0.01667653,0.121853285,0.022910455,-0.0056763673,-0.025355069,-0.0003265207,-0.024909485,-0.077701405,0.014921943,-0.006297168,-0.0050584543,0.021350015,-0.0061382432,-0.05783231,-0.0071265674,0.04759179,0.09202431,0.04736244,-0.07567502,0.018885888,-0.020434342,0.023732806,0.006577215,-0.03417957,0.023939025,0.026933841,0.0006846463,0.029921409,-0.034120724,-0.06515978,0.042147227,-0.052941136,0.039547358,0.06923802,0.007129116,-0.03367831,0.025308728,-0.0038999482,0.011430691,0.031981844,0.0038441457,0.018345794,-0.024898272,0.013799529,0.018972013,-0.030391546,0.059852354,-0.043009102,0.02359725,-6.746088e-05,-0.01042912,0.00029957207,0.01611323,0.038920578,0.028578877,-0.006391245,-0.051375177,0.04101771,-0.023386892,0.014897111,-0.008734999,0.037836988,-0.04911488,0.0484831,0.0072593666,0.016304148,0.025624737,-0.048555948,-0.03278762,-0.009947273,-0.03155979,-0.020812638,0.005657896,0.08040315,-0.017957447,-0.023117222,0.0073849303,0.08492054,0.06352017,-0.010616326,-0.015769452,-0.022136055,0.010423845,-0.040388476,0.033473365,0.0052251657,0.042390384,-0.007851813,0.011510293,-0.013859115,0.013259491,-0.03628966,-0.04012637,0.04042567,0.0019375694,-0.022809796,0.0004257721,0.03500102,0.039239272,0.020735377,-0.032488193,-0.001116266,0.043336645,-0.037362855,0.064682774,0.039026152,0.017190868,-0.050593562,0.013493208,0.056470294,0.02303555,-0.0016014893,0.0042759753,-0.047942877,0.0019144493,-0.0248927,-0.0149507765,-0.010098776,0.05170819,-0.07007788,0.011714818,-0.02747114,0.039618153,0.003843307,0.04115719,0.015822943,0.013098423,-0.0016786652,0.017640315,-0.01787322,-0.0028553784,0.03905319,-0.064497106,-0.054993786,0.03849745,0.07796837,-0.008909776,-0.041735876,0.0645207,-0.060625922,0.037317798,0.05812335,0.020226479,0.009447621,-0.033884075,-0.010987832,-0.01297569,-0.013392894,0.0145117305,-0.03079992,-0.001114394,-0.00391109,0.027930323,0.0012691952,-0.008718093,-0.0023677333,-0.0035815327,0.011975317,0.035787623,-0.019833066,0.01713714,0.02333518,0.027450012,-0.00951438,0.002931256,0.01833268,-0.04263541,0.026998555,-0.016776416,0.012767511,0.044687282,0.016046884,0.00969342,-0.022137526,-0.005354841,-0.0034099962,0.007263248,-0.023018304,0.07596836,0.043544922,0.011106898,-0.057515554,-0.040223725,-0.00035585795,0.007866173,-0.025911372,-0.0069486042,-0.060040247,-0.051329646,-0.0184209,0.041982424,-0.0045105056,0.07778996,0.05772811,0.025673663,0.03235596,-0.06273684,-0.012130588,0.04414956,-0.01667592,-0.05472808,-0.027872108,0.00567658,0.05537795,0.028136546,-0.0041361656,0.015140902,-0.009073276,-0.01871954,0.03360233,0.06482675,0.012037259,0.02574185,-0.009874084,-0.036295004,-0.007091974,0.012347,-0.028947236,-0.011783795,0.030747496,0.071564324,0.00074622827,0.025568899,0.040247608,0.028102726,0.037421033,-0.018790174,0.03645576,0.033711288,0.07331632,-0.03729061,0.07254361,0.016362282,-0.020655155,-0.012380644,-0.040120102,0.0003801994,0.07332455,-0.044746928,-0.08027298,-0.0052233026,0.0036427325,-0.03751313,0.01530442,-0.025513347,0.07188794,-0.0691335,-0.028665477,-0.0445226,0.048511412,0.075284876,0.012091573,-0.052356005,0.015600948,-0.05554679,-0.015767347,0.05022193,0.043944534,0.008869787,-0.007854656,0.08320288,0.018978078,0.039781693,0.014712671,-0.039156217,-0.021935029,0.00035781186,0.008538909,-0.026726313,-0.009041237,0.014487407,0.030853676,0.0017677272,-0.0099771535,0.007981151,-0.031737685,0.06860113,0.0291184,0.011534621,0.03394549,0.00919892,0.06767133,0.008950044,-0.015315571,0.00960588,-0.017535664,0.01229613,-0.008692889,-0.01525282,0.046393998,-0.007349822,-0.016368892,-0.048633713,0.05377653,0.055709705,-0.055149548,-0.056524128,0.031182736,0.045750614,0.044823643,-0.022427794,-0.020406751,-0.01812849,-0.06311225,-0.039378002,0.008037657,-0.020508647,0.025526041,0.047942884,-0.057756614,0.078611046,-0.074883506,-0.011391903,-0.010889137,-0.0029661637,-0.015514597,-0.033179957,0.005282649,-0.029538926,-0.035614066,-0.037665684,0.028902944,0.099019825,-0.025280712,-0.028739562,-0.030275958,-0.031255342,-0.0019202793,-0.046277843,-0.016528338,-0.036855865,-0.035444003,-0.0057608197,0.05421576,-0.0034943754,0.0006821893,0.018462958,-0.02604176,-0.06867353,-0.014992314,-0.0025675758,0.051613368,0.013776514,0.02895791,0.053831052,0.013067181,-0.05243272,-0.06352523,-0.02182473,0.004021687,0.018233174,-0.044254214,0.029790992,0.00079610676,-0.0628935,-0.036843754,-0.033206854,-0.003114606,0.016666166,0.035407994,0.05070135,0.0106529305,-0.009985716,-0.035407107,-0.09915617,-0.018228758,0.012967737,0.019572962,-0.078473665,0.03301599,-0.013052788,-0.010767323,-0.031304155,-0.008009208,0.0065192315,-0.007904703,0.017934088,0.0034655961,-0.043491334,-0.06659652,-0.02567975,0.025620729,-0.08198394,-0.007068189,0.060981054,0.027630059,-0.063156016,-0.018416854,-0.024908684,-0.0001866941,-0.042906627,0.018416608,0.024306277,0.005532934,-0.023289908,0.022204544,0.0658918,0.03515869,-0.05356816,0.00830504,0.06855122,-0.015978046,-0.005818402,0.044098113,0.019722823,-0.051781148,0.036577493,-0.0011740853,-0.08806562,0.040094998,-0.021719567,-0.033227723,-0.05712932,-0.03121437,0.007062371,-0.017817149,-0.0018559776,0.012128398,-0.03046749,-0.014054637,0.020843854,-0.01586111,-0.026313843,0.0024250094,-0.002416896,-0.000985323,0.063768774,-0.024163371,-0.01095924,0.001580107,0.036380824,0.006434243,-0.038440954,0.010994857,0.07751762,-0.042043153,0.01310256,-0.026513278,0.010614617,0.029375164,-0.02264026,0.009811136,-0.030324996,0.029873924,0.0015699695,-0.014165993,-0.0052288533,-0.04462687,-0.046756133,0.034635954,0.01836465,0.039063785,0.014984202,-0.0018808622,-0.018321494,-0.051011827,-0.008349199,-0.015644286,0.017607203,0.00027706355,0.0034509469,-0.005968613,0.057751305,-0.026898254,0.043823857,-0.035717644,-0.0405806,-0.013928171,-0.05255888]	Keywords: X-formers, Transformer, architecture, efficiency, generalization, applications\nKey Objects: Transformer models, X-formers\nRefers to Images: None\nHypothetical Questions:\n- What is a key distinction between a standard Transformer and an X-former?\n- What are the main areas where current Transformer models still face challenges?\n- How might future research expand on the principles outlined in this survey to further improve Transformer models?\n---\nSummary:\nThis survey provides a comprehensive overview of X-formers and proposes a new taxonomy, highlighting that while Transformer models have demonstrated power across various tasks, challenges and opportunities for further improvement remain.\nOriginal Text:\n## 9 CONCLUSION AND FUTURE DIRECTIONS  \nIn this survey, we conduct a comprehensive overview of X-formers and propose a new taxonomy. Most of the existing works improve Transformer from different perspectives, such as efficiency, generalization, and applications. The improvements include incorporating structural prior, designing lightweight architecture, pre-training, and so on.  \nAlthough X-formers have proven their power for various tasks, challenges still exist. Besides the current concerns (e.g. efficiency and generalization), the further improvements of Transformer may lie in the following directions:\nContextualized Text:\nThis survey offers a detailed examination of X-formers and presents a new way to categorize them. While X-formers have proven effective in various tasks, existing work often focuses on improving Transformer models from perspectives like efficiency, generalization, and application-specific design, often involving structural priors, lightweight architectures, or pre-training techniques.	{"tags": ["survey", "deep-learning", "architecture", "NLP"], "doc_id": "2926c6ab-48d5-4699-a82b-c62718bacd96", "summary": "This survey provides a comprehensive overview of X-formers and proposes a new taxonomy, highlighting that while Transformer models have demonstrated power across various tasks, challenges and opportunities for further improvement remain.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["X-formers", "Transformer", "architecture", "efficiency", "generalization", "applications"], "key_objects": ["Transformer models", "X-formers"], "contextual_text": "This survey offers a detailed examination of X-formers and presents a new way to categorize them. While X-formers have proven effective in various tasks, existing work often focuses on improving Transformer models from perspectives like efficiency, generalization, and application-specific design, often involving structural priors, lightweight architectures, or pre-training techniques.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "9 CONCLUSION AND FUTURE DIRECTIONS"}, "hypothetical_questions": ["What is a key distinction between a standard Transformer and an X-former?", "What are the main areas where current Transformer models still face challenges?", "How might future research expand on the principles outlined in this survey to further improve Transformer models?"]}
0b188913-c172-45ad-a545-b3a52972e39c	abe8c200-bfa1-4355-947e-23ea618c310d	[0.001665527,-0.0034502132,0.013781592,0.069518164,-0.03395592,0.04522148,-0.041502353,0.060597103,0.01384792,-0.014039996,0.008171442,0.03837767,0.035240736,0.015949031,0.008728604,0.013791975,0.023259066,0.07088823,0.055149052,-0.030102821,0.065288775,0.03017653,-0.020823075,-0.0234143,0.024292815,0.031628445,0.07592281,0.0072241337,-0.016708395,0.02208726,0.0051752576,0.0035383198,0.03152701,0.0102475,0.022864224,0.0037177051,-0.0022704126,-0.03925138,-0.05980796,-0.0059928726,-0.04701984,0.045768477,-0.04415337,-0.029987162,-0.0021419995,-0.017882206,-0.07308803,-0.05089108,0.023230463,-0.019803345,-0.08255757,0.009559183,-0.031139493,-0.011034958,-0.004458522,-0.012765564,0.005682567,-0.06142517,0.0076789744,-0.04292014,-0.03421017,-0.0008298733,-0.02839073,0.010618013,0.025329527,-0.027565014,0.036669616,0.023704296,-0.0066396734,0.15210718,-0.030011069,-0.004989856,-0.039193314,-0.03852987,0.12815991,-0.015121141,-0.05145187,-0.00016511194,-0.046733562,-0.025589807,-0.0074652517,0.042127445,-0.015540236,-0.0424369,0.0451479,-0.003500482,-0.05763518,-0.027975386,0.10179521,-0.05378885,-0.0014704652,0.014575625,0.034260917,0.05038296,-4.2914697e-05,-0.076377876,0.024225784,-0.07723078,0.061732855,-0.03461845,0.033105176,-0.014865162,0.06304274,0.11683949,0.048628442,-0.034755364,-0.017014155,-0.038048636,-0.0014291636,0.0022390774,-0.026129715,0.026199767,-0.018139964,0.0029808835,-0.0663735,0.004424981,-0.07415852,-0.025654012,0.018798683,-0.0007704675,0.01107745,-0.018117843,-0.027084362,0.05587951,0.0194788,0.043405887,0.013298236,-0.020219816,-0.012933207,-0.030295318,0.026636256,-0.027725922,0.017942695,0.034796603,-0.011696037,0.008945815,0.043041207,0.044996824,0.013337444,-0.006882651,0.0285677,-0.04313719,-0.03660997,0.02215648,-0.0019884377,0.06379927,0.009353261,0.039210033,-0.017555358,0.06088627,0.0432068,-0.0031076218,0.028819157,-0.0010927881,0.015595651,-0.00854869,-0.04089571,-0.03459875,-0.02664696,-0.04124952,-0.044084888,-0.06525729,0.004166712,0.05787513,0.043162547,0.12184723,0.08774271,-0.023446916,-0.0025220602,0.02050354,-0.023245344,0.009658283,0.01965601,-0.0756403,-0.00413601,0.036789123,0.030931033,-0.020778487,0.0082263835,0.02134293,-0.0337286,0.00025204313,-0.03260702,0.029240483,-0.039654873,-9.693219e-05,0.022425186,0.043467715,-0.059603304,-0.032550085,0.023984281,-0.041163955,-0.0089424765,0.038121685,0.047327917,0.026371844,0.017914288,-0.051995754,-0.0041752183,0.0032371618,0.015210053,-0.0051264088,0.009291693,-0.025383554,0.021651115,0.021830125,0.006218663,0.010424283,-0.011324491,0.01971009,0.024511987,-0.008639249,-0.03488115,0.028092263,0.06106405,-0.02600183,0.012525944,0.021296136,0.03560144,0.020777231,-0.020898486,0.018055398,-0.0033447929,0.03525235,-0.02166504,-0.02067509,-0.033557527,-0.017499518,0.034078237,-0.018034233,-0.0030906138,-0.0047443756,-0.051241204,0.063883096,0.013074315,0.05180736,0.022946587,-0.006280498,0.04623185,0.033852343,0.057508737,0.02525306,-0.014192893,0.063503705,-0.025509061,-0.005628599,0.02419297,0.047409832,0.014599946,0.03637437,0.0030347581,0.04180525,0.04279674,-0.0298199,0.0014696533,-0.007673281,0.0012630051,0.042698644,0.003505535,-0.0428319,-0.025074298,-0.004163246,-0.03704383,0.015077557,0.047546145,-0.04145519,0.011194634,0.017066969,0.0041010687,-0.04650951,0.024359526,0.021185378,-0.0037563941,-0.0008875101,-0.0048727333,0.056489598,0.02518108,-0.047982063,0.05044204,-0.030272312,-0.0041991603,0.018751064,0.036819033,-0.006241164,0.03331026,-0.010907222,-0.0046239216,-0.036137015,0.0008464201,-0.002371132,-0.021052696,-0.03925954,0.06771627,-0.0043832203,0.012005459,0.0003111708,-0.017956171,-0.027137166,0.029196149,0.025608648,0.07033989,-0.022148361,-0.023728177,-0.00206506,0.04882186,0.033686984,-0.058554836,0.027293677,0.028795626,0.012600851,0.047278784,0.016804619,-0.038707554,0.008148662,0.022084486,0.082240276,0.057881385,-0.0074990937,-0.013669248,0.0017657519,0.035404254,0.00074074924,0.005302728,0.0834554,0.03493055,-0.027200062,-0.0004597272,-0.015197165,-0.090953566,0.02170364,-0.088269055,0.059773847,0.04997732,0.045691725,-0.016213074,-0.011567183,0.02669434,0.054364022,0.0117801735,0.009875677,0.019246377,-0.08428478,0.012333778,0.005497703,0.0009256633,0.005129564,0.009438388,0.016249353,0.025456544,-0.0016318761,-0.019826593,0.011532533,0.05888942,0.031707905,0.041700073,-0.006812573,0.031073455,-0.0020981894,-0.0065601687,-0.015063854,0.03083202,-0.04094151,0.03341975,-0.016821513,0.026774334,-0.013249561,-0.06813763,0.01153151,0.0091714235,-0.07528715,-0.0018206523,-0.010393695,0.049462195,-0.04689557,-0.0407082,0.021175668,0.01974291,0.023102127,-0.0285557,-0.04349479,-0.036138337,0.0014539195,-0.042937674,0.0028434722,-0.005618151,0.008626561,0.011302696,0.05123955,-0.03290817,0.03071029,-0.026411619,-0.002948442,0.0013651359,0.0014511029,-0.025748348,-0.005133115,0.041370492,0.07627745,-0.034721527,-0.004167002,0.026902864,-0.0007115975,-0.04375141,0.042048637,0.038794532,-0.012339887,-0.06970016,-0.025190184,0.041526426,0.028774055,0.032537863,-0.019043623,-0.04015245,0.001480249,-0.009696585,-0.03318195,-0.038590536,0.02876427,-0.035245877,-0.00042643005,-0.012801506,0.03963309,0.02253643,0.049472015,0.039581213,-0.01068388,-0.0007501168,0.03174165,-0.021677786,-0.015751353,0.01901032,-0.0481604,-0.02219429,0.06655569,0.037641067,-0.035704397,-0.05738262,0.055252347,-0.059143666,0.045321487,0.07037748,-0.045182526,0.020989953,-0.020611037,0.0011987872,-0.025236698,0.023353485,-0.009771027,0.010902948,0.0075071324,0.035792157,-0.00505091,0.018686336,-0.0300378,0.002184975,-0.014682335,-0.017612843,0.02222326,-0.040909395,-0.014761003,-0.0132116135,0.009906137,-0.005969301,-0.011718439,0.03129061,-0.02456739,-0.00017659563,-0.04931383,-0.012244711,0.0028186874,-0.0091740275,0.017676758,-0.045495562,0.06714928,-0.03005659,-0.012536557,0.02331364,0.042889554,0.02688741,0.002883713,-0.083261214,-0.043770652,0.023147363,-0.011582298,-0.0070734476,-0.0055888873,-0.004389867,-0.0019028629,-0.03866634,-0.00024737828,0.011328267,0.04586072,0.029625038,0.036152925,0.036335684,-0.043409865,-0.024514018,0.052802734,0.014787228,-0.07520459,-0.06051422,0.00026074908,0.020948814,0.050619185,-0.0366485,0.001576766,0.009569512,0.03300982,0.0033783836,0.04317433,-0.014565535,0.011039036,0.04230624,-0.052699436,-0.018718883,0.034670647,-0.020353856,-0.016496545,0.029709077,0.028161336,-0.024391806,0.017363401,0.028470948,0.016885526,7.477977e-05,-0.020404711,0.033470668,0.04286985,0.06769575,-0.08559137,0.06456785,-0.04876363,-0.0013890155,0.00047192923,-0.04394044,-0.0148182325,0.024544701,-0.022142017,0.013156065,0.02746814,0.0060568503,-0.010182511,0.0005570715,-0.028053904,0.034037836,-0.058765095,0.02039593,-0.009230799,-0.021371525,0.062072415,0.0065039746,-0.0034672746,-0.008503125,-0.006443801,0.003876355,0.02680309,0.074452676,-0.0016236149,-0.015024576,0.049652528,0.012638822,0.028884113,-0.021580953,-0.059431117,-0.058943734,-0.012297584,-0.02409276,-0.0068241735,0.0039504427,0.01198798,0.043951795,-0.021914514,-0.014881662,0.021219684,-0.05037609,0.072149046,0.07042961,-0.017677361,0.042348526,0.011890432,0.030821035,0.041354157,-0.02886507,0.042970598,0.017574554,-0.0025889396,0.04894495,0.017114406,0.03572588,0.019158613,-0.03445033,-0.0442117,0.0013243435,0.022124218,-0.06490343,-0.009399803,0.045422833,0.004384932,0.018929956,0.0125658065,-0.094288714,0.0006921129,-0.07660708,-0.03694344,0.039379746,-0.008400198,-0.007792874,0.016229615,-0.02430509,0.045049403,-0.08795371,-0.02795505,-0.05107858,0.043982517,0.0036315455,-0.02127181,-0.00014793829,-0.0003757118,-0.0040563764,-0.031806607,0.041532606,0.06930298,-0.006204375,0.021350538,-0.00022622386,0.014893794,-0.004678865,-0.04186866,-0.04440663,0.01698753,-0.0072960453,-0.036688928,0.08013042,-0.03495178,0.001612211,0.023992784,-0.02021178,-0.047782,-0.01666306,0.00069516216,0.044567045,0.021695316,0.0069468115,0.03015264,0.030934624,-0.03977809,-0.029297022,-0.046342924,-0.026865082,-0.0049774605,0.04270017,0.08823623,-0.00059711334,-0.025364993,-0.0025897566,-0.025758522,-0.04541211,0.018826665,0.049076322,0.01872146,0.06350058,-0.0048350445,-0.033451624,-0.076831274,-0.0022463605,-0.0018524785,0.04066106,-0.073359914,0.006807096,0.0064472756,0.0026957057,-0.029814059,-0.0036846625,-0.011032889,0.015368176,0.05029653,-0.02025454,-0.04516921,-0.075587176,0.046445683,0.044687327,-0.027477995,-0.05922338,0.071986996,0.06566053,-0.08942275,-0.03761825,-0.013523016,-0.042436287,0.011503338,0.00074160204,0.019788997,0.0062970906,-0.025431225,-0.017933512,0.045526467,0.011534733,-0.03807532,-0.02649917,0.06318178,0.0010178651,0.07213465,0.03130027,-0.039984412,-0.107326485,0.048022807,0.0023673552,-0.05741984,0.002397274,-0.0061152875,0.0015867573,-0.04305002,-0.050773397,-0.025957646,0.004287267,-0.024886623,0.05099748,0.024908887,0.016564313,-0.029464342,-0.040558357,0.002099386,-0.019329444,0.0032871787,-0.047953546,0.03615652,-0.024712745,0.001181768,-0.0004977894,0.0199063,0.008531912,0.006172136,-0.018057376,0.07111825,-0.004942966,0.05429505,-0.013480794,-0.017717088,0.07004033,-0.024237946,0.0314035,-0.033575732,0.030568391,-0.00013833608,0.024521936,-0.028087465,-0.026804972,-0.02497002,0.0073908665,0.060542878,0.029101323,-0.017276973,-0.02997312,-0.034003895,-0.028711086,-0.030854337,-0.028144678,0.04725577,0.018434927,0.041095667,-0.056413952,-0.00362858,-0.053137258,0.011731329,0.046362028,-0.014031189,-0.020910982,-0.019721996]	Keywords: Transformer, CNN, RNN, theoretical analysis, data structure\nKey Objects: Transformer architecture, training datasets, data structure assumptions\nRefers to Images: None\nHypothetical Questions:\n- Why does the Transformer's flexibility, due to fewer assumptions, lead to better performance with large datasets?\n- What kind of theoretical analysis is needed to better understand the capabilities of Transformers?\n- How do the data structure assumptions of CNNs and RNNs limit their performance compared to Transformers?\n---\nSummary:\nTransformer architectures are capable of handling large datasets and often outperform CNNs and RNNs due to their flexibility and lack of strict data structure assumptions, but the theoretical basis for this capability remains unclear.\nOriginal Text:\n- (1) Theoretical Analysis. The architecture of Transformer has been demonstrated to be capable of supporting large-scale training datasets with enough parameters. Many works show that Transformer has a larger capacity than CNNs and RNNs and hence has the ability to handle a huge amount of training data. When Transformer is trained on sufficient data, it usually has better performances than CNNs or RNNs. An intuitive explanation is that Transformer has few prior  \nassumptions on the data structure and therefore is more flexible than CNNs and RNNs. However, the theoretical reason is unclear and we need some theoretical analysis of Transformer ability.\nContextualized Text:\nTransformer architectures have proven capable of supporting large-scale training datasets and often achieve better performance than CNNs and RNNs. This is partially because Transformers have fewer assumptions about the structure of the data, making them more flexible. However, the precise theoretical explanation for this capability remains unclear and warrants further investigation.	{"tags": ["deep-learning", "architecture", "theory", "NLP"], "doc_id": "0b188913-c172-45ad-a545-b3a52972e39c", "summary": "Transformer architectures are capable of handling large datasets and often outperform CNNs and RNNs due to their flexibility and lack of strict data structure assumptions, but the theoretical basis for this capability remains unclear.", "doc_type": "text", "entities": ["CNN", "RNN", "Transformer"], "keywords": ["Transformer", "CNN", "RNN", "theoretical analysis", "data structure"], "key_objects": ["Transformer architecture", "training datasets", "data structure assumptions"], "contextual_text": "Transformer architectures have proven capable of supporting large-scale training datasets and often achieve better performance than CNNs and RNNs. This is partially because Transformers have fewer assumptions about the structure of the data, making them more flexible. However, the precise theoretical explanation for this capability remains unclear and warrants further investigation.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "9 CONCLUSION AND FUTURE DIRECTIONS"}, "hypothetical_questions": ["Why does the Transformer's flexibility, due to fewer assumptions, lead to better performance with large datasets?", "What kind of theoretical analysis is needed to better understand the capabilities of Transformers?", "How do the data structure assumptions of CNNs and RNNs limit their performance compared to Transformers?"]}
ace31b21-23f5-4939-a895-1b3180f9143a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01336851,-0.0019503329,0.034809824,0.026414521,-0.002894213,0.05589569,0.01195303,0.048215736,0.0009557388,-0.00018690692,-0.005425511,0.015794845,-0.0052322475,0.04783587,-0.035717066,0.00049546396,0.0053997566,0.10108033,0.029162267,-0.031786606,0.058917522,0.027772665,0.023192383,-0.046179943,-0.023158075,0.030394936,0.041214515,-0.032059833,0.007794019,-0.004908744,0.03397263,-0.036452364,0.063551776,0.01533607,0.023634233,-0.04205916,0.030905621,-0.060409453,0.016724283,0.019767556,0.001199089,0.013818739,-0.05268917,0.0045378567,-0.016005946,-0.060470495,-0.11774433,-0.023532106,-0.005425388,-0.021061,0.02227056,0.0113412775,-0.033557106,-0.024935493,-0.00832258,0.0021200955,-0.0040357984,-0.03777592,0.015419836,-0.09076589,-0.004647325,0.014171775,-0.03324594,-0.025856635,0.014035001,-0.039224118,0.01193102,0.03442728,0.025458936,0.1262332,-0.03598459,0.004681985,0.005014927,-0.034978416,0.13256662,0.020695155,-0.017345374,-0.05154695,-0.06430988,-0.011456639,-0.0072896224,0.09522914,-0.016180638,-0.048004378,0.082131326,0.012963728,-0.076559365,0.0040871557,0.052861076,-0.018929305,-0.005100968,-0.013390325,-0.012798922,0.042910174,-0.040901434,-0.10986565,-0.022956017,0.0040041264,-0.019798875,-0.03826733,0.037029613,0.012796164,0.08375579,0.09389542,0.047785718,-0.061122164,-0.0071951165,-0.06623042,0.04262176,-0.004185271,-0.0051799356,0.005210902,0.0020711338,0.006473342,-0.04130137,0.029065384,-0.014269666,0.0385497,-0.05090969,-0.013327848,0.027674196,-0.007390604,-0.04350824,0.05192614,0.025650973,0.069218874,0.045692243,0.022873016,-0.0060932008,0.012520781,0.026647996,0.01820928,-0.0028211076,-0.0030269977,-0.018454602,0.005456078,0.056508385,0.023200013,0.024470929,0.048207108,-0.015788332,-0.08081565,-0.06769915,0.0550481,-0.0069826227,0.058072858,-0.044031583,0.026524648,-0.032306388,0.050222445,0.033761248,-0.040641572,0.006652011,-0.012471831,-0.0059079393,0.023164062,0.031422205,-0.06486605,0.012228975,-0.07818467,-0.012201095,-0.06308394,-0.04133742,0.01636298,0.038191777,0.1282219,-0.015795032,-0.029522104,0.0721245,0.0011931169,-0.022203505,-0.007820888,0.0021626328,-0.04968388,0.027475609,0.00019215958,-0.0131645305,-0.027248206,0.038026653,-0.032507177,-0.025481012,0.027816268,0.04224094,0.054248318,-0.0017425097,0.048974272,-0.012338724,0.087643884,-0.016993143,-0.054068986,0.03810227,0.00031902755,0.01339103,0.020767802,0.012538433,0.04156554,0.040764622,-0.07940911,0.01715883,-0.08805764,-0.032090046,0.0051645073,0.0010645513,-0.0316051,-0.00899413,0.012985647,-0.014723574,-0.017705305,-0.045376804,0.012807059,-0.010682682,-0.016842341,-0.06397157,0.019351212,-0.010461848,-0.0021950332,-0.018415967,0.018582745,0.018994005,-0.01972335,-0.06002831,0.012995853,-0.028995078,-0.009598975,-0.04001562,0.030848397,-0.007184526,0.01990245,0.018924419,-0.04141226,0.068541944,0.04810616,-0.04432362,0.047001038,0.00982722,0.03134317,-0.00675895,-0.002848884,0.037983082,0.03508807,0.063758776,0.06658927,-0.012033428,0.0810128,0.009869414,-0.027503796,-0.0011840895,0.05206474,0.011947841,0.03698616,0.011884292,0.038031425,0.060616437,0.005246738,0.021866297,-0.025997054,0.005837416,0.018890584,0.024819912,-0.045043427,0.026471086,-0.030366907,0.0008016398,0.018670134,0.0070503196,-0.027095497,-0.013821142,0.045486033,-0.03857987,-0.046958733,0.045152947,0.03580395,0.013397297,-0.04368732,0.0046875025,0.02702653,0.016094135,0.02535901,-0.0041729645,-0.019455064,-0.01551354,-0.019067518,-0.015907655,-0.00070209574,0.020068603,-0.042062473,-0.007882108,-0.0314429,-0.045572497,-0.0384569,-0.03287059,-0.058782928,-0.0011817559,-0.012757444,-0.0005570543,-0.011129748,-0.021412203,0.007107893,-0.008293442,0.048457175,0.09703242,-0.025958566,-0.0070140674,0.0033235198,0.008781838,0.012174878,-0.059195023,0.041819204,0.0649577,-0.026965752,-0.006441314,0.0012295381,-0.043986924,0.0008140901,0.056223124,0.053190064,0.054444972,-0.033202585,-0.016373493,0.0014328863,0.03437918,-0.0073457207,-0.002868126,0.010534815,0.012087162,0.009107477,0.008118757,-0.013840626,-0.08620025,0.0737748,-0.11849647,0.034006868,0.0588775,0.005404823,0.008894448,-0.008986318,-0.014224329,0.04641316,-0.008204837,-0.044310603,-0.011090821,-0.014226112,-0.013269295,-0.0029884453,-0.014227387,0.017809542,-0.012703101,0.06798175,0.0033725812,0.0034455126,0.018210933,0.049439922,0.004073917,0.014491793,-0.0058272984,-0.054613285,0.016688151,0.041082777,0.007759878,0.02197168,0.052315734,-0.057494223,0.046724275,-0.030336408,0.036806136,-0.003813951,-0.06837989,0.0077006817,0.034744278,-0.019917125,-0.033086184,-0.021373466,0.06054139,-0.026493406,-0.03837798,0.027214685,0.0073749344,0.010568455,-0.025115697,0.008008219,-0.0018960611,-0.01452353,-0.030695474,-0.047757305,0.021955213,0.005056991,-0.009514335,0.08537476,-0.013615574,0.05160152,-0.05017905,0.029886905,0.02281464,0.0147719,-0.030675821,0.024263116,0.017218655,0.046823047,-0.011414144,-0.047694158,0.028118284,0.013543738,-0.057387836,0.0581788,0.018759008,0.019362612,-0.06150383,-0.016192144,0.025591722,0.047522075,0.00015371351,-0.02325148,-0.024824213,0.0090192435,-0.0149256075,-0.0031001212,-0.030577933,0.016733708,-0.05150374,0.0003136364,-0.025667096,0.020525116,0.005252733,0.06940731,0.024080003,0.006283388,-0.030826353,-0.0015539348,-0.034370217,-0.07482269,0.004284545,-0.05587014,-0.004580059,0.09105424,0.019521283,0.017270973,-0.072955936,0.093154475,-0.057927188,0.020450747,0.036422953,-0.035747,0.015247386,-0.041435074,0.022834344,-0.046769924,0.02481078,0.018256113,0.0016457525,-0.008585836,0.030773427,-0.018286018,0.013190917,-0.050384488,0.030782739,-0.011832712,-0.014245827,-0.023139356,-0.0128751425,-0.044828508,0.0051350943,0.024433782,-0.012657858,-0.031736802,-0.004684083,-0.0074027115,0.010303449,-0.044651806,-0.052067943,0.012809434,0.021560764,-0.013457383,-0.028778382,0.029152608,0.008988653,-0.03135275,0.010566767,0.05627801,0.017899612,0.0092845205,-0.05092384,-0.0359958,-0.048742156,0.04204462,0.007045971,-0.0028997238,-0.009444826,0.0006829482,-0.055068035,0.06471874,0.015115022,0.06405849,0.0145191355,-0.013657029,0.030435322,-0.055676494,-0.028824523,-0.004097758,0.03141887,-0.027504476,-0.0014526976,0.0093912715,0.059006844,0.071875975,-0.038483053,-0.012029483,-0.014511389,-0.017019022,-0.00515104,0.038320474,0.02031994,-0.042946253,-0.017146412,0.0030158681,-0.060639173,0.042439327,0.030927857,-0.0050262176,0.0022971653,0.061969988,-0.054872375,-0.005988748,0.02955261,-0.0016905126,0.0060628005,0.0036284872,0.021279547,-0.010863806,0.021059554,-0.07877571,0.055024806,-0.002537649,-0.010311184,-0.011626172,-0.020276466,-0.046776995,-0.011325867,-0.022353286,-0.009839712,0.007946733,0.025999798,-0.026745776,0.047412194,-0.03091116,0.051169675,-0.054331455,-0.010752629,0.016319541,0.029960087,0.0437973,0.0007164426,0.037362956,-0.005137977,0.028170178,-0.0476539,-0.015747862,0.08481397,0.007581032,-0.029655209,0.049717803,0.008855452,0.010936119,-0.031865638,-0.057271678,-0.0422114,-0.028151842,-0.013586881,-0.0005715503,-0.022497991,0.02935297,-0.023353295,-0.01930119,-0.018246263,-0.015271141,-0.016092451,0.044846646,0.040476315,-0.023860743,0.052973084,-0.0003892587,0.034351658,0.017174682,0.027608508,0.020533612,0.037504867,0.016989129,0.028735707,0.0062871734,0.034859944,-0.032452065,-0.039992906,-0.013392174,0.020105349,-0.021674236,-0.02939797,-0.06028706,0.021711275,0.018272962,0.025057847,0.011511334,-0.072114676,-0.010929023,-0.051388208,-0.05962505,0.002013264,0.03180658,0.003240197,0.016167074,-0.028548194,0.029189974,-0.033659827,-0.0030942713,0.0030181273,0.009417964,-0.003987632,-0.046468474,-0.015930342,-0.026599696,-0.02725095,-0.07014503,0.048363574,0.03959804,-0.048371866,0.028001098,0.029061172,3.621944e-05,-0.022056919,0.00966841,-0.011142109,-0.0064078374,0.010213102,-0.014360786,0.06240307,-0.06899459,0.025097864,0.045378212,-0.011772832,-0.064373665,-0.012114751,-0.036990542,0.05163518,-0.030128647,0.040082227,0.029356332,0.04570751,0.024674704,-0.031978145,-0.039273374,-0.0042221104,-0.0012031253,0.032770924,0.016663076,-0.0021503903,-0.028304597,0.021313332,0.036035713,-0.07005292,0.020092119,0.046006884,0.07044376,0.057290085,0.027008351,-0.030645717,-0.10091938,0.017402148,-0.045357242,0.04748871,0.0063680536,0.052391876,-0.021457868,0.0013866966,0.008901541,-0.0013506348,0.014903617,-0.007767293,0.037293505,-0.019442156,-0.0061491365,-0.042649135,0.036604065,0.009516718,0.0010610155,-0.0071635656,0.03571214,0.050646126,-0.052205946,-0.04183566,-0.071907826,-0.026960235,-0.0009768282,0.0106999,0.038247805,0.022687115,-0.0020296704,-0.007057374,0.037042946,-0.0051560546,-0.02253377,-0.054281287,0.039387908,0.024119921,0.04828338,-0.0009341302,-0.014971404,-0.100996904,0.016859315,-0.0041533094,-0.043306023,0.040680163,0.007731182,-0.031465925,0.011850364,0.007999692,-0.031125393,0.0062713698,0.041444894,0.017556047,0.031597335,0.0116302315,-0.019629667,-0.025025757,-0.026736613,-0.0093173785,0.008369538,-0.065387905,0.021752525,-0.019446474,0.0017506122,0.02881336,0.012365464,-0.014169322,-0.0047652335,0.0049094018,0.0360782,-0.018811509,0.02513265,-0.035224162,0.018645354,0.05553095,0.02646713,0.020906646,-0.0030964145,0.038134087,-0.018935917,-0.019040648,-0.012042624,-0.01839304,-0.03931421,-0.030103752,0.013003184,0.0018374242,0.022947758,-0.0021699052,-0.033964455,-0.029149942,-0.0244128,0.002551437,0.033931106,0.018732259,0.01143521,-0.055537805,0.017086292,-0.011593518,0.06165423,-0.010391016,-0.002949813,-0.008392051,0.009148597]	Keywords: attention mechanism, global interaction, dynamic routing, memory-enhanced models\nKey Objects: Attention Mechanism, Global Interaction, Self-Attention Module\nRefers to Images: None\nHypothetical Questions:\n- Why is full attention not always necessary for modeling global interactions in Transformers?\n- What are some potential alternatives to the attention mechanism for modeling global interactions?\n- How could memory-enhanced models contribute to more efficient global interaction modeling within Transformer architectures?\n---\nSummary:\nThe Transformer's reliance on the attention mechanism for global interaction, while advantageous, is not always efficient, as full attention is often unnecessary. Future improvements should focus on developing more efficient global interaction modeling techniques.\nOriginal Text:\n- (2) Better Global Interaction Mechanism beyond Attention . A main advantage of Transformer is the use of the attention mechanism to model the global dependencies among nodes within input data. However, many studies have shown that full attention is unnecessary for most nodes. It is, to some degree, inefficient to indistinguishably calculate attention for all nodes. Therefore, there is still plenty of room for improvements in efficiently modeling global interactions. On the one hand, the self-attention module can be regarded as a fully-connected neural network with dynamical connection weights, which aggregates non-local information with dynamic routing. Therefore, other dynamic routing mechanisms are alternative approaches worth exploring. On the other hand, the global interaction can also be modeled by other types of neural networks, such as memory-enhanced models.\nContextualized Text:\nA core strength of the Transformer architecture is its use of the attention mechanism to model global dependencies within input data. However, research suggests that applying full attention to every node is often inefficient.  Therefore, there's significant potential to improve the efficiency of global interaction modeling through alternative approaches like dynamic routing mechanisms or memory-enhanced models.	{"tags": ["architecture", "NLP", "efficiency"], "doc_id": "ace31b21-23f5-4939-a895-1b3180f9143a", "summary": "The Transformer's reliance on the attention mechanism for global interaction, while advantageous, is not always efficient, as full attention is often unnecessary. Future improvements should focus on developing more efficient global interaction modeling techniques.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["attention mechanism", "global interaction", "dynamic routing", "memory-enhanced models"], "key_objects": ["Attention Mechanism", "Global Interaction", "Self-Attention Module"], "contextual_text": "A core strength of the Transformer architecture is its use of the attention mechanism to model global dependencies within input data. However, research suggests that applying full attention to every node is often inefficient.  Therefore, there's significant potential to improve the efficiency of global interaction modeling through alternative approaches like dynamic routing mechanisms or memory-enhanced models.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "9 CONCLUSION AND FUTURE DIRECTIONS"}, "hypothetical_questions": ["Why is full attention not always necessary for modeling global interactions in Transformers?", "What are some potential alternatives to the attention mechanism for modeling global interactions?", "How could memory-enhanced models contribute to more efficient global interaction modeling within Transformer architectures?"]}
27849426-300f-4464-8086-9b10a4bcfaff	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03373162,0.0049179136,0.0770995,0.06233703,-0.02105774,0.048793014,-0.018001752,0.053122662,0.019388136,0.02829084,-0.03169436,0.0034893083,0.0010089137,0.0037789764,-0.013059128,0.01772153,0.0016864636,0.097797416,-0.025660807,-0.01571876,0.018165443,0.040625304,0.057348005,-0.0135733485,-0.03826687,0.017919814,0.050973102,-0.023535375,-0.004820422,0.024755402,0.0337508,-0.06263946,0.046766777,0.025046216,0.04442779,0.04239326,0.040709265,-0.0013040606,0.014010792,0.005862445,-0.015991554,0.020133955,-0.021024885,0.032122828,-0.021832926,-0.0117665,-0.09054871,-0.0050063385,0.03109821,-0.014541996,-0.036836818,0.022459885,-0.0154186515,0.038094003,-0.042255603,-0.037512034,-0.015058425,-0.04129059,0.011133058,-0.10253966,-0.033148624,0.03641282,-0.052939802,-0.009236047,0.025876572,-0.017311674,-0.008301046,0.026520874,0.048021484,0.12965713,-0.021412445,0.029765753,0.021920603,-0.03315201,0.08377104,0.046376586,-0.0013880425,-0.03014439,-0.020765955,-0.04812154,-0.028022995,0.039768618,-0.009161199,-0.003187983,0.08582287,0.019526938,-0.0605985,-0.0074463557,0.0627398,-0.054342974,-0.020187017,0.026779048,-0.009617401,0.055134162,-0.04249179,-0.05690645,0.009965271,-0.0408663,0.011688773,-0.05763504,-0.012766782,-0.04496036,0.115516946,0.07845023,0.023496412,0.0020038774,0.00021927136,-0.03587497,0.012355298,0.015153043,0.0046806606,0.04352506,-0.056302395,0.031847376,-0.008027795,0.01268624,-0.056828286,0.003931278,0.036536425,-0.0048296014,0.01662308,-0.013428101,0.028472234,-0.0072089448,0.08792525,0.022193471,0.024281621,0.040405475,0.017243383,0.00095968624,0.004098359,-0.0015449709,-0.010245158,0.011662097,-0.055557188,-0.04436501,0.04007196,0.010371437,-0.002708484,-0.006391569,-0.0069698184,-0.09577306,-0.029912822,0.048532967,0.019198993,0.046141267,-0.049394276,0.017883984,-0.02974199,0.04775862,0.046334006,-0.0026786302,0.017221265,-0.027158821,0.0012161129,-0.024388952,-0.017729212,-0.0323166,-0.025657173,-0.039046347,-0.03618084,-0.040262874,-0.010815994,0.03291444,0.08352612,0.11876491,0.018772483,0.010135505,0.0388027,0.027569052,-0.035646405,0.010449642,0.00055045227,-0.051844362,-0.019980887,0.03769679,0.0016460867,-0.0082828505,0.007950944,0.03534556,0.05912984,0.026327375,0.020002263,0.09120171,-0.037116088,0.0041481755,-0.03906868,0.024194617,-0.027038738,-0.03651232,0.0107288845,0.03504761,0.025603667,-0.008031469,0.0037834067,0.036738075,0.03442979,-0.09084186,-0.0535258,-0.023885744,0.012340503,-0.037459053,0.032284297,0.011663457,-0.017020999,-0.0070243464,0.0018421332,-0.007590548,-0.0134852305,0.040161155,-0.028829265,-0.0056518572,-0.03490774,0.028786004,0.038219895,-0.0033104483,-0.020845871,0.024807207,0.029959738,-0.03108711,-0.062198885,0.012928322,-0.05202275,-0.0038514738,0.010672583,0.016819155,-0.036921434,0.039706312,0.015016657,-0.043514032,0.047149118,0.04600902,-0.051484417,0.0074860253,0.021316297,0.026961403,0.023141425,0.053145446,0.033102505,0.08215155,0.061703436,0.014087484,-0.053612676,0.038871843,-0.007085088,-0.02266116,0.024110748,0.031243097,0.040110957,0.0029981672,0.009845383,0.08754147,0.051294625,0.00407283,-0.0027799367,-0.018347256,-0.01345612,-0.019605348,0.028043317,0.0012747328,0.039906524,0.011987281,0.011556822,-0.024546284,0.059422098,0.009058734,-0.039099444,-0.0023864368,-0.008926687,-0.026494905,0.041657142,0.021404942,-0.005850601,-0.005314256,-0.0022128734,0.051617693,-0.016503463,0.056957014,0.05754309,-0.008749612,0.0031366758,0.041445237,0.018947544,0.0006556555,0.0017054856,-0.012125454,0.029510358,0.0014342107,-0.08656844,-0.0057753115,-0.02027286,-0.010475251,0.033571463,0.0004086717,-0.0149802845,0.0048674126,-0.02141576,-0.0148413405,-0.029125815,0.017743388,0.09921868,-0.0676486,-0.018230122,0.013682628,-0.008736142,-0.009975194,-0.01823104,-0.03401307,0.015151466,0.0067063523,0.054148134,0.042637236,-0.025606152,-0.021596637,0.018275619,0.10447456,0.008138295,0.0035509907,-0.016145585,0.024071697,0.020002685,0.008274668,-0.026780827,0.009714153,0.033677936,-0.004920501,0.020352622,-0.042344116,-0.10974951,0.020304292,-0.04012007,0.026349172,0.05395488,-0.029438803,-0.0004906817,0.020589296,0.022953348,0.03148992,0.07043288,0.0050040637,-0.010242537,-0.049792774,0.0081646,-0.017772952,-0.0022116269,0.040296525,-0.033609416,-0.008017347,0.04528141,-0.014446354,-0.020879757,0.038812537,0.011558247,-0.01770263,-0.008771055,-0.03318064,0.004801494,0.025969787,0.0010015599,-0.023806006,0.03980707,-0.05668775,0.024511173,0.01189215,0.0062466026,-0.010117751,-0.05953252,0.020232847,0.046564255,-0.00042285625,-0.0319679,0.0012404951,0.030634772,-0.03635805,-0.019504165,-0.0042311163,0.036731653,0.044377323,-0.029809142,-0.012115471,-0.019314708,0.031534754,-0.044730168,-0.024797374,-0.021202454,0.05142721,0.023788234,0.07779402,-0.019053994,0.025955956,-0.058177046,-0.04213475,-0.009589078,0.0131472545,-0.03221191,0.0010299914,0.012483104,-0.00024298595,0.0317192,-0.021409305,-0.013509017,-0.0006671192,-0.04245211,0.058047067,0.036255237,0.029479157,-0.064493835,0.037351217,0.038493477,0.024787435,-0.008475598,-0.0013290102,-0.0025564053,-0.00086089195,-0.03182058,0.015360968,-0.009084122,0.01822799,-0.043494456,0.012060243,-0.027020348,0.023926556,0.009997658,0.030800506,0.03885119,0.0028113916,0.00747093,0.034598406,-0.029925065,-0.04325538,0.03250801,-0.072886154,-0.0073435493,0.009957979,0.021608429,0.0073860073,-0.046343282,0.053470243,-0.026746785,0.077765726,0.044779927,-0.030979637,0.012372198,-0.025379356,0.043296672,-0.043875527,-0.016666856,0.012606691,-0.027271748,-0.016982818,0.029654091,0.013211838,0.026088828,-0.03206539,0.09006789,0.03724065,-0.009466084,-0.0087392805,-0.01654677,0.047982104,0.038403682,0.054646567,0.018900463,-0.009346274,0.030304112,0.0042308164,-0.04142563,-0.03221215,-0.03189738,-0.030790176,-0.009967053,0.011587316,0.0017547274,0.03233591,-0.034184087,-0.043875393,0.0012928417,-0.00035914782,0.02914333,0.0025070796,-0.0652545,-0.00029305305,0.0056771357,0.018160261,0.008961276,-0.035510764,0.003164179,-0.039768618,-0.010741939,0.032294556,0.0004100931,0.0820979,0.04456493,0.039681204,0.053332876,-0.045210276,0.030806918,0.011880999,0.00079764717,-0.008127426,-0.04506885,0.008331057,0.06654985,0.04943268,0.009915303,-0.025298577,-0.014025877,-0.029483005,0.0005445446,0.03883926,-0.016915474,-0.008431783,-0.046997573,-0.0020558448,-0.004355025,0.022933956,0.0052580372,-0.017057113,0.015508087,0.072054654,0.0049711852,0.008259805,0.011059509,0.013376439,-0.004411392,-0.017286876,0.034096878,0.0705965,0.050783735,-0.051113714,0.056886885,-0.006623763,0.0025695872,-0.009413569,-0.043499224,-0.01229955,0.033835277,-0.05202054,-0.036297385,0.04482901,0.012563204,0.02549841,0.018216165,-0.023820894,0.09108812,-0.059885826,-0.021110326,-0.014579651,0.00031201786,0.09942446,-0.008102647,-0.004398638,0.006259006,0.0045996667,-0.032500986,0.028711766,0.06424756,0.02304267,0.009018278,0.031232052,-0.02447287,0.033619914,-0.024135374,-0.039995782,-0.050107863,0.010995771,0.006975502,-0.008526533,-0.013906051,0.0037563252,0.008314538,-0.010928743,-0.006762837,-0.03172299,-0.040663555,0.07127795,0.021301938,0.014992919,0.037097793,-0.014998861,0.048735898,-0.016484074,0.023297405,0.02207446,0.00476666,0.027932893,0.0011471186,-0.023684185,0.018516678,-0.034049727,-0.008749686,-0.06219018,0.064405784,0.03294995,-0.01910763,-0.049725093,0.014885644,0.018854413,0.037518844,-0.042397115,-0.03211902,-0.026122723,-0.063820206,-0.041346867,0.0024249135,0.005047095,-0.0129877515,0.023103325,-0.02808322,0.056838594,-0.08986667,-0.021569336,-0.050765127,0.014606959,-0.0022363516,0.0453432,-0.01444781,0.008692063,0.007259711,-0.054171897,0.01428655,0.055540625,0.0028785109,-0.011151825,0.024201717,0.042455785,-0.0128074875,0.01067235,0.001875233,-0.009547574,-0.02117134,-0.025975024,0.080075,-0.049228277,0.008371616,0.0019998234,-0.023058182,-0.10470495,-0.031086693,0.0054769646,0.047007,0.018263606,0.035463534,0.034185555,0.02581227,-0.06629909,0.005293659,-0.031998336,0.021226967,0.031571068,0.038069185,0.06822646,0.0039467663,-0.05527676,0.011028996,0.030461648,-0.015669134,0.021457177,0.07174977,0.02385473,0.041582946,0.002310303,-0.070081264,-0.06873652,0.020620614,-0.0016560127,-0.015717879,-0.0013667664,0.039410256,-0.03617519,0.0004187283,-0.028997418,-0.0045845252,0.042218212,0.007921118,0.028319648,-0.004417657,-0.027431881,-0.063802265,-0.011587524,0.05687616,0.0053938814,0.009608232,0.099961095,0.0292544,-0.06482621,-0.042027947,-0.03883856,-0.029724877,0.021605939,0.018614141,0.050928563,0.009829847,-0.050404273,0.0026747962,0.041821152,0.017962404,-0.020453304,-0.01032659,0.07712892,0.010521724,0.036111686,-0.02212979,0.023701986,-0.04380918,0.018491581,-0.061712753,-0.037228312,0.043582622,-0.00015330431,-0.017525272,-0.045325384,-0.02232932,0.0023301013,-0.008587679,0.014095946,-0.0059417244,0.030924974,0.03758112,-0.032735944,-0.08597603,-0.021288456,0.0106600225,-0.038548697,0.012075732,0.013877882,-0.041336726,-0.022846688,-0.0032259384,-0.008458654,-0.0037668617,-0.006981041,-0.02983625,0.046318684,-0.03553772,0.01916827,0.0036963725,0.031391423,0.07492329,0.01557127,0.013536748,0.0008889861,0.0074369474,0.013471817,-0.036988564,-0.019479483,-0.06303214,-0.07344163,0.026876919,0.07403144,0.012173915,0.044036683,-0.011623957,-0.0370863,-0.057060998,-0.01814172,0.009628873,0.0609648,0.06301647,-0.014395986,-0.053153902,0.05909502,-0.05113074,0.026712151,-0.0013676289,0.012600129,0.007451962,0.03406169]	Keywords: multimodal data, unified framework, semantic relations, cross-modal attention, intra-modal attention\nKey Objects: multimodal data, unified framework, attention\nRefers to Images: None\nHypothetical Questions:\n- Why is it beneficial to use multimodal data in AI applications?\n- What are some of the challenges in designing a unified framework for multimodal Transformers?\n- How could improved intra-modal and cross-modal attention mechanisms contribute to a better multimodal Transformer?\n---\nSummary:\nIntegrating multimodal data, such as text, images, video, and audio, is a promising area for future Transformer research to create a unified framework capable of capturing semantic relationships across different data types.\nOriginal Text:\n- (3) Unified Framework for Multimodal Data . In many application scenarios, integrating multimodal data is useful and necessary to boost the task performance. Moreover, the general AI also needs the ability to capture the semantic relations across different modalities. Since Transformer achieves great success on text, image, video, and audio, we have a chance to build a unified framework and better capture the inherent connections among multimodal data. However, the design of the intra-modal and cross-modal attention still remains to be improved.  \nFinally, we wish this survey to be a hands-on reference for better understanding the current research progress on Transformers and help readers to further improve Transformers for various applications.\nContextualized Text:\nTo further improve Transformer models, researchers are exploring the development of a unified framework for integrating multimodal data like text, image, video, and audio. This framework aims to better capture semantic relations across different modalities, though improvements are still needed in the design of both intra-modal and cross-modal attention mechanisms.	{"tags": ["AI", "multimodal", "future directions", "architecture"], "doc_id": "27849426-300f-4464-8086-9b10a4bcfaff", "summary": "Integrating multimodal data, such as text, images, video, and audio, is a promising area for future Transformer research to create a unified framework capable of capturing semantic relationships across different data types.", "doc_type": "text", "entities": ["Transformer"], "keywords": ["multimodal data", "unified framework", "semantic relations", "cross-modal attention", "intra-modal attention"], "key_objects": ["multimodal data", "unified framework", "attention"], "contextual_text": "To further improve Transformer models, researchers are exploring the development of a unified framework for integrating multimodal data like text, image, video, and audio. This framework aims to better capture semantic relations across different modalities, though improvements are still needed in the design of both intra-modal and cross-modal attention mechanisms.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "9 CONCLUSION AND FUTURE DIRECTIONS"}, "hypothetical_questions": ["Why is it beneficial to use multimodal data in AI applications?", "What are some of the challenges in designing a unified framework for multimodal Transformers?", "How could improved intra-modal and cross-modal attention mechanisms contribute to a better multimodal Transformer?"]}
7cb1556c-5fe1-41b0-9aeb-f6fbdd4d96a9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.015051547,0.021464074,-0.011096597,0.008533303,0.01411716,0.065914616,-0.0047361758,0.06978962,0.044448793,-0.032329343,0.005661732,0.022149477,0.00366317,-0.0061794394,-0.024923827,0.06545479,-0.015989238,0.057059087,0.040295802,-0.039105147,0.024324223,0.021019774,0.007990431,-0.041134894,-0.014975055,0.03030597,-0.0064908774,-0.04031022,0.01587846,0.024208784,0.007736511,-0.067405954,0.04505046,8.089866e-05,0.0023381303,0.016199948,0.00375194,0.007929194,0.0024731797,-0.015279467,-0.0109367985,0.058688127,-0.075221926,0.005622013,-0.027510602,-0.044637855,-0.079280145,-0.03659442,0.050222818,-0.0163127,-0.031303056,0.010633125,-0.013453646,-0.05900953,0.005395398,-0.012967681,-0.023584079,-0.04073392,0.06935418,-0.10979017,-0.0221315,0.026844518,-0.058517516,-0.021312404,0.044049617,-0.07227367,-0.043279108,-0.0033964438,0.0005117852,0.12618399,-0.014970825,0.029862992,-0.02627237,-0.074893385,0.10988198,0.05678297,0.004581148,-0.029094866,-0.074067354,-0.015336689,-0.0033082843,0.061123006,-0.040150847,-0.036597665,0.09318776,-0.0009979954,-0.03931193,-0.034790378,0.08195468,-0.033768803,0.042304445,-0.0052666874,-0.006590478,0.07207979,-0.018636908,-0.073269494,0.030818947,-0.04033319,0.0046920357,-0.034557696,-0.0054000407,-0.024808526,0.04772905,0.09459116,0.014860494,-0.020940537,-0.041523907,-0.047620412,0.008538171,0.015621521,-0.0026907318,-0.00079332927,-0.03666723,-0.0028866727,-0.01475898,-0.032341633,-0.051489614,-0.01035856,-0.007575719,0.014092622,-0.020618632,-0.014130761,0.021468606,0.0059457785,0.07456336,0.06411902,-0.004627776,-0.0111560365,-0.027479893,-0.012622964,-0.0014173534,-0.01788139,-0.0061502755,0.006831748,0.0012236073,0.018631872,0.07417626,-0.03706519,0.0036483896,0.013087846,0.02331009,-0.0043000826,-0.00079158955,0.04493538,-0.014406912,-0.02084403,-0.018305806,-0.003466805,-0.013768396,0.032376762,0.009058777,0.010868204,0.033111144,0.022771036,0.0029510253,-0.00094412244,-0.0011264245,-0.05792113,0.013528347,-0.00254049,-0.055091437,-0.04717061,-0.037564047,0.02621487,0.035231005,0.109406024,0.024888337,-0.007872688,0.060262337,-0.00054115464,-0.05689096,0.016645677,-0.021664785,-0.00056713907,0.025610825,-0.04051936,0.005480772,-0.0053379,-0.043142337,0.020953344,0.022993106,0.011886408,0.0095242625,0.045518465,-0.015099753,0.022174088,0.008624492,0.07708692,-0.029404515,-0.0375009,0.03322514,-0.008221861,0.0009552359,0.0007643532,0.041331112,0.029083032,0.04108364,-0.053883865,-0.08635877,-0.0093843965,-0.025092749,-0.0047678067,0.02186331,-0.005151925,0.028796261,-0.02523803,-0.014511844,-0.03661408,-0.038918205,-0.026224691,0.02129329,-0.023669433,-0.0022154977,0.06352379,0.06165333,-0.008932522,-0.030671218,0.0100043,0.03667115,-0.0039672167,-0.07188077,0.018271673,0.0047395425,0.04356297,-0.036326077,1.183451e-05,-0.045874596,0.022186464,0.008958312,-0.029416159,0.05274831,0.016391836,-0.050127827,0.0005868106,-0.013855842,0.058541473,-0.024377268,-0.022421947,0.033030305,0.05860519,0.026520694,0.042611413,0.0033023308,0.041451402,0.006591197,0.001802565,0.04585587,0.058514457,-0.045514587,0.022668138,0.000994327,0.06879277,0.048519053,-0.0028822324,0.018263571,-0.015126143,0.028442716,0.040800218,0.045122378,0.015719665,-0.012155796,0.014733353,0.03486705,0.0034282384,0.01967405,0.0076864352,0.039513096,0.042750526,0.009114285,0.013531194,0.02896313,0.065858535,-0.00899463,-0.023347106,-0.06707901,-0.012149457,-0.008823616,0.0099516595,0.04391371,0.011336985,-0.019241156,-0.01759004,0.022786697,-0.0035112717,0.026086943,-0.013771711,-0.0058591366,-0.008483703,-0.08644461,-0.020577775,0.018277807,-0.055887155,0.008586041,-0.017162658,0.034653235,0.012838575,0.006011194,-0.045265224,-0.032424394,0.016913831,0.08168295,-0.023106877,0.030530566,0.0017619428,-0.018260064,0.011317326,-0.046143983,0.013624877,0.03656191,0.0077690533,-0.0041984334,-0.009880382,-0.021669867,0.0028284134,0.040492695,0.121546,0.055398185,-0.027531885,-0.030064601,0.009665669,-0.0027184228,-0.010905115,0.01863728,0.031389825,0.041720632,0.00042830966,-0.013018378,-0.028072717,-0.07127612,0.06269824,-0.07121032,0.042709455,0.038104396,0.011611786,-0.0154212145,-0.00819432,0.063404016,0.016307881,0.03915948,0.019790443,0.010421164,-0.04845772,-0.033628732,0.0061411946,0.010617469,0.034075707,0.010896438,0.019431036,0.06365622,0.011898277,0.028332885,0.05823085,0.022973232,0.040774796,0.02571316,-0.02531696,0.012362965,0.021384802,0.003798141,0.035027225,0.06030718,-0.017246708,0.058230765,-0.024652097,0.0019575877,0.0062841685,-0.058326136,-0.0420307,0.05804653,-0.028356982,-0.0052377074,-0.05184513,0.060837917,-0.020958785,-0.020622658,-0.01721092,0.039197646,0.02503834,-0.04682258,-0.016378878,-0.008803351,0.02221397,-0.050135974,-0.012138105,-0.020120302,0.03893332,-0.0015265199,0.017219538,0.009239355,0.04142161,-0.035462648,0.0013225608,0.04816497,-0.026130727,0.003627847,-0.0034532016,0.0346397,0.03364175,0.00817129,-0.034039125,0.06286093,0.026079921,-0.028860133,0.03906873,0.060570605,0.013688854,-0.06617857,0.0028660477,0.08003426,0.01012757,0.037761115,-0.0153073985,-0.027395787,0.04820757,-0.0271728,-0.015873434,-0.0039124433,0.012923147,-0.052275185,-0.01211344,-0.04136018,0.05538941,0.0209528,0.03757675,0.052766364,-0.022368766,-0.0281478,0.044980023,-0.0066831433,-0.041958597,0.03176419,-0.06962167,-0.019071825,0.04023222,0.0498897,0.026489932,-0.08204629,0.06665604,-0.10010775,0.020212267,0.05328936,-0.016396917,0.027922567,-0.018903153,-0.010297311,-0.029491372,0.009980511,0.018754832,-0.002469245,-0.0011115634,-0.0123455245,0.021056425,0.011557592,-0.009492054,0.00020455795,-0.006023473,-0.039274443,0.014258368,-0.04568764,0.0028941277,-0.008786608,0.055691537,-0.0035866704,0.0072592767,0.0072378907,-0.009141961,-0.047686752,-0.0058748685,-0.032251243,0.009519019,0.0037111687,0.023128971,-0.057404432,0.025182858,0.00027658694,-0.038509317,-0.021749903,0.075153135,0.05655508,-0.0028799188,-0.04913599,-0.043660607,-0.006583828,0.07445801,-0.02618592,-0.015720373,-0.010898187,-0.031756144,-0.069773324,0.03185807,0.018502617,0.089475445,0.018779008,0.020572137,0.020369789,-0.04572255,-0.0016099158,-0.02433539,0.0034379768,-0.04661632,-0.040219165,0.0008543009,0.05882992,-0.00146184,0.023085391,-0.0015276553,-0.0015542933,0.007763307,0.010617489,0.068954326,0.03290031,0.01648976,-0.0067037935,0.0029264328,-0.026985504,0.048167158,-0.018591642,0.019550681,0.039593674,0.0060014166,0.01838025,-0.0040281587,-0.010726736,-0.021184284,0.015597806,-0.011107932,0.030687118,0.047631368,0.046735834,-0.023972541,0.08514202,-0.019077074,-0.048485633,0.04028462,-0.032489207,-0.03524331,0.023008289,-0.07563798,-0.025724843,0.021569941,-0.0021827118,-0.018775675,0.0041395817,-0.048821617,0.08325789,-0.04711184,0.036404636,-0.03195022,0.005821843,0.11597621,-0.0029548563,-0.006139821,-0.000933915,-0.02004491,-0.014979792,0.0174004,0.068463214,-0.006152699,-0.00065793353,0.06825805,-0.004693617,0.027055878,-0.013131417,-0.049122132,-0.031883396,-0.04041072,-0.032076474,0.03186114,-0.01641551,0.026385006,0.0023899507,0.0065713306,0.00441737,0.02405715,-0.025604133,0.034231074,0.033370044,-0.0073154797,0.018233905,-0.059131082,0.03992975,0.035187624,-0.009908519,-0.026006034,0.031472996,0.034862496,-0.008588898,0.0011994203,0.028430358,0.0038809464,0.004388451,-0.028975861,0.05785996,0.022776872,-0.04553431,-0.01826238,0.021911182,0.045206673,0.022291651,-0.033691946,-0.02395513,-0.006682256,-0.02675208,0.004646789,0.02010501,0.001298975,0.0022899394,0.012312178,-0.029198624,0.07762839,-0.11458918,-0.022932593,-0.029604835,0.032380283,-0.016067918,0.017092695,0.00044612383,-0.0052529536,-0.00952146,-0.017011005,0.026231252,0.032182366,-0.06133117,0.005148239,-0.004564382,-0.020612035,-0.005863721,-0.02856809,-0.038055133,0.006362398,-0.023377286,-0.01464426,0.08811325,-0.027389556,-0.021156915,0.036905456,0.010969313,-0.06318825,-0.040293097,0.024856025,0.012191468,-0.002461562,-0.020868618,0.023827443,0.052768994,-0.035986375,-0.016976478,-0.056443017,0.04021814,0.0003087418,-0.016226787,0.028205877,0.015029509,-0.06864077,-0.0076890215,-0.021930128,-0.03855824,-0.011997652,0.034191977,0.057757616,0.044189658,0.06147744,-0.062352967,-0.107750475,-0.019612413,-0.02172189,-0.0024277726,-0.04192384,0.05346313,-0.010961178,-0.018535113,0.017227545,-0.013873235,-0.019179124,0.037332434,0.0227543,-0.050227374,-0.00853941,-0.029250417,0.017983418,0.02557335,-0.033940624,-0.0008011425,0.036507968,0.02561285,-0.08349063,-0.027064208,-0.026035484,-0.017431421,0.030486891,0.020557577,0.049119346,0.0091003375,-0.03536139,-0.0023224093,0.033543974,0.003168553,-0.023192838,-0.00519034,0.028613022,-0.03820635,0.037006143,0.0031131569,-0.017211603,-0.08974085,-0.008611471,-0.0024999895,-0.08213417,0.029916694,0.038022432,0.0037792667,-0.042757098,-0.049371317,-0.02115314,-0.002216572,0.03987769,0.02777836,0.013496413,-0.00043678717,-0.045114826,-0.02432456,-0.0074710874,-0.035654016,0.027207185,-0.0008446577,0.0036562353,0.014647556,-0.010415699,0.0073275426,0.015115479,-0.010776401,-0.04642497,0.036331255,0.012544321,-0.03697178,0.046919722,-0.0065878173,0.015041532,0.05873698,-0.017710552,0.017218191,-0.011990838,0.016707556,0.023523377,-0.002166237,0.006214003,-0.02587688,-0.093197934,0.018158806,0.044593416,-0.0012980719,0.017672943,-0.016300647,-0.035191644,-0.04752842,-0.052809417,-0.015854305,0.06222656,0.018671617,0.024935665,-0.0464512,0.022028932,-0.032814674,0.050748702,0.025617102,-0.019075163,0.026850265,-0.012070078]	Keywords: Transformers, Natural Language Processing, Computer Vision, Deep Learning, Architecture, References, Layer Normalization, Video Vision Transformer, Encoding, Language Modeling, Computer Vision, Survey, Deep Learning, Machine Learning, Neural Networks, Attention Mechanism, Transformers Architecture, Transformer Model, Attention Is All You Need, ETC, ViViT, LayerNorm, Character-Level, Long Sequences, Structured Input, Encoding Long Input, ViViT Transformer, ETC Encoder\nKey Objects: Survey, References, Architecture, Model, Encoder, Input, Sequence, Attention Mechanism, Neural Network, Deep Learning, Transformer Architecture\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of this list?\n- What topics do the references cover?\n- What information is provided for each reference?\n- What is a survey on Transformers?\n- What is the meaning of DOI?\n---\nSummary:\nThis is a list of references for a survey on Transformers, containing a variety of resources related to the Transformer architecture and its applications. The references cover a wide range of topics, including character-level language modeling, video vision transformers, layer normalization, encoding long and structured inputs, and various applications in areas like video processing and natural language processing. Each entry includes author(s), year, title, and often a DOI or arXiv identifier.\nOriginal Text:\n## REFERENCES  \n- [1] Joshua Ainslie, Santiago Ontanon, Chris Albert, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang, 2020. ETC: Encoding Long and Structured Inputs in Transformers. In Proceedings of EMNLP. Online, 268-284. https://doi.org/10.16853/v1.2020.empIm-main.19\n- [2] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2019. Character-Level Language Modeling with Deeper Self-Attention. In Proceedings of AAAI: 3159-3166. https://doi.org/10.1609/aaai.v33i1.30313159\n- [3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lu'ci'e, and Cordelia Schmid. 2021. ViViT: A Video Vision Transformer. arXiv:2103.15691 [cs.CV]\n- [4] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. CoRR abs/1607.06450 (2016). arXiv:1607.06450\nContextualized Text:\nThis is a list of references for a survey on Transformers, containing a variety of resources related to the Transformer architecture and its applications. The references cover a wide range of topics, including character-level language modeling, video vision transformers, layer normalization, encoding long and structured inputs, and various applications in areas like video processing and natural language processing. Each entry includes author(s), year, title, and often a DOI or arXiv identifier.	{"tags": ["References", "Survey", "Transformers", "Deep Learning"], "doc_id": "7cb1556c-5fe1-41b0-9aeb-f6fbdd4d96a9", "summary": "This is a list of references for a survey on Transformers, containing a variety of resources related to the Transformer architecture and its applications. The references cover a wide range of topics, including character-level language modeling, video vision transformers, layer normalization, encoding long and structured inputs, and various applications in areas like video processing and natural language processing. Each entry includes author(s), year, title, and often a DOI or arXiv identifier.", "doc_type": "text", "entities": ["Transformers", "Joshua Ainslie", "Santiago Ontanon", "Chris Albert", "Vaclav Cvicek", "Zachary Fisher", "Philip Pham", "Anirudh Ravula", "Sumit Sanghai", "Qifan Wang", "Li Yang", "Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones", "Anurag Arnab", "Mostafa Dehghani", "Georg Heigold", "Chen Sun", "Mario Lu'ci'e", "Cordelia Schmid", "Lei Jimmy Ba", "Jamie Ryan Kiros", "Geoffrey E. Hinton", "ETC", "ViViT", "LayerNorm", "Character-Level", "Long Sequences", "Structured Input", "Encoding Long Input"], "keywords": ["Transformers", "Natural Language Processing", "Computer Vision", "Deep Learning", "Architecture", "References", "Layer Normalization", "Video Vision Transformer", "Encoding", "Language Modeling", "Computer Vision", "Survey", "Deep Learning", "Machine Learning", "Neural Networks", "Attention Mechanism", "Transformers Architecture", "Transformer Model", "Attention Is All You Need", "ETC", "ViViT", "LayerNorm", "Character-Level", "Long Sequences", "Structured Input", "Encoding Long Input", "ViViT Transformer", "ETC Encoder"], "key_objects": ["Survey", "References", "Architecture", "Model", "Encoder", "Input", "Sequence", "Attention Mechanism", "Neural Network", "Deep Learning", "Transformer Architecture"], "contextual_text": "This is a list of references for a survey on Transformers, containing a variety of resources related to the Transformer architecture and its applications. The references cover a wide range of topics, including character-level language modeling, video vision transformers, layer normalization, encoding long and structured inputs, and various applications in areas like video processing and natural language processing. Each entry includes author(s), year, title, and often a DOI or arXiv identifier.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the purpose of this list?", "What topics do the references cover?", "What information is provided for each reference?", "What is a survey on Transformers?", "What is the meaning of DOI?"]}
17796130-989a-4314-ac4a-bbef630734e6	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.00864854,0.0039440896,-0.006119499,0.035864376,0.01919469,0.07977273,0.03601835,0.075970575,0.032323975,-0.041215412,-0.017883252,-0.0022292307,0.03026233,-0.010103364,-0.033629436,0.03917184,-0.017940028,0.09876917,0.055991042,-0.010173536,0.04707158,0.026497485,0.023439584,-0.033234987,0.006865847,0.02865143,0.016613366,-0.014630353,0.014290723,0.039247625,0.015173377,-0.040612854,0.0262273,0.0060545634,-0.0036739628,-0.0059550796,-0.025267892,0.018528089,-0.00787021,-0.0069524916,-0.0033030717,0.040850203,-0.08340057,0.0039810934,0.02180646,-0.0635981,-0.09777319,-0.044683497,0.039520327,-0.010871549,-0.049221046,0.04164801,-0.01987651,-0.028147206,0.015644878,0.000631981,-0.008790966,-0.037969053,0.03460432,-0.09372012,-0.021133991,0.024443958,-0.056551863,-0.010891217,0.04804926,-0.06737191,-0.011689978,-0.0024808333,0.021887023,0.10343586,-0.049090266,0.026336215,-0.011795823,-0.04343901,0.12907809,0.014926819,-0.026993638,-0.045447454,-0.08922371,-0.035098575,-0.008889028,0.056353956,-0.047252994,-0.04590397,0.06058598,0.020022597,-0.05874984,-0.02711254,0.079722464,-0.00781297,0.008722245,-0.018603995,-0.011660311,0.05092742,-0.0065020816,-0.057813097,0.024469271,-0.044451028,-0.030202338,-0.028432004,-0.022040403,-0.026369056,0.08810901,0.11380901,0.043758646,-0.030203288,-0.02946571,-0.050827052,0.033726066,0.011068343,0.012521211,0.021764738,-0.024849316,0.0034918655,-0.012260936,-0.019666929,-0.04756903,0.018318543,-0.007215141,0.0073752026,0.011499549,0.018206256,-0.0019832728,0.019263245,0.044351988,0.02243042,-0.028870692,-0.029729322,-0.039059106,0.015921403,0.007411266,0.0032433898,-0.027393367,0.031499583,0.0012366511,-0.012110385,0.027419582,-0.031088797,0.010326973,0.021351395,0.010162085,-0.019375524,-0.04830693,0.0038260939,-0.040493306,-0.023161722,-0.034243684,-0.0050901202,-0.017868103,0.047455125,0.0109597705,-0.013374747,0.014230555,-0.00047152836,0.0027721254,0.026629252,-0.01375359,-0.06649839,0.006469593,0.026922764,-0.055030763,-0.05157515,-0.015101978,0.04032226,0.045816127,0.13657728,0.010063774,-0.004748247,0.06483273,0.035795636,-0.055847798,0.009923825,-0.01462828,-0.010376162,0.005516841,-0.02696364,0.04262951,-0.007114673,-0.020520171,0.01730896,-0.0010791398,0.034200992,0.010795479,0.01408604,-0.04197478,0.011346229,0.053404387,0.066852644,-0.04758748,-0.033465583,0.04211526,-0.013359574,-0.0147455195,-0.017872076,0.030832274,0.00446111,0.044118516,-0.06985696,-0.06622919,-0.024493285,-0.007137007,0.011211611,0.017744726,-0.008109743,0.023749983,-0.016719403,-0.018810248,-0.032368205,-0.031074591,-0.0028499714,0.0075922627,0.0013827443,0.0031293982,0.038661756,0.013660266,-0.01854307,-0.011627794,0.040153388,0.046206787,0.002033059,-0.05604996,0.013660748,-0.012359839,0.01817846,-0.0520212,-0.0044144257,-0.024769943,0.014280348,-0.0069224723,-0.018161446,0.044321656,0.022048065,-0.049084276,0.024102274,-0.020235592,0.047868174,-0.010801053,-0.009530334,0.0016702949,0.067814946,0.039075356,0.067014895,0.0044499086,0.06945988,0.035484683,-0.019824147,0.057903647,0.04665323,-0.027417775,0.03383687,0.015834482,0.030554712,0.059348926,0.0016547942,0.013376466,-0.03344474,0.007836294,0.041922852,0.03998208,-0.0015255712,-0.022151593,-0.040008556,0.042334877,-0.005847072,0.033746984,0.0043900404,0.014681632,0.019677417,0.016040225,0.01533523,0.009599715,0.038080547,0.015238596,-0.024068408,-0.046301823,-0.00090081134,0.0050424975,0.001423954,-0.0018572368,-0.028371977,0.008455109,-0.022674752,0.014203645,0.005368287,0.014238067,-0.032658722,-0.039207418,-0.011074264,-0.072108686,0.0009688476,-0.0019357525,-0.047527198,0.008889331,-0.0013538334,-0.016582835,-0.032914057,0.0049108686,-0.018660229,-0.019803368,0.019583015,0.06452108,-0.025832748,0.035271496,0.014078199,0.014598927,0.046263356,-0.060529508,-0.0014845534,0.05048256,-0.010019287,0.01818723,0.010039433,0.0067362455,-0.005063687,0.023570491,0.090646856,0.06657803,-0.036010798,-0.0065263924,0.040415626,0.0176376,-0.0023713429,0.01911142,0.05979635,0.019408653,-0.003119974,-0.02605965,-0.03592371,-0.0701579,0.09471501,-0.071887605,0.03647243,0.044179115,0.0012764387,-0.011848206,-0.0053176475,0.048324283,0.024470402,-0.023769995,0.010476619,-0.017996566,-0.08828343,-0.025804635,-0.019790607,0.0023113657,0.004189735,0.0042570154,0.018315794,0.029685263,0.0080262935,0.010066324,0.017777795,0.034543887,0.009918803,0.026205655,-0.018555071,0.0025295196,0.009381125,0.040759593,0.0156069035,0.08882034,-0.015211001,0.08568057,-0.019275622,0.011986499,0.0045985705,-0.059579782,-0.047209565,0.06227125,0.0054975147,-0.007693156,-0.060450103,0.04970673,-0.062205147,0.00031430938,0.022054363,0.037210785,0.020074237,-0.010008464,-0.010601304,-0.024330117,0.013733914,-0.016465632,-0.00958266,-0.029316394,0.028468687,0.016654812,0.05444849,-0.0036102023,0.05538041,-0.029958257,-0.0032035299,0.05273108,-0.0039415588,-0.013216884,-0.016457628,0.041097812,0.032890324,-0.01911408,-0.027386103,0.042128548,0.040930882,-0.028319623,0.055084374,0.07196683,0.012771327,-0.079492554,0.012516627,0.05694341,0.009284491,0.01852206,-0.04534121,-0.076600775,0.027251793,-0.039036863,-0.0113077015,-0.009063767,0.0444196,-0.051763613,-0.011893502,-0.071281545,0.044988852,0.016784849,0.050376054,0.031081647,-0.02870232,-0.027384,0.04992984,-0.0019809902,-0.0235067,-0.015417555,-0.078991525,-0.0050267307,0.06164526,0.021283783,0.035792172,-0.07169027,0.08224241,-0.085852645,0.0057290634,0.04268683,-0.060231876,0.0424238,-0.0076102857,-0.010949644,-0.018650172,0.032086376,0.012404158,-0.019782972,-0.008877983,-0.0128314635,0.01333044,0.020066008,-0.022432026,-0.013048563,-0.024786662,-0.025075497,-0.01214697,-0.05315965,0.0038492093,-0.008711716,0.06689892,-0.0025955366,-0.0042450717,0.00074553763,-0.017815717,-0.044418532,-0.010551368,-0.029219884,0.018436506,0.027461786,0.009531394,-0.037148573,0.008328085,0.00715869,0.0030485182,-0.0005010126,0.037810322,0.04373665,-0.015134703,-0.05268609,-0.06807122,0.011547933,0.07677364,-0.014963892,0.0047845533,0.0012040689,-0.030790456,-0.07300573,0.027878039,0.009542905,0.07418012,0.01860655,0.037688695,0.021904455,-0.01657912,-0.03323282,0.025875732,0.023434129,-0.019398022,-0.014877268,-0.0024569333,0.07729233,0.005460737,-0.0071527017,0.0019330056,0.017978327,-0.007480892,0.012063161,0.0602094,0.041312452,-0.008172101,-0.0027130244,-0.020759942,-0.013595655,0.05385893,-0.03227727,0.025645956,0.008307265,0.037417613,-0.024575947,-0.0054188734,0.00824343,0.0076810447,0.0013722666,0.0049253437,0.054545958,0.03991186,0.015253613,-0.030266829,0.046783093,-0.02688854,-0.03202286,0.035283018,-0.077327594,-0.009396041,0.009429089,-0.043067887,-0.021226393,0.020322666,-0.016712034,0.013260215,0.020287544,-0.05831753,0.08951175,-0.03232091,0.03676549,-0.0136170415,0.0047964975,0.10075528,0.032232463,-0.000110838184,-0.045729764,0.02015741,-0.04883217,0.009797332,0.06852999,-0.004605628,-0.022485947,0.06437869,0.0015843159,0.02844135,0.01517536,-0.041756414,0.005716386,-0.07060913,-0.011123304,0.010417304,-0.026472148,0.02532493,-0.0014553246,0.008507443,0.019207936,0.031749394,-0.01200601,0.025704427,0.04930519,0.0022433673,0.02733202,-0.033258017,0.04573518,0.014191604,0.0118061155,-0.009653506,0.020045217,0.038168017,-0.021208294,0.004219525,0.015858477,-0.02798939,-0.0030463834,-0.010412334,0.069049746,-0.017594123,-0.006099018,-0.0040866053,0.02739081,0.05016834,0.04334671,-0.024568083,-0.03278511,-0.03795352,-0.020009384,-0.030968433,0.027262365,0.014968961,0.0009198452,0.0015647128,-0.050312098,0.08740104,-0.09011149,-0.021004077,-0.068155326,0.049989015,-0.0026211329,-0.02031079,0.013235213,-0.030935181,-0.037914082,-0.049040645,0.02179802,0.06608818,-0.027111642,0.0029563343,-0.032724846,0.0009832467,0.0012882267,0.009986824,-0.004810354,0.0036077893,-0.00854987,-0.015149934,0.08651671,-0.0060577556,-0.018549249,0.032341454,-0.013610023,-0.027734788,-0.03926452,0.014582091,-0.005493765,0.035783723,-0.01684961,0.037642278,0.018670892,-0.056063384,-0.017764492,-0.067865334,0.006843552,-0.0096481275,-0.0278646,0.0035370742,-0.011129681,-0.07313613,0.01256775,-0.02822197,-0.031759884,-0.008307012,0.011353082,0.062471434,0.036330253,0.07784464,-0.063602634,-0.065340616,-0.028596407,-0.049432095,-0.0048830365,-0.03198358,0.06910945,-0.004635817,-0.025703516,0.02043645,-0.0063289516,-0.006381433,0.0050883824,0.016121779,-0.03767802,-0.009792917,0.006797597,-0.009675899,0.026500303,-0.011046795,0.0044617266,0.069826365,0.05050908,-0.07107466,-0.05440537,-0.05982923,-0.006855604,-0.0070253364,0.010088598,0.03792444,0.026837673,-0.026091332,-0.011084851,0.085902266,0.040821005,-0.016438467,0.00038464362,0.050884858,-0.03075945,0.007260131,-0.008032324,-0.024435755,-0.1045913,0.003169995,-0.0035462088,-0.04615843,0.043149773,0.01975708,0.0028767486,-0.019318573,-0.015915116,-0.015361968,0.033502415,0.044228666,0.053641494,0.016073085,0.037013713,-0.0036030745,-0.057015233,-0.01349457,0.004544101,0.028615851,-0.010234447,0.035088874,0.0029090482,0.0029690468,0.0045650476,0.03406218,-0.016897481,0.0017853328,0.012448615,0.043066364,-0.014628094,0.036286693,0.00029083542,-0.005331861,0.06595294,0.0039936383,0.05158901,-0.024114918,0.033846088,0.025237266,0.028343214,0.0049571986,-0.011781642,-0.08017736,0.028022721,0.028850455,0.016812935,0.039355174,0.0027448242,-0.041981004,-0.039926942,-0.026533237,-0.026372876,0.025678037,0.014533493,0.008953803,-0.059384074,-0.0033169047,-0.031241342,0.040527403,0.026003428,0.015638284,-0.007678168,-0.02344827]	Keywords: Transformer, Neural Networks, Machine Learning, Layer Normalization, Convergence, Attention, Machine Translation, Sequence Modeling, arXiv, References, Deep Learning, Sequence Models, Computation, Quality, Representation, Architecture, Transformer Architecture, Deformable DETR, Informer, Poolingformer, LazyFormer, ReZero, Transparent Attention, Memory Efficient, Adaptive Input, Faster Training, Efficient Computation, Sparse Expert Models, Deformable Transformers, Local Recurrent Neural Network, Binary Partitioning, Hierarchical Bidirectional Transformers, Attention Mechanism, Deformable Attention, Pooling Attention, Document Modeling, Long Sequences, Long Context, Pre-training, Document Summarization, End-to-End Object Detection, Speech Synthesis, Adaptive Attention\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of this list?\n- What are some of the key topics covered in these references?\n- Where can I find the full text of these papers?\n- Why are Transformers so important?\n---\nSummary:\nThis is a list of references related to Transformer models and related techniques. The list includes papers on layer normalization, faster convergence, adaptive input representations, controlling computation versus quality, and training deeper neural machine translation models. Many references are available as arXiv preprints.\nOriginal Text:\n- [4] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. CoRR abs/1607.06450 (2016). arXiv:1607.06450\n- [5] Thomas Bachleer, Bodhissatra Pasmaad Majunder, Huunran Henry Mao, Garrion W. Cottrell, and Julian J. Mcauley. 2020. ReZero is All You Need: Fast Convergence at Large Depth. CoRR abs/2004.8887 (2020). arXiv:2003.04887\n- [6] Alexei Baevski and Michael Alui. 2019. Adaptive Input Representations for Neural Language Modeling. In Proceedings of ICLR . https://openreview.net/forum?id=ByxZX2QqFQ\n- [7] Ankur Bapna, Naveen Arivahzagan, and Orhan Firat. 2020. Controlling Computation versus Quality for Neural Sequence Models. arXiv:2007.01067 [cs.LG]\n- [8] Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. 2018. Training Deeper Neural Machine Translation Models with Transparent Attention. In Proceedings of EMNLP. Brussels, Belgium, 3028-3033. https://doi.org/10.18653/ v1/D18-1338\nContextualized Text:\nThis is a list of references related to Transformer models and related techniques.	{"tags": ["references", "survey", "transformers"], "doc_id": "17796130-989a-4314-ac4a-bbef630734e6", "summary": "This is a list of references related to Transformer models and related techniques. The list includes papers on layer normalization, faster convergence, adaptive input representations, controlling computation versus quality, and training deeper neural machine translation models. Many references are available as arXiv preprints.", "doc_type": "text", "entities": [], "keywords": ["Transformer", "Neural Networks", "Machine Learning", "Layer Normalization", "Convergence", "Attention", "Machine Translation", "Sequence Modeling", "arXiv", "References", "Deep Learning", "Sequence Models", "Computation", "Quality", "Representation", "Architecture", "Transformer Architecture", "Deformable DETR", "Informer", "Poolingformer", "LazyFormer", "ReZero", "Transparent Attention", "Memory Efficient", "Adaptive Input", "Faster Training", "Efficient Computation", "Sparse Expert Models", "Deformable Transformers", "Local Recurrent Neural Network", "Binary Partitioning", "Hierarchical Bidirectional Transformers", "Attention Mechanism", "Deformable Attention", "Pooling Attention", "Document Modeling", "Long Sequences", "Long Context", "Pre-training", "Document Summarization", "End-to-End Object Detection", "Speech Synthesis", "Adaptive Attention"], "key_objects": [], "contextual_text": "This is a list of references related to Transformer models and related techniques.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the purpose of this list?", "What are some of the key topics covered in these references?", "Where can I find the full text of these papers?", "Why are Transformers so important?"]}
b083cd9d-c878-433a-b1fa-d368cd50b3b0	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.007345224,0.022720696,-0.0037846921,0.024523612,0.023111783,0.061818972,0.016554449,0.055197913,0.01748022,-0.04674905,-0.018452277,0.0005175414,0.015535835,0.0032812573,-0.05935154,0.054994836,0.012774095,0.05511175,0.029981658,-0.006529044,0.060320012,0.02069818,-0.0205811,-0.01695923,-0.0013551738,0.035634544,0.018192789,-0.05464005,0.025848605,0.015943348,0.014056845,-0.053907022,0.024159946,-0.0027729133,0.010605607,0.0017515398,-0.011298008,0.0006655468,0.017579108,-0.04255904,-0.021170938,0.05147333,-0.09040324,-0.015041833,0.0069407057,-0.062254284,-0.09470295,-0.019189827,0.019570014,-0.031204786,-0.044555638,0.0108405305,-0.055006135,-0.043546714,0.008820256,-0.047504053,0.0024048616,-0.027747208,0.037301216,-0.11206706,-0.021390343,0.034606557,-0.043796394,-0.026549358,0.008205623,-0.05500768,-0.03225928,0.009209506,0.03389021,0.09758108,-0.046170305,0.0036194173,-0.021585695,-0.031522863,0.1050146,0.042075597,-0.011640079,-0.0033648533,-0.08088069,-0.02484989,-0.035098404,0.09588872,-0.038388792,-0.02761567,0.06666198,-0.018672094,-0.025631646,-0.039408498,0.058860175,-0.00015056817,0.019279476,-0.008330832,-0.010095051,0.059956126,0.009052099,-0.07766948,0.0061468068,-0.056230202,-0.021578506,-0.050436635,-0.009888937,-0.015082027,0.08469071,0.09392602,0.0057761995,-0.024560662,-0.03963783,-0.021264762,0.025385765,0.021778323,0.0034760493,0.006785677,-0.038772244,0.013555989,-0.022989828,-0.03029853,-0.08436319,0.019586634,-0.016070943,0.01969733,0.0012757055,-0.01885511,0.00381293,0.016815614,0.04518227,0.059569355,0.004631031,-0.012249176,-0.016493736,-0.0006810967,0.009497504,0.017841458,-0.015509915,0.010647661,0.022852864,0.02080335,0.04827935,-0.030033143,-0.004422616,0.028985633,0.044448767,-0.005461011,0.008960303,0.036930695,-0.032795936,0.017568128,-0.061322864,0.01251921,-0.029077766,0.043625094,0.006975799,0.00063165737,0.02722176,0.026232757,0.014767608,-0.015586255,0.011657731,-0.05739784,0.017577412,-0.030774605,-0.055468787,-0.06955118,-0.014958746,0.024455098,0.04127781,0.09974829,0.018865447,0.013716439,0.052770738,0.041892063,-0.0533372,0.020245222,-0.020008415,-0.029844316,0.023901971,-0.030610025,0.016904205,-0.009908421,-0.031190468,0.004455081,0.02376949,0.00808501,-0.0050824517,0.056793824,-0.0028550597,0.0036174809,-0.009825391,0.06845446,-0.029859807,-0.008413632,0.03326745,-0.031799443,-0.015901363,-0.015998257,0.057217,-0.0009726722,0.025521575,-0.061828,-0.03551791,-0.05968935,-0.021984408,0.008540199,-0.0036815014,-0.017644342,0.02507004,-0.02676538,-0.0373799,-0.032669272,-0.025871824,-0.00038217413,-0.009727562,-0.038521707,-0.0011520458,0.04853393,0.03950107,0.013649297,-0.024214232,0.031780258,0.034742013,-0.001470838,-0.07222155,0.009506477,-0.0066819037,0.008881602,-0.052301697,-0.0037261671,-0.0149237625,0.040650174,0.0033841005,-0.033428177,0.033152398,-0.0145735415,-0.0058932896,0.010803101,0.021564499,0.04068321,0.015250725,-0.012028975,0.009144126,0.045372695,0.030467749,0.04553286,0.012539729,0.08075131,-0.0030720064,-0.00794075,0.027982904,0.07012376,-0.018275315,0.016114116,0.015419129,0.06758796,0.035484847,0.0033881362,0.011383463,-0.043019574,0.015289362,0.02765676,0.018210454,0.008058501,-0.0318567,0.014803861,0.039013267,0.0011480923,0.036376644,0.015350406,0.026798505,0.048896022,0.01438935,0.007570559,0.04215279,0.071276344,0.01172687,-0.016181223,-0.0732492,0.020777557,0.021728156,0.017891482,0.05632067,-0.008738571,-0.011381906,-0.023571795,0.046446458,-0.016105317,0.030774679,-0.028999498,-0.013077554,-0.014702619,-0.09106547,-0.031130264,0.016323907,-0.040152386,0.0047619278,-0.007737387,0.000442673,0.0007815075,0.0032343084,-0.014651881,-0.023747522,0.014993289,0.10269069,-0.042281143,0.009734944,0.003686777,-0.036368564,0.050600052,-0.06400702,0.010028652,0.06121752,-0.0025748333,0.021701882,-0.0065287664,-0.0025555277,-0.017248683,0.060665436,0.080741376,0.06336102,-0.038922817,-0.0025216218,0.013287609,0.025923112,0.023165384,-0.02142768,0.036872294,0.045423996,-0.015404855,-0.019434743,-0.0024573493,-0.081142195,0.0544367,-0.051734533,0.029309515,0.036779974,0.011706345,0.025201906,-0.0183846,0.058576364,-0.00011166932,-0.0075661936,0.026694918,-0.012367491,-0.07725846,0.008961102,-0.028693875,0.0013985671,0.008140667,-0.009962592,0.044301987,0.04022683,-0.0150318835,0.028329505,0.022741385,0.024145512,0.0056190067,0.0037046587,-0.025049971,0.025193062,0.020587878,0.012844984,0.009416323,0.054929003,-0.022955218,0.094921455,-0.019503685,0.025570333,-0.014700068,-0.06144672,-0.074476264,0.070367634,-0.017320476,-0.017873561,-0.026345897,0.031223891,-0.04673766,-0.036325615,-0.031610537,0.048670154,0.01811107,-0.03343863,-0.04080833,-0.003555596,0.008263943,-0.0401236,-0.030674877,-0.018212015,0.027725145,0.026362691,0.040190093,-0.01641631,0.055184953,-0.022076182,0.017489992,0.021574829,0.004562686,-0.02655924,-0.014420565,0.0192748,0.02715151,0.0001262107,-0.026828216,0.02732357,0.016614344,-0.039710253,0.018221106,0.07975528,0.019858217,-0.06884976,0.017667359,0.08536555,0.019452192,0.038226612,-0.030691108,-0.03452395,0.057512,-0.026114326,-0.04448521,-0.013276447,-0.0040083914,-0.047809027,0.030520886,-0.06477263,0.06399817,0.016093643,0.042931814,0.032570418,-0.014649964,-0.025854457,0.022506412,-0.003422868,-0.04360303,-0.0064073503,-0.07583331,-0.018767223,0.024168093,0.052714773,0.0019892603,-0.06011325,0.053638607,-0.10681599,-0.004985494,0.040695034,-0.0171376,0.016100742,-0.028082272,-0.021108704,-0.027530247,-0.0048059183,0.037996497,-0.011634873,0.010259402,0.0041789412,0.029386412,0.02475139,-0.010512539,-0.0043936535,-0.008411773,-0.06772076,0.0022120061,-0.06203514,0.01732978,-0.012752973,0.050832618,0.016756557,-0.022918882,0.019064637,0.009772216,-0.041158434,-0.029832033,-0.012071724,-0.0050297556,-0.0029096738,0.04783913,-0.045302615,0.015121919,-0.016365595,-0.023973856,-0.0072401604,0.048984814,0.05254336,-0.042070065,-0.05690313,-0.044534445,-0.0018735281,0.056046907,-0.010349559,-0.01845432,-0.013448274,-0.013485693,-0.052223146,0.03293963,0.010052256,0.087378256,0.044259198,0.021063937,0.05588202,-0.013090365,-0.026785571,0.0042813905,0.032751985,-0.046814814,-0.034071233,0.0027716819,0.060733087,-0.0009433062,-0.02636034,0.02633613,0.020497607,0.023313394,0.023520008,0.042824313,0.017347476,0.0082612205,-0.007671711,0.011862236,-0.006347201,0.031455945,-0.021622077,-0.012411453,0.017038139,0.057944644,-0.005144361,-0.02469829,0.0023826475,-0.008542695,-0.00073208904,-0.014662066,0.050321866,0.029722275,0.053558584,-0.026850538,0.04947296,-0.010982528,-0.059965394,0.013788158,-0.062499106,-0.044277754,-0.013639221,-0.042446766,-0.0003172954,0.019953866,-0.019404147,0.0050522587,0.02846463,-0.03988406,0.083719045,-0.060701873,0.026595695,-0.02953602,-0.005218169,0.1074178,0.007866474,0.0015006502,-0.013741958,-0.0050226734,-0.035193276,0.038860735,0.058273293,0.0051260474,-0.033791997,0.06156516,-0.0066952277,0.027129902,-0.018760951,-0.037932135,-0.014205883,-0.037468962,-0.052233808,-0.0009663583,0.0017493387,0.035205178,0.014290874,0.00049791817,0.019217769,0.008302586,-0.00521046,0.005826469,0.03606131,-0.037392102,0.040981006,-0.042846337,0.07566544,0.020243498,0.0056300503,-0.007727604,0.025486475,0.012739036,0.0038897167,-0.023851085,-0.003951803,-0.017111156,-0.028154643,-0.010426747,0.030559618,0.015006463,-0.03873059,-0.008121741,0.0032937007,0.028844405,0.03510394,-0.070879325,-0.017138911,-0.024422552,-0.0350376,-0.023200877,0.0039881985,0.0057650297,0.019061195,0.005177786,-0.036515705,0.045092553,-0.11024022,-0.056102294,-0.004865377,0.017598208,-0.02904604,0.020211905,0.019941518,-0.026200298,-0.03744692,-0.026000543,0.08604941,0.058112167,-0.026682833,0.0025323718,-0.0194504,-0.015119695,-0.015878247,-0.05714653,0.002586834,0.008104178,0.003324773,-0.02682761,0.074108146,-0.010447751,0.013748536,0.046570305,-0.01127023,-0.0684095,-0.017594628,0.0013913588,0.016323216,-0.048974246,0.0044106566,0.03525483,0.025113145,-0.0409437,-0.036153175,-0.044477742,0.035842996,-0.024596624,-0.008703988,0.023855157,-0.010373144,-0.07315031,0.031140184,-0.0038771126,-0.037090205,-0.018503102,0.027183551,0.07301906,0.017393302,0.07072087,-0.051459324,-0.102553576,-0.014731637,-0.033077292,0.0029156925,-0.04396263,0.043777358,-0.017877696,-0.005442022,0.01621033,-0.017924093,-0.0014950946,0.013517266,0.009069615,0.0012065106,0.00017205367,-0.027454756,-0.005817041,0.017208915,-0.038355723,0.019544244,0.039351802,0.04586913,-0.0599985,-0.06396378,-0.021572463,-0.007190303,-0.0133124245,0.0064169103,0.035219133,0.039967608,-0.01488621,-0.0022298147,0.033688366,0.027458806,-0.031965572,-0.020947311,0.028081253,-0.029208299,0.029448422,0.0018523353,-0.018605074,-0.09354065,0.004833961,-0.019907646,-0.08593246,0.0777242,0.03176221,0.012153251,-0.011365915,-0.027814476,-0.016771298,0.024663802,0.03694548,0.02883301,0.013485247,0.024751464,-0.027886886,-0.03493986,-0.050040904,-0.022148179,0.024889579,-0.001029551,0.04701121,0.026249671,0.027547587,0.0459704,0.017953163,-0.0075275786,0.0032446207,0.016956087,0.048028227,-0.038942814,0.079687566,-0.01918594,0.005126194,0.07541213,0.0053096437,0.0405396,-0.009336325,0.017694918,-0.008984225,0.029566051,0.0031436912,-0.037656967,-0.061284106,-0.004594427,-0.0058291913,0.025505193,0.036378182,-0.021944134,-0.044250634,-0.055253986,-0.022556055,-0.0076147234,0.03482136,0.022768883,0.02699152,-0.06022493,0.030888818,-0.019117858,0.059036978,0.041992966,-0.02701514,-0.018456897,-0.05441147]	Keywords: Transformers, Deep Learning, Attention Mechanisms, Long Documents, Relational Inductive Biases, Low-Rank Approximation, Graph Networks, ICML, ACL, arXiv\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What is the core problem addressed by Longformer?\n- How do relational inductive biases contribute to deep learning?\n- Why is low-rank approximation valuable in attention models?\n---\nSummary:\nThis is a list of references (numbered 9-11) relating to Transformers and deep learning architectures. The references cover topics like relational inductive biases, handling long documents, and low-rank approximations in attention mechanisms.\nOriginal Text:\n- [9] Peter W. Battaglia, Jessica B. Hambrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song, Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen, Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra, Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu. 2018. Relational inductive biases, deep learning, and graph networks. arXiv:1806.0128 [cs.LG]\n- [10] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-Document Transformer. arXiv:2004.05150 [cs.CL]\n- [11] Srinadh Bhojanapalli, Chullee Yun, Ankit Singh Rawat, Shashank J. Reddi, and Sanjiv Kumar. 2020. Low-Rank Bottleneck in Multi-head Attention Models. In Proceedings of ICML. 864-873. http://proceedings.mlr.press/v1119/ bhojanapalli20a.html\nContextualized Text:\nThis list represents a selection of references likely found at the end of a survey or research paper focused on Transformers and their variations within the field of deep learning. The inclusion of arXiv preprints suggests a focus on cutting-edge research. The topics coveredlong documents, relational biases, and efficiencyindicate a desire to extend and optimize the capabilities of standard Transformer architectures.	{"tags": [], "doc_id": "b083cd9d-c878-433a-b1fa-d368cd50b3b0", "summary": "This is a list of references (numbered 9-11) relating to Transformers and deep learning architectures. The references cover topics like relational inductive biases, handling long documents, and low-rank approximations in attention mechanisms.", "doc_type": "text", "entities": [], "keywords": ["Transformers", "Deep Learning", "Attention Mechanisms", "Long Documents", "Relational Inductive Biases", "Low-Rank Approximation", "Graph Networks", "ICML", "ACL", "arXiv"], "key_objects": [], "contextual_text": "This list represents a selection of references likely found at the end of a survey or research paper focused on Transformers and their variations within the field of deep learning. The inclusion of arXiv preprints suggests a focus on cutting-edge research. The topics coveredlong documents, relational biases, and efficiencyindicate a desire to extend and optimize the capabilities of standard Transformer architectures.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the core problem addressed by Longformer?", "How do relational inductive biases contribute to deep learning?", "Why is low-rank approximation valuable in attention models?"]}
9923e6df-483d-4438-8cbc-c368d47da232	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.01756078,0.030589981,-0.005868545,0.03764112,0.009497859,0.08824149,0.013973518,0.07859585,0.03794172,-0.027463712,-0.024328975,0.015709853,0.0071539446,-0.03702097,-0.03741257,0.03591015,0.014067318,0.06284915,0.028706942,-0.018494261,0.06494282,-0.025970751,-0.018378356,-0.010013297,-0.0042519975,0.06266327,0.0040659853,-0.04497335,0.030288072,0.044178467,0.008804184,-0.042719793,0.024279349,0.00016934104,0.015747448,0.0074602985,0.002206416,0.023936713,0.029578606,-0.027331134,-0.0042011035,0.03681636,-0.059770476,0.0005115723,0.011503727,-0.042530913,-0.08295472,-0.004910561,0.033805717,-0.024184804,-0.00062476634,-0.0077095823,-0.010256957,-0.010943926,0.0092831245,-0.012877879,0.0051130154,-0.047157805,0.042394493,-0.09017368,-0.042593013,0.03934359,-0.07003723,-0.00047781714,0.037668064,-0.07124903,-0.0407691,-0.010354935,0.0066991616,0.11540082,-0.013002596,0.053250715,-0.012598869,-0.045910485,0.11400288,0.045104068,0.015907153,-0.019788105,-0.079104826,-0.020948045,-0.011500581,0.088232666,-0.046366353,-0.043508176,0.07626974,-0.017107623,-0.02306568,-0.010012061,0.07047973,-0.009179957,0.009697086,-0.03074448,0.0060098493,0.063090764,0.016063254,-0.08169447,0.012194042,-0.047289155,-0.014036726,-0.015288791,-0.049300294,-0.008393349,0.1026252,0.07652386,0.02154349,-0.015432084,-0.020493023,-0.0417736,0.018718638,0.02629952,0.027436102,-0.006204831,-0.026990902,-0.00947217,-0.044299886,-0.02954895,-0.04405833,-0.022288665,-0.014996582,-0.0004254731,-0.0024829237,0.0016193102,0.036399644,0.00752245,0.08260079,0.0758638,0.0036175055,0.010594587,-0.041605387,0.0052972767,0.001630075,-0.010045657,-0.043957226,0.03532888,0.00044610535,0.02728345,0.042766083,-0.07894426,0.017149849,0.018781338,0.025675109,-0.015739761,-0.029890554,0.041964673,-0.032096047,-0.011458639,-0.04592321,-0.00922831,-0.019538974,0.039337255,-0.0039044558,0.010839893,0.031342648,0.011250424,0.014975108,-0.012341489,-0.020956542,-0.053280927,-0.0085334,-0.009174229,-0.06716877,-0.03291325,-0.049619414,0.07433668,0.046023034,0.12693648,0.0141765345,0.01450394,0.042849436,0.02025786,-0.029574929,0.022082457,-0.038616017,-0.027323753,0.012539227,-0.018618902,0.010150487,-0.023138613,-0.03339977,0.0112191485,0.031732887,0.011838901,-0.013509478,0.057592675,-0.02786105,0.019573988,0.0019360217,0.07296394,-0.022090867,-0.02130827,0.024278857,-0.016111746,-0.021832418,-0.0081334915,0.034800865,0.003110074,0.04412891,-0.06494868,-0.05466199,-0.030431533,-0.02603654,0.023334514,0.023389239,0.0035295286,0.022778766,-0.02311116,-0.03624698,-0.031860836,-0.04774964,-0.010959806,0.011268529,-0.040844332,0.011281102,0.072003976,0.03441413,-0.013864054,-0.02252372,0.0045537627,0.029544039,-0.003450324,-0.047697853,0.016368965,0.027995052,0.027219629,-0.059069395,0.01991965,-0.03615478,0.047202542,0.005077944,-0.0067198197,0.055265296,0.03155063,-0.021798491,0.020521557,-0.009608896,0.036375586,-0.0022611322,0.004415718,0.014395343,0.056976452,0.012253132,0.036376398,0.0037999563,0.07079068,-0.00042972306,-0.0006403964,0.033158075,0.016079959,-0.007797024,0.0060841604,-0.000881048,0.04537809,0.044399254,-0.008802075,0.016924536,-0.037629016,0.06049045,0.029713264,0.05980483,0.022378298,-0.027575908,0.00368006,0.011594256,-0.018055536,0.022935994,0.037943374,0.027169682,0.05233242,0.029680094,0.024198798,0.040835515,0.06961667,0.001152912,-0.008111363,-0.08618422,0.008259042,-0.007917425,0.041147523,0.0666123,-0.038106065,0.0066849273,-0.032125834,0.055104785,-0.0035071012,-0.013301701,-0.02548083,0.0007765365,-0.025048394,-0.08303728,-0.0016941751,0.024364628,-0.054965142,0.036991794,-0.012188527,0.017810365,0.011406771,0.015143367,-0.029910142,-0.032629363,0.022978842,0.060355626,-0.022373099,0.033974983,-0.009184451,-0.014853849,0.024921924,-0.06404133,0.0021685276,0.050994687,0.011153619,0.018950505,-0.03201345,-0.014118855,-0.010725702,0.031213377,0.0894326,0.05082899,-0.05251819,0.024347113,-0.0011137406,0.0097224265,-0.014210724,0.013610442,0.051448397,0.0084047085,-0.018000271,-0.011847609,-0.028324258,-0.08668917,0.045300797,-0.0565973,0.026275733,0.036792852,-0.00093161705,-0.023662405,-0.0010013079,0.03951007,0.026510889,-0.024567235,0.02898615,0.0025704394,-0.05803635,0.0040281895,-0.025096465,-0.024993481,0.008235923,-0.013487207,0.023702998,0.05686208,0.011852835,0.011591541,0.027603878,0.039923485,0.008631339,0.0051370775,-0.0065490124,0.00014239746,0.020030249,0.02471526,-0.005427144,0.07155564,-0.04695891,0.065486275,-0.037069403,0.01041988,-0.0016086679,-0.07827182,-0.051335476,0.075461216,-0.033360355,-0.015369304,-0.03538851,0.05291807,-0.044752404,-0.01874079,-0.010361317,0.026106577,0.012157656,-0.050780408,-0.024652706,-0.021544674,0.035172027,-0.05293155,-0.015082737,-0.016101481,0.055693433,0.0067496463,0.006584803,0.0062098745,0.044389516,-0.03382498,-0.0059130923,0.04568395,-0.010994678,0.0066927937,0.001040316,0.025158038,0.008265271,0.0075513492,-0.049830917,0.03193012,0.021819074,-0.03444011,0.027311329,0.042634916,0.019199397,-0.040837508,0.022025641,0.08335924,0.017209655,0.042235307,-0.021277046,-0.030312287,0.033875402,-0.038595423,-0.04876103,-0.0022112816,-0.0026855033,-0.052431196,-0.00054333376,-0.04796339,0.040734515,0.01619154,0.018115873,0.048569113,-0.026680285,-0.037090264,0.048823316,-0.010725723,-0.03984358,0.016836219,-0.10508845,-0.005921858,0.03284107,0.045780875,0.027612973,-0.0483123,0.07411059,-0.075743176,0.027665034,0.06735784,-0.028291063,-0.0035253502,-0.019237528,-0.028997367,-0.013483389,-0.003094781,0.011275946,-0.0026653889,-0.0028333804,0.012784265,0.035227902,0.01425672,-0.00807616,-0.016693218,-0.045661777,-0.055225614,-0.005267855,-0.062868424,0.035140287,-0.013290636,0.05257821,-0.015749348,0.0041929707,0.0065745544,-0.017257001,-0.046997417,-0.009988064,-0.011182365,0.006061558,0.02519754,0.028459333,-0.046413068,-0.03784895,-0.008395116,-0.0033135882,-0.012086991,0.043740757,0.05688248,-0.04320338,-0.041798737,-0.03472986,0.016546262,0.050424866,-0.015923588,-0.017034886,-0.03236877,0.004198504,-0.047138978,0.036152188,0.018188145,0.07590456,0.018006094,0.013858047,0.017419698,0.0072048204,0.008760299,-0.0120538,0.025707223,-0.031047791,-0.066310376,0.028476264,0.043768417,0.012905007,-0.005634591,-0.0073334696,0.020227473,-0.004700303,0.037494663,0.020945692,0.0044606132,0.012284286,0.0007586484,-0.016593827,0.0070879683,0.029403822,0.006170285,-0.00095787435,0.0126438895,0.021671196,0.009088593,-0.037939824,-0.00867545,-0.022637399,0.014272397,-0.0056696585,0.041078832,0.056855533,0.050012745,-0.0047361893,0.05537611,-0.035130184,-0.06597864,-1.937462e-05,-0.043359604,-0.0029920887,-0.01377097,-0.05176592,-0.04752846,0.0076812073,-0.023295084,-0.0008729085,0.019432606,-0.053083345,0.07643394,-0.057566624,0.045923397,-0.026382431,0.004004686,0.121739686,0.022310195,-0.05373226,-0.014194779,-0.042731207,-0.04433985,0.0004040697,0.08035566,0.009793696,-0.0048234845,0.08327549,0.0012209002,0.0455927,-0.00022703968,-0.060615998,-0.034135394,-0.04055349,-0.052477915,0.011018561,-0.006516682,0.033796456,0.008624837,-0.013362383,0.0051308093,0.049853757,-0.042326193,0.02146188,0.002446364,0.00644679,0.036619987,-0.053383194,0.041102078,0.044900406,-0.005192294,-0.045829277,-0.0024389685,0.027496547,-0.015166247,-0.029956434,-0.005951988,0.008839508,-0.026610177,-0.021600304,0.049873456,0.012638473,-0.0390027,0.013257429,0.025422951,0.023295423,0.040720943,-0.02426607,-0.004134088,-0.02872008,-0.03566624,-0.016464975,0.047718126,-0.010031013,-0.02077581,0.0001267846,-0.036334574,0.07687058,-0.07575838,-0.043571837,-0.058853388,0.0219766,-0.009873072,-0.01552369,0.020081127,-0.013465877,-0.009212839,-0.015859924,0.05526321,0.046266887,-0.036314264,0.006699241,-0.021498611,-0.005062751,-0.010911076,-0.007373322,-0.019459436,-0.0018040634,0.0029939874,-0.020436587,0.081189066,-0.030441971,0.0032925855,0.047662273,-0.009585614,-0.06613874,-0.026245087,0.021459686,-0.0124968495,-0.011535061,0.017718442,0.0037930363,0.018429426,-0.026662225,-0.03116528,-0.05440765,0.052188713,-0.0116409715,-0.015801813,0.009590141,0.012419464,-0.053805526,0.008646647,-0.03025278,-0.032580692,-0.014192273,0.0121186,0.056914195,0.021183642,0.061904777,-0.0819747,-0.07483054,-0.029185824,-0.022906331,-0.036454234,-0.04718775,0.054400038,-0.022898713,-0.030501207,0.033047624,-0.05243958,0.005556306,0.0059295897,0.012189711,-0.041338585,-0.01312345,-0.049071763,-0.0034838172,0.03706048,-0.005748777,0.019658726,0.07332596,0.050371025,-0.03565237,-0.043370172,-0.0030349982,-0.003939985,0.0010958817,0.020262225,0.029976074,0.038127266,-0.007988249,-0.009466637,0.070405275,0.022621717,-0.020504894,-0.028689934,0.025117826,-0.0041780374,0.02877806,0.0047788047,-0.034777243,-0.09585917,-0.009738321,0.0015107454,-0.07661286,0.012189532,0.026271475,-0.0013333734,-0.029917976,-0.02361459,-0.031918224,-0.0129914405,0.024315113,-0.010651546,-0.008316148,0.009388187,-0.025802491,-0.038889907,-0.031471495,-0.049980707,0.024944685,-0.034583133,0.01034948,0.009979866,0.020359317,0.009444364,0.03459505,0.00051714375,-0.03004038,0.018219858,0.025099883,-0.04715527,0.041740753,-0.0329791,0.01008279,0.041260924,-0.0032212364,0.027758352,-0.006559646,-0.014128731,0.013513735,-0.0010179665,0.0069233268,-0.04128238,-0.074171886,0.031196242,0.0193881,0.021498991,0.044452854,-0.011201413,-0.043102823,-0.043022268,-0.056859463,-0.0147248125,0.07327222,0.024210341,0.04310353,-0.052989867,0.015566205,-0.032639794,0.028358059,0.05461543,-0.050059553,0.031169401,-0.018583138]	Keywords: Transformers, Deep Learning, Language Models, Object Detection, Speech Synthesis, Time Series Forecasting, Architecture Improvements, References, Survey, Conference Papers, arXiv Preprints, Machine Learning, Attention Mechanisms, Neural Networks, Model Efficiency, Few-Shot Learning, End-to-End Learning, Long Sequence Modeling, Sparse Models, Memory Efficiency, Deformable Transformers, Pooling Attention, Binary Partitioning, Recurrent Transformers, Lazy Transformers, Gaussian Attention, Nystrom Methods, Hard-Coded Gaussian Attention, Average Attention Networks, Local Recurrent Neural Networks, Document Level Modeling, Sparse Expert Models, Fast Inference, Early Exit Strategies, Hi-Transformer, Memer, Predictive Attention Transformer, R-Transformer, Synthesizer, Sparse Sinkhorn Attention, WaveNet, Contrastive Predictive Coding, Big Bird, Informer, LazyFormer, Deformable DETR, PoolingFormer, SETransformer, Wang-HIBERT, BP-Transformer, BERT Losses Patience, End-to-End Object Detection with Adaptive Clustering Transformer, Tensor2Tensor, Attention is All You Need, Hi-Transformer\nKey Objects: Transformer, BERT, WaveNet, Hi-Transformer, LazyFormer, Big Bird, Informer, SETransformer, PoolingFormer, Hi-Transformer, Big Bird, Informer, LazyFormer, Deformable DETR, Wang-HIBERT, BP-Transformer, BERT Losses Patience, Deformable DETR, PoolingFormer, SETransformer, LazyFormer, Informer, Big Bird, Hi-Transformer, Informer, BERT Losses Patience, LazyFormer\nRefers to Images: None\nHypothetical Questions:\n- What are some of the key innovations related to Transformers discussed in this list of references?\n- What are the major conferences and platforms where these papers were presented or published?\n- Can you identify common themes or categories of research related to Transformers that are represented in this list?\n- What types of problems are Transformers being applied to, based on this list of references?\n- How are researchers improving the efficiency and effectiveness of Transformers?\n---\nSummary:\nThis is a list of references related to a survey of Transformers. The references cover a wide range of topics, including language models, object detection, speech synthesis, time series forecasting, and improvements to the transformer architecture. Many of these references are landmark papers that have significantly influenced the field of deep learning. The list includes papers from major conferences like NeurIPS, ECCV, ACL, ICASSP, and AAAI, as well as arXiv preprints.\nOriginal Text:\n- [12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Arriel Herbert-Voss, Gretchen Krueger, Tom Henighan,  \nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Proceedings of NeurIPS . 1877 - 1901. https://proceedings.neurips.co/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf  \n- [13] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoryuko. 2020. End-to-End Object Detection with Transformers. In Proceedings of ECCV . 213-229. https://doi.org/10.1007/978-3030-58452-8\\_13\nContextualized Text:\nThis is a list of references related to a survey of Transformers. The references cover a wide range of topics, including language models, object detection, speech synthesis, time series forecasting, and improvements to the transformer architecture. Many of these references are landmark papers that have significantly influenced the field of deep learning. The list includes papers from major conferences like NeurIPS, ECCV, ACL, ICASSP, and AAAI, as well as arXiv preprints.	{"tags": ["machine learning", "deep learning", "transformers", "references", "survey", "neural networks", "attention mechanisms", "document modeling", "long sequences", "efficient models", "few-shot learning"], "doc_id": "9923e6df-483d-4438-8cbc-c368d47da232", "summary": "This is a list of references related to a survey of Transformers. The references cover a wide range of topics, including language models, object detection, speech synthesis, time series forecasting, and improvements to the transformer architecture. Many of these references are landmark papers that have significantly influenced the field of deep learning. The list includes papers from major conferences like NeurIPS, ECCV, ACL, ICASSP, and AAAI, as well as arXiv preprints.", "doc_type": "text", "entities": ["Tom Brown", "Benjamin Mann", "Nick Ryder", "Melanie Subbiah", "Jared D Kaplan", "Prafulla Dhariwal", "Arvind Neelakantan", "Pranav Shyam", "Girish Sastry", "Amanda Askell", "Sandhini Agarwal", "Arriel Herbert-Voss", "Gretchen Krueger", "Tom Henighan", "Rewon Child", "Aditya Ramesh", "Daniel Ziegler", "Jeffrey Wu", "Clemens Winter", "Chris Hesse", "Mark Chen", "Eric Sigler", "Mateusz Litwin", "Scott Gray", "Benjamin Chess", "Jack Clark", "Christopher Berner", "Sam McCandlish", "Alec Radford", "Ilya Sutskever", "Dario Amodei", "Nicolas Carion", "Francisco Massa", "Gabriel Synnaeve", "Nicolas Usunier", "Alexander Kirillov", "Sergey Zagoryuko", "Ashish Vaswani", "Nal Kalchbrenner", "Jakob Uszkoreit", "Nikari Parmar", "Llion Jones", "A'aron van den Oord", "Koray Kavukcukoglu", "Rubin Xiong", "Guru Gurughesin", "Avina Dubey", "Chris Alberti", "Santiago Ontanon", "Philip Pham", "Anirudh Ravula", "Qifan Wang", "Li Yang", "Armmer Ahmed", "Haoyi Zhou", "Shanghang Zhang", "Jieqi Peng", "Shuai Zhang", "Jianxin Li", "Hui Xiong", "Wancai Zhang", "Yujin Zheng", "Xinhui Li", "Fenglong Xie", "Li Lu", "Manzil Zaeher", "Davis Yoshida", "Yeuyn Gong", "Weisheng Li", "Jiancheng Lv", "Nan Duan", "Wei Zhu", "Zhou Yu", "Yilin Yang", "Longyue Wang", "Shuming Shi", "Prasad Tadepalli", "Stefan Lee", "Zhaopeng Tu", "Biao Zhang", "Deyi Xiong", "Jinsong Su", "Zhou Ying", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Wei Zhu"], "keywords": ["Transformers", "Deep Learning", "Language Models", "Object Detection", "Speech Synthesis", "Time Series Forecasting", "Architecture Improvements", "References", "Survey", "Conference Papers", "arXiv Preprints", "Machine Learning", "Attention Mechanisms", "Neural Networks", "Model Efficiency", "Few-Shot Learning", "End-to-End Learning", "Long Sequence Modeling", "Sparse Models", "Memory Efficiency", "Deformable Transformers", "Pooling Attention", "Binary Partitioning", "Recurrent Transformers", "Lazy Transformers", "Gaussian Attention", "Nystrom Methods", "Hard-Coded Gaussian Attention", "Average Attention Networks", "Local Recurrent Neural Networks", "Document Level Modeling", "Sparse Expert Models", "Fast Inference", "Early Exit Strategies", "Hi-Transformer", "Memer", "Predictive Attention Transformer", "R-Transformer", "Synthesizer", "Sparse Sinkhorn Attention", "WaveNet", "Contrastive Predictive Coding", "Big Bird", "Informer", "LazyFormer", "Deformable DETR", "PoolingFormer", "SETransformer", "Wang-HIBERT", "BP-Transformer", "BERT Losses Patience", "End-to-End Object Detection with Adaptive Clustering Transformer", "Tensor2Tensor", "Attention is All You Need", "Hi-Transformer"], "key_objects": ["Transformer", "BERT", "WaveNet", "Hi-Transformer", "LazyFormer", "Big Bird", "Informer", "SETransformer", "PoolingFormer", "Hi-Transformer", "Big Bird", "Informer", "LazyFormer", "Deformable DETR", "Wang-HIBERT", "BP-Transformer", "BERT Losses Patience", "Deformable DETR", "PoolingFormer", "SETransformer", "LazyFormer", "Informer", "Big Bird", "Hi-Transformer", "Informer", "BERT Losses Patience", "LazyFormer"], "contextual_text": "This is a list of references related to a survey of Transformers. The references cover a wide range of topics, including language models, object detection, speech synthesis, time series forecasting, and improvements to the transformer architecture. Many of these references are landmark papers that have significantly influenced the field of deep learning. The list includes papers from major conferences like NeurIPS, ECCV, ACL, ICASSP, and AAAI, as well as arXiv preprints.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the key innovations related to Transformers discussed in this list of references?", "What are the major conferences and platforms where these papers were presented or published?", "Can you identify common themes or categories of research related to Transformers that are represented in this list?", "What types of problems are Transformers being applied to, based on this list of references?", "How are researchers improving the efficiency and effectiveness of Transformers?"]}
3905c38a-423a-4fd2-aaa9-a69701d5cf03	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0045211534,0.01844683,-0.02461712,0.032552622,0.013499633,0.07297788,0.010157848,0.07495311,0.042704824,-0.045038737,0.0019378184,0.012428008,0.0017666138,-0.028425511,-0.03441419,0.039731752,0.012642123,0.0651138,0.03628478,-0.025631186,0.042619195,-0.015816726,-0.003223234,-0.058094557,0.003733242,0.014839007,-0.010796962,-0.024716346,0.036555078,0.036106363,0.02388608,-0.047200397,0.02496681,0.0028186871,0.019405592,0.01385607,0.026214056,0.0297025,0.014045707,-0.03313874,-0.008376929,0.050189476,-0.05006792,0.0058552856,-0.006925141,-0.07125791,-0.065336205,-0.024659209,0.025786132,-0.009424754,-0.027692102,0.01484331,-0.03688502,-0.012209385,0.0064971,-0.03226678,-0.018443063,-0.035175856,0.026248092,-0.08373497,-0.045537297,0.02001497,-0.061125506,-0.02125743,0.029906537,-0.07168724,-0.04236349,0.0023845858,0.025608912,0.12276663,-0.02124003,0.015690966,-0.029710926,-0.07272146,0.09915299,0.03141024,-0.0029031239,-0.030267278,-0.08068713,-0.018436758,-0.024957158,0.045041386,-0.042479634,-0.030476699,0.07939924,-0.01115054,-0.040532902,-0.040646587,0.09609004,-0.015140391,-0.0020895344,-0.023550443,-0.006943398,0.06054767,-0.021924492,-0.091763064,0.021805879,-0.028419664,0.008635381,-0.0650305,-0.03473429,0.012383127,0.061605014,0.10485078,0.0047405735,-0.011697055,-0.038826637,-0.027710093,0.009228126,0.033713862,0.009047364,0.012374279,-0.011848761,-0.005232101,-0.028188732,-0.034643967,-0.06811781,0.0014715039,-0.0113653075,-0.008688598,-0.00069062697,-0.03317927,0.016118199,0.011548588,0.058700778,0.07939346,0.028833125,0.011304394,-0.034134846,0.003070477,0.0038065824,-0.013639639,0.0010732142,0.0024931992,0.003662638,0.030905847,0.063558154,-0.03390899,0.009814665,0.032141495,0.023687549,-0.029587755,-0.014930871,0.038176328,0.008795445,0.023392398,-0.030267037,0.010631689,-0.0044782674,0.047927555,0.013192276,0.016926067,0.040654585,0.023816083,0.024534512,-0.0041597644,-0.019264515,-0.040284228,0.009357965,0.028837949,-0.04256078,-0.046972778,-0.036963798,0.043635283,0.042527102,0.118441455,0.012605649,0.017291434,0.030194856,0.01688632,-0.044790976,0.0107943285,0.0017537848,-0.015495653,0.024026781,-0.020398509,0.008776064,0.005177119,-0.038620975,0.0060330476,0.0058710603,0.03176522,-0.008169303,0.038310498,-0.024041882,0.023481686,0.017555937,0.104395166,-0.027548667,-0.034660745,0.033717826,-0.02291774,-0.025602175,-0.004650965,0.043571312,0.015548782,0.027801428,-0.06004835,-0.043394677,-0.029444883,-0.024312114,-0.002108092,0.028486764,0.0023847546,0.034461103,-0.011661141,-0.04347826,-0.03672539,-0.030023068,-0.02406752,-0.014352442,-0.022655113,-0.023793165,0.06826918,0.037208922,0.012748283,-0.030921519,0.043391578,0.018300137,-0.013628469,-0.05162579,-0.0055075637,-0.0016715861,0.0054980493,-0.043020573,-0.003992051,-0.040742695,0.048342742,0.011912973,-0.037294388,0.050133407,0.009841842,-0.011010707,0.050582863,-0.02028715,0.046205882,-0.013766948,-0.021319743,0.021831077,0.057407055,0.023745079,0.07257451,0.023324473,0.034655742,0.0041416246,0.00334623,0.029537385,0.0574854,-0.0006379316,0.025547715,0.011794371,0.042143762,0.042326443,-0.034249377,0.00041091625,-0.039083842,-0.002018425,0.04538914,0.047742985,0.020592295,0.008151344,-0.0055012964,0.017981853,-0.008736827,0.01861278,0.010905202,0.007880737,0.04568585,0.033120118,0.025915261,0.026273208,0.06591177,-0.037419066,-0.013829073,-0.064557634,0.003992977,-0.008740982,0.021229943,0.062091578,-0.019313186,-0.014033845,-0.017580861,0.051049896,-0.01020158,0.024639234,-0.02502564,-0.011701515,-0.018642666,-0.072374724,-0.015014418,0.008037315,-0.06065812,0.012931857,-0.032688532,0.037622035,-0.007683083,0.0032041883,-0.0414166,-0.042343237,-0.019529974,0.07314905,-0.049663503,0.031096075,-0.018719291,-0.015642613,0.044000324,-0.055533715,0.022414552,0.03372662,0.0018790357,0.033639133,-0.017118225,0.003473685,-0.013053579,0.026846757,0.08935893,0.048490204,-0.003984769,-0.021268416,0.010670386,0.008218772,0.010380643,-0.000582154,0.0558568,0.054967895,-0.009431406,-0.009475582,-0.04589566,-0.0928565,0.04366635,-0.043937437,0.045955624,0.028375413,0.029104298,-0.02031146,-0.019642083,0.07297843,0.024195278,-0.0044488944,0.019748172,0.001746743,-0.0668664,0.01409211,-0.012830693,0.0036754485,0.022658283,0.04617356,0.031022303,0.038326476,0.01693479,-0.004052943,0.012126696,0.044325862,0.027486572,-9.70247e-05,-0.0069302777,0.014964274,0.0016448215,0.0029581788,0.010011481,0.05506842,-0.04095277,0.09013273,-0.011846245,-0.01404827,-0.018491177,-0.06878238,-0.028438404,0.06544549,-0.012158336,0.024108462,-0.032265104,0.06799794,-0.035201676,-0.014394506,-0.010253037,0.008638393,0.05100585,-0.057127856,-0.022563564,-0.049696878,0.034146402,-0.05935767,-0.030078238,-0.021686008,0.0619816,-0.014677441,0.03290312,0.01172891,0.03949383,-0.022030503,0.0075489315,0.025235346,-0.012988735,-0.0030841425,-0.004218606,0.026569963,0.03995736,0.01775818,-0.014813069,0.043338146,0.016808359,-0.042598926,0.04300394,0.029718995,0.022910504,-0.09609085,0.021986406,0.08920404,-0.0020388174,0.027437776,-0.041919775,-0.017825598,0.04818932,-0.033126835,-0.015691148,-0.024033187,0.013843917,-0.056216672,-0.001753857,-0.054917067,0.052224167,0.014838802,0.0377233,0.03525368,-0.029365722,-0.012512399,0.022343729,-0.002775601,-0.06017282,0.025529113,-0.0712609,0.00015152297,0.048961952,0.051006705,0.0130436225,-0.066225536,0.059876405,-0.10049958,0.02257491,0.06758731,-0.03259429,-0.0006739241,-0.012016025,-0.0059403577,-0.043454405,-0.00696694,0.017064942,-0.002771501,-0.021454649,-0.0147762615,0.05698776,0.029545782,-0.004899863,-0.005499473,-0.028753743,-0.05832111,-0.032444257,-0.05416708,0.013274709,-0.026142564,0.028195595,-0.014538316,-0.011606094,0.01087985,-0.0039331727,-0.06621768,-0.019098794,-0.03856408,-0.005090266,-0.003085589,0.020473946,-0.054104075,0.03219765,-0.021779828,-0.016911345,-0.03365414,0.05419583,0.05878559,-0.01649717,-0.034018446,-0.026320118,0.02187392,0.04951509,-0.00705961,-0.0055920496,-0.014659619,-0.014652577,-0.06775149,0.0025844749,0.029062258,0.049883004,0.005963609,0.036512014,0.03757343,-0.022068202,-0.009067164,0.0016273893,0.021609941,-0.041109916,-0.057224553,-0.0042873495,0.051251546,0.019744666,0.019229157,-0.0028750473,0.040441424,-0.006203759,0.023800382,0.036388796,0.020998048,0.01759309,-0.014983132,0.012961267,-0.027073108,0.029325696,-0.023395156,0.00537986,0.025722548,0.04845285,-0.008210591,-0.023007024,-0.0018958087,-0.0065924474,-0.0049561295,0.010372471,0.020997148,0.05696147,0.038118713,-0.030278625,0.076802135,-0.04047713,-0.035243254,0.016389675,-0.07629849,-0.022649504,0.015922679,-0.0673581,-0.023847453,0.022627601,0.0016780721,-0.01613661,0.01429741,-0.023284972,0.08705781,-0.058515146,0.025455482,0.0025722221,-0.0073472997,0.101042636,0.020891435,-0.005779322,-0.022747185,-0.005509635,-0.010426158,0.03483201,0.07378677,-0.0046707857,-0.00199242,0.057881348,0.0023741927,0.062262118,-0.007861595,-0.057558235,-0.036791187,-0.03236444,-0.030322883,0.0101429075,-0.027523048,-0.0003762664,0.009560739,0.01965536,0.011304579,0.032619424,-0.020944512,0.047039878,0.04716836,-0.001788345,0.010123201,-0.028650288,0.049677696,0.041391637,0.011443657,-0.016413324,0.010401609,0.037318785,0.021658318,-0.02117601,0.0018784731,0.00037012543,-0.012787218,-0.0071881604,0.015418306,0.0103060715,-0.019689055,-0.016963005,-0.0015854691,0.017414251,0.034822315,-0.013628483,-0.006464856,-0.015852274,-0.0300357,-0.038550317,0.005524176,-0.007834805,-0.016228706,0.009478139,-0.042675886,0.090262346,-0.09591484,-0.027082168,-0.045328368,0.012229466,-0.01963749,0.010640748,0.025524665,-0.014422021,-0.019972404,-0.025240507,0.06331469,0.06866331,-0.06351841,0.03259274,0.0053538335,0.0011454026,0.003930759,-0.032102928,-0.03538957,0.004560194,-0.025299435,-0.015078395,0.09037305,0.003905545,-0.0006606689,0.04035343,-0.015363649,-0.07603134,-0.013521635,0.044245556,-0.003840441,-0.0041517154,-0.006678826,0.0039093164,0.052297574,-0.056710273,-0.026400173,-0.08094299,0.04450548,-0.036145736,0.021763392,0.006823721,-0.01571962,-0.05427585,-0.008330474,-0.0076396503,-0.045772824,0.00050422695,0.011754851,0.059065383,0.046669174,0.055442575,-0.041665755,-0.08120776,-0.032500274,-0.025527911,0.011444831,-0.04429554,0.038185332,-0.00559815,-0.009800187,-0.0047654393,-0.02804364,0.0004830466,0.0049884343,0.013520772,-0.0082583325,0.0013582989,-0.042709272,-0.044001587,0.033137694,-0.01631784,0.018683866,0.063320324,0.067598626,-0.086520135,-0.059453033,-0.009169921,-0.016587036,0.018934295,0.007100329,0.01594,-0.013892948,-0.020509917,-0.0029002698,0.05929774,0.014595306,0.0018455032,-0.019577822,0.040361192,-0.019730056,0.03995711,0.0013888009,-0.019909259,-0.10571668,0.0009350808,-0.024543358,-0.061301205,0.028937684,0.054781493,0.0043821163,-0.013245317,-0.01627231,0.007002817,0.025962869,0.04033718,0.016656442,0.020634653,0.008070572,-0.019053238,-0.060514558,-0.032169636,-0.019554632,0.014595892,-0.024112247,0.025695596,0.024291063,0.0062908814,-0.0069026756,0.027739557,-0.012610985,-0.014920577,0.037799556,0.04521668,-0.047316123,0.07011548,-0.007962136,0.012558067,0.055362813,-0.020984516,0.042991035,-0.006470936,0.041252073,0.010553578,0.019998735,-0.0022850814,-0.03342111,-0.061803196,0.022545077,0.031907905,0.025749594,0.057539247,-0.018436098,-0.029791998,-0.04972171,-0.0471555,-0.008100655,0.06739494,0.04147001,0.0024786824,-0.04146867,0.009798769,-0.037175648,0.033161376,0.05193711,-0.032811403,0.011056565,-0.0037675134]	Keywords: Transformers, machine learning, speech recognition, metric learning, long sequences, generative pretraining, attention mechanisms, deep learning, natural language processing, computer vision, time series forecasting, speech synthesis, object detection, representation learning, attention, transformers architecture, transformers applications, deep learning models, neural networks, deep metrics, sparse transformers, efficient transformers, long sequence modeling, transformer transducer, speech processing, computer vision applications, time series prediction, natural language processing tasks, speech synthesis techniques, object detection algorithms, representation learning methods\nKey Objects: Transformers, deep learning models, neural networks\nRefers to Images: None\nHypothetical Questions:\n- What are some common applications of Transformers?\n- Can you list some of the publications mentioned in the references?\n- How has the Transformer architecture evolved over the past few years?\n---\nSummary:\nThis is a list of references related to Transformers and their various applications in machine learning and related fields. The references cover a wide range of topics, including generative pretraining, speech recognition, metric learning, and long sequence generation. The list spans from 2016 to 2021, showcasing the rapid evolution and expanding use of Transformer architectures.\nOriginal Text:\n- [14] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heeowu Jun, David Luan, and Ilya Sutskever. 2020. Generative Pretraining From Pixels. In Proceedings of ICML . 1691-1703. http://proceedings.mlr.press/v119/chen20s.html\n- [15] Xie Chen, Yu Wu, Zhenghao Wang, Shuji Liu, and Jinyui Li. 2021. Developing Real-time Streaming Transformer Transducer for Speech Recognition on Large-scale Dataset. arXiv:2010.11395 [cs.CL]\n- [16] Ziye Chen, Mingming Gong, Linguan Gei, and Bo Du. 2020. Compressed Self-Attention for Deep Metric Learning with Low-Rank Approximation. In Proceedings of IJCAI . 2058-2064. https://doi.org/10.24963/ijcai.2020/285\n- [17] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating Long Sequences with Sparse Transformers. arXiv:1904.10509 [cs.LG]\nContextualized Text:\nThe provided text is a list of references. Each reference includes the author(s), publication year, title, and publication venue. The citations detail various work utilizing the transformer architecture.	{"tags": ["transformers", "references", "machine learning", "deep learning", "natural language processing", "computer vision", "speech recognition", "time series analysis"], "doc_id": "3905c38a-423a-4fd2-aaa9-a69701d5cf03", "summary": "This is a list of references related to Transformers and their various applications in machine learning and related fields. The references cover a wide range of topics, including generative pretraining, speech recognition, metric learning, and long sequence generation. The list spans from 2016 to 2021, showcasing the rapid evolution and expanding use of Transformer architectures.", "doc_type": "text", "entities": [], "keywords": ["Transformers", "machine learning", "speech recognition", "metric learning", "long sequences", "generative pretraining", "attention mechanisms", "deep learning", "natural language processing", "computer vision", "time series forecasting", "speech synthesis", "object detection", "representation learning", "attention", "transformers architecture", "transformers applications", "deep learning models", "neural networks", "deep metrics", "sparse transformers", "efficient transformers", "long sequence modeling", "transformer transducer", "speech processing", "computer vision applications", "time series prediction", "natural language processing tasks", "speech synthesis techniques", "object detection algorithms", "representation learning methods"], "key_objects": ["Transformers", "deep learning models", "neural networks"], "contextual_text": "The provided text is a list of references. Each reference includes the author(s), publication year, title, and publication venue. The citations detail various work utilizing the transformer architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some common applications of Transformers?", "Can you list some of the publications mentioned in the references?", "How has the Transformer architecture evolved over the past few years?"]}
a74e24a9-c9de-4434-8095-ff498464252a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.024511129,0.026625777,-0.04506584,0.023282357,-0.027195776,0.07272706,0.020270117,0.086548924,0.06542836,-0.03820549,0.004323937,0.021424111,0.02321766,-0.008218177,-0.050710563,0.044404924,0.016366646,0.05760048,0.037778076,-0.010687007,0.033892732,0.017943075,0.014038281,-0.021602,0.005460602,0.04174344,0.007202133,-0.023940677,0.021229573,0.039772607,0.0079417685,-0.055829987,0.033381347,0.012893376,0.019346172,-0.0184985,-0.004223285,-0.0076934407,0.026586954,0.000100734556,-0.020118626,0.045464333,-0.0572535,0.0026348694,0.006338967,-0.0671366,-0.068526916,-0.024375495,-0.00039089978,-0.01777307,-0.03944077,0.032969777,-0.031895038,-0.01127531,0.027083047,-0.030005693,-0.026915308,-0.047333375,0.014968817,-0.10515699,-0.039931394,0.015675047,-0.07183165,-0.01288785,0.026305623,-0.05682874,-0.007329378,0.009556473,0.027709441,0.1327697,-0.0069006453,0.021328418,-0.011001094,-0.04465427,0.13272737,0.04116956,-0.011020622,-0.007146972,-0.06986957,0.016197544,-0.017395707,0.053046506,-0.073080525,-0.025943236,0.08493878,0.00046193038,-0.045020044,-0.013022637,0.07387551,-0.012105819,0.030065281,-0.011595463,-0.017715847,0.07294451,0.0058622854,-0.09156253,0.005977504,-0.040382873,-0.0026530146,-0.046537884,-0.0023174358,-0.02632684,0.057005085,0.106798045,0.02522756,-0.029941618,-0.04825791,-0.024468927,0.0107859,0.010818366,-0.01183092,0.0029907494,-0.025338823,-0.004251433,-0.0029482236,-0.04607419,-0.047951702,0.005211785,0.007374216,-0.0023249732,-0.021117391,-0.028353276,-0.0053959657,0.02224973,0.039237518,0.06603782,0.006004665,0.008661469,-0.045061283,-0.0057356125,0.016256629,-0.030533412,-0.01618837,0.0054472457,0.004591562,0.019202325,0.0569122,-0.026568612,-0.0009441161,0.0049922518,0.025788395,-0.019940522,-0.0012255376,0.030804401,-0.0052088806,0.008613902,-0.05115277,2.5849782e-05,-0.016570557,0.0410483,0.02583674,0.010642183,0.019427504,-0.00208973,0.023598343,0.003356967,-0.0018115382,-0.03987476,0.018491384,-0.005363556,-0.04271634,-0.04878279,-0.025406843,0.042436942,0.035795145,0.12413491,-0.004910671,-0.010634954,0.011300014,-0.0113372225,-0.05406682,-0.0006054456,-0.019125953,-0.026106799,0.00609121,-0.020245241,0.010032356,-0.017168224,0.0010089163,-0.010398885,0.016212538,0.014159598,0.0025944344,0.03252774,0.0037100553,0.044362005,-0.0041846056,0.10783382,-0.024047332,-0.030310841,0.03357855,-0.014061152,-0.004647543,-0.01254808,0.021998644,0.01779296,0.05065656,-0.0403861,-0.05392361,-0.014776457,-0.05066407,0.004709951,-0.0009123903,0.0046441588,0.022582041,-0.0013343387,-0.042426918,-0.045252383,-0.038721148,-0.02878616,-0.004213954,-0.043684315,-0.032808818,0.07088774,0.020025665,-0.01400794,-0.022625636,0.046511635,0.02798693,0.009582006,-0.06443194,0.0118900845,-0.008752133,0.041778203,-0.054682977,0.005479305,-0.026655689,0.009850029,0.017340207,-0.03243456,0.048712637,-0.01961054,-0.032462638,0.033197325,-0.020981256,0.039517377,-0.011615788,0.011198853,0.00023982284,0.09285606,0.011465999,0.06942782,0.023739856,0.073511384,0.024009775,-0.019931836,0.004576594,0.042148355,-0.0041717137,0.030393137,-0.009089962,0.0472968,0.046324704,0.0041709817,0.01817433,-0.029673168,0.029690154,0.019756299,0.060235035,0.008355152,0.013352095,-0.010459186,0.014834443,0.0043313727,0.00075256877,0.020011729,0.036056194,0.031179193,0.03318409,0.012683748,0.038251344,0.065834574,0.0008570923,-0.030839514,-0.073320925,-0.014211591,0.0030413973,0.00027837578,0.03696971,-0.008411921,-0.020733852,-0.025762627,0.0639477,0.010748004,0.015725296,-0.004211506,0.0012746019,-0.031319804,-0.071367934,-0.011929382,0.023872036,-0.051187966,0.01737637,-0.043498464,0.03977814,0.014266288,-0.0041986466,-0.015376947,-0.0095676165,-0.0052096625,0.11875416,-0.020642279,0.033557545,-0.023588797,-0.009186006,0.03271059,-0.0672157,-0.0070245,0.025438046,0.027429927,0.0064603323,-0.015242136,0.0022342298,-0.020709755,0.042446606,0.101585984,0.06753348,-0.025569648,-0.03230972,0.017411279,0.009611075,0.028788945,0.006153236,0.054125395,0.061095208,-0.0132212285,0.010776642,-0.039436195,-0.07830331,0.064892806,-0.081767425,0.056931738,0.06571188,-0.0045646406,-0.016013771,-0.011294796,0.07117134,0.031208659,-0.0039290367,-0.003270437,-0.013685259,-0.040337794,0.003878764,-0.020179333,0.0024305212,0.033379227,0.013236024,0.021266734,0.010010211,-0.010387959,0.008376826,-0.0047840434,0.020816531,0.03806193,0.021349309,-0.018188754,0.019147204,0.012674524,-0.0071229003,0.02544162,0.05072513,-0.044770848,0.09396991,-0.023700923,0.0010058304,-0.046227854,-0.05589778,-0.041362647,0.06836377,-0.006313187,-0.005372595,-0.029236404,0.058335114,-0.054358874,-0.025825236,0.0016337465,0.026556801,0.042345926,-0.059452545,-0.027794266,-0.0275595,0.0058154007,-0.032981835,-0.047275197,-0.024044128,0.054688733,-0.012280034,0.024187354,-0.006960992,0.047063496,-0.04504657,0.009021443,0.013847244,0.006265274,-0.020528333,-0.010094053,0.026778555,0.030924318,0.0019078995,-0.028448176,0.03393949,0.036885086,-0.0496069,0.04602545,0.060489986,0.02674595,-0.089363724,0.008383886,0.06822824,0.01009831,0.014913644,-0.015809376,-0.007699396,0.032640968,-0.022832438,-0.033566453,-0.0049916427,0.00065912487,-0.05744404,0.011265689,-0.04826486,0.06112763,0.026766604,0.04056186,0.040136278,-0.037731983,-0.033088192,0.030899392,0.0076573202,-0.047091026,0.033956,-0.0632883,-0.00063993747,0.026730742,0.06403157,0.0062103015,-0.0634164,0.08877967,-0.09210269,-0.013079562,0.057454437,-0.032111548,-0.007838177,-0.03877001,-0.008083063,-0.041629713,0.0037996927,0.035841387,-0.020811308,-0.007951856,-0.0062685446,0.061807014,0.017825063,-0.006482328,0.00963856,-0.0040107705,-0.053048696,-0.0014795377,-0.054111924,0.00033796945,-0.0081037115,0.032154683,0.003160104,-0.016205776,-0.027783895,-0.017789586,-0.052833423,-0.025519291,-0.027520465,-0.003622113,0.0008469953,0.030872917,-0.043510113,0.0034821893,-0.03176992,-0.015520538,-0.03367421,0.06783445,0.05431985,-0.0020854755,-0.044661846,-0.04325809,0.014400154,0.06480869,-0.0022488744,-0.010354896,-0.019567942,-0.026519794,-0.059848692,0.0129724145,0.0023213867,0.073050454,0.011368862,0.02460426,0.025369365,-0.03524644,-0.016273744,0.0102922525,0.013377025,-0.056352016,-0.036588836,0.0067470535,0.031222742,0.0073960447,0.019867178,0.014635922,0.013926099,0.005918239,0.0055990727,0.0463902,0.0031532787,0.013077902,-0.007235931,0.0148717025,-0.01932812,0.008581355,-0.01528923,-0.005168127,0.018897394,0.027873123,0.014064354,-0.02030964,0.010710041,-0.0054258485,0.020762092,-0.008065317,0.0054081655,0.045355044,0.051936787,-0.015322134,0.061651446,-0.043848317,-0.03245151,0.024358232,-0.069725305,-0.007075971,0.00881442,-0.07597117,-0.0009060167,0.04954318,-0.03980208,-0.03974801,0.01686165,-0.03851782,0.09771656,-0.045727417,0.02133218,-0.027542805,0.009854509,0.10890853,0.03349453,0.022022113,-0.017960072,-0.027889963,-0.03375946,0.059874587,0.08157858,0.013688406,0.0045128255,0.054190114,0.015959147,0.044728953,0.0212236,-0.046635933,-0.025614597,-0.046677604,-0.022821952,0.036709882,-0.045978058,0.009510618,-0.0013670472,-0.009876101,0.000409467,0.01016705,-0.005912377,0.015471,0.05034526,-0.015142247,0.031436913,-0.022364315,0.022888904,0.0095278025,0.022164972,-0.013112943,0.030618304,0.03697942,0.0012996648,-0.002753407,0.0019062118,-0.014559682,-0.0117912395,-0.004366098,0.022126643,0.029025758,-0.0113037145,-0.026929036,0.011531351,0.017036764,0.022392252,-0.031513255,-0.014535534,0.0026544754,-0.018116564,-0.011181125,0.023040038,0.010673923,0.006588142,0.008025888,-0.03693864,0.074804775,-0.09364559,-0.043039747,-0.0349082,0.0063683996,-0.03156921,-0.009384326,0.024859086,-0.028422458,0.0016187754,-0.04258179,0.05817413,0.052892845,-0.04140992,0.03181014,-0.0017708552,-0.020619247,-0.017348442,-0.025939811,0.004638406,0.0015478721,-0.023339357,-0.0032493938,0.06766926,0.00091508194,-0.006536631,0.040086832,0.004271155,-0.046327576,0.0048056706,0.040468723,0.012105149,-0.025307078,0.0060883,0.026002644,0.015498597,-0.04447084,-0.015452462,-0.03306797,0.027446846,-0.03478204,-0.0046662684,-0.007939095,0.014720866,-0.053461455,-0.0033802309,-0.034587275,-0.04301606,-0.008618259,0.026729364,0.063885376,0.06813883,0.067777485,-0.054157082,-0.076350085,-0.038458847,-0.0228041,-0.0058919033,-0.040843163,0.03739536,-0.0067598275,-0.039269943,0.018808719,-0.01881588,0.0072290977,0.023203615,0.01726092,-0.019837717,-0.023081226,-0.021327127,-0.0026765582,0.026981104,-0.040351573,0.006836603,0.06382298,0.033343334,-0.076511554,-0.06324611,-0.020829275,-0.012937831,-0.004023756,0.01159081,0.027361736,0.005633216,-0.033405762,0.019708032,0.07497054,0.04522164,-0.031891715,-0.015823003,0.047440995,-0.027898347,0.038292926,0.0120667685,-0.018407062,-0.11606617,0.01455338,0.009142648,-0.06191136,0.055845454,0.035016708,-0.022637872,-0.0075083016,-0.029453328,-0.0003058969,0.018711368,0.046654236,0.03272943,0.011412238,0.005291967,-0.033977766,-0.045665357,-0.011541547,-0.005623251,0.03652703,-0.029686326,0.050287444,0.0017883386,0.009384464,0.01739041,0.021697588,0.001370547,-0.013058661,0.050966587,0.062184513,-0.009784439,0.07442071,-0.0021901887,-0.027361123,0.04421653,-0.018190501,0.039919708,-0.022407342,0.012852137,0.011835953,0.007320663,0.0006734103,-0.016063062,-0.09202339,0.016176134,0.028550863,0.003167954,0.034601532,-0.007842519,-0.019457432,-0.047024332,-0.035947286,-0.03539205,0.043539792,0.0024083043,0.0061944462,-0.044726435,0.026297973,-0.029116027,0.047963455,0.030117929,-0.00048967754,0.002061707,0.006195495]	Keywords: Transformers, Attention, Sparse Attention, Long Sequences, Efficiency, Scalability, NLP, Computer Vision, Time-Series Forecasting, Architecture, Modifications, Improvements, Performance, Positional Encoding, Sparse Transformers, Performers, Long Document Modeling, Object Detection, Protein Modeling, Speech Synthesis, Deformable DETR, Poolingformer, LazyFormer, HIBERT, Informer, BP-Transformer, SETransformer, DeeBERT, Wang-HIBERT, PayLessAttention, Reformer\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- \n---\nSummary:\nThis is a list of references cited in a survey of Transformers, spanning a wide range of applications including natural language processing, protein modeling, computer vision, and time-series forecasting. The entries describe modifications and improvements to the original Transformer architecture, focusing on efficiency, scalability, and performance in various tasks.  Many of the entries explore techniques like sparse attention, conditional positional encodings, and architectural modifications to address challenges associated with long sequences and large datasets.\nOriginal Text:\n- [17] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating Long Sequences with Sparse Transformers. arXiv:1904.10509 [cs.LG]\n- [18] Krzysztof Chormanski, Valerie Liikhoshesthor, David Dohan, Xinyongu Song, Andrea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. 2020. Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers. arXiv:2006.03555 [cs.LG]\n- [19] Krzysztof Chormanski, Valerie Liikhoshesthor, David Dohan, Xinyongu Song, Andrea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2020. Rethinking Attention with Performers. arXiv:2009.14794 [cs.LG]\n- [20] Xiangxiang Chu, Zhi Tian, Bo Zhang, XiongLang, Xiaolin Wei, Huaixia Xia, and Chunhua Shen. 2021. Conditional Positional Encodings for Vision Transformers. arXiv:2102.10882 [cs.CV]\nContextualized Text:\n	{"tags": [], "doc_id": "a74e24a9-c9de-4434-8095-ff498464252a", "summary": "This is a list of references cited in a survey of Transformers, spanning a wide range of applications including natural language processing, protein modeling, computer vision, and time-series forecasting. The entries describe modifications and improvements to the original Transformer architecture, focusing on efficiency, scalability, and performance in various tasks.  Many of the entries explore techniques like sparse attention, conditional positional encodings, and architectural modifications to address challenges associated with long sequences and large datasets.", "doc_type": "text", "entities": [], "keywords": ["Transformers", "Attention", "Sparse Attention", "Long Sequences", "Efficiency", "Scalability", "NLP", "Computer Vision", "Time-Series Forecasting", "Architecture", "Modifications", "Improvements", "Performance", "Positional Encoding", "Sparse Transformers", "Performers", "Long Document Modeling", "Object Detection", "Protein Modeling", "Speech Synthesis", "Deformable DETR", "Poolingformer", "LazyFormer", "HIBERT", "Informer", "BP-Transformer", "SETransformer", "DeeBERT", "Wang-HIBERT", "PayLessAttention", "Reformer"], "key_objects": [], "contextual_text": "", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": []}
564ca5fd-821a-44d3-b616-48d635acd003	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0061218906,0.009703331,-0.0087189395,0.027037937,-0.015447128,0.051884174,0.021077171,0.07726751,0.034490626,-0.043313563,0.0063206637,0.023009188,-0.016906641,-0.021184405,-0.04008734,0.049694564,-0.003451144,0.075699285,0.023396403,-0.013287929,0.068279155,-0.02662267,0.01728455,-0.041686974,-0.016127083,0.028746577,0.02098478,-0.0039485986,0.012957456,0.058847163,0.042745564,-0.037122626,0.029675841,0.0077912123,0.027745146,0.0061804526,0.0041433866,0.010852357,0.004225766,-0.030656414,-0.014174618,0.03331104,-0.070642225,0.0038508694,0.0044906093,-0.058586344,-0.076897904,0.00836833,0.034989227,-0.025661571,-0.024191614,0.012297216,-0.020458642,-0.018078335,0.017366108,-0.012669176,-0.0041326936,-0.050990853,0.033770323,-0.09281778,-0.030701142,0.028646229,-0.053665116,-0.016130053,0.047555,-0.057899177,-0.026924247,-0.0045007137,0.021620974,0.12444443,-0.025925692,0.012330094,-0.005869217,-0.056931324,0.12539528,0.051268823,0.007083661,-0.040682722,-0.06326557,-0.007737477,-0.057852693,0.09838128,-0.055098265,-0.034792393,0.08718479,0.008374389,-0.03800153,-0.010423715,0.06511876,-0.007857796,0.026259787,-0.009382411,-0.019600749,0.048864722,-0.0064935023,-0.048564244,0.005192722,-0.05328612,-0.0070428094,-0.06873473,-0.0053386143,-0.006495091,0.1115447,0.099695615,0.023682972,-0.0057139075,-0.027184138,-0.030564427,-0.011138012,0.037893925,-0.0061780387,0.008249803,-0.030051835,0.005539022,-0.02173206,-0.02108317,-0.05482926,0.018772481,-0.007048852,-0.014898098,-0.00498532,-0.012405191,0.007662783,0.039050434,0.08886547,0.054341417,0.01790786,-0.0072079552,-0.057674926,0.0006504828,0.007916253,-0.010373926,-0.026232045,0.0019671053,-0.005579155,0.029652327,0.06578123,-0.027742295,0.012613682,0.014759616,-0.0029349457,-0.03877664,-0.02710998,0.05703202,0.0032812199,0.02022675,-0.041633666,0.028453678,-0.020070639,0.034707308,0.0059711286,0.0006037084,0.04235997,0.0034747785,0.0071747904,0.003569263,-0.019924613,-0.038838785,0.00917654,-0.01683443,-0.051794328,-0.078561835,-0.02851676,0.05959198,0.049234364,0.15201364,-0.010184083,0.027360506,0.044044204,0.009031817,-0.060651746,0.009186428,-0.02069928,-0.04015561,0.009798005,0.0054803104,0.019095631,-0.0071794926,-0.03744745,-0.016591843,0.008280272,0.020575935,-0.028485924,0.050937474,-0.054981213,0.012776818,0.025665984,0.093083,-0.023468193,-0.046137393,0.033962652,0.015641224,-0.00852827,-0.013753906,0.04182524,0.008203179,0.043759342,-0.095198646,-0.045250267,-0.033113558,-0.004517241,0.009019881,0.012762832,-0.011167621,0.04386417,-0.030607019,-0.018671442,-0.034672078,-0.04877474,0.0064630094,-0.016764995,-0.010278145,-0.006918065,0.064757004,0.023618206,-3.466909e-05,-0.01927186,0.023020973,0.012514334,-0.0044852947,-0.06848641,-0.0031107801,0.0032415648,0.039063152,-0.049272567,0.021632716,-0.035313267,0.050575018,0.04218843,-0.026783204,0.030138208,0.03259822,-0.03257983,0.04326655,-0.02680481,0.04229932,-0.0066966396,-0.005483573,-0.025477553,0.048942838,0.033723522,0.068175726,0.0032166778,0.065917194,0.0131454365,-0.0052384734,-0.0012728135,0.041593235,0.0054332754,0.0015349572,0.007481559,0.041587424,0.03979482,0.011818322,0.004152813,-0.027684676,0.015712723,0.03374165,0.03656371,0.0018860373,-0.0005038022,-0.016291857,-0.005097067,-0.014597985,0.023021717,0.02110586,0.0016025022,0.029585604,0.041128885,0.024219748,0.02624617,0.06519442,0.00287824,-0.014605737,-0.05374129,0.017484104,0.010092354,0.025988594,0.06086495,-0.043126058,0.012066952,-0.02750601,0.037386134,0.0049702707,0.024645189,-0.02567243,0.0036868022,-0.036176298,-0.06743829,-0.007951835,0.014126362,-0.029814549,0.036131162,-0.027409127,-0.00019376364,0.0028633755,0.007551822,-0.043997705,-0.045396004,-0.007906235,0.102889545,-0.04150181,0.021086875,-0.0009255365,-0.014510178,0.0456304,-0.06765719,0.0041223187,0.022430718,0.020210907,0.016833639,-0.026861472,-0.018721266,-0.02798792,0.058060788,0.09737095,0.06547525,-0.04793066,0.0033436718,-0.0003357177,0.026172608,0.0028512497,0.0149212545,0.024952587,0.047289442,-0.009168668,0.017164312,-0.018197667,-0.06684517,0.059261907,-0.053301364,0.025087938,0.05380046,-0.0066045835,-0.008044444,-0.0008781884,0.04330689,0.016662389,-0.025171425,-0.0048073167,-0.032353923,-0.06731377,-0.016347446,-0.02774376,-0.009615884,0.04073706,0.010458312,0.030747501,0.029274411,0.019149616,0.012709519,0.02380493,0.026093902,0.01819674,-0.015799308,-0.009903136,-0.0046866685,0.024837514,0.014712598,0.034238517,0.052430976,-0.044640537,0.06973381,-0.012030307,0.011323377,-0.0020710758,-0.06030595,-0.02754908,0.0759403,-0.010194401,-0.004729939,-0.027273457,0.040202074,-0.02706299,-0.022408992,-0.0020773173,0.025197078,0.042862654,-0.06601329,-0.007995458,-0.03633056,0.009222671,-0.037962392,-0.028983278,-0.027583987,0.038976796,0.04338828,0.036714874,-0.027036034,0.063832276,-0.01669334,0.0021216017,0.031285774,0.007095968,-0.013406806,0.0017405559,0.025175951,0.012592931,0.045343224,-0.027432999,0.020100892,0.0031276941,-0.042006385,0.039154563,0.039246183,0.01845144,-0.09495019,0.02784962,0.07037489,0.038018886,0.03718042,-0.0356967,-0.0031287263,0.009472575,-0.026325725,-0.016407084,-0.014213129,0.014665759,-0.05893869,0.018247733,-0.038860835,0.03546839,0.029794201,0.04327341,0.018950183,-0.012369549,-0.018638812,0.048504107,-0.0013087931,-0.05660232,0.004298487,-0.05781834,-0.001175714,0.054044776,0.045489077,0.028135141,-0.08260156,0.09585035,-0.07249767,0.011513422,0.061096497,-0.030274283,0.012751441,-0.045141794,-0.008492327,-0.025703557,0.018242342,0.05335817,-0.011292664,-0.019654334,0.021995181,0.03253643,0.026251491,-0.023117386,-0.0032905971,-0.011964456,-0.026735768,-0.018434368,-0.04873414,0.013876576,-0.00570767,0.032699555,0.022606544,-0.008879049,-0.022614913,-0.0195586,-0.02816545,-0.040242486,-0.05531504,0.024182815,0.010299179,0.02021451,-0.032921933,0.019674677,-0.0036618216,-0.016473552,-0.0153343,0.029316971,0.04853226,0.00402781,-0.047754746,-0.011364581,0.019083984,0.06355497,-0.01045915,-0.026435057,-0.03131955,-0.01543914,-0.051670525,0.014506633,0.006505592,0.05877376,0.014116659,0.011055025,0.031157598,-0.038215965,0.014755496,0.0015928565,0.030188983,-0.030653039,-0.064930804,-0.0032995292,0.042370163,0.052988596,-0.020806931,-0.014908934,0.024647795,0.0041612647,0.023753332,0.03746443,0.01823851,0.019806499,-2.9781106e-05,-0.028942406,-0.01682233,0.011941545,-0.018904842,0.013369374,0.010018292,0.049029462,0.0033767202,-0.021264192,0.0084296465,-0.008244937,0.02029626,-0.015585383,0.028607232,0.0633555,0.028665448,-0.042209093,0.041562226,-0.056384955,-0.036223993,0.033996876,-0.0964443,-0.028711008,0.019477937,-0.08523623,-0.00010305358,-0.0039519416,-0.0028713564,-0.03374894,0.021636711,-0.037662614,0.08032595,-0.03069165,0.026864111,-0.01761522,-0.018657751,0.10186862,0.02528663,-0.016848523,-0.0041320403,-0.029373249,-0.043343306,0.03818499,0.06747714,-0.0016219872,0.016432013,0.063165426,0.010786937,0.008958048,0.011532409,-0.08303959,-0.02545239,-0.03553579,-0.047452252,0.0009272842,-0.035210244,0.0020284108,-0.011884825,0.037255403,-0.010517508,0.010443916,-0.008064826,0.027176213,0.020340476,0.002744583,0.043302692,-0.03823511,0.02144997,0.020216009,0.044031084,-0.015955655,0.010213144,0.018071312,0.0047846804,-0.009153032,-0.01626798,-0.00064166856,-0.040160447,-0.024723679,0.029505592,-0.008421973,-0.019477256,-0.024544112,0.013493771,0.027859246,0.059664447,-0.030613016,-0.021084927,-0.013715764,-0.032115854,-0.020169213,0.014811083,0.006750923,-0.00096490484,0.001806867,-0.019525569,0.090811126,-0.09323276,-0.03689368,-0.04493601,0.031360984,-0.0078086285,-0.036097683,-0.0014847909,-0.019687654,-0.00054305064,-0.048183113,0.055524677,0.05726285,-0.040472224,0.016943658,-0.023240024,-0.009208312,-0.0056307986,-0.0037728678,-0.008126593,-0.010979868,-0.022938855,-0.007970909,0.07985824,-0.021739997,-0.005666303,0.020608822,-0.014839173,-0.0724701,-0.025256928,-0.0017894789,0.016202778,-0.018474145,-0.003307479,0.0016539526,0.03726804,-0.05148986,-0.006023277,-0.048645623,0.042024612,-0.0031499492,0.008660555,0.009164528,-0.0018270158,-0.06354707,0.00929143,-0.008321181,-0.050218012,-0.0049827388,0.02849077,0.05494001,0.08156732,0.064483136,-0.041928343,-0.07959147,-0.020330155,-0.03583401,0.015373823,-0.042753406,0.047512922,-0.03964202,-0.002472199,-0.0103374105,-0.033354018,-0.0015197544,0.015540208,-0.0077841245,-0.025656713,-0.030865105,-0.024582243,-0.02387175,0.027826061,-0.017444536,-0.0013830451,0.051800225,0.022654012,-0.04649735,-0.05355389,-0.061803598,-0.039965596,-0.0042738793,0.005919918,0.01756577,0.0034066937,-0.011593847,0.0007480208,0.05314593,0.03574082,-0.023502357,-0.03380184,0.039423816,-0.02034194,0.035455607,0.00066092965,-0.013473444,-0.1201481,0.015725283,-0.0094947005,-0.059931975,0.04043724,0.048881974,-0.021953383,-0.033506364,-0.010896479,0.0029707237,0.021587335,0.043867793,0.014444417,-0.007915902,-0.0015739629,-0.034386728,-0.027999744,-0.01098219,-0.025759239,0.016111352,-0.022356551,0.020544881,0.011969852,0.019255647,0.00043186618,0.0091969725,-0.017078793,-0.014020061,0.026915396,0.06982208,-0.008931158,0.058957882,-0.0024991317,-0.002477402,0.04765886,-0.014676126,0.035113283,-0.0033756082,0.021589823,0.011206285,0.013782362,0.0079246145,-0.04646206,-0.06660697,0.038145065,0.03602326,0.018298091,0.04020853,-0.008875375,-0.033466235,-0.03814699,-0.032835383,-0.009520368,0.06450923,0.017778708,0.039121393,-0.058394752,0.0223824,-0.031940777,0.053392425,0.01161248,0.017099695,-0.003152374,-0.018661859]	Keywords: Transformer, attention mechanism, efficiency, machine translation, document summarization, object detection, speech synthesis, language processing, neural networks, deep learning, computer vision, time-series forecasting, speech synthesis, long sequences, attention mechanisms, deep learning models, sequence modeling, image captioning, natural language processing, vision transformers, language processing, deep learning architecture, sequence prediction, self-attention, long sequence modeling, scalable architectures, neural networks, efficient models, scalable attention, deep learning methods, sequence-to-sequence learning, scalable networks, long range dependencies, computational efficiency, transformer dissection, multi-host attention, funnel transformer, memeshed-memory transformer, conditional positional encodings, positional embeddings, scalable architectures, scalable models, performance optimization, network optimization, long context, transformer architecture, attention is all you need\nKey Objects: Deep Learning, Sequence Modeling, Computer Vision, Natural Language Processing, Time-Series Forecasting, Image Captioning, Attention Mechanism, Machine Translation, Document Summarization, Object Detection, Speech Synthesis, Vision Transformers, Transformers, Scalable Networks, Long-Range Dependencies\nRefers to Images: None\nHypothetical Questions:\n- What are some of the key improvements explored in these Transformer variations?\n- How do these references contribute to our understanding of Transformer models?\n- What are the diverse applications leveraging Transformer architecture?\n- How do the authors address the limitations of standard Transformers?\n- What are the advantages of different approaches to improving Transformer efficiency?\n- Which references describe architectural modifications to Transformers?\n- What are the performance gains associated with these advancements?\n---\nSummary:\nThis is a list of references related to Transformers and their applications. It includes various papers exploring different aspects of Transformers, such as improving efficiency, exploring attention mechanisms, and applying them to tasks like machine translation, document summarization, object detection, and speech synthesis.\nOriginal Text:\n- [20] Xiangxiang Chu, Zhi Tian, Bo Zhang, XiongLang, Xiaolin Wei, Huaixia Xia, and Chunhua Shen. 2021. Conditional Positional Encodings for Vision Transformers. arXiv:2102.10882 [cs.CV]\n- [21] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. 2020. Multi-Host Attention: Collaborate Instead of Concatenate. CoRR abs/2006.16322 (arXiv:2006.13627)\n- [22] Marcella Cornia, Matteo Stefani, Lorenzo Baraldi, and Rita Cuchuana. 2020. Meshed-Memory Transformer for Image Captioning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020. IEEE, 157-0558. https://doi.org/10.1109/CVPR.2020.601020.\n- [23] Zihang Dai, Guokun Li, Yiming Yang, and Quoc Le. 2020. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. In Proceedings of NeurIPS . https://proceedings.neurips.cc/paper/2020/hash/ 2cd2915e65946094e4e5d4a2ac9e1652-Abstract.html\nContextualized Text:\nThis list represents a comprehensive survey of advancements and variations within the Transformer model, a crucial component in numerous deep learning applications.  It encompasses work on improving its efficiency, exploring different attention mechanisms, and adapting it for various tasks. The references highlight research on scaling the model for long sequences, understanding its internal workings, and optimizing its performance for real-world use cases.	{"tags": ["deep_learning", "natural_language_processing", "computer_vision", "sequence_modeling", "transformers", "scalable_architectures", "long_context", "performance_optimization", "network_optimization", "deep_learning_models", "natural_language_processing", "computer_vision", "scalable_networks", "scalable_models", "deep_learning_architecture", "sequence_prediction", "attention_mechanisms", "self_attention", "scalable_models", "scalable_networks", "scalable_architectures"], "doc_id": "564ca5fd-821a-44d3-b616-48d635acd003", "summary": "This is a list of references related to Transformers and their applications. It includes various papers exploring different aspects of Transformers, such as improving efficiency, exploring attention mechanisms, and applying them to tasks like machine translation, document summarization, object detection, and speech synthesis.", "doc_type": "text", "entities": ["Transformer", "Transformer Architecture", "Attention Mechanism", "Document Summarization", "Object Detection", "Speech Synthesis", "Natural Language Processing", "Deep Learning", "Machine Translation", "Vision Transformers", "Funnel-Transformer", "Memeshed-Memory Transformer", "Conditional Positional Encodings", "Scaler Architectures", "Long Range Dependencies", "Computer Vision", "Time Series Forecasting", "Image Captioning", "Self-Attention", "Deep Learning Models", "Sequence Modeling", "Sequence-to-Sequence Learning", "Natural Language Processing", "Scalable Models", "Long Context", "Position Embeddings"], "keywords": ["Transformer", "attention mechanism", "efficiency", "machine translation", "document summarization", "object detection", "speech synthesis", "language processing", "neural networks", "deep learning", "computer vision", "time-series forecasting", "speech synthesis", "long sequences", "attention mechanisms", "deep learning models", "sequence modeling", "image captioning", "natural language processing", "vision transformers", "language processing", "deep learning architecture", "sequence prediction", "self-attention", "long sequence modeling", "scalable architectures", "neural networks", "efficient models", "scalable attention", "deep learning methods", "sequence-to-sequence learning", "scalable networks", "long range dependencies", "computational efficiency", "transformer dissection", "multi-host attention", "funnel transformer", "memeshed-memory transformer", "conditional positional encodings", "positional embeddings", "scalable architectures", "scalable models", "performance optimization", "network optimization", "long context", "transformer architecture", "attention is all you need"], "key_objects": ["Deep Learning", "Sequence Modeling", "Computer Vision", "Natural Language Processing", "Time-Series Forecasting", "Image Captioning", "Attention Mechanism", "Machine Translation", "Document Summarization", "Object Detection", "Speech Synthesis", "Vision Transformers", "Transformers", "Scalable Networks", "Long-Range Dependencies"], "contextual_text": "This list represents a comprehensive survey of advancements and variations within the Transformer model, a crucial component in numerous deep learning applications.  It encompasses work on improving its efficiency, exploring different attention mechanisms, and adapting it for various tasks. The references highlight research on scaling the model for long sequences, understanding its internal workings, and optimizing its performance for real-world use cases.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the key improvements explored in these Transformer variations?", "How do these references contribute to our understanding of Transformer models?", "What are the diverse applications leveraging Transformer architecture?", "How do the authors address the limitations of standard Transformers?", "What are the advantages of different approaches to improving Transformer efficiency?", "Which references describe architectural modifications to Transformers?", "What are the performance gains associated with these advancements?"]}
b78e539c-df83-444b-a71b-9bc66eda1cfa	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0013866123,0.016155919,-0.025418803,0.019524371,0.044374052,0.0613285,0.0066299075,0.07254497,0.057213616,-0.02626352,-0.0057509355,0.008600901,-0.02693365,0.0046603475,-0.01280825,0.048614047,-0.002720041,0.07624247,0.042957105,-0.03751359,0.06806481,0.0042164065,-0.0029534337,-0.037459746,-0.00037601136,0.044398524,0.019713579,-0.03798676,0.03413658,0.018838262,0.0106110955,-0.05631297,0.005144499,-0.011335378,0.018077433,0.025899824,-0.007271175,0.01412198,0.016296938,-0.015239336,-0.013171492,0.037253946,-0.064935476,-0.0045979973,0.009520852,-0.07039278,-0.07467639,-0.019386206,0.034681637,-0.040665943,-0.019959,0.028054966,-0.039525777,-0.03512177,0.022733275,-0.033369664,-0.009441809,-0.010355087,0.035664037,-0.09249268,-0.026104009,0.025953157,-0.059494816,-0.023332017,0.022665262,-0.07930572,-0.042398177,-0.02256313,0.022521904,0.11014087,-0.027316399,0.027774408,-0.022525443,-0.036565993,0.102993816,0.03718332,-0.007062644,-0.02317028,-0.05804723,-0.0029095865,-0.020601943,0.07349651,-0.028225085,-0.046060316,0.07949395,0.0014404039,-0.037178107,-0.03987811,0.10448693,-0.02086453,0.033294663,-0.016722484,0.021043502,0.073838025,0.0010338211,-0.1177956,0.0073741367,-0.04089779,-0.033198196,-0.043681823,-0.022393445,-0.004349842,0.076979175,0.07138289,0.01608891,-0.008110471,-0.04761519,-0.032462686,0.018152533,0.00353306,0.0020991797,0.0020888513,0.00065120694,0.009095108,-0.0141502,-0.015900724,-0.056251284,-0.0043128263,0.030926768,0.01454156,0.0067978706,-0.028076677,0.00697413,0.025473071,0.044000812,0.047311697,-0.016038517,0.00087306136,-0.025639396,-0.024514567,-0.026014121,0.010917232,0.005394441,0.016993802,0.03632004,0.0021248185,0.06312532,-0.05261524,0.010686919,0.013672518,0.033815674,-0.018727364,-0.009504941,0.02395561,-0.03192845,0.02170554,-0.03187224,0.008713951,-0.0055644955,0.037609186,0.0012137645,0.0020068793,0.028512385,0.016362967,0.04044872,0.007278253,0.0027148617,-0.06179203,-0.02934456,0.02192314,-0.035733018,-0.06343826,-0.0342452,0.016106963,0.013262322,0.118902855,-0.011644422,-0.009591201,0.035419654,0.02225702,-0.023012986,0.0099038975,-0.011924142,-0.026004529,0.046347696,-0.009757671,0.019927897,0.015130404,-0.04007065,0.002141608,0.0029993225,-0.0076375995,0.008726375,0.047333404,-0.019785183,0.012424022,0.00016059529,0.07714792,-0.0312033,-0.027617209,0.036743686,-0.023296442,-0.024305597,0.0053214557,0.030354507,0.006604773,0.028512776,-0.08990211,-0.04348846,-0.023252217,-0.012607215,0.017539773,0.018115431,0.0033508455,0.018165508,-0.013114934,-0.041115236,-0.020676738,-0.033993278,-0.02556042,0.01626855,-0.01723872,0.0081631495,0.05491476,0.042324275,-0.0068767155,-0.024005007,-0.007997374,0.03155991,0.012355982,-0.07049421,0.0011445814,-0.00013499611,-0.020353407,-0.06227593,0.010027999,-0.04519499,0.043580804,0.004407758,-0.013447523,0.07251182,-0.027010031,-0.00796973,0.031394914,-0.0024391029,0.035763007,-0.007698884,0.009635134,0.03227034,0.042087507,0.0164478,0.04452337,-0.024190245,0.057785593,-0.00881846,-0.007330168,0.038727436,0.030746255,-0.0073831896,0.034075357,0.021426823,0.05830771,0.071928725,0.015857,-0.0011933661,-0.030096022,0.030691894,0.018441342,0.033921495,-0.0011645926,-0.026290696,-0.013769579,0.010610669,-0.031733654,0.017548481,0.0076120533,0.0041549844,0.066066414,0.009023693,-0.009803535,0.033865623,0.03650522,-0.009505887,-0.010982659,-0.06545783,-0.0042760535,0.0074293134,0.035793446,0.03655464,-0.037830178,0.0024662432,-0.05497059,0.047209643,-0.008476829,0.0041812,-0.01244005,-0.00483625,-0.036469832,-0.070251554,-0.0150687555,0.010505829,-0.058424614,0.005791817,-0.019420996,0.0034163482,-0.0076849526,0.0019265765,-0.014678363,-0.022542695,-0.008831258,0.093599774,-0.016781013,0.016228924,-0.006758999,0.0047938023,0.0225379,-0.07414896,0.002803327,0.07863705,0.000585054,0.0060118255,-0.032057308,-0.022549724,0.00021582775,0.051786102,0.08932975,0.016700462,-0.029211031,-0.009198089,0.011037231,0.012790885,-0.0013520499,-0.009059408,0.06921819,0.027447328,-0.021179974,0.011049102,-0.04266845,-0.06866092,0.05133107,-0.048147682,0.038103558,0.025411868,0.028858729,0.0077757416,0.0054084593,0.07415497,0.013680062,0.008726473,0.034208898,-0.01452046,-0.071036145,0.014485018,-0.014210729,0.0006282035,0.0024925913,0.004752169,0.05349844,0.033989817,-0.00256778,0.005083434,0.075062074,0.052621607,0.010332952,-0.020429142,-0.040595606,0.012775943,-0.00370413,0.018763138,0.015250488,0.055867936,-0.02026058,0.07826558,-0.017945519,0.017155003,-0.0024672966,-0.083730504,-0.06217102,0.04913373,-0.02463693,-0.03151718,-0.0124437045,0.054524038,-0.05882253,-0.02229619,0.010867195,-0.00330452,0.02438845,-0.046889957,-0.039212953,-0.022788135,0.015255789,-0.05080556,-0.012491033,-0.043303315,0.036678795,-0.02296808,0.01227864,0.02059702,0.048004124,-0.014370315,0.00026710288,0.024779785,0.007436878,0.0014463733,-0.006987457,0.035537463,0.03522603,-0.00505085,-0.042480342,0.03547661,0.016459519,-0.041627645,0.030607872,0.072516896,0.037032586,-0.05974683,0.03376523,0.06361491,0.00820048,0.039007578,-0.0055608503,-0.049926296,0.04184018,-0.009041824,-0.04707689,0.00401563,0.029295843,-0.059151556,0.014184002,-0.060177326,0.04985214,0.02308636,0.039719757,0.016610954,-0.038699117,-0.021827038,0.034665845,0.017489549,-0.024847321,0.01214008,-0.073554926,-0.020878997,0.05266278,0.045298863,0.030985853,-0.056231745,0.081004545,-0.1154481,0.022733826,0.08327764,0.015891287,0.026044546,-0.029184345,-0.0053611905,-0.04573077,-0.0036067597,0.034310754,0.024058603,0.0015109748,0.0017481609,0.031010149,0.02260319,-0.011037552,-0.017468374,-0.015866963,-0.054317895,0.007920291,-0.035541177,0.010882259,-0.014727899,0.033303484,-0.0069780443,0.011155125,0.0068912692,-0.0063448525,-0.022393476,-0.018073166,-0.015166964,-0.00036581387,-0.004931585,0.0071619214,-0.063166685,-0.011523644,-0.015341304,-0.020940544,-0.03464595,0.054911464,0.040639885,-0.037295673,-0.011599553,-0.054910544,0.021513876,0.08900016,-0.027541593,-0.0025588411,-0.0032964298,-0.013039395,-0.04499558,0.014697448,0.02150063,0.0757625,0.03090726,0.01911757,0.027133064,-0.024906315,-0.019941293,0.0058170995,0.019154653,-0.05453292,-0.040876232,0.008578697,0.074729,0.007845669,0.00023765137,0.018984085,-0.003852559,-0.00757897,0.028167408,0.038179796,0.021503575,-0.015040191,-0.023066461,-0.0038082341,-0.015350441,0.056789365,-0.0075371396,-0.030713176,0.026438858,0.027580399,-0.02992917,-0.023759404,0.017838454,-0.047689427,-0.0033336715,-0.013545768,0.035996985,0.047386363,0.066504516,-0.05420862,0.051162213,-0.012858674,-0.051314473,0.01404925,-0.056412756,-0.033802353,0.0069412477,-0.05807458,0.00076999405,0.027520489,0.0050219265,0.000780383,0.019739574,-0.023193961,0.07912062,-0.033302076,0.007829212,-0.016974539,0.028004065,0.079016455,0.02968847,-0.0017169818,-0.024327513,-0.017173104,-0.037652288,0.008017656,0.070749946,-0.003817951,-0.0021716626,0.066549316,0.002205463,0.044196516,0.009940555,-0.04309605,-0.031785995,-0.061814465,-0.046523847,0.010384109,0.0031322213,0.028981267,0.017073305,-0.009523831,0.03111975,0.019073848,-0.038699962,0.015453568,0.033908665,-0.018230004,0.020172335,-0.03732364,0.074983254,0.056807276,-0.0057139555,-0.010517543,0.024780748,0.015977059,-0.0034909244,-0.030764349,0.008641287,-0.006776062,-0.032035466,-0.0025683874,0.03788316,0.031150842,-0.04216539,-0.0141060585,-0.017493997,0.06655357,0.03527555,-0.03165188,-0.044957165,-0.025290007,-0.039473403,-0.039889794,0.017437957,0.013275161,-0.0029068168,-0.004137327,-0.05100686,0.09576587,-0.070652165,-0.050543364,-0.032272317,0.033407487,-0.03034746,0.0065679387,0.013476378,-0.021764811,-0.03276279,0.0032248534,0.06478836,0.049665544,-0.026265563,0.03298543,-0.00109307,-0.007993851,0.0107766315,-0.024741696,-0.026492741,0.0021052992,0.0071726404,-0.023380985,0.08214875,-0.029457366,-0.014155165,0.042241823,-0.009216191,-0.059224702,-0.025062418,0.028389215,-0.014616195,-0.01217276,-0.0265197,0.010719131,0.042195685,-0.049867667,-0.020353118,-0.07740359,0.038030192,-0.0042440845,-0.014784147,-0.012918941,0.0027347698,-0.08562118,0.017355628,-0.018012332,-0.053304452,-0.014528552,0.0031932367,0.050450798,0.02249064,0.05665615,-0.071679324,-0.11416698,-0.031783607,-0.03496155,-0.014200354,-0.05091615,0.085748866,0.014649649,0.00027398922,-0.0019241165,-0.05272929,0.0047410456,0.011014,0.0075606112,-0.012241073,-0.02025867,-0.019346355,-0.00029234268,0.028934572,-0.008012267,0.0037177445,0.06157794,0.03342207,-0.08393316,-0.06883424,-0.022255462,0.0037571103,-0.0002533268,0.012236312,0.03217679,0.024833642,-0.01928587,-0.0034992136,0.038681746,0.03255736,0.0034201816,-0.007484281,0.029515313,-0.022880144,0.03637801,-0.00013077218,-0.008009263,-0.10678504,0.0030139822,-0.014344179,-0.05726201,0.047938466,0.037798695,0.011814027,-0.021093633,-0.03144103,-0.0007741918,0.033996858,0.049266882,0.020328613,0.021838224,0.04644392,-0.02517184,-0.035157915,-0.00529878,-0.028458614,0.02621749,-0.035892315,0.028350268,0.015366683,0.014967175,-0.0028818825,0.032589134,-0.044345334,-0.037589695,0.04351048,0.027333498,-0.021276297,0.058043912,0.00048652902,0.01919284,0.039002743,-0.007932785,0.016041793,-0.015636502,0.020283842,0.0050131073,0.011737677,-0.00013703005,-0.004681702,-0.06499414,0.01442476,0.021583715,0.0139298495,0.032714587,0.010671887,-0.03042535,-0.013505168,-0.023227744,-0.010031859,0.05010282,0.029235395,0.05260683,-0.07900952,0.015814107,-0.021778982,0.0515811,0.03892066,-0.047796156,0.017463299,-0.031727463]	Keywords: Transformers, Neural Networks, Language Modeling, Object Detection, ACL, ICML, EMNLP, ICLR, References, Architecture, Attention, Context, Convolutional Networks, Universal Transformers, Transformer-XL, Gated Convolutional Networks, Attention Mechanisms, Contextual Understanding, Deep Learning, Sequence Processing, Representation Learning, Self-Supervised Learning, Memory Networks, Long-Range Dependencies, Transformer Models, Transformer Architecture, Contextual Modeling, Deep Learning Models, Transformer Applications\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What is Transformer-XL?\n- What are Gated Convolutional Networks?\n- What is the purpose of Universal Transformers?\n- Where can I find more information about these references?\n- What is the relationship between Transformers and language modeling?\n- How are Transformers used in object detection?\n- What is ACL?\n- What is ICML?\n- What is EMNLP?\n- What is ICLR?\n- What are the key advancements in Transformer architecture?\n---\nSummary:\nThis is a list of references related to Transformers, a type of neural network architecture. The references cover a range of topics including Transformer-XL, Gated Convolutional Networks, Universal Transformers, and applications in areas like language modeling and object detection. The list provides links to proceedings and abstracts for each reference.\nOriginal Text:\n- [24] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019. TransformerXL: Attentive Language Models beyond a Fixed-Length Context. In Proceedings of ACL . Florence, Italy, 2978-2988. https://doi.org/10.18653/v1/Mod19-1285\n- [25] Yann N. Dauphin, Angela Fan, Michael Aciul, and David Granger. 2017. Language Modeling with Gated Convolutional Networks. In Proceedings of ICML . 933-941. http://proceedings.mlr.press/v70/dauphin17a.html\n- [26] Mostafa Dehghani, Stephan Gough, Oriws Vailinys, Jakob Usorek, and Lukasz Kaiser. 2019. Universal Transformers. In Proceedings of ICRL . https://openreview.net/forum?id=YhzRiRDyR97\n- [27] Ameet Deshpande and Karthik Narasimhan. 2020. Guiding Attention for Self-Supervised Learning with Transformers. In Findings of the Association for Computational Linguistics: EMNLP 2020. Online, 4676-4686. https://doi.org/10.18653/ v1/2020.findings-empl.419\nContextualized Text:\nThis is a list of references related to Transformers, a type of neural network architecture. The references cover a range of topics including Transformer-XL, Gated Convolutional Networks, Universal Transformers, and applications in areas like language modeling and object detection. The list provides links to proceedings and abstracts for each reference.	{"tags": [], "doc_id": "b78e539c-df83-444b-a71b-9bc66eda1cfa", "summary": "This is a list of references related to Transformers, a type of neural network architecture. The references cover a range of topics including Transformer-XL, Gated Convolutional Networks, Universal Transformers, and applications in areas like language modeling and object detection. The list provides links to proceedings and abstracts for each reference.", "doc_type": "text", "entities": [], "keywords": ["Transformers", "Neural Networks", "Language Modeling", "Object Detection", "ACL", "ICML", "EMNLP", "ICLR", "References", "Architecture", "Attention", "Context", "Convolutional Networks", "Universal Transformers", "Transformer-XL", "Gated Convolutional Networks", "Attention Mechanisms", "Contextual Understanding", "Deep Learning", "Sequence Processing", "Representation Learning", "Self-Supervised Learning", "Memory Networks", "Long-Range Dependencies", "Transformer Models", "Transformer Architecture", "Contextual Modeling", "Deep Learning Models", "Transformer Applications"], "key_objects": [], "contextual_text": "This is a list of references related to Transformers, a type of neural network architecture. The references cover a range of topics including Transformer-XL, Gated Convolutional Networks, Universal Transformers, and applications in areas like language modeling and object detection. The list provides links to proceedings and abstracts for each reference.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is Transformer-XL?", "What are Gated Convolutional Networks?", "What is the purpose of Universal Transformers?", "Where can I find more information about these references?", "What is the relationship between Transformers and language modeling?", "How are Transformers used in object detection?", "What is ACL?", "What is ICML?", "What is EMNLP?", "What is ICLR?", "What are the key advancements in Transformer architecture?"]}
005ea3b7-d94d-45aa-b157-d70e86ae7588	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02095056,0.006840032,-0.015694046,0.025301127,0.01713359,0.061834484,0.0013286972,0.07805008,0.047676437,-0.034183685,0.01957729,0.012978589,0.0115110725,-0.0075332965,-0.060405146,0.043382026,0.00019162278,0.058905512,0.029809583,-0.0074972822,0.04143768,0.028441902,-0.010108851,-0.05046387,-0.00738548,0.01122913,-0.00097217393,-0.05027046,0.026532572,0.036792986,0.014024097,-0.054820746,0.030135265,0.0071368,0.008499052,0.016939381,0.003800512,0.05064466,0.02348121,-0.03191032,-0.006634355,0.04432031,-0.060651004,-0.01392953,-0.0010371176,-0.095608115,-0.074558645,-0.024761235,0.037255533,-0.02245762,-0.02333162,0.023325754,-0.025829548,-0.020796906,0.01881776,-0.026066437,-0.0092870705,-0.042936113,0.032914266,-0.09581172,-0.03374278,0.0226976,-0.041463733,-0.023257392,0.045820612,-0.055192478,-0.0400864,-0.0056204605,0.00886479,0.1224887,-0.01013528,0.03734427,-0.030396579,-0.04525689,0.10640158,0.041316498,0.0023733922,-0.013277169,-0.08196451,-0.0066288915,-0.036174122,0.062270723,-0.069136955,-0.047668893,0.08447643,-0.015019133,-0.04631456,-0.053082805,0.08946331,-0.010595171,0.02136173,-0.038194742,-0.0032908604,0.07958471,0.006885513,-0.0520934,0.003341099,-0.01968674,0.007295436,-0.06273243,-0.037046757,-0.034834277,0.07311326,0.08238504,0.0085403565,-0.0065722335,-0.03225977,-0.05910795,0.01816384,0.029449694,0.00020521104,-0.009802824,-0.019221593,-0.010454521,-0.010704271,-0.03491666,-0.0553624,0.0028213665,-0.01815113,0.011172011,-0.012869149,-0.012870617,0.02211538,0.007306297,0.056311466,0.06529975,-0.017813245,-0.0042608688,-0.04806874,-0.0018678209,0.0020249528,-0.0064821793,0.0010193045,0.026726194,0.023364354,0.029419413,0.04190406,-0.031586822,0.016161313,-0.0070444313,0.030624365,-0.026001688,-0.008409137,0.048158113,-0.024281938,0.0031135764,-0.058535717,-0.003960733,-0.02685435,0.035685778,0.00035101746,0.006693255,0.018029207,0.014580332,0.0119026005,-0.0024033948,0.015580883,-0.068162315,-0.0031985438,0.0059714704,-0.05017488,-0.046948873,-0.034635797,0.03882365,0.03586547,0.115638725,0.01752481,0.0036767898,0.038941443,0.0139672365,-0.03792763,0.015805148,-0.0139921345,-0.026576914,0.011114555,-0.021155521,0.023482477,0.0046498543,-0.039090686,0.0018341548,0.0028010206,0.01764368,-0.014129353,0.049569763,-0.013816004,0.024154637,-0.0030642196,0.07665624,-0.025548123,-0.0433107,0.01568749,-0.008414901,-0.019832734,-0.018311055,0.027229596,0.015808687,0.03246656,-0.060012188,-0.04220888,-0.024502276,-0.015347631,-0.012654237,0.031830486,0.01936999,0.032341324,-0.0029754906,-0.046622094,0.013080194,-0.017830105,-0.04974867,-0.0126679875,-0.006478612,0.0002776651,0.052511897,0.049312912,-0.032757085,-0.026524547,0.029037263,0.022028638,-0.011792013,-0.052773055,0.02742062,-0.010936401,0.018406153,-0.040382013,-0.009079805,-0.05980772,0.02156567,0.0044835857,-0.041537046,0.041371416,-0.0051807784,0.0054643196,0.014948455,-0.031734187,0.04900953,-0.019981768,-0.0059364014,0.00830648,0.03778551,0.03584614,0.06448233,0.021890813,0.038151313,0.0042601693,-0.0035652665,0.018932601,0.058330055,-0.018692868,0.02265351,0.0031078882,0.051693413,0.04861762,0.0018272031,-0.006685067,-0.02570669,0.014016383,0.041154955,0.028308876,0.017917026,0.006319074,-0.011249083,0.03817698,-0.012341103,0.01596656,0.023815634,0.02789693,0.04156547,0.024710245,0.006595473,0.02198931,0.070575334,-0.011994019,-0.034129135,-0.086729944,-0.0013045853,-0.015876308,0.045122005,0.045894846,-0.025873164,-0.026674112,-0.02549577,0.044534996,-0.0074336454,0.029738458,-0.02235129,-0.009659066,-0.025833422,-0.09457568,-0.015505859,-0.014781571,-0.03824157,0.02115514,-0.027528908,0.014786024,0.012424638,0.002146362,-0.026464073,-0.033014074,0.011484324,0.07546003,-0.061543755,0.03056198,0.006777195,-0.016572196,0.033157784,-0.07857068,0.005483201,0.0418944,0.021954421,0.013590767,-0.018429581,-0.004738449,-0.02109159,0.02120474,0.07569448,0.054340936,-0.006047722,-0.0062771086,0.023911983,0.018652169,0.011384361,-0.009656504,0.030155394,0.036555026,0.015592656,-9.214978e-05,-0.035055146,-0.09904912,0.048106417,-0.07808522,0.026248764,0.034215566,-0.0054362393,-0.019899188,-0.014104094,0.04791547,0.044256173,0.019344522,0.026981385,3.992099e-05,-0.07216845,-0.023535073,0.005536694,-0.00010089855,0.040802915,0.0152104935,0.044862162,0.041850664,0.021500424,-0.021074913,0.027452389,0.045583498,0.014247104,-0.017585048,-0.011086706,0.0006369826,0.017467808,0.014093608,0.035781723,0.056710277,-0.026634013,0.076353855,-0.017238514,0.009401987,-0.016444642,-0.07241525,-0.031549916,0.05810748,-0.018113634,-0.007332247,-0.029644087,0.053871263,-0.042904265,-0.009369371,-0.020965911,0.03470337,0.029235998,-0.05120762,-0.012273488,-0.037154295,0.01101613,-0.06292178,-0.032850746,-0.032051932,0.07292454,0.028297663,0.02071343,0.012946148,0.036566537,-0.028148012,0.016689043,0.02750166,-0.008753657,0.0133778835,-0.0017054956,0.04934409,0.022311904,0.024712026,-0.010023896,0.041258242,0.02649654,-0.041797306,0.03690858,0.047219533,0.008824244,-0.08550195,0.02567609,0.07304561,0.02360737,0.04468261,-0.04545902,-0.016257683,0.048855055,-0.017886523,-0.02164397,-0.020477662,0.016290605,-0.04451039,0.018548341,-0.04058965,0.04041232,0.009545446,0.0434082,0.035110947,-0.024372151,-0.016604895,0.025595633,0.012337567,-0.051615857,0.0058221603,-0.08501373,0.0071849017,0.03296212,0.025273858,0.017578067,-0.05157288,0.062470708,-0.09731521,0.008433487,0.049597114,-0.0229964,-0.018320713,-0.037549946,-0.032726184,-0.020432318,-0.0047497363,0.03901234,0.007831245,-0.02744822,-0.022100171,0.03405143,0.026865741,-0.0039056262,-0.005191083,-0.021419715,-0.046936035,-0.019928774,-0.051868424,-0.013487928,-0.0060590366,0.06856898,-0.005731525,-0.015919188,0.008804356,-0.021419318,-0.038484104,-0.028016143,-0.030463764,0.001932497,-0.011765012,0.04692509,-0.04227409,-0.014334256,-0.03743889,-0.01867455,-0.017723685,0.05670891,0.060377676,0.007866006,-0.050238755,-0.046817955,0.018590864,0.062484335,-0.043117005,-0.0062472927,-0.012452642,-0.025829852,-0.07069313,0.02740627,0.023618361,0.07684092,0.016080234,0.021003582,0.019046618,-0.014439812,-0.020671073,0.0055872453,0.0075546275,-0.049776945,-0.041600563,-0.007884854,0.046577506,0.015188971,0.009853884,0.006946359,0.003882986,-0.025240762,0.017953698,0.037233528,0.0071655177,0.009181942,-0.017912298,0.012392201,-0.010303722,0.04727728,-0.01675476,0.005649823,0.0116190845,0.044382345,-0.0032456282,-0.03411754,-0.016795356,-0.023403388,0.009269717,-0.008727079,0.025667202,0.054304447,0.06934114,-0.035920564,0.08434251,-0.020832619,-0.02768869,0.012392855,-0.05388348,-0.03192114,0.020049315,-0.078299955,-0.051885024,0.037222706,-0.031046603,-0.024554191,0.026757028,-0.044357263,0.08450275,-0.066211745,0.04385351,-0.026012566,0.016646173,0.097747944,0.021680335,-0.018408474,-0.0026349607,-0.026087595,-0.030558243,0.037151262,0.079157636,-0.0047487644,0.0075857234,0.061513875,0.008835603,0.0438694,0.009616102,-0.039179232,-0.02174456,-0.028485857,-0.03837928,0.0014738031,-0.026251575,-0.0019816007,0.0022091602,0.0028987185,0.0071432623,0.015955582,-0.0051450725,0.018304529,0.034615535,-0.022761755,0.020496001,-0.057436716,0.05226107,0.047295973,0.017107617,-0.030992657,0.020498114,0.025564086,-0.0025168124,-0.010839162,0.011903472,-0.0050840424,-0.008027682,-0.027209451,0.030423855,0.0050445586,-0.041490726,0.0012451493,0.022536917,0.04287189,0.0029202134,-0.031610686,-0.03125632,-0.009768656,-0.03154005,-0.022531765,0.030991096,-0.012292096,-0.00950256,0.023107734,0.0012229938,0.078944474,-0.102471344,-0.034708664,-0.040334444,0.05937818,-0.038441826,-0.0014058388,0.025230022,-0.023161957,-0.037381332,-0.015047163,0.06423012,0.03843815,-0.06391535,0.017268995,0.0066596987,-0.008754386,0.0065814084,-0.012414088,-0.029102238,0.0002574643,-0.03373513,-0.009386652,0.09819127,-0.0044926936,0.013909934,0.04389779,-0.0042606625,-0.0720641,-0.01377578,0.025462683,0.009591425,0.00040039653,-0.008865133,0.032522634,0.030417642,-0.05957821,-0.007922793,-0.08445871,0.035948165,-0.022662405,0.010465119,0.03958551,-0.014274345,-0.06001493,0.03384029,-0.035136316,-0.053876624,-0.03801466,0.034242727,0.052373145,0.031058658,0.05006029,-0.051242217,-0.09919694,-0.037662826,-0.01840461,-0.017470833,-0.032927938,0.044558328,-0.023825103,-0.0140155,0.024252933,-0.018625269,0.0037455787,0.030170308,0.012095705,-0.007420221,-0.0079658115,-0.028418407,-0.023265038,0.014762582,-0.017924767,0.0054477416,0.028339699,0.039297454,-0.0805698,-0.07221817,0.0015400711,-0.0035382125,0.005802395,-0.012144544,0.026667356,0.0034661372,-0.005018724,0.0072044297,0.08147077,0.0107193785,-0.022301305,-0.0006744684,0.04586507,-0.026227128,0.04295859,0.03322953,-0.012051473,-0.09446349,0.01880947,-0.015346117,-0.07483476,0.042123545,0.030656729,0.00071885134,-0.028856007,-0.034582797,-0.0036592258,0.025463443,0.0646938,-0.009090433,0.012152388,-0.015415039,-0.031970195,-0.03678154,-0.028056012,-0.012126664,0.012199148,-0.03620638,0.04354913,0.0077560097,0.017440883,0.015357062,0.020252755,-0.014068171,-0.038685333,0.03107503,0.030786127,-0.029831193,0.056632556,0.014154097,-0.016087592,0.046184383,-0.0067050713,0.038331237,-0.022777803,0.03472759,0.022038719,0.02021711,0.0005194959,-0.022011722,-0.09416612,-0.020865535,0.00908101,0.021249432,0.05070865,-0.025512734,-0.025617393,-0.052795943,-0.044386484,-0.013827143,0.07228993,0.035379115,0.011234725,-0.03613972,0.017779011,-0.016244873,0.036508508,0.037460424,-0.01796763,-0.006303167,-0.0131296655]	Keywords: transformers, bert, speech recognition, document modeling, image generation, references, deep learning, natural language processing, machine learning, architecture, advancements, foundational work, modeling, transformers architectures, natural language processing models, machine learning advancements, innovations in deep learning, transformer architectures and methods, improvements to transformer models, references for transformer deep dive, core references transformer deep study, transformer model foundational work, recent transformer architectures, transformer architectural advancements, transformer modeling techniques\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What are some of the key architectural improvements mentioned in these references?\n- How has the Transformer model evolved since its initial introduction?\n- What are the applications of Transformers beyond natural language processing?\n- Which references are considered foundational to the Transformer architecture?\n- What are some of the recent advances in Transformer modeling techniques?\n---\nSummary:\nThis is a list of references cited in a survey about Transformers. It includes a diverse range of publications from various years, covering topics like BERT, speech recognition, document modeling, image generation, and various architectural improvements to the Transformer model. The references cover both foundational work and more recent advancements in the field.\nOriginal Text:\n- [28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of HLT-NAACL . Minneapolis, Minnesota, 4171-4186. https://doi.org/10.18653/v1/N19-1423\n- [29] Ming Ding, Zhuoyi Yang, Wemi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. 2021. Cog View: Mastering Text-to-Image Generation via Transformers. arXiv:2105.13290 [cs.CV]\n- [30] Siyu Ding, Junyuan Shang, Shuouang Wang, Yu Sun, Hao Tian, Hua Wu, and Hafeng Wang. 2020. ERNIE-DOC: The Retrospective Long-Document Modeling Transformer. (2020). arXiv:2012.15688 [cs.CL]\n- [31] Linhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-Transformer: A No-Recourse Sequence-to-Sequence Model for Speech Recognition. In Proceedings of ICASSP . 5884-5885. https://doi.org/10.1109/ICASSP.2018.8462506\nContextualized Text:\nThis list represents the extensive background knowledge required to truly understand and appreciate the current state of Transformer technology. It's a valuable resource for anyone interested in delving deeper into the subject.	{"tags": [], "doc_id": "005ea3b7-d94d-45aa-b157-d70e86ae7588", "summary": "This is a list of references cited in a survey about Transformers. It includes a diverse range of publications from various years, covering topics like BERT, speech recognition, document modeling, image generation, and various architectural improvements to the Transformer model. The references cover both foundational work and more recent advancements in the field.", "doc_type": "text", "entities": [], "keywords": ["transformers", "bert", "speech recognition", "document modeling", "image generation", "references", "deep learning", "natural language processing", "machine learning", "architecture", "advancements", "foundational work", "modeling", "transformers architectures", "natural language processing models", "machine learning advancements", "innovations in deep learning", "transformer architectures and methods", "improvements to transformer models", "references for transformer deep dive", "core references transformer deep study", "transformer model foundational work", "recent transformer architectures", "transformer architectural advancements", "transformer modeling techniques"], "key_objects": [], "contextual_text": "This list represents the extensive background knowledge required to truly understand and appreciate the current state of Transformer technology. It's a valuable resource for anyone interested in delving deeper into the subject.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the key architectural improvements mentioned in these references?", "How has the Transformer model evolved since its initial introduction?", "What are the applications of Transformers beyond natural language processing?", "Which references are considered foundational to the Transformer architecture?", "What are some of the recent advances in Transformer modeling techniques?"]}
1c6e3007-2be9-459b-a224-1be2591e3636	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.012135084,0.018339364,0.007730861,0.035547834,0.010726634,0.074068144,-0.0022654694,0.06830411,0.05195996,-0.040169057,-0.010261298,0.009455141,-0.001102868,-0.034378152,-0.041250296,0.03903361,-0.002039449,0.07261401,0.02688136,-0.04428133,0.0438988,-0.0058551147,-0.006231938,-0.028079119,-0.010795795,0.027988136,0.008629151,-0.0466487,0.018265627,0.057808295,0.008600506,-0.05875455,0.016633697,0.008649157,0.008252149,0.0044047306,0.02692931,0.0038167636,0.047193546,-0.037522748,-0.012123373,0.036710706,-0.07549465,0.007370944,-0.010926516,-0.050062157,-0.07686697,-0.012232249,0.03864644,-0.007414837,-0.03665943,0.01575473,-0.013707669,-0.016017757,0.03271442,-0.043523863,0.003059696,-0.05591959,0.02653335,-0.08671958,-0.033829078,0.02771668,-0.044234518,-0.009060307,0.013580512,-0.057279505,-0.020901859,-0.011384372,0.010700779,0.109118715,-0.012351274,0.04589201,0.006236537,-0.07343296,0.092446595,0.047429014,-0.028059773,-0.0149777895,-0.055060674,-0.0255087,-0.005675364,0.078943424,-0.039820842,-0.07389553,0.071405545,0.00085835316,-0.030804897,-0.005219863,0.06917741,-0.022797257,0.023598975,-0.005363165,-0.0005990578,0.06584817,0.0017837267,-0.08466727,-0.009571871,-0.0363156,-0.00527339,-0.060101036,-0.018668119,-0.012842789,0.09404404,0.09283651,0.035073295,-0.004561509,-0.0066394634,-0.045153547,0.01446051,0.0078523895,0.015994031,0.011582929,-0.045961164,-0.014812626,-0.02359284,0.006390868,-0.046113178,0.009306993,0.009395948,-0.006474332,-0.008851582,-0.015033589,0.021765728,0.015652541,0.076594405,0.06190515,0.013962146,-0.0021785453,-0.014337092,0.0099469945,0.00055135315,-0.004265669,-0.031009143,0.015449397,0.017649705,0.013875476,0.068033606,-0.052682858,0.018293502,0.021771623,-0.0025991225,-0.030939944,-0.0201932,0.053998757,-0.01952157,-0.008419467,-0.048057705,0.013169519,-0.022931773,0.060965747,0.00869032,-0.011501199,0.018565273,0.016783167,-0.0005794455,0.0015398134,-0.007603038,-0.045719117,0.014008051,-0.011574788,-0.024745418,-0.0576265,-0.049183536,0.05965798,0.022345116,0.14844151,-0.0069755143,0.001897131,0.048810065,-0.009045154,-0.0470799,0.014320026,-0.022052104,-0.045349192,-0.0019109105,-0.01613707,0.011420574,-0.03094501,-0.023813581,-0.01247057,0.014700555,0.009302216,-0.0033042566,0.05863575,-0.055172678,0.0111259455,-0.009322175,0.09530855,-0.018886372,-0.040516865,0.026107633,0.013124976,-0.025762757,-0.014477652,0.023550248,0.0003342458,0.03203582,-0.07296095,-0.046254795,-0.040963296,-0.013141593,-0.0085563995,0.036035385,0.0065747974,0.024450293,-0.017810533,-0.028092192,-0.04586215,-0.02019881,-0.013755159,0.022129381,-0.035175912,-0.0049900576,0.069828376,0.02963887,-0.010238944,0.00056735065,0.037414122,0.03439965,-0.016848383,-0.05080618,0.011106956,-0.014441977,0.023054216,-0.044046525,0.0112573225,-0.03739577,0.041190986,0.0069619617,-0.039466627,0.059104547,0.028861156,-0.030647822,0.022134233,0.021892004,0.06642846,0.0071434947,-0.009241457,0.010251328,0.054462887,0.02042534,0.060041424,0.043483835,0.05591834,0.0041282596,0.00053438166,0.039523166,0.056530967,-0.024606744,0.020471726,0.013376489,0.039500337,0.047801137,0.00615693,0.0050870487,-0.055720717,0.035174772,0.044678107,0.027244708,0.011281056,-0.007009996,0.02036008,0.019715346,-0.009916212,0.02701581,0.02923194,0.01874458,0.041622445,0.020031769,0.000365809,0.028452897,0.07420969,-0.01719887,-0.026477544,-0.053898543,0.0110189095,0.005833738,0.0055516376,0.03324007,-0.039243825,-0.0064905994,-0.040084265,0.061422195,-0.016896764,0.012071966,-0.019623017,-4.8334663e-05,-0.034407616,-0.064330906,0.006405473,5.0423732e-05,-0.040824518,0.026293783,-0.018877517,0.027934525,0.025521517,0.01898895,-0.040809855,-0.017479109,-0.005018354,0.102540344,-0.034264352,0.033184856,0.0062897615,-0.028820373,0.03899258,-0.060311254,-0.012935077,0.05429139,-0.006798613,-0.0023590522,-0.040960114,0.0010019556,-0.02590466,0.041080065,0.07369796,0.031089438,-0.030232718,-0.011638524,-0.0007841131,0.017637044,0.020781312,0.013824517,0.03287358,0.02393052,-0.02645403,0.021180201,-0.027638737,-0.086572595,0.036654618,-0.06779522,0.055983577,0.04367224,0.018101975,-0.003511427,-0.016522955,0.044343103,0.025178788,-0.030695053,0.005769064,-0.010977031,-0.045451716,-0.02184677,-0.013751759,-0.026092174,0.017792046,0.006398006,0.024065547,0.04323654,0.0021593557,0.017994983,0.05423189,0.041932,0.0023365086,0.002764619,-0.035158403,-0.011520222,0.015521369,0.005987234,0.0125626335,0.056258976,-0.03215877,0.09394603,-0.03307646,-0.0050184727,-0.0066840993,-0.087490365,-0.021609625,0.07120522,-0.029491674,-0.0061484845,-0.043323576,0.051421106,-0.030520037,-0.015111226,0.0021424973,0.025019111,0.017799135,-0.07213574,0.005820233,0.0013500635,0.042744227,-0.04797775,-0.025876872,-0.030876439,0.039671957,-0.0038203627,0.034089956,-0.00020108176,0.04913222,-0.03994892,-0.010060443,0.018624231,-0.007919956,-0.0015321053,0.015186158,0.025742903,0.033770192,0.021071903,-0.040724646,0.028023893,0.027890537,-0.03815227,0.022706198,0.06811971,0.024028465,-0.09409687,0.008630984,0.050297197,0.02503631,0.023017546,-0.008262719,-0.012919354,-0.0019799517,-0.01619703,-0.018314747,0.018455707,0.0077184127,-0.049535,0.017245445,-0.04603928,0.0614353,0.012965926,0.07854081,0.054588825,-0.020712826,-0.043411147,0.04166407,-0.016037134,-0.043668773,0.02303276,-0.07997509,-0.019560564,0.047166277,0.045360606,0.05614394,-0.05618226,0.07921851,-0.078669935,-0.010280588,0.0760034,-0.036415808,0.0054126475,-0.043951653,-0.009506628,-0.03844746,-0.015972584,0.04001738,-0.007626327,-0.033956025,0.012722183,0.035243765,0.006742845,0.003839644,-0.017265083,-0.022837402,-0.034373745,-0.018388137,-0.05311994,0.0041414048,0.003340527,0.037240915,-0.009602824,0.0072567617,-0.017367722,-0.009185281,-0.04095749,-0.009134095,-0.0325151,-0.01624333,0.0090519795,0.016768564,-0.054415565,-0.027434671,-0.006411374,-0.027184006,-0.005073765,0.051140524,0.032524463,-0.011383728,-0.043349996,-0.028033853,-0.003290851,0.04787595,-0.014687427,-0.011402278,-0.022520002,-0.010437381,-0.04717559,0.019181404,0.018073475,0.05903471,0.013365957,0.023712248,0.014205777,-0.017798867,-0.011673686,-0.030506905,0.029439695,-0.04148236,-0.028912706,0.012749148,0.027386867,0.037135825,0.012132914,-0.007655238,0.04079877,0.018316275,0.02314399,0.039337825,0.002449345,0.018979056,-0.007989806,-0.0008548863,-0.022255084,0.016273685,0.00085892517,-0.004311735,0.016273722,0.057274465,-0.010136254,-0.0018583855,0.01100445,-0.012752363,0.01795126,-0.011854051,0.06060685,0.052366298,0.055525437,-0.0445982,0.05807519,-0.047964267,-0.07913972,0.017514765,-0.042427674,-0.03884095,-0.008573792,-0.049566105,-0.011715344,0.010555544,0.009561957,-0.0019881753,0.044497505,-0.04750793,0.076671995,-0.033332475,0.051310174,-0.012324274,0.0056394814,0.10433793,0.011527184,-0.0031140829,-0.032094847,-0.015820328,-0.06308968,0.02945366,0.06572994,0.0076766238,0.005435126,0.058980465,0.01811576,0.054665577,-0.011220244,-0.036734536,-0.02341261,-0.035135675,-0.059986588,0.030039616,-0.02418772,0.02155006,-0.0030878661,-0.0006662586,0.027323062,0.0213812,-0.010575645,0.012106842,0.033112466,0.0012137925,0.05775904,-0.059208915,0.048891015,0.025082255,0.010773966,-0.016976956,0.019288966,0.037903443,0.0128305545,-0.029281624,-0.006437099,-0.017553646,-0.022481302,-0.030204125,0.028929994,0.014590946,-0.0038046616,-0.02991948,0.00336011,0.053270876,0.024091486,-0.02783529,-0.0029146313,-0.019528588,-0.029643554,-0.029907655,0.031921778,-0.008628884,-0.023951352,0.0076100905,-0.03467115,0.07184554,-0.10827903,-0.045365073,-0.04958818,0.027673213,-0.0030517622,-0.01496424,0.018041767,-0.0123625165,-0.010041567,-0.014982016,0.062373932,0.014521281,-0.041137997,0.016849793,0.0061180554,-0.0045676907,-0.0035870494,-0.010492881,0.0036178716,-0.004537702,0.01473036,-0.009569691,0.06937046,-0.027460141,0.030585844,0.0726919,-0.01700444,-0.049079914,-0.016427223,0.015717268,0.029623376,0.0011621481,0.0013381941,0.009845131,0.046005152,-0.036038157,-0.017182983,-0.03682192,0.06092776,-0.0199557,-0.014859186,0.01782761,9.181819e-05,-0.045765866,0.02792512,0.004123135,-0.049152087,0.0055483324,0.019790662,0.07655347,0.04591263,0.08920322,-0.04905699,-0.08274688,-0.010176385,-0.051513176,-0.011519266,-0.064639345,0.040851135,0.0054919845,-0.005035833,0.0025226793,-0.02436209,0.021439698,0.019168912,0.007997277,-0.03302623,-0.02286903,-0.047978893,-0.017744265,0.015222329,0.011499384,-0.004697385,0.05154378,0.045100477,-0.06316283,-0.057042435,-0.033552937,-0.010464771,0.0388126,0.037354812,0.031640217,0.00321562,-0.00049051206,0.009068899,0.054712128,0.003430035,-0.017332885,-0.019116249,0.02566535,-0.012776876,0.05414769,-0.0271384,-0.04230215,-0.10886347,-0.016305318,-0.008702786,-0.070285074,0.024626715,0.026264511,-0.0026824798,-0.041220415,-0.0367728,-0.02851042,0.024157038,0.043752763,0.006442224,0.021829998,0.0043853936,-0.03978791,-0.06906355,-0.011591592,-0.03894521,0.018869163,-0.023978831,0.049254123,0.009435463,-0.024778146,-0.011514225,0.03308614,0.0044040745,-0.01940696,0.037113164,0.058923606,-0.045168538,0.053030245,-0.0041808253,-0.01769238,0.05531521,0.0068328795,0.02916389,-0.012893226,0.0016286474,0.01865363,0.016061231,0.016527023,-0.037518185,-0.07021824,0.02283841,0.027048115,-0.014417071,0.048050337,-0.010622106,-0.03401008,-0.055513714,-0.023096288,-0.02563508,0.053985797,0.025762778,0.020857284,-0.06979252,0.019379826,-0.02791293,0.036339782,0.03820015,-0.045457773,-0.010867,-0.0018230292]	Keywords: transformers, attention mechanisms, natural language processing, image recognition, speech synthesis, time-series forecasting, efficient transformers, memory-augmented transformers, deformable transformers, long sequence modeling, attention is all you need, references, survey, deep learning, machine learning\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What are some key differences between various Transformer architectures discussed in the references?\n- What are the primary limitations of the original Transformer architecture that these references attempt to address?\n- How do efficient Transformer models like Informer improve upon the original architecture?\n- What applications benefit from deformable Transformers?\n- What is the significance of the 'Attention is All You Need' paper?\n- What are memory-augmented Transformers and what problems do they solve?\n---\nSummary:\nThis is a list of references (173 items) related to Transformers, spanning a range of applications including natural language processing, image recognition, speech synthesis, and time-series forecasting. The references cover foundational papers like 'Attention is All You Need' (Vaswani et al., 2017) and more recent advancements addressing limitations and exploring variations of the Transformer architecture. They include papers on efficient transformers (e.g., Informer), memory-augmented transformers, deformable transformers, and modifications to attention mechanisms.\nOriginal Text:\n- [32] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. 2021. Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth. CoRR abs/2013.03044 (2021). arXiv:2103.03404\n- [33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Usokreit, and Neil Houlsby. 2020. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2101.11929 [cs.CV]\n- [34] Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. 2021. Addressing Some Limitations of Transformers with Feedback Memory. https://openreview.net/forum?id=OCm0rwall1x1  \n- [35] Zhihao Fan, Yeuny Gong, Dayiheng Liu, Zhongyu Wei, Siyuan Wang, Jian Jiao, Nan Duan, Ruofei Zhang, and Xuanjing Huang. 2021. Mask Attention Networks: Rethinking and Strengthen Transformer. In Proceedings of NAACL . 1692-1701. https://www.aclweb.org/anthology/2021.naacl-main.135\nContextualized Text:\nThis is a list of references (173 items) related to Transformers, spanning a range of applications including natural language processing, image recognition, speech synthesis, and time-series forecasting. The references cover foundational papers like 'Attention is All You Need' (Vaswani et al., 2017) and more recent advancements addressing limitations and exploring variations of the Transformer architecture. They include papers on efficient transformers (e.g., Informer), memory-augmented transformers, deformable transformers, and modifications to attention mechanisms.	{"tags": [], "doc_id": "1c6e3007-2be9-459b-a224-1be2591e3636", "summary": "This is a list of references (173 items) related to Transformers, spanning a range of applications including natural language processing, image recognition, speech synthesis, and time-series forecasting. The references cover foundational papers like 'Attention is All You Need' (Vaswani et al., 2017) and more recent advancements addressing limitations and exploring variations of the Transformer architecture. They include papers on efficient transformers (e.g., Informer), memory-augmented transformers, deformable transformers, and modifications to attention mechanisms.", "doc_type": "text", "entities": [], "keywords": ["transformers", "attention mechanisms", "natural language processing", "image recognition", "speech synthesis", "time-series forecasting", "efficient transformers", "memory-augmented transformers", "deformable transformers", "long sequence modeling", "attention is all you need", "references", "survey", "deep learning", "machine learning"], "key_objects": [], "contextual_text": "This is a list of references (173 items) related to Transformers, spanning a range of applications including natural language processing, image recognition, speech synthesis, and time-series forecasting. The references cover foundational papers like 'Attention is All You Need' (Vaswani et al., 2017) and more recent advancements addressing limitations and exploring variations of the Transformer architecture. They include papers on efficient transformers (e.g., Informer), memory-augmented transformers, deformable transformers, and modifications to attention mechanisms.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some key differences between various Transformer architectures discussed in the references?", "What are the primary limitations of the original Transformer architecture that these references attempt to address?", "How do efficient Transformer models like Informer improve upon the original architecture?", "What applications benefit from deformable Transformers?", "What is the significance of the 'Attention is All You Need' paper?", "What are memory-augmented Transformers and what problems do they solve?"]}
b61b41a5-64e6-46eb-8bf1-a9ccbd224b5f	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0010093342,0.0076828212,-0.021206656,0.027378514,0.016361909,0.038685516,0.0010076126,0.061482474,0.04531828,-0.021810304,0.003209362,0.022616772,0.011172803,-0.020489626,-0.041155405,0.060638618,0.023823494,0.074244276,0.04307261,-0.047093082,0.053587653,0.01588529,0.024426961,-0.047427937,0.0026686953,0.011470712,0.012506105,-0.0152364755,0.008943276,0.02533595,0.029336171,-0.0735173,0.03146992,-0.007172063,0.03788082,-0.0058415662,-0.022918422,0.0051722205,0.016016547,-0.017956179,-0.0095286835,0.057715748,-0.056538913,-0.015073037,-0.01038388,-0.06725012,-0.08846726,-0.027279899,0.046198092,-0.0292304,-0.03684279,0.021643108,-0.0319104,-0.011656862,0.006481828,-4.25887e-05,-0.018128354,-0.03887187,0.031600177,-0.096260674,-0.018480206,0.00565543,-0.024916925,0.009864265,0.018667739,-0.0511034,-0.021687584,0.0059197433,0.0054600183,0.12430934,-0.022592982,0.022176554,-0.025888141,-0.04954452,0.10259725,0.051741265,-0.010922699,-0.00952246,-0.08226194,-0.020674802,-0.027295167,0.063224316,-0.04299491,-0.036147717,0.075446166,0.013875773,-0.036699563,-0.03693047,0.08900205,-0.027106235,0.017917141,-0.024456292,0.01574783,0.06231879,-0.029183911,-0.053994406,0.016228897,-0.043813124,-0.000636011,-0.04908982,-0.006251265,-0.0164295,0.058501873,0.09217829,0.022259988,-0.034328308,-0.025888639,-0.054950967,0.022706296,0.024543235,0.009585152,0.0278975,-0.03937294,-0.006116292,-0.0023431103,-0.033021368,-0.045472685,0.013290297,0.0099809915,0.028193451,0.016996358,-0.04341553,-0.018341579,0.03246997,0.06348446,0.06712955,0.005236545,0.010923939,-0.05433801,0.004316992,0.007743849,0.010911673,-0.016666036,0.01497635,0.025145182,0.013207122,0.078072235,-0.014121328,0.017552204,0.008862067,0.023743391,-0.026115518,-0.02623155,0.01762336,-0.022123469,-0.0014142328,-0.04212381,0.006847187,-0.011148368,0.07092102,0.014181375,0.005658315,0.033654235,0.02132142,0.02004247,-0.00102219,0.0031386998,-0.03624049,-0.007284719,0.011106597,-0.054854058,-0.07568483,-0.017498257,0.03607948,0.033649433,0.13661067,0.013357336,0.0008843512,0.018626042,0.00665718,-0.054387525,0.013564578,-0.0063855825,-0.012056564,0.0075394013,-0.02363864,0.017141648,-0.014904054,-0.03291466,0.005293879,-0.00031958334,0.02246771,-0.015594978,0.048874073,-0.03341437,0.0034379344,0.019616732,0.07451716,-0.018641315,-0.045024484,0.019927284,-0.017351579,-0.011321895,-0.01775005,0.021863742,0.013434787,0.02095066,-0.08515108,-0.03904748,-0.018193869,-0.030225432,0.008217127,0.016793132,-0.011872651,0.028155899,-0.004952678,-0.009613341,-0.021208985,-0.032742254,-0.0052191,-0.010384886,-0.0085557625,-0.0074778567,0.054930154,0.034625478,-0.0018778135,0.0030200072,0.027665993,0.024270235,-0.010855856,-0.056741655,-0.013269889,-0.035219327,0.021968838,-0.025998991,-0.0046853004,-0.05034034,0.04039882,0.0014982638,-0.046341047,0.023409033,-0.0077983537,-0.02349556,0.0340528,-0.0041969903,0.03781306,0.010953656,-0.0022551962,0.001849774,0.03547367,0.042465452,0.08229365,0.022614395,0.07498705,0.0134504475,-0.006588395,0.04241406,0.05077169,-0.009708049,0.022296827,-0.010102895,0.050879642,0.056457296,-0.0028680242,0.0041443054,-0.042726647,0.0143400775,0.023625467,0.04421074,-0.018245514,-0.008324412,-0.009508314,0.012038433,0.022567613,0.027815577,0.0011413803,0.0071289376,0.033788797,0.016675169,0.0060564196,0.038747758,0.06231559,0.010833159,-0.017127268,-0.08306096,0.03394889,0.012304787,0.028327117,0.022002338,-0.0059167556,0.0067027058,-0.036395274,0.06785983,-0.010231106,0.040278737,-0.0102414265,-0.016541032,-0.037298694,-0.07234486,-0.011212858,-0.0026952422,-0.051576257,0.021051813,-0.015482547,-0.0053020907,0.0014691531,0.0022407614,-0.01469958,0.011819015,0.005377035,0.09492536,-0.016962023,0.008996099,-0.022993965,0.015261163,0.016638337,-0.0756755,0.0050540026,0.0378515,0.01343141,0.011725219,-0.015240048,0.014101344,0.017313404,0.016330762,0.096782096,0.067381054,-0.0076332847,-0.0025063436,0.038058348,0.009902248,0.007092948,-0.013654984,0.06673304,0.037531357,-0.021181557,-0.0016803027,-0.06604988,-0.093887195,0.06985973,-0.072778784,0.056943215,0.03553429,0.012650257,0.009196369,-0.004148258,0.027361218,0.018944023,-0.0005003938,0.02871994,-0.012472029,-0.07881238,0.019486848,0.0012311782,0.011629981,0.03339544,0.025968263,0.029948536,0.055918407,0.013909585,0.008994544,0.057984393,0.03922609,0.032570973,-0.0012257189,-0.020098656,0.0067126886,0.024085075,-0.004830883,0.021811375,0.051379085,-0.036064737,0.07817711,-0.04618221,0.027783018,-0.018864116,-0.07212604,-0.032094974,0.059710715,-0.005078456,-0.008541608,-0.041398752,0.03953458,-0.05096878,-0.011321235,-0.009091155,0.024508363,0.03925181,-0.058449615,-0.037141994,-0.01813688,-0.0031918278,-0.040985957,-0.017786747,-0.039682064,0.041332547,-0.0025427863,0.041090887,0.00014462015,0.033494506,-0.034807153,0.023357235,0.017879313,0.015082497,-0.015560849,-0.002377321,0.035495058,0.051837865,0.0020046262,-0.030208355,0.046529233,0.03210774,-0.04982137,0.03176382,0.048246294,0.0014478086,-0.08380646,0.017816894,0.06815987,-0.00074205454,0.03201838,-0.041432183,-0.03092121,0.018972524,-0.02055048,-0.020792915,-0.013085907,0.036254395,-0.06454609,0.0051945043,-0.043909185,0.06865108,0.027120976,0.06814553,0.030745674,-0.03064159,-0.037417147,0.042109355,-0.01108641,-0.053196583,0.013877728,-0.07184603,-0.0113336975,0.0541479,0.018102432,0.008483649,-0.06579939,0.08315337,-0.08548408,0.0067852247,0.061757833,-0.010335564,0.0166769,-0.04575423,-0.014610564,-0.010539086,0.0059136082,0.047643464,-0.0024122824,0.009618438,-0.00017653605,0.028085409,0.022220744,-0.00981605,0.01075238,0.0015081227,-0.021598807,-0.015437576,-0.06520179,-0.013782373,0.0034825893,0.037634756,0.011016752,-0.021816222,0.0004145319,0.005526371,-0.026613,-0.046518352,-0.0052821757,0.010812916,0.0040481384,0.0353751,-0.05705498,0.022182226,-0.015359703,-0.016277459,-0.05712989,0.05806465,0.052897766,0.0055297767,-0.060331665,-0.037946545,0.02227186,0.06195381,-0.05343814,-0.012783715,-0.026280561,-0.0059663253,-0.039585043,0.020682026,0.017877366,0.03830716,0.014962473,0.016481966,0.022846311,-0.046857283,-0.019996822,0.011283257,0.021010533,-0.059678584,-0.01652531,-0.010236689,0.067453176,0.021688856,0.0058749574,-0.011427461,-0.0005690456,0.012858509,0.030709658,0.076948404,0.020156212,-0.0102666,-0.03114699,-0.007669604,-0.038906913,0.03259033,-0.021695063,-0.033837028,0.027476788,0.045832567,0.0033781424,-0.016719976,0.008607945,-0.024664583,-0.0028180291,0.0026067356,0.022718264,0.042734236,0.053011604,-0.06833275,0.054341976,-0.024062281,-0.01087861,0.015928807,-0.04889127,-0.029610496,0.0127779255,-0.06522219,0.01750006,0.022012934,-0.024621809,-0.026540421,-0.006757372,-0.03289161,0.114635035,-0.06724914,0.02341664,-0.030601453,0.0050471295,0.113351144,0.010781442,0.024080798,-0.023111975,-0.0076314863,-0.015872426,0.03156346,0.07179116,0.009497475,-0.008551903,0.06500022,-0.00860454,0.036939126,-0.0033176464,-0.05640275,-0.014098545,-0.028552657,-0.020928873,-0.008166043,-0.0027778812,0.037186306,0.005379115,0.004468163,-0.0076820133,0.015079556,-0.018428447,0.032901198,0.049188364,-0.010879505,0.00014222723,-0.040693548,0.057184167,0.035883218,0.018170405,0.006024314,0.021969577,0.03320927,0.023309402,-0.0151550565,0.027972149,-0.00052151456,-0.02503626,-0.004997276,0.011917413,0.020637892,-0.036428083,-0.012599628,0.026606988,0.04213128,0.042567115,-0.050053958,-0.06345269,-0.012516801,-0.024546912,-0.017634358,0.03476156,0.0051962445,-0.0050151222,0.0065985727,-0.013629155,0.084798075,-0.11036057,-0.030233199,-0.04070914,0.025655601,-0.00466569,-0.006041468,-0.0007189443,-0.011680033,-0.011787535,-0.030689,0.04581275,0.040493768,-0.046969913,0.0056589535,-0.0072616823,-0.035851207,-0.018093642,0.017707163,-0.023317078,-0.005802534,-0.011963551,-0.015041035,0.099318035,-0.040421918,0.0019377448,0.031863935,-0.023915373,-0.06877804,-0.019096255,0.031728376,0.01761427,-0.017233567,-0.016065285,0.033232458,0.013164885,-0.048959114,-0.012634757,-0.0859387,0.048323475,0.0020782081,0.0021558728,0.030029558,0.0043252455,-0.07849251,0.012172135,0.0083422065,-0.047302786,-0.00028907717,0.033407986,0.048180483,0.06781428,0.057858113,-0.020964826,-0.09547988,-0.028836355,-0.044319592,0.005738972,-0.042196054,0.02812156,0.0109979315,-0.022805205,-0.005992917,-0.0077741193,-0.013878216,0.009751531,0.019022403,-0.012509218,-0.02505156,-0.025399059,-0.003494943,0.011114938,0.0006952846,0.001250531,0.06631136,0.038971983,-0.019668125,-0.063357405,-0.033732582,-0.019710401,0.004439424,-0.010912056,0.011786676,0.02020305,-0.029397067,0.008139365,0.06762052,0.0017719435,-0.028976742,-0.012540755,0.055098347,-0.020140382,0.0657755,0.022241974,-0.03451404,-0.09298028,0.0012721444,-0.026929827,-0.051220823,0.049731594,0.01907318,0.0032582125,-0.05856471,-0.011901809,-0.017466832,0.032237485,0.046994023,0.021877673,0.01769063,0.021351663,-0.017477414,-0.050903145,-0.0006823252,-0.027189614,0.0077510765,-0.018289588,0.05109658,0.024903465,0.023641635,-0.01031896,0.023277327,1.0724606e-05,-0.009319931,0.019112622,0.049156982,-0.022841312,0.06497556,-0.011373474,0.0076968777,0.06940245,-0.014492962,0.029607663,-0.012219769,0.025797697,0.004821256,0.021041952,-0.0019985584,-0.03330345,-0.07512605,0.0062860465,0.0088442145,0.0073135374,0.032817695,-0.0047372826,-0.03593775,-0.054729022,-0.035938002,-0.009199234,0.055524103,0.03662953,0.009915409,-0.06261617,0.01775693,-0.018210063,0.045723632,0.031451628,-0.008728081,0.0063391053,-0.017031291]	Keywords: Transformer, Attention, Scaling, Adaptive Computation, Generation, Capsule Networks, Architectures, Machine Learning, Deep Learning\nKey Objects: Switch Transformers, Convolutional Sequence to Sequence Learning, Adaptive Computation Time for Recurrent Neural Networks, Insertion-based Decoding\nRefers to Images: None\nHypothetical Questions:\n- What are some of the recent innovations in Transformer architectures?\n- How are Transformer models being scaled to handle larger datasets?\n- How do different Transformer architectures compare in terms of efficiency and performance?\n- What are some of the limitations of current Transformer models?\n---\nSummary:\nThis is a list of references related to Transformer models and related architectures. It spans a wide range of applications and architectural innovations, including scaling, adaptive computation time, generation order, capsule networks, and more.\nOriginal Text:\n- [36] William Fedus, Barret Zoph, and Noam Shazeer. 2021. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. CoRR abs/2101.03961 (2021). arXiv:2101.03961\n- [37] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional Sequence to Sequence Learning. In Proceedings of ICML . 1243-1252.\n- [38] Alex Graves. 2016. Adaptive Computation Time for Recurrent Neural Networks. CoRR abs/1603.08983 (2016). arXiv:1603.08983\n- [39] Jiaotao Gu, Qi Liu, and Kyunghyu Cho. 2019. Insertion-based Decoding with Automatically Inferred Generation Order. Trans. Assoc. Comput. Linguistics 7 (2019), 661-676. https://transcript.org/os/index.php/tacl/article/view/1732\n- [40] Shuhao Gu and Yang Feng. 2019. Improving Multi-head Attention with Capsule Networks. In Proceedings of NLPCC . 314-326. https://doi.org/10.1007/978-30-32323-5\\_25\nContextualized Text:\nThis list appears to be part of a larger document surveying Transformer models and their advancements.	{"tags": ["MachineLearning", "DeepLearning", "NLP", "TransformerArchitecture", "ReferenceList"], "doc_id": "b61b41a5-64e6-46eb-8bf1-a9ccbd224b5f", "summary": "This is a list of references related to Transformer models and related architectures. It spans a wide range of applications and architectural innovations, including scaling, adaptive computation time, generation order, capsule networks, and more.", "doc_type": "text", "entities": ["William Fedus", "Barret Zoph", "Noam Shazeer", "Jonas Gehring", "Michael Auli", "Denis Yarats", "Yann N. Dauphin", "Alex Graves", "Jiaotao Gu", "Qi Liu", "Kyunghyu Cho"], "keywords": ["Transformer", "Attention", "Scaling", "Adaptive Computation", "Generation", "Capsule Networks", "Architectures", "Machine Learning", "Deep Learning"], "key_objects": ["Switch Transformers", "Convolutional Sequence to Sequence Learning", "Adaptive Computation Time for Recurrent Neural Networks", "Insertion-based Decoding"], "contextual_text": "This list appears to be part of a larger document surveying Transformer models and their advancements.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the recent innovations in Transformer architectures?", "How are Transformer models being scaled to handle larger datasets?", "How do different Transformer architectures compare in terms of efficiency and performance?", "What are some of the limitations of current Transformer models?"]}
e85f931b-4e74-431a-a982-d42fd8bbeb3e	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.019066231,0.035319667,0.004581728,0.013874758,-0.010738576,0.07976761,0.016770013,0.077859275,0.02110823,-0.018602109,-0.004599806,0.026886858,-0.0026803298,-0.02224948,-0.04453206,0.019902762,-0.0029237845,0.07404758,0.028649515,-0.0029706438,0.0731631,-0.007277373,-0.009384534,-0.027491007,-0.018351285,0.027230687,0.027493514,-0.020618683,0.01494006,0.030905448,0.024165912,-0.035370212,0.02314321,0.021893527,0.01733325,0.012136013,0.007967036,0.014090802,0.017925197,-0.01338524,-0.0015570861,0.04183509,-0.07742294,-0.0007508252,0.0035056735,-0.057221506,-0.08948472,-0.031554177,0.033017103,-0.039417803,-0.023550937,0.007754663,-0.029397814,-0.008081908,0.0076402933,-0.031154076,0.004043195,-0.04370755,0.040844917,-0.09080832,-0.048761036,0.038896717,-0.060483385,0.0069572153,0.03633065,-0.06459911,-0.032261807,-0.005302094,0.032465484,0.11542046,-0.017254954,0.021079535,0.0032625059,-0.059925754,0.1235153,0.045080185,-0.0044977916,-0.028720656,-0.06917123,-0.015579651,-0.015157334,0.07820473,-0.06307479,-0.053434316,0.09086828,0.009829514,-0.037322145,-0.019135263,0.08320794,-0.005105515,0.01787787,-0.020529196,-0.006844823,0.057771236,-0.013191541,-0.08242143,-0.007476995,-0.041574936,-0.013794847,-0.045860853,-0.039701458,-0.007543037,0.09019947,0.09415835,0.017014869,-0.010976181,-0.015251256,-0.02792727,-0.0051934714,0.030272061,0.015873574,0.013783145,-0.004753084,0.010568165,-0.043894473,-0.016705386,-0.04029012,0.012931611,-0.008225492,0.00755927,0.005455737,-0.003994114,0.00080655125,0.017535042,0.08474993,0.08096114,0.013717951,-0.0015921909,-0.027580116,-0.009146844,-0.017765233,-0.016197521,-0.021324048,0.029671919,-0.014691726,0.013962885,0.048724316,-0.03812669,0.0029233722,0.028153883,0.0075035524,-0.044564568,-0.040242612,0.016145952,-0.023085907,0.01250097,-0.037978776,-0.014503891,-0.025626054,0.048344813,-0.008262176,-0.016613867,0.02879923,0.010412055,0.00034489844,0.00077067176,0.019235397,-0.06797431,0.0045173634,-0.03125535,-0.05439728,-0.055534508,-0.01203348,0.06254653,0.03843241,0.13061762,-0.017860113,-0.008926663,0.053407684,0.03403912,-0.051142715,0.007848238,-0.040133055,-0.04302455,0.029770404,-0.002508199,0.020787008,0.00688832,-0.022386013,0.00025768194,0.011395896,-0.0049218717,-0.011579251,0.067922056,-0.053355966,0.004387529,0.004577052,0.07546765,-0.013926242,-0.021722458,0.033281926,-0.0106615415,-0.011593755,0.01222185,0.045924313,0.006544966,0.050170366,-0.07893591,-0.084246896,-0.03527504,-0.0006354276,0.018814698,0.04126034,-0.015284467,0.011321497,-0.009705964,-0.016018001,-0.009148373,-0.039655678,-0.009571516,0.016526131,-0.022180961,0.0017859756,0.05932084,0.035864916,0.008019898,-0.023360215,0.034632847,0.016477572,-0.008479893,-0.07705564,0.021690086,-0.011675526,0.0042534517,-0.058928393,0.014132352,-0.039274525,0.042040024,9.277679e-05,-0.025493417,0.04973348,0.01842524,-0.028802216,0.017146908,-0.02399825,0.041094672,0.0007077575,0.0073393,-0.00205036,0.056561377,0.025283618,0.056930583,-0.0170274,0.06181599,0.020487154,-0.0086186705,0.029798696,0.037625644,-0.010386636,0.003048255,-0.003184807,0.03574171,0.047848657,0.018292803,0.018332085,-0.0360682,0.04493736,0.04941776,0.02572509,0.0147057315,-0.021386134,-0.006701088,0.014278985,-0.012684814,0.018455632,0.029528465,0.01981097,0.050345693,0.03116193,-0.009352248,0.034714065,0.04617405,-0.013115741,-0.01160752,-0.054811995,0.0115760295,0.0031197371,0.024528377,0.065426245,-0.032332864,0.011146009,-0.029432166,0.03293823,0.011062264,-0.011767459,-0.04558761,0.0076994775,-0.026372934,-0.07600691,-0.0031141972,0.015468843,-0.02805907,0.014691119,-0.02659939,0.007965782,0.00703116,0.011437355,-0.0062065586,-0.05743605,0.023908608,0.08348202,-0.033948567,0.026610274,0.020436553,-0.022857135,0.0364893,-0.066117376,0.015805574,0.06498612,0.007914978,0.02317379,-0.0063291807,-0.021412732,-0.010769267,0.04666826,0.08387922,0.038170464,-0.038758866,0.011448036,0.006411944,0.026805636,-0.0015191206,0.0012733062,0.023023693,0.03502121,-0.0108320005,0.0033115218,-0.052901268,-0.07855769,0.05242516,-0.07385836,0.027442113,0.05062553,-0.009045561,-0.009147369,0.0044129607,0.07039535,0.02453705,-0.017382924,0.014178602,-0.0047562183,-0.06592544,-0.016384453,-0.026264451,0.008333989,0.038910385,-0.009348825,0.024410479,0.04424415,0.017593067,0.015703207,0.03161489,0.04214692,0.021068536,-0.0068850936,-0.00034319868,-2.0844855e-05,0.018235575,0.02803888,0.03060949,0.06294397,-0.051917076,0.080684885,-0.0066871177,0.016275872,-0.0027318625,-0.08639553,-0.015307639,0.08408548,-0.023443293,-0.014083133,-0.033244915,0.0686651,-0.029776428,-0.035979178,0.01876766,0.03936963,0.020847525,-0.060410585,-0.016441531,-0.026390467,0.034726705,-0.044740178,-0.029158913,-0.0058733504,0.04347628,0.011785674,0.02988377,-0.013488385,0.044955455,-0.028061748,0.0060884776,0.03866598,-0.014189016,-0.013527452,0.009863004,0.016805718,0.02986695,0.0025571203,-0.041625965,0.029872939,0.013204246,-0.032633524,0.027552187,0.033619788,0.031702306,-0.057356548,0.03211347,0.07337781,0.014445662,0.043992862,-0.023494335,-0.041719228,0.019138195,-0.011505641,-0.025123201,-0.012837547,0.011251459,-0.04321856,0.018190818,-0.065426685,0.037579156,0.008107495,0.027771842,0.034753356,-0.022556596,-0.03193253,0.041351296,-0.01986736,-0.028351523,-0.0041354676,-0.10306295,-0.00043632952,0.038790066,0.036583237,0.024735276,-0.06540739,0.07475508,-0.076155834,0.025271477,0.06487302,-0.04094959,0.016679661,-0.015096011,-0.027492926,-0.02876131,0.022668522,0.036042824,-0.008846216,-0.01264505,0.016711682,0.016229548,0.022006156,-0.014236844,-0.0042166,-0.030778283,-0.03742617,-0.0092926705,-0.05543014,0.0016192799,0.00080300367,0.06755365,0.015655134,-0.019073637,0.007425937,-0.021315342,-0.059496783,-0.029449394,-0.028351229,0.0067922296,0.018334001,0.02625179,-0.032186024,0.0024800855,0.007946473,0.0011712933,-0.024324134,0.034368914,0.059993207,-0.019280994,-0.036657844,-0.023440467,0.0027658883,0.06465539,0.009030786,0.0056397337,-0.012501008,-0.021584544,-0.054007445,0.028027592,0.002338099,0.07042681,0.014877929,0.026550129,0.030472336,-0.006497014,-0.011780068,-0.0124745695,0.040313285,-0.047717493,-0.05040742,0.01770306,0.057773847,0.047635984,-0.017661363,0.0070268414,0.028807264,-0.006216456,0.029646637,0.04222751,0.021884572,-0.021817723,-0.0001409408,-0.0134940725,-0.0051711723,0.035008024,-0.012943515,0.0027707273,0.021344015,0.070491485,-0.006361818,-0.032299466,0.0019108396,-0.025118569,-0.0048984233,-0.001586226,0.05938165,0.05009734,0.06595358,-0.022799857,0.05043692,-0.03921662,-0.057628132,0.024420511,-0.06536583,-0.0323325,0.025830379,-0.051944904,-0.015665544,0.013537884,0.0065135187,-0.0086538615,0.03607432,-0.052491874,0.0542167,-0.059956487,0.034126863,-0.008849274,0.02175121,0.118700296,0.016304875,-0.02618025,-0.010627411,-0.021599371,-0.034650173,0.022081356,0.062595986,0.0014353528,-0.0068691135,0.072773606,0.00069250667,0.014793008,-0.008864786,-0.08293851,-0.036402777,-0.06814382,-0.052721485,0.018302042,0.0003466605,0.017341273,-0.006082663,-0.022470228,-0.0014378815,0.02196859,-0.024771092,0.01992652,0.04099945,0.0063157114,0.050214574,-0.027551526,0.05683735,0.049942333,-0.0016220681,-0.024095196,-0.005837698,0.04288848,-0.014963232,-0.03145067,-0.0034979114,-0.0019443666,-0.035917405,-0.04429476,0.047523692,0.012712072,-0.032839183,-0.001756396,0.010081326,0.051844697,0.0013667976,-0.010818273,-0.008064011,-0.017250247,-0.033853713,-0.03270818,0.017179966,-0.00035864086,-0.012872013,-0.0042822156,-0.03216882,0.06509642,-0.109051175,-0.026919926,-0.058188647,0.010616825,-0.023899186,-0.018979033,0.0010401398,-0.015154689,-0.013775067,-0.018017946,0.058552496,0.05348633,-0.025944449,0.0050266874,-0.012647006,0.006808834,-0.036119014,-0.013410681,-0.0047194185,-0.009755018,-0.0005225191,-0.009440135,0.075936265,-0.0045615984,0.02025617,0.035461787,-0.03749073,-0.08518492,-0.020667536,0.0066310605,0.006420523,-0.022274008,0.029870465,0.00795398,0.03878892,-0.038343705,-0.027379949,-0.04804916,0.026108438,-0.0036068435,-0.017851185,0.013252954,-0.012779963,-0.048510417,0.01600463,-0.027560282,-0.040838223,-0.02299774,0.026226273,0.07078651,0.039328095,0.050185267,-0.03734273,-0.09737382,-0.033738185,-0.018801002,-0.014375231,-0.051556878,0.04576251,-0.01615069,-0.008596123,0.0071026366,-0.035977215,0.0014619558,0.00792607,0.018967895,-0.023092927,0.00082054356,-0.035700873,-0.016365087,0.022859953,-0.016634278,6.004467e-06,0.075961486,0.023031043,-0.07048679,-0.045812007,-0.04360095,-0.015102837,-0.0030798102,0.0057092845,0.057358246,0.013701297,0.001117813,-0.008813811,0.06780402,0.012372966,-0.00608337,-0.02474557,0.03094608,0.0072349356,0.032042682,0.0020070167,-0.028884478,-0.112478696,0.0017322763,9.669303e-05,-0.059152845,0.04751225,0.040376652,-0.019026453,-0.045072168,-0.019641241,-0.002470245,0.023020532,0.02478072,0.00032512372,-0.002730361,0.01046654,-0.019027565,-0.03490363,-0.010170075,-0.021319682,0.011012879,-0.031402215,0.014552152,0.011036918,0.010334017,0.019234424,0.028338939,-0.0017252069,-0.027014792,0.021208314,0.04527703,-0.04435528,0.055581518,0.0013829254,0.005541781,0.04499158,0.014376735,0.021118348,0.014142036,0.000667846,0.01969931,0.00607475,0.00060508493,-0.022883639,-0.069919124,0.0106181055,0.023344887,0.04977048,0.03336883,0.00022249769,-0.039404053,-0.047544327,-0.030911705,-0.01999564,0.05833808,0.03342167,0.035654947,-0.04703733,0.020578282,-0.042979963,0.053871553,0.04872471,-0.02430283,0.0012471173,-0.0047085024]	Keywords: Transformers, Attention Mechanisms, Natural Language Processing, Speech Recognition, Architecture, Efficiency, Scalability, References, Survey, Deep Learning, Machine Learning, Convolutional Neural Networks, Capsule Networks, Speech Synthesis, Object Detection, Time-Series Forecasting, Deep Learning, Language Models, Self-Attention, Architectural Innovations, Optimization, Computational Efficiency, Sequence Modeling, Transformer Architecture, Neural Networks, Long Sequence Processing, Language Inference, Context Modeling, Sequence-to-Sequence Learning, Speech Understanding, Audio Processing, Computer Vision, Neural Machine Translation, Encoder-Decoder Models, Document Summarization, Representation Learning, Sparse Models, Memory Efficiency, Dynamic Architectures, Early Exit Strategies, Gaussian Processes, Recurrent Neural Networks, Hybrid Architectures, Gradient Descent, Optimization Techniques, Model Compression, Edge Computing, Embedded Systems, Real-Time Applications, Edge Intelligence, Federated Learning, Distributed Computing, Big Data, Data Parallelism, Model Parallelism, Transfer Learning, Domain Adaptation, Few-Shot Learning, Zero-Shot Learning, Reinforcement Learning, Imitation Learning, Generative Adversarial Networks, Variational Autoencoders, Autoencoders, Latent Variables, Feature Extraction, Dimensionality Reduction, Clustering, Classification, Regression, Decision Trees, Support Vector Machines, K-Means, Principal Component Analysis, Factor Analysis, Canonical Correlation Analysis, Multidimensional Scaling, Non-Negative Matrix Factorization, Singular Value Decomposition, Eigenvalue Decomposition, Latent Dirichlet Allocation, Topic Modeling, Word Embeddings, Semantic Similarity, Knowledge Graph, Reasoning, Symbolic AI, Explainable AI, Fairness, Bias Detection, Mitigation, Privacy-Preserving Machine Learning, Differential Privacy, Homomorphic Encryption, Secure Multi-Party Computation, Blockchain Technology, Internet of Things, Robotics, Autonomous Vehicles, Healthcare, Finance, Education, Entertainment, Cybersecurity, Social Media, Scientific Discovery, Innovation, Future Trends, Grand Challenges\nKey Objects: References, Transformer Models, Deep Learning Architectures, Natural Language Processing Techniques, Machine Learning Algorithms, Computational Methods, Mathematical Frameworks, Statistical Models, Engineering Solutions, Technological Innovations\nRefers to Images: None\nHypothetical Questions:\n- What are the common themes and areas of research explored in these references?\n- Can you identify any specific architectural modifications or optimization techniques that are frequently mentioned?\n- What are the key limitations or challenges that are being addressed in the Transformer research?\n- What are the most promising future directions for Transformer research?\n- How do these references contribute to the broader understanding of deep learning and artificial intelligence?\n---\nSummary:\nThis document appears to be a list of references for a survey or paper on Transformers. The references span a wide range of topics related to Transformers, including improvements to attention mechanisms, architectural modifications, applications to speech recognition and natural language processing, and explorations of efficiency and scalability. The list includes a diverse set of authors and publications, primarily from conferences and online repositories like ACL, AAAI, NeurIPS, and arXiv.\nOriginal Text:\n- [40] Shuhao Gu and Yang Feng. 2019. Improving Multi-head Attention with Capsule Networks. In Proceedings of NLPCC . 314-326. https://doi.org/10.1007/978-30-32323-5\\_25\n- [41] Anmol Gulati, James Qin, Chung-Cheng Chi, Niki Parmar, Yurang, Ziahju Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. In Proceedings of Interspeech . 5036-5040. https://doi.org/10.21437/Interspeech.2020-3015\n- [42] Maosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaussian Transformer: A Lightweight Approach for Natural Language Inference. In Proceedings of AAAI . 6489-6496. https://doi.org/10.1609/aaa.v3iii.031306489\n- [43] Qipeng Guo, Xipeng Qiu, Peignei Lu, Yinfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-Transformer. In Proceedings of HLT-NAACL . 1135-1235. https://www.aclweb.org/anthology/N19-1133\nContextualized Text:\nThis document is a meticulously compiled list of references focusing on the Transformer model, a foundational architecture in modern deep learning.  The diversity of listed works demonstrates the breadth of ongoing research surrounding Transformers, encompassing improvements to their core mechanisms, their adaptation to various tasks, and the exploration of their limitations and future directions. The references cover techniques ranging from architectural modifications (e.g., capsule networks, star transformers) to optimization strategies and the integration of Transformers into diverse applications.  The inclusion of references from prominent conferences (ACL, AAAI, NeurIPS) and online repositories (arXiv) underscores the significance of this work within the research community.	{"tags": ["deep-learning", "transformers", "nlp", "architecture", "reference-list", "computing", "artificial-intelligence", "machine-learning", "research", "publications", "survey", "technology", "innovation"], "doc_id": "e85f931b-4e74-431a-a982-d42fd8bbeb3e", "summary": "This document appears to be a list of references for a survey or paper on Transformers. The references span a wide range of topics related to Transformers, including improvements to attention mechanisms, architectural modifications, applications to speech recognition and natural language processing, and explorations of efficiency and scalability. The list includes a diverse set of authors and publications, primarily from conferences and online repositories like ACL, AAAI, NeurIPS, and arXiv.", "doc_type": "text", "entities": ["ACL", "AAAI", "NeurIPS", "arXiv", "BERT", "Capsule Networks", "Conformer", "Gaussian Transformer", "Star-Transformer", "NLPCC", "Interspeech", "HHLT-NAACL", "Google", "Microsoft", "Facebook", "Amazon", "Apple", "IBM", "Oracle", "Intel", "NVIDIA", "AMD", "Tesla", "Qualcomm", "Huawei", "Samsung", "Sony", "LG", "Panasonic", "Sharp", "Canon", "Fujifilm", "Toshiba", "Hitachi", "Mitsubishi", "Toyota", "Honda", "Nissan", "Ford", "General Motors", "Volkswagen", "BMW", "Mercedes-Benz", "Audi", "Porsche", "Ferrari", "Lamborghini", "Rolls-Royce", "Bentley", "Maserati", "Bugatti", "Koenigsegg", "Pagani", "McLaren", "Aston Martin", "Lotus", "Alpine", "Polestar", "Rivian", "Lucid", "VinFast", "BYD", "Geely", "Changan", "Great Wall", "SAIC", "Chery", "Dongfeng", "GAC", "Haval", "Zotye", "Roewe", "MG", "Wuling", "JAC", "BAIC", "DFL", "FAW", "Dongfeng Motor Corporation", "Shanghai Automotive Industry Corporation", "China Automotive Technology Development Center", "China Association of Automobile Manufacturers", "Society of Automotive Engineers of China", "China National Standards Administration", "Ministry of Commerce of China", "National Development and Reform Commission of China", "Ministry of Industry and Information Technology of China", "Ministry of Science and Technology of China"], "keywords": ["Transformers", "Attention Mechanisms", "Natural Language Processing", "Speech Recognition", "Architecture", "Efficiency", "Scalability", "References", "Survey", "Deep Learning", "Machine Learning", "Convolutional Neural Networks", "Capsule Networks", "Speech Synthesis", "Object Detection", "Time-Series Forecasting", "Deep Learning", "Language Models", "Self-Attention", "Architectural Innovations", "Optimization", "Computational Efficiency", "Sequence Modeling", "Transformer Architecture", "Neural Networks", "Long Sequence Processing", "Language Inference", "Context Modeling", "Sequence-to-Sequence Learning", "Speech Understanding", "Audio Processing", "Computer Vision", "Neural Machine Translation", "Encoder-Decoder Models", "Document Summarization", "Representation Learning", "Sparse Models", "Memory Efficiency", "Dynamic Architectures", "Early Exit Strategies", "Gaussian Processes", "Recurrent Neural Networks", "Hybrid Architectures", "Gradient Descent", "Optimization Techniques", "Model Compression", "Edge Computing", "Embedded Systems", "Real-Time Applications", "Edge Intelligence", "Federated Learning", "Distributed Computing", "Big Data", "Data Parallelism", "Model Parallelism", "Transfer Learning", "Domain Adaptation", "Few-Shot Learning", "Zero-Shot Learning", "Reinforcement Learning", "Imitation Learning", "Generative Adversarial Networks", "Variational Autoencoders", "Autoencoders", "Latent Variables", "Feature Extraction", "Dimensionality Reduction", "Clustering", "Classification", "Regression", "Decision Trees", "Support Vector Machines", "K-Means", "Principal Component Analysis", "Factor Analysis", "Canonical Correlation Analysis", "Multidimensional Scaling", "Non-Negative Matrix Factorization", "Singular Value Decomposition", "Eigenvalue Decomposition", "Latent Dirichlet Allocation", "Topic Modeling", "Word Embeddings", "Semantic Similarity", "Knowledge Graph", "Reasoning", "Symbolic AI", "Explainable AI", "Fairness", "Bias Detection", "Mitigation", "Privacy-Preserving Machine Learning", "Differential Privacy", "Homomorphic Encryption", "Secure Multi-Party Computation", "Blockchain Technology", "Internet of Things", "Robotics", "Autonomous Vehicles", "Healthcare", "Finance", "Education", "Entertainment", "Cybersecurity", "Social Media", "Scientific Discovery", "Innovation", "Future Trends", "Grand Challenges"], "key_objects": ["References", "Transformer Models", "Deep Learning Architectures", "Natural Language Processing Techniques", "Machine Learning Algorithms", "Computational Methods", "Mathematical Frameworks", "Statistical Models", "Engineering Solutions", "Technological Innovations"], "contextual_text": "This document is a meticulously compiled list of references focusing on the Transformer model, a foundational architecture in modern deep learning.  The diversity of listed works demonstrates the breadth of ongoing research surrounding Transformers, encompassing improvements to their core mechanisms, their adaptation to various tasks, and the exploration of their limitations and future directions. The references cover techniques ranging from architectural modifications (e.g., capsule networks, star transformers) to optimization strategies and the integration of Transformers into diverse applications.  The inclusion of references from prominent conferences (ACL, AAAI, NeurIPS) and online repositories (arXiv) underscores the significance of this work within the research community.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the common themes and areas of research explored in these references?", "Can you identify any specific architectural modifications or optimization techniques that are frequently mentioned?", "What are the key limitations or challenges that are being addressed in the Transformer research?", "What are the most promising future directions for Transformer research?", "How do these references contribute to the broader understanding of deep learning and artificial intelligence?"]}
c8d5d438-efbd-4034-b215-636dadb87128	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.015701964,0.0018542198,-0.0123061845,0.022562837,0.017074797,0.08294595,0.020374276,0.074143276,0.03847521,-0.031203585,0.017749574,0.025430163,0.00088194903,-0.0078037535,-0.016647313,0.018433418,0.020422209,0.083516724,0.032286286,-0.0356257,0.03735762,-0.0013309835,0.0022836637,-0.04242537,-0.013963674,-0.0018356838,0.0141070625,-0.026143348,0.0046730973,0.023988008,0.019228097,-0.041215926,0.03563597,0.0015079002,0.016851509,-0.0028980635,0.010616954,0.004827877,-0.0023753499,-0.0063342587,-0.027421907,0.0487806,-0.059726708,0.0041333614,-0.02571142,-0.054754432,-0.062933475,-0.02602999,0.020808786,-0.017419212,-0.017517624,0.010444277,-0.0132178,0.014914542,0.02574564,-0.022918938,0.001502153,-0.058522392,0.021385841,-0.09425821,-0.04132805,0.0026003662,-0.06526922,0.017954083,0.043342773,-0.059748344,-0.012101932,-0.02126597,0.008500683,0.124385454,-0.020907851,0.02408139,0.0069900607,-0.052212544,0.114706755,0.0348773,-0.030889781,-0.038841944,-0.05617582,-0.013396489,-0.017446006,0.07862754,-0.052456945,-0.048265114,0.093349524,0.02138853,-0.046401136,-0.004702186,0.07153805,-0.023942633,0.024647895,-0.0137848165,0.003312434,0.06509826,-0.024178602,-0.07283889,0.011938779,-0.029467212,0.012107723,-0.051688667,-0.020584587,0.00023143845,0.08438099,0.0723269,0.019488592,-0.029750952,-0.025010752,-0.039831534,0.00092664745,0.025897138,0.018115345,0.028299402,-0.0041676406,0.0007129523,-0.023673486,-0.0396917,-0.053500235,0.03161817,0.019019065,-0.013705576,0.0065077753,-0.02397095,-0.00056568766,0.04315218,0.083790064,0.077881455,0.021702383,0.020900715,-0.037202027,-0.0061112554,-0.007536254,0.0005287081,0.0067942687,0.011452888,-0.0063438625,0.021950664,0.07408034,-0.03223242,0.02502374,0.019995458,0.0062723937,-0.038577538,-0.04005039,0.033330634,-0.013152187,0.00055867044,-0.018791612,0.002793354,-0.0038629123,0.06908312,0.02046418,-0.017030468,0.016396938,-0.0058422918,-0.0075554075,0.013711882,0.008542717,-0.06049617,0.007870653,-0.020509113,-0.02922986,-0.08644807,-0.038243428,0.058574203,0.051104084,0.14690895,-0.014969524,0.015292557,0.036340684,-0.0021770354,-0.05273841,0.00236284,0.0015575191,-0.031508446,0.021783022,-0.02256207,0.005950714,-0.0015681002,-0.0497394,-0.02782815,0.02248424,0.014233204,-0.020396475,0.07006084,-0.054404322,0.017588817,0.034047432,0.08763398,-0.025301447,-0.028095782,0.046125617,-0.0037285003,-0.012208478,-0.0054220194,0.02999891,0.0072561223,0.050912663,-0.07832742,-0.0431475,-0.025913816,-0.01736073,0.007929473,0.028533913,-0.014850605,0.015802862,-0.030669792,-0.017631385,-0.029306097,-0.058172755,-0.027172914,-0.013559903,-0.03211967,-0.023192765,0.07957128,0.016628085,0.01046671,-0.03389999,0.012503155,0.04096836,0.00038431387,-0.06655949,0.00072678475,-0.015230223,0.028071618,-0.05983377,0.024873052,-0.041654136,0.044028025,0.0087683275,-0.032392085,0.033107765,0.017672278,-0.03674707,0.045140564,-0.009216251,0.031792395,-0.003826584,-0.012717152,-0.009641638,0.05443724,0.025745584,0.0753342,0.0036863915,0.063691355,0.0049043284,-0.011481906,0.0063145515,0.040489685,0.01446683,0.022134759,-0.009149603,0.03775679,0.061512064,0.0062207817,-0.010158728,-0.05187763,0.03661861,0.06466181,0.04796913,-0.023091422,0.014034746,-0.013919214,0.0054014875,0.0029495175,0.013879358,-0.001085063,0.008744371,0.029291894,0.031543113,0.012194014,0.053287655,0.04894133,-0.016061164,-0.028363053,-0.06296766,0.021513496,-0.0033084552,0.01944167,0.04092305,-0.02626602,-0.0032988698,-0.032696247,0.045658875,0.011058097,0.015424167,-0.018130084,0.0050264983,-0.029851677,-0.04297123,0.0021824827,0.02497241,-0.03650518,0.04388238,-0.045678493,0.01713511,0.00902001,0.005328333,-0.03021054,-0.014217987,0.007856668,0.08208678,-0.050046824,0.012602942,0.014561541,0.006512114,0.023240833,-0.06350393,-0.020736262,0.04396374,-0.0017129352,0.02793145,-0.027783936,-0.011610285,-0.0019912163,0.041746806,0.08907345,0.05251626,-0.038933042,-0.014666502,-0.0019484734,0.015783384,0.010133673,-0.004978605,0.053361572,0.04737489,-0.009320671,0.009921164,-0.057249337,-0.09196741,0.05382536,-0.07701527,0.04122979,0.049461696,0.013742323,-0.0045183846,-0.008742915,0.031519882,0.021120032,0.008237005,0.030830825,-0.019040419,-0.0637911,-0.014303623,-0.027343778,-0.0050251624,0.043880917,0.029794957,0.030218882,0.038216956,0.0075827185,0.01116406,0.059170358,0.019450923,0.035548363,9.8369084e-05,-0.010628353,-0.00484047,0.003475858,0.0006208472,0.010046471,0.0487671,-0.037266567,0.07800089,-0.008993348,-0.016801892,-0.0031256864,-0.08068778,-0.039800625,0.06647416,-0.014930063,-0.0074076056,-0.047201075,0.06760526,-0.026427373,-0.015085876,0.030364644,0.02373244,0.020040667,-0.058442216,-0.0047575315,-0.02113807,0.02230241,-0.03361278,-0.04393558,-0.035339482,0.0520919,0.014895199,0.03703438,-0.027680898,0.037909277,-0.037994247,0.015624895,0.039351698,0.0069573084,0.0023142558,0.0009560549,0.029243577,0.034770455,0.0046390328,-0.028717017,0.044384684,0.0063657355,-0.03868777,0.039023478,0.037435297,0.016613074,-0.09483908,0.02808365,0.07952848,0.031156898,0.015134524,-0.03410108,-0.037945,0.01864436,0.00038958451,-0.020836547,-0.017959733,0.03133275,-0.06278435,0.0168087,-0.05110244,0.06377024,0.0042968304,0.058097698,0.023670841,-0.037961252,-0.030095791,0.04349948,-0.0287643,-0.048657853,0.027317883,-0.07226162,-0.010356789,0.06681615,0.04756866,0.03329253,-0.07569518,0.06810912,-0.09166382,0.016227407,0.06422654,-0.020212522,0.008790292,-0.047613494,-0.00054536434,-0.028287254,0.015892506,0.03260074,-0.0014713554,-0.034020133,0.007615297,0.028544737,0.023526087,-0.01904685,-0.0015405222,-0.03125201,-0.028419018,-0.004641907,-0.04647743,0.0014856309,-0.0057137394,0.03882815,0.010235115,-0.00986401,0.0008961846,-0.030541988,-0.048166517,-0.04147298,-0.017300978,0.007102268,9.792065e-05,0.015824987,-0.046765123,-0.00078800984,-0.0054821977,-0.038648963,-0.04330635,0.051685646,0.060926553,-0.001505797,-0.060222212,-0.024923423,0.022372691,0.056785487,-0.0028698642,-0.022540277,-0.045472052,-0.011338914,-0.03975683,0.028981801,0.01494929,0.059530795,0.028854702,0.026734352,0.018652495,-0.03741515,-0.00524219,-0.006290963,0.011924471,-0.060206152,-0.024423137,0.0042927177,0.050923526,0.037046544,-0.0044580204,-0.016088245,0.029245198,-0.010236989,0.045608923,0.031514157,0.029426826,-0.004492835,-0.022606427,-0.0053482065,-0.037834264,0.021993779,-0.027178137,-0.009590517,0.032341097,0.07197954,-0.013768611,-0.042331245,-0.0005178878,-0.027751096,0.014575004,-0.017278824,0.0422505,0.056210864,0.05759599,-0.042662937,0.06457511,-0.03795108,-0.029143339,0.027837174,-0.061620433,-0.021358186,0.01076425,-0.04785846,0.011887792,0.0256707,-0.010023465,-0.017384982,0.015060908,-0.020278715,0.07940344,-0.052314866,0.0015613856,-0.016509129,0.0030891127,0.11782609,0.01700204,0.0033768956,-0.0097293435,-0.02710056,-0.044967804,0.03638712,0.06364345,0.00625837,-0.013307757,0.07940638,-0.010359308,0.04647347,0.0068136714,-0.04793803,-0.025234325,-0.020915216,-0.029314138,0.013885821,-0.018993288,0.008019767,0.015738666,-0.011608807,0.014528124,0.008575463,-0.0008843207,0.03733653,0.03226771,-0.012230557,0.019599514,-0.018973066,0.03229387,0.02079927,0.041306045,-0.018571977,-0.0054570646,0.037702825,0.005875848,-0.024504628,0.015608611,-0.014480177,-0.025120227,-0.0074866386,0.029631572,0.034989044,-0.0012579737,-0.009565155,0.008853883,0.031788647,0.01271224,-0.011947092,-0.02988422,-0.023831308,-0.02299469,-0.033969913,0.009905417,-0.0028422626,-0.01271082,-0.002918464,-0.009720661,0.10570291,-0.08622679,-0.06281068,-0.06391659,0.020400409,0.006987557,-0.01676729,0.016465787,-0.008516576,-0.0040835096,-0.023475112,0.08119443,0.04422601,-0.026854351,0.019076822,0.0036294619,-0.004355716,-0.013385704,-0.001825678,-0.006295301,-0.024209965,-0.0038424141,-0.0030234607,0.08031943,-0.023297967,0.029630743,0.038557153,-0.03159248,-0.08038797,-0.01537449,0.022751959,0.015322773,-0.01995588,-0.010892494,0.0069209114,0.023551978,-0.0477164,-0.019589566,-0.062655844,0.05264403,0.0055963215,-0.019131515,0.019441877,-0.0014066548,-0.05040202,0.01306025,-0.01902338,-0.037298962,0.013279073,0.018124377,0.041680843,0.061598126,0.06316404,-0.038925927,-0.092233546,-0.021667596,-0.056413867,-0.01689347,-0.03980342,0.056880604,-0.008579793,-0.008154718,-0.008399438,-0.0130607765,0.016257832,0.017142339,0.023934096,-0.011205965,-0.019001933,-0.04379398,-0.024789335,0.011467863,-0.0025634402,0.0029209044,0.060908746,0.042386778,-0.032670192,-0.04295379,-0.052385643,-0.024984216,0.006930504,-0.007254392,0.012658161,0.020199642,-0.00017833717,0.011892391,0.06439242,0.0163957,-0.0075585395,-0.0075210053,0.041846327,0.001759816,0.034574777,0.012994383,-0.026359895,-0.10488831,0.009826361,-0.0054340935,-0.050312076,0.03869988,0.014470922,-0.01573535,-0.03931598,-0.021978905,-0.016806575,0.018983876,0.046034485,0.024649534,-0.0054127946,-0.0014947933,-0.020980831,-0.038521558,-0.021483943,-0.023576688,0.014770966,-0.029884623,0.030341797,0.027296405,0.009109094,-0.012003628,0.031494863,0.007825137,-0.033319123,0.028114216,0.046981636,-0.0254649,0.044783987,-0.021803832,0.00041873785,0.027927006,0.0073333583,0.043708023,0.018371528,0.024334857,0.0010359949,0.026343722,-0.0033608172,-0.052644115,-0.10007525,0.03219932,0.009931054,0.018168623,0.024190577,-0.009941837,-0.027870072,-0.045778163,-0.029726235,-0.008325138,0.04380778,0.029503139,0.028267635,-0.050934497,-0.0044067064,-0.03759919,0.054468617,0.028477686,-0.0069197887,0.003927853,-0.026398923]	Keywords: Transformer, Attention, Self-Attention, Text Classification, Speech Synthesis, Object Detection, Efficiency, Scalability, Machine Learning, Natural Language Processing, Computer Vision, Deep Learning, Architecture, Model, Reference, Survey, Deep Learning Model, Architecture Modification, Model Performance, Performance Optimization, Sequence Modeling, Text Analysis, Language Processing, Computer Graphics, Image Recognition, Audio Processing, Speech Recognition, AI, Artificial Intelligence, Model Innovation, Reference List, Deep Neural Network, Neural Network Architecture, Computer-Aided Design, Image Processing, Language Model, NLP, Neural Network, Model Training\nKey Objects: Transformer Model, Self-Attention Mechanism, Text Classification Algorithm, Speech Synthesis System, Object Detection Framework, Neural Network Model, Deep Learning Architecture, Language Model, Model Training, Model Innovation, Model Performance, Architecture Modification, Sequence Modeling\nRefers to Images: None\nHypothetical Questions:\n- What are the common modifications to the Transformer architecture?\n- How are Transformer models applied across different domains (NLP, Computer Vision, Audio Processing)?\n- What are the key drivers behind research into Transformer models?\n- How has the Transformer architecture evolved since its initial development?\n---\nSummary:\nThis is a list of references related to Transformer models and their applications. The list includes numerous papers detailing various modifications and improvements to the original Transformer architecture, focusing on efficiency, scalability, and performance across different tasks like text classification, speech synthesis, and object detection.  The references are presented in numerical order.\nOriginal Text:\n- [43] Qipeng Guo, Xipeng Qiu, Peignei Lu, Yinfan Shao, Xiangyang Xue, and Zheng Zhang. 2019. Star-Transformer. In Proceedings of HLT-NAACL . 1135-1235. https://www.aclweb.org/anthology/N19-1133\n- [44] Qipeng Guo, Xipeng Qiu, Peignei Lu, Xiangyang Xue, and Zheng Zhang. 2020. Multi-Scale Self-Attention for Text Classification. In Proceedings of AAAI . 7487-7543. https://aaai.org/os/index.php/AAAI/article/view/6290\n- [45] Qipeng Guo, Xipeng Qiu, Xiangyang Xue, and Zheng Zhang. 2019. Low-Rank and Locality Constrained SelfAttention for Sequence Modeling. IEEE/ACM Trans. Audio, Speech and Lang. Proc. 27, 12 (2019), 213-2222. https: //doi.org/10.1109/TASLP.2019.2944078\n- [46] Chi Han, Mingxuan Wang, Hejong Ji, and Lei Li. 2021. Learning Shared Semantic Space for Speech-to-Text Translation. arXiv:2105.03095 (cs.CL)\nContextualized Text:\nThis list details research and developments relating to Transformer architectures, focusing on improvements to efficiency, scalability and task-specific performance across NLP, computer vision, and audio processing tasks. Numerous modifications and applications are highlighted, demonstrating the widespread adoption and evolution of the original Transformer model.	{"tags": ["References", "Deep Learning", "NLP", "Computer Vision", "Transformers", "Architecture", "List", "Research", "Model"], "doc_id": "c8d5d438-efbd-4034-b215-636dadb87128", "summary": "This is a list of references related to Transformer models and their applications. The list includes numerous papers detailing various modifications and improvements to the original Transformer architecture, focusing on efficiency, scalability, and performance across different tasks like text classification, speech synthesis, and object detection.  The references are presented in numerical order.", "doc_type": "text", "entities": ["Transformer", "Self-Attention", "Text Classification", "Speech Synthesis", "Object Detection", "NLP", "Deep Learning", "Sequence Modeling", "AI", "Computer Vision", "Speech Recognition", "Image Processing", "Text Analysis", "Language Processing", "Computer Graphics", "Language Model", "Neural Network", "Architecture"], "keywords": ["Transformer", "Attention", "Self-Attention", "Text Classification", "Speech Synthesis", "Object Detection", "Efficiency", "Scalability", "Machine Learning", "Natural Language Processing", "Computer Vision", "Deep Learning", "Architecture", "Model", "Reference", "Survey", "Deep Learning Model", "Architecture Modification", "Model Performance", "Performance Optimization", "Sequence Modeling", "Text Analysis", "Language Processing", "Computer Graphics", "Image Recognition", "Audio Processing", "Speech Recognition", "AI", "Artificial Intelligence", "Model Innovation", "Reference List", "Deep Neural Network", "Neural Network Architecture", "Computer-Aided Design", "Image Processing", "Language Model", "NLP", "Neural Network", "Model Training"], "key_objects": ["Transformer Model", "Self-Attention Mechanism", "Text Classification Algorithm", "Speech Synthesis System", "Object Detection Framework", "Neural Network Model", "Deep Learning Architecture", "Language Model", "Model Training", "Model Innovation", "Model Performance", "Architecture Modification", "Sequence Modeling"], "contextual_text": "This list details research and developments relating to Transformer architectures, focusing on improvements to efficiency, scalability and task-specific performance across NLP, computer vision, and audio processing tasks. Numerous modifications and applications are highlighted, demonstrating the widespread adoption and evolution of the original Transformer model.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the common modifications to the Transformer architecture?", "How are Transformer models applied across different domains (NLP, Computer Vision, Audio Processing)?", "What are the key drivers behind research into Transformer models?", "How has the Transformer architecture evolved since its initial development?"]}
92e62920-984e-4848-bf27-4ffa6306d46a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02200586,0.022130605,-0.020870697,0.00041425045,-0.009773462,0.061501313,-0.009549063,0.045300346,0.072821446,-0.026627507,0.001891426,0.023486298,0.021802561,0.005215031,-0.030910015,0.03304216,0.03694392,0.04364354,0.031971555,-0.022496775,0.037778694,0.047127955,-0.012239779,-0.034048583,-0.018874953,0.017788522,0.017305642,-0.057198208,0.0418174,0.03826784,-0.0057287933,-0.062206797,-0.0020418202,-0.0309358,0.0002564922,0.027109222,0.006379528,0.039723206,0.03358921,-0.01651611,-0.010522996,0.04773427,-0.07142196,0.008100006,0.016762959,-0.042696018,-0.028633447,-0.04522387,0.017192708,-0.021172982,-0.020331072,0.026425112,-0.023981463,-0.0045482446,-0.0052594966,-0.036292166,-0.010372997,-0.03327926,0.045761026,-0.09681513,-0.039793316,0.04372576,-0.024475947,-0.029143658,0.028052498,-0.06654544,-0.024749067,0.011290257,0.0064080977,0.12768093,0.00025255757,0.03281215,-0.016542211,-0.04901925,0.10144506,0.036597185,-0.015403572,-0.007013839,-0.03789905,-0.018449869,-0.008617255,0.05712904,-0.03715669,-0.038064156,0.07009349,0.010354127,-0.046781227,-0.030700937,0.12157191,-0.00912104,0.029025152,-0.025611473,0.00832738,0.08430235,-0.011421982,-0.06670602,0.03208372,-0.047640193,-0.010882396,-0.054339543,-0.0071155303,-0.020595327,0.04149647,0.0720131,0.02807662,-0.02300777,-0.048759095,-0.051580552,-0.006411781,0.009666137,-0.027921215,-0.007317693,-0.009482,0.035568416,0.00016747174,-0.01618283,-0.06544923,-0.0012449287,0.0012021207,0.00092313933,0.005110228,-0.022620916,0.010580251,0.02486528,0.036394134,0.06903968,-0.012146949,0.0028596874,-0.032891463,-0.026885238,-0.008394071,0.023225386,0.000109583874,0.029941909,0.039431717,0.0018237694,0.07986441,-0.04511911,0.02109535,-0.005752914,0.04299535,0.0037660003,-0.013752559,0.02875553,-0.042756896,0.03048572,-0.047707126,0.01348482,-0.022793943,0.035880547,0.015090542,0.023177018,0.04260771,0.009775799,0.024377948,0.011275681,0.011459036,-0.05387777,0.003538537,0.015832938,-0.03565444,-0.072549686,-0.054255694,0.036503013,-0.0060514095,0.13401243,0.00983554,-0.031758677,0.035929266,0.0015069994,-0.04858119,0.012681987,-0.005230639,-0.035595186,0.03789006,-0.019352555,0.037940346,0.0037967763,-0.027313912,-0.0007190625,0.029352536,-0.008288787,-0.017448364,0.07120385,-0.0035950216,0.026762549,-0.015262983,0.06781285,-0.024406351,-0.032171927,0.027661487,-0.030120613,-0.027728057,0.026039826,0.010406886,0.0044958685,0.020246737,-0.060706634,-0.042012926,-0.006101517,-0.040871013,0.010962365,0.018977748,0.013518379,0.025822205,-0.02754461,-0.04398547,-0.0039225128,-0.023638824,-0.048038535,0.008691505,-0.018382143,0.0027955142,0.05134563,0.035995312,-0.018619083,-0.013015867,0.027189134,0.012712446,-0.009930743,-0.07096445,0.0071845027,-0.034657657,0.019080704,-0.038413823,-0.014655425,-0.050282937,0.036923323,0.016649688,-0.047093786,0.052575145,-0.012100674,-0.0347108,-0.0066084564,-0.02645206,0.02716218,0.007608972,0.010532641,0.030463325,0.04728718,0.042211324,0.047367822,0.01804894,0.05493819,0.011703576,-0.0038857588,0.029876698,0.05722064,-0.03227463,0.01952271,0.00034224853,0.06359746,0.03218507,0.031023053,-0.0032685164,-0.01888733,0.038138416,0.028394762,0.038482897,0.03201493,-0.0068171076,-0.017794592,0.030616548,-0.009797642,0.013953613,0.03221856,0.0028877228,0.049330294,0.0127423825,-0.021529483,0.01595268,0.032093298,0.011432003,-0.030707033,-0.10378589,-0.004903167,0.011371462,0.032155428,0.013836006,-0.0014262715,-0.0006718734,-0.04366803,0.037638936,-0.012639401,0.028865593,-0.022187416,-0.023986299,-0.0020085224,-0.063240856,-0.019254282,-0.01602233,-0.026610151,0.015858116,-0.020295236,0.03438687,0.0034450009,0.008463911,-0.028171254,-0.0013541307,0.002844599,0.101117164,-0.017569168,0.018215746,-0.025611788,-0.003521844,0.021478126,-0.060005087,0.0053607957,0.057538766,-0.00031038543,0.003481253,-0.029894145,-0.009803395,-0.0010892613,0.052827172,0.10028308,0.061679658,-0.012184466,-0.012220941,0.04306323,0.00055254105,0.023534909,0.015319023,0.04961539,0.0456198,-0.0077159093,-0.0028980605,-0.03675403,-0.085828654,0.040582046,-0.05656504,0.037128143,0.037193727,0.0053249323,0.0040656095,-0.005458369,0.039103836,0.022288738,0.015480556,0.011708192,0.023828795,-0.08117004,-0.0049284864,0.023929456,0.012504904,0.02348364,0.011953117,0.044734035,0.029133383,0.0021758652,-0.0013288928,0.031740922,0.04015138,0.016659554,-0.017972352,-0.022598455,0.00457429,0.01244751,-0.017029425,0.027371306,0.036880657,-0.034772374,0.07953629,-0.029858483,0.011890125,-0.0006998425,-0.06742947,-0.053963948,0.03154877,-0.011357016,-0.01592622,-0.027409535,0.054418415,-0.045412164,-0.061011977,-0.01215434,0.05181745,0.03275292,-0.05607591,-0.04221265,-0.025222072,0.019749943,-0.0519733,-0.012965315,-0.030214177,0.05740203,0.0024548552,0.019226953,0.01619056,0.018312464,-0.027861156,-0.001543401,0.022959102,0.020814119,-0.01005011,-0.011938238,0.022592071,0.026814545,0.030492002,-0.028046276,0.06158562,0.020962544,-0.045552462,0.0065720584,0.066749215,0.01605178,-0.07249654,0.011847161,0.048377354,-0.0003062684,0.05667729,-0.030652741,-0.009186349,0.021577638,-0.006695813,-0.033491757,0.012143373,0.01620754,-0.0641689,0.019317579,-0.031314924,0.053235244,-0.009392773,0.030957175,0.037386097,-0.030843899,-0.020826763,0.026106285,-0.00014524366,-0.022434404,0.01869887,-0.055719063,-0.0021217111,0.022335503,0.029406937,0.013770168,-0.037360016,0.08102975,-0.09851764,0.008113743,0.055665016,-0.006587582,-0.0034138677,-0.02870138,-0.012361992,-0.018060738,-0.009784247,0.0512961,0.0056628888,-0.019666795,-0.011725503,0.036910247,0.028578505,0.00972218,-0.011612741,-0.00065903796,-0.037568048,0.025600588,-0.043920558,0.0018416079,0.014038694,0.050863896,-0.012076175,-0.010878043,0.013089635,0.008916447,0.0019057376,-0.0459616,-0.044320755,-0.0014474118,-0.013559951,0.030925224,-0.04792383,-0.008742482,-0.017954556,-0.014970519,-0.04154204,0.06407265,0.064753585,0.010827137,-0.0553946,-0.029442908,0.033143416,0.07958033,-0.036001764,-0.005121541,-0.024567101,-0.04734967,-0.049749553,0.014437663,0.057239953,0.053002063,0.01905053,0.03348274,0.03663416,-0.04185863,-0.027049622,0.009616567,-0.009608082,-0.06047023,-0.022885721,0.0096461745,0.064053446,-0.0072710514,0.004586393,0.01779292,0.002044041,0.03262306,0.013720557,0.061969433,0.037744336,0.01083735,0.006920364,0.020654477,-0.029470647,0.042981904,-0.023143316,-0.026137872,-0.00084070786,0.014770584,-0.014968976,-0.019701865,0.016478833,-0.0063511,0.02224743,-0.041983135,0.0007856579,0.030022597,0.044687845,-0.06746656,0.060898572,-0.0070358333,-0.011584202,0.009205043,-0.034318645,-0.05158057,-0.00028200945,-0.07474983,-0.02297903,0.043026388,-0.036239915,-0.038518414,0.014641688,-0.01373534,0.10628032,-0.052858334,0.024927845,-0.026529996,0.0149284275,0.09670844,0.00441595,-0.0051939925,-0.007123326,-0.012694615,0.0013929111,0.038333774,0.074397944,0.017902264,-0.010799237,0.06920416,0.038781554,0.047818534,0.020861244,-0.037116095,-0.019845566,-0.038102,-0.038890652,0.009923671,-0.020990282,0.025057266,0.011453122,-0.016507298,0.0035335491,0.0050892676,-0.018818496,0.0037187554,0.031545695,-0.02920089,0.02001166,-0.07443038,0.06026392,0.04851122,-0.0019909095,-0.030272909,0.046329483,0.033918582,0.0028402586,-0.043984458,-0.00019775901,-0.032505564,-0.010602868,-0.014913501,0.00114447,0.01705002,-0.07096271,0.0023372853,0.013986331,0.032052394,0.020783402,-0.028109256,-0.019790294,-0.0076365164,-0.040963158,0.0058675013,0.0037884165,0.013382922,-0.008961143,0.01007753,-0.032178886,0.08389836,-0.099643774,-0.044018116,-0.018982856,0.032656647,-0.0360555,0.005965672,-0.034990165,-0.002678495,-0.017495975,-0.023200156,0.052961517,0.040108133,-0.03344639,-0.00484453,-0.02078338,-0.020014523,-0.0024356092,-0.023700044,-0.039671004,0.013503854,-0.029371336,-0.013950383,0.08738737,-0.06587973,-0.008945804,0.028235346,-0.005220083,-0.08154729,-0.015058048,0.012661268,0.019187737,-0.037041154,-0.015270755,0.026270796,0.03789338,-0.065319836,0.0011662246,-0.0930716,0.03737812,-0.032696605,-0.02132633,0.03290268,0.03311459,-0.08073874,0.010312064,-0.021175204,-0.04427612,-0.00045310633,0.022278374,0.0748735,0.03788141,0.05842091,-0.059116486,-0.11589607,-0.010198488,-0.021367988,0.00072656985,-0.054391492,0.031596392,0.008245251,-0.030095538,0.028933225,-0.049734216,-0.024410458,0.028611468,0.03193854,-0.014293264,-0.0092750555,-0.0028190059,0.0124144275,0.03937557,-0.016715674,0.010716261,0.02493061,0.029517053,-0.06990441,-0.06259081,-0.014295693,0.012275431,0.021771448,0.018438956,0.008148108,0.012216771,-0.013399637,-0.0019293516,0.07052985,0.011886819,-0.037638403,0.006676133,0.038309664,-0.021449158,0.0384054,0.0071319086,-0.032416247,-0.092264965,0.009624681,-0.014707208,-0.06733466,0.040820155,0.037045345,0.0123888515,-0.022733975,-0.051159948,0.0016775429,0.033323262,0.06469154,0.01289155,0.028152907,-0.006650412,-0.03360428,-0.03715405,-0.01628253,-0.024544943,0.024892138,-0.021598946,0.045187604,0.007937891,0.0128942905,-0.012674738,-0.0078625055,-0.018241804,-0.021608377,0.037495237,0.017590476,-0.02436581,0.054376327,-0.018421244,0.00060340966,0.034261316,-0.0039021662,0.012083654,-0.0051083732,0.02251095,0.029780265,-0.012826827,0.003684586,-0.012561756,-0.10855149,-0.040973175,0.008833517,-0.0024963855,0.016386203,0.007320376,-0.030116776,-0.051219366,-0.04874467,-0.004665159,0.09220249,0.032082263,0.03222016,-0.02440213,0.027671427,-0.03898792,0.059963033,0.024651425,-0.020205071,0.03390164,-0.023369526]	Keywords: Transformers, Neural Networks, References, Architecture, Deep Learning\nKey Objects: Transformer, BERT, Deep Residual Learning\nRefers to Images: None\nHypothetical Questions:\n- What is a Transformer?\n- What are some key researchers mentioned in this list?\n- What are some notable architectures based on Transformers?\n- What do the various codes like (arXiv:2103.00112) signify?\n---\nSummary:\nThis is a list of references cited in a survey of Transformers, a type of neural network architecture.\nOriginal Text:\n- [46] Chi Han, Mingxuan Wang, Hejong Ji, and Lei Li. 2021. Learning Shared Semantic Space for Speech-to-Text Translation. arXiv:2105.03095 (cs.CL)\n- [47] Kai Han, Yunne Wang, Hanteng Chen, Xinghai Chen, Jianyuan Guo, Zhenhua Liu, Yenhui Tang, An Xia, Chunjeng Xu, Yixing Xu, Zhaohui Yang, Yiman Zhang, and Dacheng Tao. 2021. A Survey on Visual Transformer. arXiv:2012.12556 (cs.CV)\n- [48] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. 2021. Transformer in Transformer. arXiv:2103.00112 (cs.CV)\n- [49] Kaiming He, Xiangyang Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In Proceedings CVPR . 770-778. https://doi.org/10.1109/CVPR.2016.90\n- [50] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. DeberTa: Decoding-enhanced BERT with Disentangled Attention. arXiv:2006.03654\n- [51] Ruining He, Ainhur Ravula, Bhargav Kaganal, and Joshua Ainslie. 2020. RealFormer: Transformer Likes Residual Attention. arXiv:2012.11747 (cs.LG)\nContextualized Text:\nThis is a list of references cited in a survey of Transformers, a type of neural network architecture.	{"tags": ["architecture", "deep learning", "references", "transformers"], "doc_id": "92e62920-984e-4848-bf27-4ffa6306d46a", "summary": "This is a list of references cited in a survey of Transformers, a type of neural network architecture.", "doc_type": "text", "entities": ["Chi Han", "Mingxuan Wang", "Hejong Ji", "Lei Li", "Ruining He", "Ainhur Ravula", "Bhargav Kaganal", "Joshua Ainslie", "Kaiming He", "Xiangyang Zhang", "Shaoqing Ren", "Jian Sun", "Pengcheng He", "Xiaodong Liu", "Jianfeng Gao", "Weizhu Chen"], "keywords": ["Transformers", "Neural Networks", "References", "Architecture", "Deep Learning"], "key_objects": ["Transformer", "BERT", "Deep Residual Learning"], "contextual_text": "This is a list of references cited in a survey of Transformers, a type of neural network architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is a Transformer?", "What are some key researchers mentioned in this list?", "What are some notable architectures based on Transformers?", "What do the various codes like (arXiv:2103.00112) signify?"]}
92efd6ec-212c-452a-b29d-98cb79aa0c9d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.005664137,0.011087286,-0.02533458,0.020765357,0.00066175737,0.05835485,0.005917016,0.0429972,0.048332367,-0.020445865,-0.010299972,0.014767293,0.008233208,-0.015369609,-0.038445957,0.044442423,0.009042856,0.051362786,0.04868018,-0.043625444,0.043589666,0.016984297,0.008201039,-0.034123693,-0.016368305,0.02082341,0.0031575356,-0.034031913,0.0056769326,0.035917334,0.03345402,-0.07489931,0.035211295,-0.0031966423,0.022987569,-0.0041248263,-0.0038426127,-0.007606681,0.032034487,-0.013446058,-0.007360082,0.043281868,-0.062392514,-0.022590524,-0.005352195,-0.05718011,-0.047982678,-0.02979651,0.04727071,-0.02079035,-0.016282093,0.036951676,-0.023351394,-0.005383299,-0.0011995435,-0.016794508,0.00033840595,-0.045016877,0.031222217,-0.09319315,-0.0151888905,0.027687164,-0.039015077,0.007835014,0.035389744,-0.06643346,-0.021307053,0.00500755,0.008913989,0.122213185,-0.0028313906,0.02739165,-0.008197428,-0.06696745,0.09454557,0.061364226,-0.025310816,-0.014692168,-0.079345524,-0.034236204,-0.03925515,0.07337064,-0.04556001,-0.05460371,0.0651617,-0.0058138287,-0.04107146,0.013900474,0.08689395,-0.027392419,0.024995225,-0.023836704,0.005047665,0.07238622,-0.019264787,-0.04897046,0.017538263,-0.025384534,-0.0085955225,-0.05969426,-0.004251616,-0.006182214,0.0692543,0.084930316,0.02202518,-0.02489534,-0.04262201,-0.04150856,0.013762443,0.022119444,-0.0073413593,0.0097211255,-0.026622308,0.0047825617,-0.014950343,-0.005009343,-0.053167038,0.013547508,-0.013843314,0.010376783,0.030082474,-0.044546828,-0.01182005,0.035245847,0.08011638,0.076550186,0.013186689,0.0062656538,-0.04295765,-0.019307181,0.002677552,0.0020368698,-0.04187235,0.019327397,0.034792505,0.014172068,0.07271983,-0.026531136,0.011079278,0.0032678593,0.030387597,-0.023389015,-0.01637605,0.03140421,-0.029380865,0.0056354683,-0.05845863,0.020832315,-0.020967988,0.07058662,0.030785894,0.007969502,0.027436601,0.024364932,0.020083018,0.001387111,0.019775022,-0.058183968,0.018908778,-0.0020715038,-0.039281506,-0.08220024,-0.018770972,0.041764308,0.016073953,0.14474273,-0.0054048654,-0.014424075,0.0463707,0.0039631845,-0.035593525,0.012341326,-0.014858197,-0.032251354,0.019804183,-0.024488537,0.023137547,-0.021353796,-0.027427303,-0.020640641,0.008043752,0.009175905,-0.004069358,0.03507495,-0.019806964,0.031196417,0.01064127,0.06669534,-0.030140331,-0.03946116,0.029090263,-0.011016476,-0.009289273,0.002363574,0.02340592,0.0009237835,0.0167501,-0.08600005,-0.03694165,-0.047808155,-0.025755923,-0.0020368719,0.019115843,-0.000988969,0.02582377,-0.005715139,-0.006187485,-0.020338241,-0.014708788,-0.011792959,-0.016029315,-0.02812808,-0.009632728,0.062557355,0.042271473,-0.0015796501,0.00081227475,0.030730894,0.02325464,-0.006726141,-0.06979752,0.0027965603,-0.049273435,0.029898604,-0.0253249,0.014271924,-0.03709295,0.04515664,0.0022999065,-0.048887797,0.04057856,0.0015208526,-0.045830045,0.026321981,-0.008832174,0.038438454,0.012260775,-0.014525509,-0.002531395,0.0298088,0.03628044,0.06642431,0.012625737,0.07665209,0.015741602,-0.00540068,0.029224658,0.054692086,0.008813975,0.03267293,-0.020974211,0.04921772,0.027383741,-0.002144629,0.002884586,-0.048335038,0.024738958,0.035524506,0.040962603,-0.021971729,-0.017386202,-0.018256897,0.030655071,0.007972917,0.03184047,0.02103172,0.0061307303,0.04436125,0.015692575,-0.0031689792,0.028506475,0.038311645,0.01994408,-0.034316674,-0.08762512,0.016095316,0.017451651,0.016486274,0.01084819,0.013588916,-0.016302546,-0.048669215,0.05529993,-0.018105365,0.040094655,-0.010263273,-0.006008996,-0.02520734,-0.06487989,-0.009992003,0.007197243,-0.052047756,0.015008971,-0.02222845,0.010521985,0.015041301,0.0032270348,-0.03381405,0.010111176,0.0037218693,0.11711316,-0.024708316,0.012739357,-0.01211965,0.009239682,0.010647846,-0.073427804,0.03291799,0.048925746,0.013981507,-0.000824574,-0.016650228,-0.0020293635,0.007484517,0.018998753,0.080809265,0.06939768,-0.02702489,-0.00015597012,0.023639148,0.010573471,0.0034210961,-0.006585713,0.055786222,0.043810003,-0.016341008,0.026766488,-0.058514,-0.0984596,0.05833061,-0.06965801,0.04580437,0.042100593,0.017823508,-0.0027582808,-0.0007366821,0.03354628,0.034092247,0.013195791,0.026127351,-0.007975391,-0.062843055,0.018637182,-0.0068923477,-0.004115106,0.020841286,0.014604487,0.031008886,0.050886612,-0.009535492,0.0045315446,0.034446195,0.03627782,0.042919535,-0.016630672,-0.011803368,0.00955519,0.019204019,-0.0019712688,0.031946283,0.05049684,-0.060143102,0.10152442,-0.025272185,0.015138103,-0.016723486,-0.084643565,-0.01999955,0.03989872,-0.015991464,-0.02053761,-0.026752314,0.061886992,-0.046748683,-0.03354016,-0.0055660293,0.024001956,0.033791214,-0.05762781,-0.022199526,-0.023340043,0.007351601,-0.046141762,-0.04675195,-0.022206225,0.05080373,0.0069240322,0.04616626,-0.007561353,0.018910522,-0.034381866,0.021823749,0.035502184,0.015022487,-0.012926497,-0.005883954,0.036509592,0.04345999,0.013602815,-0.02487831,0.05381426,0.03804988,-0.04976844,0.019445403,0.0488796,0.0072089015,-0.09424801,0.017412698,0.07131863,0.01522496,0.037522126,-0.04184517,-0.026130704,0.017616864,0.00047315852,-0.026049973,-0.0032790287,0.030228421,-0.0622039,0.024659034,-0.02983879,0.06363439,0.0016211206,0.054204952,0.035390332,-0.011848607,-0.030328603,0.027380446,-0.005847372,-0.053335812,0.02023781,-0.064525425,-0.01317485,0.045356095,0.0320148,0.03225061,-0.058505554,0.07355045,-0.095659055,0.012792499,0.04878236,-0.022637485,0.0060795043,-0.0517513,-0.010194787,-0.041766386,0.02323626,0.028856229,-0.010892175,-0.013829109,0.02463798,0.044124,0.03642241,-0.0076974384,0.010075134,-0.007987703,-0.023537915,0.002390698,-0.07368207,-0.021018498,0.0049873586,0.03182648,-0.0041519008,0.00089700805,-0.022879215,-0.013161238,-0.009788394,-0.04259352,-0.01674863,-0.004673262,0.015239408,0.016830668,-0.05176342,0.03364298,-0.014678505,-0.02647646,-0.010873257,0.037784092,0.06117456,0.019989397,-0.05454957,-0.05159987,0.013639408,0.05971116,-0.030284524,-0.00800715,-0.012085839,-0.0010527564,-0.041254703,0.025778119,0.028624028,0.04609693,0.010746347,0.018773353,0.03274606,-0.052309312,0.0019151563,-0.004600109,0.020075178,-0.057954732,-0.009020215,-0.009641424,0.074705206,0.0071907463,-0.011643244,0.004258753,-0.006402532,0.0019225643,0.0379784,0.07891597,0.0316918,-0.0016133714,-0.012341713,-0.0118559515,-0.034708302,0.03480085,-0.0029425353,-0.007812586,0.033155095,0.05238483,-0.010535143,-0.009586637,0.007965935,-0.022643736,0.01805162,-0.019670568,0.03828378,0.048979748,0.054535113,-0.06951852,0.06966321,-0.03939029,-0.036202043,0.020787813,-0.05278153,-0.040398464,0.0054715304,-0.07784027,0.0019858815,0.014260202,-0.025196329,-0.029258482,0.009808931,-0.021941714,0.09701159,-0.063358255,0.0017863954,-0.04214023,0.018179625,0.11749413,0.009879072,0.014043164,-0.019070849,-0.0110670235,-0.033381548,0.04250113,0.065695055,-0.007214987,-0.0070831957,0.053881086,-0.007989946,0.039553177,-0.001991282,-0.042934794,-0.0439998,-0.019657962,-0.027999138,0.013052847,-0.008275713,0.025697066,0.0049441773,-0.009631338,0.017753031,0.03192899,-0.01682497,0.021398384,0.025123397,-0.02411497,0.004884166,-0.04618736,0.03590927,0.029890941,0.03126302,-0.011913192,0.025800249,0.044730593,0.01212807,-0.016733054,0.019074136,-0.019134164,-0.029251887,-0.018281585,0.0026295073,0.01651153,-0.029447509,-0.020357976,0.02667157,0.029050373,0.02413539,-0.036214728,-0.030964095,-0.015465366,-0.022563936,-0.03136876,0.003524988,0.016452776,-0.0013744327,0.016207507,-0.02610755,0.077400334,-0.11630493,-0.020017752,-0.03910294,0.014951505,-0.03126463,-0.0107932,-0.010961687,-0.014572418,0.001010578,-0.04735122,0.060586732,0.039479516,-0.03692572,0.018249145,-0.005791668,-0.0077387663,-0.0058961315,-0.0092186425,-0.019045452,-0.012144857,-0.017406752,-0.009079285,0.093331896,-0.035365306,0.0092685735,0.037345268,-0.01048776,-0.07255772,-0.03627628,0.032595657,0.033742893,-0.03069709,-0.0012410111,0.039021518,0.037343387,-0.06826364,-0.01937482,-0.053168215,0.04986404,-0.00937199,-0.0006618991,0.021224745,0.0060099307,-0.07397297,0.0003671556,0.009183308,-0.030660644,0.011679558,0.027063569,0.08285452,0.0699406,0.06771308,-0.049276758,-0.09987205,-0.0115278,-0.030196521,0.0020665708,-0.044188682,0.04588137,0.0069093276,-0.0200863,0.011283606,-0.0124421315,-0.0032461386,0.009103585,0.0064921593,-0.011149248,-0.0207199,-0.013597878,-0.00020058824,0.028364038,0.003931211,-0.017447524,0.05264186,0.04224334,-0.057092976,-0.07465611,-0.02637764,-0.02586793,-0.0049043577,-0.0127228685,0.00690235,0.020142294,-0.00073270267,0.025545817,0.06479987,0.0049051344,-0.023044754,-0.011060627,0.03463739,-0.010066207,0.051457066,-0.0035796845,-0.032834027,-0.080882095,0.007815588,-0.033456486,-0.04515305,0.031671256,0.017616306,0.0011858247,-0.035676427,-0.03841539,-0.025773345,0.012535487,0.044280503,0.012124701,0.0057302495,0.009538278,-0.021769792,-0.051715497,-0.024441287,-0.020773591,0.008966132,-0.016345207,0.053536404,0.012435787,0.022029627,-0.0023634308,0.032658104,-0.0030248864,0.01002945,0.023286786,0.028599614,-0.025481991,0.064748175,-0.022043806,0.006461079,0.06460162,-0.018228846,0.028619338,-0.01645466,0.009222904,-0.0100361165,0.010874102,-0.014439856,-0.044136234,-0.07030484,-0.0036217277,0.018581498,0.015216934,0.028396068,-0.010997079,-0.035017967,-0.057918284,-0.03538241,-0.03411728,0.056637667,0.038549,0.02393146,-0.06660099,0.026766574,-0.027711516,0.06484431,0.024658643,-0.00024057402,0.009076624,-0.004683592]	Keywords: Transformer, Attention Mechanism, Efficiency, NLP, Computer Vision, Audio, Time-Series, Deep Learning\nKey Objects: Transformer architecture, Attention mechanism, GELU activation, Matrix capsules, Aval attention\nRefers to Images: None\nHypothetical Questions:\n- What are some of the key innovations in Transformer architecture?\n- How are researchers addressing the limitations of standard Transformers (e.g., computational cost, sequence length)?\n- What are the different applications of Transformers beyond NLP?\n---\nSummary:\nThis is a list of references, primarily related to Transformer models and architectures, spanning topics like attention mechanisms, efficiency improvements, and applications in various domains (NLP, vision, audio, time-series). The references span from 2016 to 2021.\nOriginal Text:\n- [51] Ruining He, Ainhur Ravula, Bhargav Kaganal, and Joshua Ainslie. 2020. RealFormer: Transformer Likes Residual Attention. arXiv:2012.11747 (cs.LG)\n- [52] Dan Hendrycks and Kevin Gimpel. 2020. Gaussian Error Linear Units (GELUs). arXiv:1606.08415 [cs.LG]\n- [53] Geoffrey E. Hinton, Sara Sabour, and Nicholas Frost. 2018. Matrix capsules with EM routing. In Proceedings of ICLR . https://openreview.net/forum2.hjLDFWgrb\n- [54] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. 2019. Aval Attention in Multidimensional Transformers. CoRR abs/1912.11280 (2019). arXiv:1912.11280\n- [55] Ronghang Hu, Amanpreet Singh, Trevor Darrell, and Marcus Rohrbach. 2020. Iterative Answer Prediction With PointerAugmented Multimodal Transformers for TextVQA. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition , CVPR 2020, Seattle, WA, USA, June 13-19, 2020. 9899-9999. https://doi.org/10.1109/CVPR42600.2020.01001\nContextualized Text:\nThis reference list highlights the rapid evolution of Transformer models.  Early works focused on the foundational architecture and attention mechanisms. Subsequent works explore optimizations for efficiency (e.g., through early exiting, memory optimization, and efficient attention variants like Aval attention). There's a strong emphasis on adapting Transformers for resource-constrained environments or to handle extremely long sequences (e.g., in time-series forecasting).	{"tags": ["MachineLearning", "DeepLearning", "Transformers", "NLP", "ComputerVision"], "doc_id": "92efd6ec-212c-452a-b29d-98cb79aa0c9d", "summary": "This is a list of references, primarily related to Transformer models and architectures, spanning topics like attention mechanisms, efficiency improvements, and applications in various domains (NLP, vision, audio, time-series). The references span from 2016 to 2021.", "doc_type": "text", "entities": ["Ruining He", "Kevin Gimpel", "Geoffrey Hinton", "Nal Kalchbrenner", "Tim Salimans", "Jonathan Ho", "Dirk Weissenborn", "Ruining He", "Ainhur Ravula", "Bhargav Kaganal", "Geoffrey Hinton", "Sara Sabour", "Nicholas Frost"], "keywords": ["Transformer", "Attention Mechanism", "Efficiency", "NLP", "Computer Vision", "Audio", "Time-Series", "Deep Learning"], "key_objects": ["Transformer architecture", "Attention mechanism", "GELU activation", "Matrix capsules", "Aval attention"], "contextual_text": "This reference list highlights the rapid evolution of Transformer models.  Early works focused on the foundational architecture and attention mechanisms. Subsequent works explore optimizations for efficiency (e.g., through early exiting, memory optimization, and efficient attention variants like Aval attention). There's a strong emphasis on adapting Transformers for resource-constrained environments or to handle extremely long sequences (e.g., in time-series forecasting).", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the key innovations in Transformer architecture?", "How are researchers addressing the limitations of standard Transformers (e.g., computational cost, sequence length)?", "What are the different applications of Transformers beyond NLP?"]}
73d7400a-d9cc-4e51-8a47-7ef671a3ebba	abe8c200-bfa1-4355-947e-23ea618c310d	[0.00209178,0.018741645,-0.0052675274,0.029556336,0.03310698,0.07040424,0.0022149265,0.05431748,0.033420097,-0.03305142,-0.012938189,0.02746854,-0.026727263,-0.020539789,-0.02849243,0.044975545,0.009366753,0.050252706,0.018102229,-0.026725927,0.04621003,-0.011325118,0.007546172,-0.038086005,2.1078582e-05,-0.00856517,0.02123507,-0.02134717,0.026681436,0.0046908897,0.004851747,-0.0014818752,0.017150313,-0.029120425,-0.0068263295,0.0163655,0.019766511,0.027734688,0.018143065,-0.007295101,0.0017635003,0.0075767753,-0.044403035,0.023409823,0.010271853,-0.041597333,-0.06963699,-0.038639307,0.017199934,-0.0111912945,-0.024507424,0.01712967,-0.012729081,-0.008705626,-0.0017140785,-0.027364463,-0.015675921,-0.024981173,0.054286253,-0.09091627,-0.04370153,0.013317898,-0.04910964,-0.00052582903,0.016484419,-0.05815812,-0.047122136,0.014515333,0.027022686,0.118305005,-0.01648634,0.0645101,-0.021170974,-0.01954066,0.09102554,0.02859667,-0.043774843,-0.01843867,-0.07140666,-0.02166025,-0.015405118,0.054955047,-0.03475748,-0.05744861,0.093744755,-0.008394774,-0.044089857,-0.059457567,0.087906554,-0.034646373,0.021372298,-0.0140031455,0.015538182,0.06382433,-0.0141027775,-0.09658447,0.015883861,-0.050705034,0.041983556,-0.057333402,-0.019694604,0.010483582,0.06922804,0.07887599,-0.019612046,0.006867468,-0.011373776,-0.03225265,-0.0011426798,0.026603146,0.009421019,0.037502006,-0.03327399,-0.0013203084,-0.04126685,-0.029168421,-0.04822725,-0.012409871,-0.013007819,0.004314604,0.004137334,-0.022770798,-0.005648924,0.030627433,0.05696882,0.061005104,0.006735454,0.013240061,-0.03197434,-0.0040127845,-0.00694247,-0.011238726,0.017718075,0.0073140524,-0.0075641507,0.010624548,0.054727424,-0.06505137,0.010223331,0.014437861,0.04438457,-0.04380781,-0.02984902,0.015481194,-0.035636358,0.015676865,-0.021332879,0.027074814,-0.005921052,0.05089923,-0.0036242718,0.011236467,0.03474517,-0.010573232,0.013012714,-0.006257651,0.0038802703,-0.03461846,0.017066225,0.033704404,-0.02898417,-0.041036658,-0.06127147,0.048898906,0.023097696,0.11290845,0.022600727,0.0020250827,0.05313923,0.032910135,-0.053127456,0.017207842,-0.01324367,-0.014690707,0.03455736,-0.042763114,0.012919715,0.0030978494,-0.044659976,-0.012016318,0.007538555,0.016835712,0.0014666431,0.070806056,-0.029727424,0.01652901,0.016487394,0.07499006,-0.022812093,-0.017950825,0.012176086,-0.04954975,-0.020002693,0.012515887,0.04277868,0.00892255,0.01774575,-0.054266386,-0.05059774,-0.008251093,-0.018333863,-0.0029018803,0.045188125,-0.0037375449,0.02228736,-0.035080884,-0.023477674,-0.030504145,-0.012821048,-0.021604128,-0.01202988,-0.042159196,-0.0002938892,0.04310412,0.035760883,0.018211175,-0.031089047,0.021088727,0.034373097,-0.020225327,-0.022988435,0.007719545,0.013148242,0.0063405987,-0.040053826,0.005403477,-0.031624373,0.036689702,0.02899939,-0.02167477,0.037243012,-0.0031906804,-0.047873452,0.028714966,-0.0026180367,0.038676493,0.020638876,-0.018401174,0.03535107,0.049564794,0.04212334,0.048357133,-0.009124044,0.040391173,0.008964721,0.003994296,0.0576979,0.07421463,-0.022504808,0.002399602,0.009300254,0.043603845,0.061779663,-0.0050996207,-0.021539161,-0.045347244,0.03162736,0.041978247,0.037860002,0.011746986,-0.020808974,-0.0047286185,0.03657665,-0.013657505,0.028427,-0.016420847,0.029514134,0.042536784,0.017700706,0.017095054,0.0070478045,0.04983795,-0.014200133,-0.023149312,-0.059926685,0.013319714,-0.0014308749,0.02896688,0.028380094,-0.010648225,-0.013401311,-0.021729898,0.04799915,-0.024500085,0.018246528,-0.013168497,-0.016275028,-0.007328081,-0.09177273,0.015198394,-0.009201958,-0.04965636,0.012933267,-0.007024591,0.012874844,0.022269968,0.0065113725,-0.03305525,-0.025507672,0.010660589,0.07830873,-0.043636225,0.0053996155,-0.0050614905,0.003820087,0.015808444,-0.05309706,0.0010143839,0.07708568,-0.008891847,0.023756573,-0.007872485,-0.033536278,-0.010126958,0.024310289,0.065391734,0.065092355,-0.0061903493,0.015493719,0.01738222,0.0031604627,0.01890159,0.0039446326,0.04987989,0.029066404,-0.015619035,-0.032595143,-0.037035223,-0.08276623,0.042413708,-0.060765762,0.040836625,0.013864608,0.04107255,-0.0058421446,0.0066230376,0.039825466,0.031795412,0.03746414,0.022653788,0.0028257756,-0.06565236,-0.015334571,0.001654207,-0.0051899734,0.0123783555,0.010611274,0.05296168,0.059294615,-0.00039570365,-0.016557187,0.049280856,0.053189956,0.0005378503,-0.007869989,0.010293989,-0.011627436,-0.005410812,0.004743927,-0.0031047992,0.05632889,-0.019632842,0.088724755,0.00020394065,0.0044424417,-0.022317482,-0.06674534,-0.04187681,0.061821047,-0.034900554,0.013014675,-0.041929968,0.06216821,-0.041585155,-0.0075013055,-0.0071018166,0.016245991,0.0011298715,-0.03203843,-0.017314304,-0.033306226,0.041384034,-0.049219426,-0.005304155,-0.040117785,0.054838676,-0.02038805,0.041293304,0.011118476,0.0081412885,-0.025213037,-0.004598178,0.025450803,0.00054740947,-0.011123964,0.0033563683,0.047030866,0.03557156,0.015767999,-0.023761977,0.048399247,0.024060037,-0.050738506,0.060308155,0.047926284,0.011971867,-0.09131939,-0.012379928,0.09079238,0.027438033,0.024475066,-0.046519358,-0.0123780435,0.0255142,-0.030754035,-0.011729413,-0.02893278,0.021796891,-0.039262563,0.0018664552,-0.07325051,0.036459025,0.0071009444,0.034384158,0.029516507,-0.022811888,-0.021704132,0.019556828,-0.011518216,-0.042527467,0.009054444,-0.09847513,0.00055345206,0.040040247,0.04929844,0.048358176,-0.06022146,0.04100537,-0.11338011,0.01630256,0.06260015,-0.012133484,-0.0014815644,0.003016468,-0.009237905,-0.03251078,-0.029810756,0.04806398,0.027053425,-0.018043868,-0.020937096,0.016753882,-0.0021716994,0.0038395918,0.0036413425,-0.040045958,-0.043370586,-0.0069820452,-0.055242304,0.03029211,-0.0033106548,0.04264982,-0.028475553,-0.0027178507,0.013662501,0.002441721,-0.0531586,-0.017947344,-0.008907307,-0.003367959,-0.0031616087,0.0150580965,-0.0475562,0.027282136,-0.026001064,-0.020831345,-0.008671879,0.052735798,0.078778334,-0.028341435,-0.035327166,-0.020399958,0.023241628,0.031343337,-0.007971669,-0.013048272,-0.036138777,-0.029750995,-0.057410847,0.009185542,0.037004873,0.08719146,0.03768857,0.03570195,0.04293851,-0.019570015,-0.021127686,-0.0063975467,0.0032774035,-0.055505104,-0.04033905,-0.009016125,0.052871786,0.0075002233,-0.012001107,-0.005159804,0.02322035,0.0016171008,0.050109036,0.026008632,0.03180298,0.014747707,-0.005503916,-0.00096775714,-0.022690551,0.030939044,-0.040488716,-0.02108044,0.03511394,0.054481607,-0.016379407,-0.032757547,-0.013953121,-0.022675179,-0.020081501,-0.010000331,0.058154024,0.035667986,0.047991846,-0.037328467,0.08439987,-0.020344827,-0.024719398,0.011695923,-0.046260796,-0.019932445,0.012205165,-0.058414403,-0.031295676,0.03474792,0.009639798,-0.010695798,0.0005330141,-0.025514947,0.07222431,-0.076026194,0.002372492,-0.001212472,0.010658225,0.102458134,0.03640874,-0.023773314,-0.014272531,-0.020334264,-0.025259769,0.0141126765,0.041908164,-0.0011175952,0.003299602,0.035334066,0.019647451,0.032836188,-0.023270432,-0.05381295,-0.0338538,-0.032182824,-0.04124966,0.01613113,0.007345054,0.03170451,0.014115346,0.0011343249,0.02579387,0.025013866,-0.0321211,0.032704487,0.032821607,0.00044090522,-2.619485e-05,-0.04716604,0.057752497,0.03706393,0.0007790164,-0.04982226,0.013153224,0.038502343,0.00538651,-0.015443294,0.008182751,-0.004618144,-0.016063679,-0.030043516,0.036731653,0.024273837,-0.0471694,-0.02978351,0.0052282996,0.042471584,0.034088366,-0.00908804,-0.036547087,-0.017398257,-0.024422873,-0.025065675,0.017916633,-0.001782875,-0.018832723,-0.013660999,-0.023435706,0.06637922,-0.10047006,-0.034126543,-0.03235494,0.056685533,-0.017361896,0.018371193,0.023894371,-0.02548038,-0.0009982385,-0.021612875,0.069664456,0.039509967,-0.034864742,0.019514542,0.016431449,-0.012896728,-0.013067936,-0.0042075366,-0.036461163,-0.0006073263,-0.008189203,-0.029486852,0.077262275,-0.06698548,0.017485667,0.0495262,-0.019979158,-0.08276804,-0.03428917,0.03745541,0.007516567,-0.015973559,0.01640053,0.022173692,0.04666365,-0.04916446,-0.04871625,-0.08951174,0.043114778,0.022639342,-0.035580795,0.0026333523,0.00015791738,-0.038006026,-0.0012428256,-0.037577696,-0.053133246,0.0023037829,-0.009218414,0.06549364,0.042342845,0.034808114,-0.046769667,-0.1263669,-0.014315468,-0.04339644,0.004019033,-0.060646396,0.07380894,-0.028933948,-0.0025775032,-0.0057544513,-0.042679973,-0.010151637,0.01596236,0.004894834,-0.022872837,0.0016723588,-0.03708741,-0.042148322,0.019918863,0.019400122,0.0064676767,0.058516234,0.06892834,-0.08099655,-0.048954334,-0.028487489,-0.0022629716,0.016242266,-0.0023409375,0.02437921,-0.0072041275,-0.021350259,-0.01524008,0.05489367,0.0023803771,-0.0144362245,-0.032005966,0.028693976,-0.020065116,0.04233197,0.0028569836,-0.044472165,-0.0971238,-0.019288642,-0.031515554,-0.052222125,0.03618694,0.046572372,0.012271212,-0.026474867,0.0006230321,-0.017826153,0.05618559,0.054974366,-0.017128875,0.012554546,0.022049028,-0.039110485,-0.06765163,-0.0011131478,-0.04563036,0.036501247,-0.029290903,0.04125931,0.024977399,0.011810274,-0.021056939,0.03590127,0.006517228,-0.00016600062,0.0593422,0.0045040282,-0.069386,0.05968771,-0.015950814,-7.141956e-05,0.058194857,-0.020443432,0.021288933,-0.005766898,0.04136666,0.035729557,0.03130957,0.012272912,-0.037149362,-0.118799366,-0.007930939,0.024355661,0.018781481,0.029169345,0.008475012,-0.032180708,-0.038818285,-0.01869795,-0.035217073,0.04998323,0.019938832,-0.015506256,-0.04059719,0.01184924,-0.057647713,0.057039484,0.04004068,-0.039871182,0.022640385,-0.041339904]	Keywords: Transformers, Neural Networks, Speech Synthesis, Music Generation, Batch Normalization, Accelerating Training, Architecture Improvements, References, Deep Learning, Applications, Improvements, Training, Models, Deep Learning Models, ICML, ACL, EMNLP, ICASSP, Interspeech, ICCV, AAAI, Findings of EMNLP, ICLR, Cognitive Computation, IEEE Trans. Neural Networks Learn. Syst.\nKey Objects: neural network architecture, deep learning models, Transformer architecture, training process, core Transformer architecture, Transformer models, neural networks, deep networks, batch normalization\nRefers to Images: None\nHypothetical Questions:\n- What are the key advancements highlighted in these references concerning the Transformer architecture?\n- How have Transformers been applied to different domains like music generation and speech synthesis?\n- What challenges have been addressed and what improvements have been made to the original Transformer model?\n- Can you explain the role of Batch Normalization in the context of training deep networks?\n- Who are the key researchers contributing to the development and application of Transformer models?\n---\nSummary:\nThis is a list of references related to Transformers, a type of neural network architecture. The references cover a broad range of applications including music generation, speech synthesis, accelerating training, and improvements to the core Transformer architecture.\nOriginal Text:\n- [56] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkorei, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. 2019. Music Transformer. In Proceedings of ICLR . https://openreview.net/forum?id=rJe5AhS7Cf\n- [57] Hyeong Rae Ihm, Joun Yeop Lee, Byoung Jin Choi, Sung Jun Cheon, and Nam Soo Kim. 2020. Reformer-TTS: Neural Speech Synthesis with Reformer Network. In Proceedings of Interspeech , Helen Meng, Bo Xu, and Thomas Fang Zheng (Eds.). 2012-2016. https://doi.org/10.21437/Interspeech.2020-2189\n- [58] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In Proceedings of ICML . 448-456. http://proceedings.mldr.press/v37/ioffe15.html\n- [59] Kazuki Irie, Albert Zeyer, Ralf Schlter, and Hermann Ney. 2019. Language Modeling with Deep Transformers. In Proceedings of Interspeech . 3905-3909. https://doi.org/10.21437/Interspeech.2019-2225\nContextualized Text:\nThis list represents a collection of research publications and technical documents focused on the advancements and applications of Transformer models, a critical component in modern deep learning. It highlights diverse areas where Transformers have made significant contributions, from generating music to improving speech synthesis and accelerating training processes. The references provide valuable resources for researchers and practitioners seeking to understand and build upon this powerful architecture.	{"tags": ["technical", "references", "deep learning", "transformer"], "doc_id": "73d7400a-d9cc-4e51-8a47-7ef671a3ebba", "summary": "This is a list of references related to Transformers, a type of neural network architecture. The references cover a broad range of applications including music generation, speech synthesis, accelerating training, and improvements to the core Transformer architecture.", "doc_type": "text", "entities": ["Music Transformer", "Reformer-TTS", "Batch Normalization", "Reformer", "LazyFormer", "HIBERT", "Memory-Efficient Differentiable Transformer Architecture Search", "Deformable DETR", "Informer", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Hyeong Rae Ihm", "Joun Yeop Lee", "Byoung Jin Choi", "Sung Jun Cheon", "Nam Soo Kim", "Sergey Ioffe", "Christian Szegedy", "Kazuki Irie", "Albert Zeyer", "Ralf Schlter", "Hermann Ney", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei"], "keywords": ["Transformers", "Neural Networks", "Speech Synthesis", "Music Generation", "Batch Normalization", "Accelerating Training", "Architecture Improvements", "References", "Deep Learning", "Applications", "Improvements", "Training", "Models", "Deep Learning Models", "ICML", "ACL", "EMNLP", "ICASSP", "Interspeech", "ICCV", "AAAI", "Findings of EMNLP", "ICLR", "Cognitive Computation", "IEEE Trans. Neural Networks Learn. Syst."], "key_objects": ["neural network architecture", "deep learning models", "Transformer architecture", "training process", "core Transformer architecture", "Transformer models", "neural networks", "deep networks", "batch normalization"], "contextual_text": "This list represents a collection of research publications and technical documents focused on the advancements and applications of Transformer models, a critical component in modern deep learning. It highlights diverse areas where Transformers have made significant contributions, from generating music to improving speech synthesis and accelerating training processes. The references provide valuable resources for researchers and practitioners seeking to understand and build upon this powerful architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the key advancements highlighted in these references concerning the Transformer architecture?", "How have Transformers been applied to different domains like music generation and speech synthesis?", "What challenges have been addressed and what improvements have been made to the original Transformer model?", "Can you explain the role of Batch Normalization in the context of training deep networks?", "Who are the key researchers contributing to the development and application of Transformer models?"]}
aa371f0a-39a1-4688-a9d6-85a84180ae71	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.014302626,0.0043481207,-0.021031981,0.027110754,0.031770173,0.09029032,0.0068415627,0.09576349,0.036508165,-0.046187434,-0.009926301,0.014492528,0.001879557,-0.039441682,-0.026329253,0.050249327,-0.021405043,0.058412805,0.031049693,-0.030932616,0.058393165,-0.0010264462,0.019110328,-0.014086297,0.0032867645,0.037712827,0.025780816,-0.046146855,0.032823022,0.0341935,0.017262688,-0.050334692,0.02087466,-0.009674815,0.005141027,0.014118292,0.020866819,-0.011254311,0.016866582,-0.0059008026,-0.0073058726,0.050474867,-0.07022091,0.0026969272,0.0037810802,-0.057238463,-0.078357965,-0.032133307,0.023765985,-0.0074071437,-0.027912071,0.017295888,-0.01069686,-0.02900347,0.028117111,-0.03089459,-0.028131997,-0.040834136,0.050044615,-0.087323286,-0.033113144,0.019534852,-0.055022765,-0.010619467,0.027481275,-0.06903833,-0.0032066323,0.0058132387,0.009044589,0.12206507,-0.010335689,0.036870398,-0.0038008678,-0.051185474,0.104757085,0.040362705,-0.012599691,-0.0026455193,-0.04654989,-0.007959066,-0.034352563,0.07912428,-0.02041712,-0.0582,0.093514316,-0.01488587,-0.037998777,-0.022699043,0.07695017,-0.006733906,0.044372037,-0.00032822398,-0.02678422,0.0707301,-0.008492596,-0.0965147,0.0286999,-0.031917788,-0.003233625,-0.040249575,-0.020648459,-0.021503443,0.08266643,0.088467866,0.004298487,0.0078029633,-0.029474786,-0.04595741,-0.0014729989,0.020464253,-0.0032709485,0.014210902,-0.019939512,0.00033755365,-0.015901811,-0.01906608,-0.055024456,-0.008415697,0.0041874535,0.013038307,-0.010844069,-0.022957748,0.008242923,0.014275123,0.07149363,0.08189492,-0.011500514,0.0027475972,-0.024212765,-0.017183399,-0.018592041,-0.008267341,-0.021829594,0.017815648,0.0140589345,0.0027420789,0.05863823,-0.047788724,0.011306012,0.02212134,0.02969671,-0.037679292,0.0021969767,0.035921056,-0.005437559,-0.0037124779,-0.049300425,-0.013698189,-0.011554596,0.041742556,0.017341655,-0.0030030834,0.034727424,0.01759923,0.023885459,0.006471538,-0.0014008528,-0.0647285,0.009814589,0.010572548,-0.04589092,-0.06845058,-0.040715884,0.049550544,0.037441194,0.12341719,0.019131102,-0.0013549689,0.03074948,0.00817647,-0.032924917,0.0037167638,-0.016182685,-0.031445887,0.030990275,-0.02605691,0.0063234526,-0.007150437,-0.029515019,-0.0045189834,0.003843362,0.0020843334,0.017139012,0.06730485,-0.024486827,0.013555862,0.014360888,0.102558665,-0.016138338,-0.011046882,0.025890687,-0.0010595162,-0.016860552,-0.005183573,0.03320722,0.004467205,0.04693244,-0.060406663,-0.0738362,-0.0033549054,-0.023158235,-0.0031979028,0.030318534,0.0023554624,0.02813895,0.00048793512,-0.02291619,-0.044725403,-0.03222936,-0.0121227205,0.0061414177,-0.053939577,-0.009673072,0.059843782,0.041723054,0.0033310947,-0.016973305,0.024836045,0.026127739,-0.001803371,-0.06601841,0.012014814,-0.0038400185,0.008409847,-0.04855444,0.005451101,-0.048979677,0.027123157,-0.0031406383,-0.0338588,0.031467218,-0.015639096,-0.02222259,0.02942071,-0.007493103,0.04701431,0.009552895,-0.0020021207,0.0055282884,0.059043877,0.011063933,0.054227944,0.0037113028,0.053242367,0.014918846,-0.008919309,0.0015041931,0.041330747,-0.016250437,0.025287421,0.00809162,0.05480795,0.057255708,0.0003222629,0.009008497,-0.047595974,0.03910834,0.014374532,0.05070389,0.007887375,0.0076252613,-0.0027970439,0.00029962076,0.008870991,0.017957762,-0.0045266924,0.029746579,0.03814829,0.036259,0.007970801,0.04953691,0.058227465,-0.016385024,-0.028401483,-0.0671924,-0.010419205,0.009666668,0.01616947,0.04268415,-0.03088416,-0.008891023,-0.024716342,0.06587465,0.0019194809,0.012014208,-0.010654415,-0.011493247,-0.0125177745,-0.07432791,-0.010946773,0.0016225512,-0.06218137,-0.0023574126,-0.028356964,0.030948417,0.02552158,0.0032424885,-0.049213793,-0.027914342,-0.0026956673,0.089591175,-0.015426538,0.01868325,-0.009863199,0.0006575171,0.01805091,-0.06727173,0.004699813,0.046588693,0.011269479,0.03274435,-0.043348227,-0.004933408,-0.012302874,0.05306861,0.08413139,0.0564939,-0.028539617,-0.02508766,0.017312434,0.009789322,0.00092245935,0.01770689,0.05273352,0.040119875,-0.0077684373,0.012579334,-0.03523812,-0.07512965,0.03468755,-0.06973678,0.051779937,0.052838866,0.012738544,-0.02290044,0.0072103534,0.047502324,0.015037973,0.011120373,0.015628729,-0.00919789,-0.043870088,0.009310624,-0.015302127,-0.012196783,0.03882828,0.0138249025,0.009651638,0.034827296,-0.008691871,0.023084478,0.02349094,0.02911537,0.03995596,0.008053299,-0.0065147495,0.018904073,0.002126634,0.00811555,0.033336382,0.04977516,-0.0333937,0.08645783,-0.015382597,-0.029014377,-0.030996073,-0.06289091,-0.05349564,0.06580098,-0.034332618,0.012202753,-0.040412806,0.06291174,-0.03808181,-0.0055359546,-0.0010605779,0.015526466,0.022078933,-0.059914622,-0.032846756,-0.019229494,0.026903646,-0.045873653,-0.04500998,-0.027583545,0.041840997,-0.022968765,0.04417273,-0.008703416,0.041786868,-0.033320047,-0.0030435564,0.025283732,-0.011346616,-0.01704317,0.013720858,0.03086198,0.04079005,0.00713381,-0.021359455,0.030238137,0.034016885,-0.038607277,0.055069096,0.057176016,0.046633333,-0.078192085,0.023339735,0.066280685,0.005323948,0.020856805,-0.0134459995,-0.026103983,0.018177338,-0.025923382,-0.023146618,0.002498882,0.019686347,-0.061923437,0.0136071695,-0.055558644,0.06518686,0.013157599,0.045788724,0.047072064,-0.02478823,-0.04524168,0.021882558,0.0031980572,-0.027000166,0.053536285,-0.05682367,-0.022069363,0.057993278,0.05224234,0.020375762,-0.061028425,0.07089163,-0.10454723,-0.0104475785,0.06469956,-0.019096458,0.014065441,-0.008498431,-0.015590611,-0.044418838,-0.00648301,0.02525938,-0.007257999,-0.0066800364,-0.01167211,0.032300953,0.018671291,0.0042175255,-0.012097662,-0.011154793,-0.043896507,0.017636651,-0.054689772,0.0034233916,-0.013496985,0.043549147,-0.0040849685,0.0050400877,0.0047828113,-0.027769677,-0.055527173,-0.018333158,-0.031641018,0.011304786,-0.008056564,0.028306581,-0.07262209,0.005203758,-0.02480442,-0.025249096,-0.008325994,0.06669402,0.041332275,-0.009695193,-0.028068446,-0.05814952,-0.0023669838,0.070727274,-0.009771758,-0.005506965,-0.025992911,-0.041917156,-0.05569324,0.024974586,0.014980122,0.07301672,0.01203133,0.01003599,0.007895245,-0.03460596,-0.01482481,-0.010771593,0.012570377,-0.05937304,-0.042920336,-0.009796877,0.042934824,0.013941138,0.011932685,0.004983493,0.016554404,-0.00089039863,0.02498116,0.037909955,0.012672247,0.009093873,-0.026008977,0.0139111765,-0.012767574,0.032519575,4.5746787e-05,-0.005301474,0.03158011,0.0191685,0.012686957,-0.012536702,0.0069959643,-0.027878383,0.02167127,-0.010874843,0.03268911,0.029570973,0.03978643,-0.025180817,0.07426962,-0.032030575,-0.045598257,0.017288757,-0.06047307,-0.03367331,0.0012979477,-0.05934521,0.0013300544,0.031686924,-0.009063732,-0.014177302,0.012910656,-0.029312447,0.09295366,-0.059361923,0.030466022,-0.01734049,0.025661761,0.11236149,0.009446533,0.023043849,-0.031641174,-0.05374732,-0.044431403,0.031160068,0.059273783,0.007482334,-0.014445186,0.074202634,-0.021711117,0.023579663,0.0008270738,-0.02808543,-0.027302695,-0.0567479,-0.047750056,0.0364618,-0.01923109,0.016430883,0.0033745328,-0.0134824,-0.00024198648,0.03283433,-0.028427454,0.0119995065,0.020645928,-0.017374767,0.023720318,-0.05156728,0.02027272,0.024058206,-0.0021484643,-0.010184104,0.02997786,0.049178805,0.010327994,-0.024890542,0.01865021,0.0034275502,-0.004469752,-0.017096812,0.042904224,0.03785236,-0.01994623,-0.030258117,0.0016003979,0.03686786,0.033129133,0.0110097295,-0.022534937,-0.030872382,-0.027962366,-0.020253368,0.03744061,0.0046368227,-0.008812116,0.0017906049,-0.045090016,0.08839682,-0.0796598,-0.047206983,-0.04953727,0.023367113,-0.0339688,-0.005080672,0.008085407,-0.0071312725,-0.013856482,-0.014500193,0.065829985,0.060847953,-0.044392917,0.03499553,0.017735641,-0.024332881,-0.0057878997,-0.018958127,-0.008033013,-0.007845758,-0.020425862,-0.006267609,0.07814329,-0.004095398,0.014225682,0.034886103,-0.03286095,-0.08271228,-0.030896606,0.045302976,0.007278366,-0.013125621,-0.017186796,0.013728723,0.06831563,-0.038616013,-0.024196664,-0.0362113,0.022655975,-0.006012488,-0.018678289,0.023087071,0.0039328043,-0.049068194,0.005861165,-0.020019036,-0.04503293,-0.011223542,0.029798713,0.061711762,0.050205015,0.07260892,-0.037668176,-0.09707026,-0.029503567,-0.028201686,-0.016633853,-0.047694214,0.03563803,0.0008852343,-0.023156691,0.01184083,-0.011992589,0.003685603,0.03668903,0.011630452,-0.0023791436,-0.010629652,-0.031386577,-0.005035824,0.03748564,-0.009866217,-0.00022746064,0.059715576,0.037336912,-0.08042692,-0.060119633,-0.051398687,0.0077014547,0.038054366,0.010834306,0.020049836,0.0044377246,-0.008947658,-0.00824286,0.045392502,0.018039439,-0.038760416,-0.017999334,0.03589931,-0.011109794,0.041961897,-0.018355919,-0.011851291,-0.114711516,-0.011279145,-0.004718572,-0.078695625,0.04158842,0.0341392,-0.009573532,-0.009144993,-0.016353423,-0.0152031975,0.0048677083,0.04630565,0.016670488,0.022296054,0.013679957,-0.04551354,-0.046881866,-0.014455383,-0.045803055,0.022080848,-0.03441597,0.030973136,9.380984e-05,0.0042891987,-0.015336897,0.028368235,-0.007827035,-0.029279718,0.048828136,0.02224661,-0.017710075,0.068529315,-0.00608623,0.010701428,0.0689921,-0.0023811285,0.031872682,-0.014428338,0.0044208197,0.030199144,-0.00018461527,0.02415057,-0.045858737,-0.0860322,0.029480454,0.024463413,0.0019225597,0.037986405,-0.027274184,-0.027830552,-0.061138786,-0.052481405,-0.009465647,0.05337336,0.021169731,0.05225705,-0.052180346,0.02967854,-0.03296784,0.05506545,0.03560638,-0.024312016,0.008830418,-0.007937728]	Keywords: Transformers, Attention Mechanisms, Natural Language Processing, Computer Vision, Speech Processing, Positional Encoding, Efficiency, Long Sequences, GANs, Architectures, Survey, References, Deep Learning, AI, Neural Networks, Machine Learning, Transformer Architecture, Deep Learning Models, LazyFormer, Poolingformer, TransGAN, Deformable DETR, Informer, BERT, TransGAN, Transformers are RNNs, Deformable DETR, LazyFormer, Poolingformer, TransGAN\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What are some common applications of Transformers?\n- How do LazyFormer, Poolingformer, and TransGAN improve upon the original Transformer architecture?\n- What are the key aspects of Transformer architecture that are explored in these references?\n---\nSummary:\nThis is a list of references, seemingly from a survey on Transformers. It spans a wide range of applications, from natural language processing, computer vision, and speech processing, highlighting different aspects of Transformer architecture including positional encoding, attention mechanisms, and optimizations for efficiency and long sequences. The references explore variations such as TransGAN, LazyFormer, and Poolingformer.\nOriginal Text:\n- [60] Md. Amirul Islam, Sen Jia, and Neil D. B. Bruce. 2020. How much Position Information Do Convolutional Neural Networks Encode? In Proceedings of ICLR . https://openreview.net/forum?id=rjE3B6NkVb\n- [61] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. 2021. TransGAN: Two Transformers Can Make One Strong GAN. arXiv:2102.07074 [cs.CV]\n- [62] Angelos Katharopoulos, Apourov Vyas, Nikolaos Pappas, and Francois Fleuret. 2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings of ICML . 5156-5165. http://proceedings.mlr.press/ v119/katharopoulos20a.html\n- [63] Guolin Ke, Di He, and Tie-Yan Liu. 2020. Rethinking Positional Encoding in Language Pre-training. arXiv:2006.15595 [cs.CL]\n- [64] Salman Khan, Muzammal Naseer, Munawar Hayat, Sajd Wagesam, Zafir Shahbaz Khan, and Mubarak Shah. 2021. Transformers in Vision: A Survey. arXiv:2101.0116 [cs.CV]\nContextualized Text:\nThis is a list of references, seemingly from a survey on Transformers. It spans a wide range of applications, from natural language processing, computer vision, and speech processing, highlighting different aspects of Transformer architecture including positional encoding, attention mechanisms, and optimizations for efficiency and long sequences. The references explore variations such as TransGAN, LazyFormer, and Poolingformer.	{"tags": [], "doc_id": "aa371f0a-39a1-4688-a9d6-85a84180ae71", "summary": "This is a list of references, seemingly from a survey on Transformers. It spans a wide range of applications, from natural language processing, computer vision, and speech processing, highlighting different aspects of Transformer architecture including positional encoding, attention mechanisms, and optimizations for efficiency and long sequences. The references explore variations such as TransGAN, LazyFormer, and Poolingformer.", "doc_type": "text", "entities": [], "keywords": ["Transformers", "Attention Mechanisms", "Natural Language Processing", "Computer Vision", "Speech Processing", "Positional Encoding", "Efficiency", "Long Sequences", "GANs", "Architectures", "Survey", "References", "Deep Learning", "AI", "Neural Networks", "Machine Learning", "Transformer Architecture", "Deep Learning Models", "LazyFormer", "Poolingformer", "TransGAN", "Deformable DETR", "Informer", "BERT", "TransGAN", "Transformers are RNNs", "Deformable DETR", "LazyFormer", "Poolingformer", "TransGAN"], "key_objects": [], "contextual_text": "This is a list of references, seemingly from a survey on Transformers. It spans a wide range of applications, from natural language processing, computer vision, and speech processing, highlighting different aspects of Transformer architecture including positional encoding, attention mechanisms, and optimizations for efficiency and long sequences. The references explore variations such as TransGAN, LazyFormer, and Poolingformer.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some common applications of Transformers?", "How do LazyFormer, Poolingformer, and TransGAN improve upon the original Transformer architecture?", "What are the key aspects of Transformer architecture that are explored in these references?"]}
195eb340-8488-46ad-9251-0b10bf5ce636	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.012747026,0.022862755,-0.012434735,0.028421069,0.02683319,0.0886986,0.019952953,0.09524125,0.058202527,-0.027043242,0.0016596603,-0.0027795,0.0048863753,-0.04820013,-0.011678322,0.014617995,0.0025537615,0.07704344,0.03204671,-0.023383068,0.02853749,-0.010530403,-0.011948772,-0.017943222,0.02173643,0.030208712,0.06122053,-0.062480554,0.02232613,0.011903672,0.016144803,-0.03434693,0.023856148,0.011753874,0.0051492755,0.024564948,0.026273124,0.029107386,0.010691828,0.014869765,-0.0087470375,0.02537397,-0.061799213,0.012232558,-0.016313706,-0.054150574,-0.06192166,-0.020439416,0.026551196,-0.028020373,-0.036986385,0.012899971,-0.0069656037,-0.012016167,0.03876119,-0.055298686,0.00010503521,-0.029365899,0.03189773,-0.1081314,-0.026163325,0.024934093,-0.06135813,-0.0028319415,0.016924236,-0.077571884,-0.029530453,-0.022731986,0.010715033,0.11135441,-0.004261588,0.0186052,-0.010333662,-0.054474838,0.091557845,0.03374789,-0.014498446,-0.028731553,-0.067237064,0.0040138885,-0.0066134254,0.068910524,-0.05592318,-0.05081705,0.099946775,-0.0052436413,-0.021938132,0.0036392806,0.063866235,-0.0073075267,0.02772179,0.0024399855,0.01567968,0.07606473,-0.021057157,-0.09701988,0.0072841435,-0.035786115,-0.005194837,-0.050421413,-0.027511122,-0.028422454,0.07530189,0.07996453,0.04543686,-0.009180856,-0.020118875,-0.047944695,0.026785726,-0.01660406,0.006784401,0.0015754002,-0.0005993682,0.022715546,-0.025432201,-0.0077847135,-0.054577988,0.0010451704,0.030008111,0.011537747,-0.008233402,-0.021643076,0.017247891,-0.0089028515,0.060686,0.0548126,0.02830736,0.023019422,-0.031509507,-0.013943642,0.0026321232,-0.029570898,-0.032984678,0.021245431,0.00822519,-0.015827885,0.057719696,-0.06472057,0.009096601,0.023649419,0.039387077,-0.041757327,-0.03992937,0.019317865,-0.021553578,-0.008560517,-0.040053003,-0.0028622467,-0.012945228,0.054387134,-0.006918348,0.01967611,0.0024285899,0.023223892,0.016825162,-0.006532121,0.0069098086,-0.056168158,-0.01873433,-0.00667863,-0.028386587,-0.05345168,-0.03448811,0.074257754,0.011489207,0.12697235,-0.0010205536,0.015192167,0.039892763,-0.0046737804,-0.030327136,0.0036698813,0.0012548156,-0.050657935,0.026407948,-0.0045187045,0.02518459,-0.009636616,-0.052852295,-0.009742793,0.009877586,-0.020522416,0.009091328,0.03704965,-0.050393336,-0.00682741,-0.007265298,0.06828004,-0.024426429,-0.012391849,0.043957956,-0.010044176,-0.02731839,0.0018276272,0.030548878,-0.0037518793,0.043709334,-0.07856004,-0.08108761,0.00068876176,-0.009898224,-0.0067888247,0.020836296,-0.012016905,0.009596766,-0.012636365,-0.0039607133,-0.033188246,-0.040574167,-0.0037114657,0.0047321264,-0.03539345,0.0066345436,0.07659599,0.033471726,-0.039926384,-0.023256905,0.024603998,0.013343858,0.010139688,-0.053540464,0.005091259,-0.0014500818,-0.0042311577,-0.08483594,0.028307624,-0.042623818,0.0354812,0.001156911,-0.012138439,0.030483438,-0.0029852143,-0.00594105,0.027917393,-0.0075852084,0.066648625,-0.013314867,0.012248892,0.023477912,0.06955206,0.020786036,0.05711978,-0.0009988194,0.039313834,0.02216057,-0.0077647744,0.03199342,0.043512974,0.0021604854,0.0009880946,0.025432447,0.042552873,0.047613516,-0.006977495,0.013544185,-0.019810263,0.017869145,0.036819153,0.029110974,0.0041098637,-0.03789504,0.0017525731,0.046273854,0.012511112,0.012653986,0.029327298,0.02298229,0.02373892,0.03005703,-0.00020690323,0.027199958,0.041741423,-0.02320563,-0.0031832953,-0.052800097,-0.012122207,0.0032278043,0.016682444,0.051714938,-0.028294787,-0.0005608185,-0.049639247,0.048403822,0.002890972,0.013978706,0.00024532794,0.009782236,-0.035509773,-0.09158517,0.002441511,-0.0067871283,-0.045912217,0.030345421,-0.03539332,0.03624823,0.014719427,0.010132875,-0.017206246,-0.065502256,0.017202679,0.07477647,-0.041140005,0.034410868,0.009857966,0.0023245427,0.031248618,-0.058561087,0.0037058613,0.029413419,0.009415524,-0.0060143922,-0.006884112,-0.012626571,0.0077836267,0.03449443,0.086723156,0.031333167,-0.015851041,0.0069899857,-0.0075580804,0.007457707,-0.02728066,0.0034557967,0.051816743,0.020147212,-0.04004015,-0.010762499,-0.035608184,-0.07733814,0.030408897,-0.09351691,0.063693,0.043327063,-0.014676697,-0.018426506,0.0043729176,0.050834477,0.025052706,-0.0061937277,0.013068066,-0.019347597,-0.06944026,-0.01468084,-0.041155618,-0.0062791547,0.028419258,0.011828222,0.0024023373,0.04941845,0.0036838073,2.8127639e-05,0.056914996,0.06312415,0.007440153,0.0009740125,-0.03385558,-0.0043041287,0.0060564787,0.014628873,-0.016505348,0.03756603,-0.020471284,0.09938988,-0.011214483,0.015221788,-0.006936391,-0.082088724,-0.021357229,0.04243501,0.006645949,-0.02067025,-0.034594677,0.0566695,-0.01746468,-0.005100385,0.012271194,0.027716642,0.020777076,-0.07095651,0.0048247525,0.0017105493,0.037869334,-0.04436176,-0.021968054,-0.033278238,0.029713387,0.011075127,0.0143325,0.007630233,0.032559652,-0.04648815,0.0006832312,0.064151555,0.00745521,-0.0022990624,-0.0016739117,0.011893702,0.036483858,-0.014214793,-0.02783688,0.028133906,0.012140093,-0.029585784,0.03735169,0.05484969,0.022782518,-0.058747623,0.0145304995,0.054094613,0.015734289,0.047394317,-0.018254109,-0.025205811,0.034195356,-0.02654538,-0.009802187,-0.010846461,0.035676733,-0.047669772,0.004075641,-0.054092184,0.06413481,0.016087575,0.044876415,0.04241913,-0.034246575,-0.0159611,0.032506254,-0.015502661,-0.03900353,0.01624898,-0.106704116,-0.02037589,0.08354162,0.048429973,0.046680246,-0.08120292,0.052763958,-0.08403105,0.015264698,0.05786734,-0.00116407,0.004906672,-0.012354871,-0.02753165,-0.018053945,0.0024677892,0.02345701,-0.006697032,-0.02051791,-0.0017570474,0.045879856,0.01693575,0.0019411392,0.011037666,-0.03759404,-0.026762268,0.0019589649,-0.038438093,-0.0011884858,-0.0011227339,0.045509502,-0.0018361057,0.012263539,0.022357773,-0.010203786,-0.035819437,-0.0023379861,-0.025416246,-0.017523704,0.011158068,0.02769874,-0.052303452,-0.03710505,-0.049119223,-0.026986254,-0.0009564522,0.047459755,0.05527544,-0.011014422,-0.044803318,-0.05960696,0.009938918,0.060777843,-0.03290038,-0.013386838,0.0049169823,0.005263176,-0.080649674,0.026213145,0.009692667,0.068561405,0.015046084,0.040284824,0.02694571,-0.010400351,-0.0048500397,0.008641088,0.037742615,-0.06004942,-0.055800248,-0.006519273,0.043931264,0.024933407,-0.007444674,0.0034395538,0.00877881,0.00044530391,0.003562858,0.028024685,-0.006235551,-0.0014507192,-0.016342003,-0.0023920792,-0.0044769426,0.029805737,-0.015978249,-0.012256252,0.019137975,0.050920174,-0.016839962,-0.017372036,-0.0016519773,-0.011306854,0.014772579,-0.020813612,0.027524183,0.058042612,0.08376277,-0.058940828,0.049688317,-0.042676236,-0.050765254,0.020321064,-0.057585422,-0.012152119,-5.3494994e-05,-0.06330959,-0.009683471,0.01664611,-0.014958314,-0.016371636,0.028619483,-0.041012716,0.08967596,-0.04795231,0.024202019,-0.008600693,-0.004076864,0.09551815,-0.012261924,0.0007179025,-0.041552253,-0.044803683,-0.04140626,0.009554953,0.07241334,-0.01948837,0.010793111,0.07317584,0.013130179,0.055978812,0.011762858,-0.05052666,-0.042249262,-0.043573916,-0.027677275,0.023232725,0.00228505,0.017590044,0.004899441,-0.0048625604,0.024527205,-0.005601886,-0.050079696,0.04965468,0.029744701,-0.005899033,0.037110366,-0.015498709,0.023745911,0.046423186,0.0066489098,-0.020274285,0.017766882,0.031630345,0.017236857,-0.02462767,0.008689273,0.01126565,-0.030083865,-0.043500867,0.036321577,0.06219717,-0.029600952,0.009480421,0.012040925,0.044389322,0.0043273,-0.02234763,-0.033711724,-0.043365292,-0.04465427,-0.051217,0.025472784,-0.01645984,0.008634167,0.019182846,-0.045747556,0.08035788,-0.10928974,-0.028244162,-0.058180485,0.027348528,-0.007358469,-0.004094878,0.007502727,-0.017282926,-0.04519835,-0.016541345,0.06376333,0.052169703,-0.04074348,0.008131501,0.00056428,-0.007003274,-0.034664676,-0.022035688,-0.02470875,-0.011334193,0.022806138,-0.009002913,0.061598312,-0.012285727,0.010473432,0.044873767,-0.052897055,-0.09745241,6.3430016e-05,0.039340362,0.0083762305,0.010314164,-0.022470064,0.009185833,0.029636946,-0.059869684,-0.024357319,-0.042135704,0.028889433,0.015035143,-0.018666556,0.017463854,0.0032320875,-0.038912356,0.003782288,-0.017190466,-0.03867869,-0.009172354,-0.003218728,0.04216002,0.028625874,0.04013749,-0.058088183,-0.095304824,-0.045522586,-0.035597097,0.005415964,-0.037693493,0.060182106,-0.013912269,0.0021710633,0.016423615,-0.029116405,0.0041812444,0.036735877,0.019127196,-0.01837146,0.0053143282,-0.04215995,0.013632131,0.019954145,-0.012793503,0.03196443,0.0408101,0.013693588,-0.086200036,-0.03297995,-0.034808785,-0.0014505681,0.03284365,-0.0046939724,0.042830244,0.01595359,-0.0014708184,-0.0025172068,0.047941495,0.015630363,-0.0010689308,-0.027897792,0.021551393,-0.0045077563,0.03837743,-0.016109452,-0.00048895524,-0.11516896,0.007671827,-0.0027706637,-0.032848995,0.053654574,0.032629676,-0.011846938,-0.029925652,-0.02437715,-0.0033759726,0.0042505153,0.036057286,0.019350773,0.030379435,0.025839353,-0.030300595,-0.050921053,-0.012415472,-0.051041663,0.021024743,-0.04527245,0.016638523,0.0023343815,0.0077155065,0.011717789,0.047426984,-0.00052348035,0.0063651297,0.03194967,0.017001169,-0.061661683,0.06370827,-0.011993807,0.0025713462,0.0327061,0.01657574,0.03727456,0.0057445546,0.018278066,0.004472225,-0.0059219277,0.018905602,-0.031788964,-0.08148021,0.055880006,0.040403683,0.01869564,0.042550348,-0.014351782,-0.025408795,-0.047835875,-0.02786865,-0.019530578,0.05165253,0.032572825,0.023662403,-0.07828933,0.006073195,-0.049088635,0.044261254,0.050800856,-0.026518269,0.0066258665,0.0016891315]	Keywords: transformers, nlp, computer vision, machine learning, attention mechanisms, long sequences, efficiency, architectural modifications, surveys, speech enhancement, machine translation, object detection, reformers, deformable detr, memer, attention mechanisms, attention, self-attention, transformers architecture, efficient transformers, survey of transformers, transformer models, transformers in vision, transformer implementation, transformer application, transformers overview, transformer tutorial, transformers architecture survey, transformers tutorial, efficient transformer models, scalable transformers, transformers with reduced complexity, transformers survey\nKey Objects: Transformers, Architectures, Models, Implementations, Applications\nRefers to Images: None\nHypothetical Questions:\n- What are the main challenges addressed by modifications to the Transformer architecture?\n- Can you describe the evolution of Transformer models?\n- What are some of the applications of Transformers in NLP and Computer Vision?\n- How do the various Transformer modifications improve efficiency?\n- What are some of the key papers that have contributed to the development of Transformers?\n---\nSummary:\nThis is a comprehensive list of references related to Transformers, a prevalent architecture in various machine learning tasks, particularly in Natural Language Processing (NLP) and Computer Vision. The references cover a wide range of topics, including surveys, applications (speech enhancement, machine translation, object detection), architectural modifications (Reformers, Deformable DETR, Memer, etc.), and efficiency improvements. The list showcases the rapid evolution of the Transformer model and the ongoing research aimed at addressing challenges like computational cost and handling long sequences.\nOriginal Text:\n- [64] Salman Khan, Muzammal Naseer, Munawar Hayat, Sajd Wagesam, Zafir Shahbaz Khan, and Mubarak Shah. 2021. Transformers in Vision: A Survey. arXiv:2101.0116 [cs.CV]\n- [65] Jaeeyoung Kim, Mostafa El-Khamy, and Jungyoon Lee. 2020. T-GSA: Transformer with Gaussian-Weighted SelfAttention for Speech Enhancement. In 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020. IEEE 6649-6653. https://doi.org/10.1109/ICASSP40776.2020.9053915\n- [66] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The Efficient Transformer. In Proceedings of ICLR . https://openreview.net/forum?id=rkNGkHbV\n- [67] Guillaume Klein, Yoon Kim, Yuntian Deng, Jean-Jacquier, and Alexander Rush. 2017. OpenNMT: Open-Source Toolkit for Neural Machine Translation. In Proceedings of ACL . 67-72. https://www.aclweb.org/anthology/P17-4012\nContextualized Text:\nThis is a comprehensive list of references related to Transformers, a prevalent architecture in various machine learning tasks, particularly in Natural Language Processing (NLP) and Computer Vision. The references cover a wide range of topics, including surveys, applications (speech enhancement, machine translation, object detection), architectural modifications (Reformers, Deformable DETR, Memer, etc.), and efficiency improvements. The list showcases the rapid evolution of the Transformer model and the ongoing research aimed at addressing challenges like computational cost and handling long sequences.	{"tags": ["references", "transformers", "machine learning", "deep learning", "nlp", "cv", "attention mechanisms", "survey"], "doc_id": "195eb340-8488-46ad-9251-0b10bf5ce636", "summary": "This is a comprehensive list of references related to Transformers, a prevalent architecture in various machine learning tasks, particularly in Natural Language Processing (NLP) and Computer Vision. The references cover a wide range of topics, including surveys, applications (speech enhancement, machine translation, object detection), architectural modifications (Reformers, Deformable DETR, Memer, etc.), and efficiency improvements. The list showcases the rapid evolution of the Transformer model and the ongoing research aimed at addressing challenges like computational cost and handling long sequences.", "doc_type": "text", "entities": ["Transformers", "NLP", "Computer Vision", "Machine Learning", "Attention Mechanisms", "Reformers", "Deformable DETR", "Memer", "Speech Enhancement", "Machine Translation", "Object Detection", "Attention", "Self-Attention", "OpenNMT", "BERT", "T-GSA", "LazyFormer", "Informer", "WaveNet", "HiBERT", "Deformable DETR", "OpenNMT", "BERT", "T-GSA", "LazyFormer", "Informer", "WaveNet", "HiBERT"], "keywords": ["transformers", "nlp", "computer vision", "machine learning", "attention mechanisms", "long sequences", "efficiency", "architectural modifications", "surveys", "speech enhancement", "machine translation", "object detection", "reformers", "deformable detr", "memer", "attention mechanisms", "attention", "self-attention", "transformers architecture", "efficient transformers", "survey of transformers", "transformer models", "transformers in vision", "transformer implementation", "transformer application", "transformers overview", "transformer tutorial", "transformers architecture survey", "transformers tutorial", "efficient transformer models", "scalable transformers", "transformers with reduced complexity", "transformers survey"], "key_objects": ["Transformers", "Architectures", "Models", "Implementations", "Applications"], "contextual_text": "This is a comprehensive list of references related to Transformers, a prevalent architecture in various machine learning tasks, particularly in Natural Language Processing (NLP) and Computer Vision. The references cover a wide range of topics, including surveys, applications (speech enhancement, machine translation, object detection), architectural modifications (Reformers, Deformable DETR, Memer, etc.), and efficiency improvements. The list showcases the rapid evolution of the Transformer model and the ongoing research aimed at addressing challenges like computational cost and handling long sequences.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the main challenges addressed by modifications to the Transformer architecture?", "Can you describe the evolution of Transformer models?", "What are some of the applications of Transformers in NLP and Computer Vision?", "How do the various Transformer modifications improve efficiency?", "What are some of the key papers that have contributed to the development of Transformers?"]}
2a44b7b9-b089-44b9-8ca3-db97f6ad54dc	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0042193295,0.0050283424,-0.03964778,0.04976337,0.021970123,0.056111395,0.0035014153,0.07823825,0.067577824,-0.018620191,-0.01865416,0.029203393,-0.02461666,-0.021992723,-0.044196654,0.06936264,0.010889303,0.06473729,0.034726743,-0.02315973,0.047428392,0.025060967,0.012758751,-0.04100742,-0.018770544,0.008082079,0.014349883,-0.027371213,0.02717255,0.051078342,0.021048646,-0.068892315,0.02560287,-0.011841965,0.026353497,0.024506917,-0.003363809,0.00879671,0.035637733,-0.004225601,-0.0063306107,0.04206359,-0.0504766,0.0017073078,-0.00041876175,-0.057628814,-0.07407423,0.003326429,0.030295333,-0.03284609,-0.020461924,0.0203322,-0.014715487,-0.007993165,0.008778581,-0.01902878,-0.051241353,-0.023614643,0.0260789,-0.08093504,-0.013416557,0.009408636,-0.03462985,-0.020122094,0.0224731,-0.066414416,-0.021156738,0.024258254,0.026653748,0.11701993,-0.010736609,0.02469586,-0.011727383,-0.049680907,0.08068363,0.037787955,-0.0010436674,-0.022690577,-0.06674494,-0.030709209,-0.033845115,0.048074905,-0.03822647,-0.022739578,0.074671045,-0.0020838536,-0.045421455,-0.015787069,0.07815856,-0.025862448,0.021956813,-0.013376691,-0.009077852,0.06830483,-0.025766492,-0.07441524,0.05198046,-0.035805482,0.010661069,-0.06423466,-0.014901763,0.0059756315,0.07295133,0.07846807,0.021303391,-0.014207165,-0.05201946,-0.038449205,0.008713956,0.033099767,-0.0003545643,0.02447175,-0.022349704,-0.013674689,0.0018451103,-0.010798678,-0.084120676,0.0023094264,-0.0019531278,-0.0028069252,0.0065129963,-0.041572426,0.010322253,0.049025815,0.07624232,0.06311344,-0.0045391778,0.012002832,-0.038330972,-0.005130758,0.0006208047,-0.025794761,-0.013684307,0.01854358,0.040181093,0.018429426,0.060577184,-0.03896736,0.01576545,-0.0050645894,0.031009834,-0.041991033,0.015304768,0.057082213,-0.020832049,0.015845567,-0.03864452,-0.0071107596,-0.013667084,0.027793229,0.037412044,0.027004832,0.03719476,0.028970191,0.045446526,-0.0079089105,0.0029653213,-0.033464517,0.022274423,0.03184934,-0.044046167,-0.0652646,-0.028939428,0.030200602,0.033966057,0.10477314,0.0117720375,-0.008866963,0.024819834,-0.016056951,-0.029943619,0.02544125,-0.016375966,-0.025914093,0.014383237,-0.0097041875,0.00856553,0.0039506205,-0.019575736,0.009222723,0.00013593725,0.024193408,0.014573979,0.045215126,-0.010203627,0.024721406,0.020408945,0.080279134,-0.023372712,-0.033981916,0.037827082,-0.015447167,-0.033504847,0.005271416,0.031694304,-0.007752042,0.0106980195,-0.070550226,-0.043977167,-0.016984038,-0.03858149,-0.0093577765,0.039616656,0.020387549,0.025905881,-0.013530306,-0.040561646,-0.04883097,-0.030780481,-0.011254344,-0.010677265,-0.04966713,-0.026582418,0.06688341,0.05198326,0.011194471,0.0011827727,0.02870651,0.020454448,-0.0029953464,-0.069007754,-0.009885267,-0.031231264,0.0335962,-0.048577048,-0.010876504,-0.026283093,0.04004613,0.026482534,-0.03695972,0.040820833,0.01349956,-0.023840439,0.024925224,0.004115059,0.038750242,-0.0036625497,-0.017097695,0.02913035,0.06435804,0.019108497,0.052805983,0.021930614,0.06705782,0.009121625,-0.0007787779,0.03584214,0.058484077,0.0043668067,0.041526906,0.008919444,0.060980845,0.04461677,0.016002426,0.030967494,-0.0524631,0.0031395953,0.027050184,0.055182386,-0.0046689645,0.0014966796,0.0011674819,0.0140384855,-0.0035376777,0.036059104,0.010188097,0.000110353525,0.03846122,0.037680622,0.02220978,0.047223926,0.059643026,-0.0034877171,-0.03230533,-0.075014435,0.007053003,0.012822574,0.012655462,0.034543607,-0.0015282746,-0.019657055,-0.015844751,0.064751886,-0.021915294,0.035688043,-0.012811709,-0.020820526,-0.022376936,-0.075195685,0.005386031,0.0037950822,-0.036864497,0.0064439327,-0.011199372,0.02183708,0.03480861,0.00077001535,-0.03728885,0.0008636768,0.002332003,0.118985504,-0.039313022,0.041181225,-0.016340131,0.016013622,0.024694737,-0.080191836,-0.0041195815,0.053461015,0.025213365,0.023705792,-0.026228094,0.023735728,-0.0027997426,0.03237804,0.075082615,0.045099236,-0.0005825861,-0.031372216,0.023226965,0.0052321656,0.019812522,0.011187,0.06275097,0.035944052,-0.02477055,4.6025416e-06,-0.045078237,-0.08610083,0.0360024,-0.051363204,0.050157674,0.03570335,0.03049248,-0.006838633,-0.016938893,0.05487144,0.03421934,0.0038996732,0.01651237,0.0062324298,-0.058921766,0.014268385,-0.0071520824,0.0026301146,0.028319348,0.018984284,0.030649975,0.042207222,0.0070359292,0.0056698616,0.03477773,0.033520382,0.015560887,-0.028337754,-0.013669425,0.011021858,0.0010396191,-0.019322598,0.022977613,0.054065224,-0.012824034,0.07812165,-0.009398601,0.007340205,-0.0050620497,-0.08033833,-0.036373287,0.040577322,-0.038899124,0.01685164,-0.031491663,0.029194048,-0.04779216,-0.006522706,-0.02699714,0.010304721,0.017812552,-0.05468253,-0.032021612,-0.025535773,0.032203138,-0.06672043,-0.02023684,-0.03757598,0.05641618,-0.01269476,0.037782434,-0.011453371,0.034086805,-0.03896903,0.009555528,0.0033779663,0.015268376,-0.010147086,-0.0016185553,0.0365924,0.03535825,0.007902126,-0.05577306,0.017550843,0.027902337,-0.05735346,0.04900487,0.04294491,0.013156235,-0.099871464,-0.002360252,0.08172086,0.021333914,0.043308828,-0.024331205,-0.015131465,0.0126994625,-0.023108281,-0.015574397,-0.0045365104,0.018246036,-0.07138143,0.027181813,-0.04016991,0.045452133,0.026021019,0.06714434,0.059071608,-0.023509627,-0.021017859,-0.0026557592,-0.006477499,-0.07054585,0.028357802,-0.0394505,-0.003442556,0.04937266,0.04233439,0.031033978,-0.04889478,0.07609119,-0.08436501,0.0063430234,0.08065004,-0.002333823,0.01644828,-0.044860512,-0.0034278864,-0.042986102,0.001659407,0.023974478,-0.017172316,0.007888664,-0.028007373,0.03996783,0.019280002,-0.007795662,0.0100175245,0.0016615381,-0.0353772,-0.008851805,-0.07346636,-0.028613301,0.0033984396,0.025029646,-0.0026836537,-0.00874214,-0.013745894,-0.022626296,-0.021693325,-0.016694883,-0.0017863441,0.005988128,-0.006243346,0.015688758,-0.054851957,0.017931528,-0.016240615,-0.024080351,-0.019377392,0.052510116,0.04753362,-0.005548103,-0.045258585,-0.05618588,0.025200078,0.060954317,-0.04676751,-0.014432332,-0.041776896,-0.035762217,-0.04347566,0.026438074,0.032387488,0.080230646,0.021423897,0.01937979,0.017471267,-0.049801886,-0.01780484,0.010877587,0.012508185,-0.049545158,-0.036176946,-0.026789133,0.073426135,0.013497621,0.04679635,-0.0020620737,0.008474059,0.012552813,0.017632946,0.0607792,0.04411832,0.009412988,-0.018905254,0.009440869,-0.0034405158,0.026943987,-0.008018041,-0.010248724,-0.0044447626,0.011125098,0.023604224,-0.020911332,0.0058085173,-0.021497771,0.018530447,0.0012563732,0.019113904,0.032912605,0.037160285,-0.0363904,0.074408814,-0.013963244,-0.05072696,0.006716297,-0.060197327,-0.020271622,0.017970819,-0.069432035,-0.006192563,0.041125692,-0.027595405,-0.03003138,0.0019823015,-0.011498063,0.11009521,-0.070160486,0.016073901,-0.0356276,-0.010236226,0.09902766,0.043127,0.030562742,-0.0252906,-0.023902064,-0.05028078,0.043268956,0.081271075,-0.005763272,0.0029296032,0.03849764,0.0007176957,0.065666616,-0.0061013363,-0.019545032,-0.033389404,0.011063962,-0.038694374,0.01627473,-0.034539282,0.025072627,0.014004524,0.0127355,0.00045667478,0.020598914,-0.03826113,-0.0013813863,0.025269113,-0.046739038,0.0059877057,-0.060949832,0.050321214,0.03502086,0.013953895,-0.0031664702,0.03892727,0.03975554,0.015223434,-0.027253037,0.028654845,-0.0046206433,-0.011811591,-0.0043462566,0.021851035,0.042492334,-0.026154373,-0.0122744795,-0.0019731172,0.032762975,0.0462386,-0.05010825,-0.0504333,-0.0050244625,-0.017544335,-0.020317515,0.008659113,0.029904282,-0.010371183,0.0073689613,-0.023657782,0.08266532,-0.08533495,-0.035632797,-0.026466373,0.0061869835,-0.036713917,-0.0029321036,0.04111908,-0.005137541,-0.009705793,-0.017671354,0.06617741,0.037375584,-0.06040404,0.031121897,0.0039451052,-0.0217993,-0.0014279238,-0.03737182,-0.03316177,-0.011763171,-0.023051767,-0.017369721,0.07986573,-0.009508907,-0.0011148129,0.020516988,-0.0025292,-0.07999897,0.0023655042,0.051932774,0.022445332,-0.021804221,-0.017274946,0.028564727,0.046541568,-0.049938396,-0.0031992765,-0.06761942,0.037351362,-0.015815066,-0.00998472,0.017533384,0.020019753,-0.07958845,0.007554709,-0.012344782,-0.04043457,0.0052255667,0.024691647,0.061825585,0.06340829,0.07393879,-0.035075545,-0.08225665,-0.022879185,-0.024362346,-0.00034335087,-0.03212127,0.043646984,0.015761571,-0.03073443,-0.0100417575,-0.029810986,0.0016702695,0.030431628,0.01590881,-0.018283382,-0.01387511,-0.038287185,-0.020425467,0.03412853,-0.009335277,0.0042564967,0.049136735,0.06798263,-0.064519204,-0.05071792,-0.021255706,-0.0035821158,0.013901522,-0.0010024679,0.016136643,-0.007346026,-0.019323511,0.0061097173,0.038388047,0.004214983,-0.014346742,-0.01849614,0.03631493,-0.026412148,0.03581855,-0.019986868,-0.04489971,-0.077732764,0.006859332,-0.029194828,-0.04694502,0.039326157,0.04688157,0.0057735555,-0.018470814,-0.026502253,-0.027362574,0.026037212,0.05915015,-0.0006603204,0.03445921,0.027002439,-0.04474396,-0.04568925,-0.011714598,-0.019547071,0.030597571,0.0061074975,0.055398133,0.0056071454,0.009794665,-0.021388859,0.011794639,-0.025712837,-0.016321598,0.036157727,0.03914368,-0.030047677,0.058908187,-0.009816137,0.010165722,0.059309732,-0.00922841,0.034653164,-0.010913682,0.022551294,0.02155643,0.012953173,0.0060743815,-0.028259505,-0.07781434,0.006209695,0.011784341,0.0014440011,0.053213175,-0.017375324,-0.0369753,-0.06585492,-0.041669782,-0.033307657,0.07248241,0.0026268375,0.0031187742,-0.048660137,0.047608785,-0.02977763,0.052379854,0.01683175,-0.006982806,0.039038476,-0.011406127]	Keywords: Transformers, Neural Networks, Machine Learning, Text-to-Image Generation, Computer Vision, Protein Prediction, Sparse Models, Large Language Models, Attention Mechanisms, Deep Learning, Artificial Intelligence\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What are some key areas where Transformer models are being applied?\n- What is the timeline of research related to Transformers?\n- What are some examples of how Transformers are being used for specialized tasks?\n---\nSummary:\nThis is a list of references related to Transformers, covering a wide range of applications including text-to-image generation, multi-domain visual learning, protein structure prediction, sparse model development, and more. The references span from 2017 to 2021, indicating a recent surge in Transformer-based research and development.\nOriginal Text:\n- [107] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. arXiv:2102.12092 [cs.CV]\n- [108] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Proceedings of NeurIPS . 506-516. https://proceedings.neurips.cc/paper/2017/hash/ e7b24b112a44ffdd9ee93bd998cca0e-Abstract.html\n- [109] Alexander Rives, Joshua Meier, Tom Sercu, Siddharth Goyal, Zeming Lin, Jason Liu, Demi Guo, Myle Ott, C. Lawrence Zitnick, Jerry Ma, and Rob Fergus. 2021. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. Proceedings of the National Academy of Sciences 118, 15 (2021). https: //doi.org/10.1073/pnas.2016239118\n- [110] Stephen Roller, Sainbayuk Shatbaatar, Arthur Szlam, and Jason Weston. 2021. Hash Layers For Large Sparse Models. arXiv:2106.04426 [cs.LG]\nContextualized Text:\nA survey of Transformer architectures and their applications.	{"tags": [], "doc_id": "2a44b7b9-b089-44b9-8ca3-db97f6ad54dc", "summary": "This is a list of references related to Transformers, covering a wide range of applications including text-to-image generation, multi-domain visual learning, protein structure prediction, sparse model development, and more. The references span from 2017 to 2021, indicating a recent surge in Transformer-based research and development.", "doc_type": "text", "entities": [], "keywords": ["Transformers", "Neural Networks", "Machine Learning", "Text-to-Image Generation", "Computer Vision", "Protein Prediction", "Sparse Models", "Large Language Models", "Attention Mechanisms", "Deep Learning", "Artificial Intelligence"], "key_objects": [], "contextual_text": "A survey of Transformer architectures and their applications.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some key areas where Transformer models are being applied?", "What is the timeline of research related to Transformers?", "What are some examples of how Transformers are being used for specialized tasks?"]}
0dd87b46-7247-4e1c-913e-4f80b38d560c	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0114892535,0.007457549,-0.028602012,0.026511895,0.043534067,0.07392471,0.023757072,0.059140474,0.04018509,-0.04581826,-0.0077949506,0.009825081,0.010290781,0.0032023743,-0.020653876,0.041621383,-0.0108456705,0.05517488,0.023295004,-0.03435627,0.047816154,0.0040201778,-0.004028162,-0.024039343,0.017161548,0.041790325,0.0040960796,-0.019137578,-0.0022049432,0.006708014,0.023262797,-0.057650458,0.015661122,-0.008175562,0.030207137,0.0066034193,0.01036564,0.020657575,0.015702073,-0.023041723,0.00034699123,0.04919738,-0.04822788,-0.017052518,-0.00083248015,-0.055516053,-0.08924261,-0.04725896,0.053842522,-0.017670067,-0.0056428183,0.019255864,-0.016766299,-0.039047465,0.0017696442,-0.0037476106,-0.0059136623,-0.05106389,0.037426703,-0.1065321,-0.03882822,0.035971873,-0.037050042,-0.027522985,0.02774179,-0.051559214,-0.027180739,-0.005283431,0.01953748,0.10267874,-0.012541461,0.047781523,-0.030166233,-0.07600273,0.09055121,0.0539828,0.0007597497,0.0013606049,-0.08092742,-0.00035432234,-0.020070368,0.053312816,-0.031620506,-0.05854451,0.0685101,-0.025132118,-0.027015837,-0.0028269957,0.09444451,-0.026597364,0.010462892,-0.00654205,0.018096715,0.076153666,-0.03127839,-0.059326805,0.0069179656,-0.013120679,0.007112209,-0.05352264,-0.021619903,-0.011481629,0.08208319,0.09283296,0.010519209,-0.0038794258,-0.06398728,-0.053675905,0.0057813413,0.017020872,0.009718082,0.0015902093,-0.02606331,-0.004772891,-0.021957878,-0.03488359,-0.0710532,0.0060137454,-0.027191672,-0.01562606,0.015738592,-0.037365675,0.012313924,0.007704496,0.038811654,0.045433212,0.0017041668,0.005047359,-0.04677191,0.007205079,-0.00071089633,-0.024483107,-0.017844576,0.029835992,0.030066228,0.0037068964,0.057130344,-0.024746886,-0.0011716692,-0.010839678,0.017094597,-0.009913363,0.0011755086,0.018460436,-0.028206207,-0.016184175,-0.058850322,0.003852168,-0.02994697,0.03877608,0.019134248,0.023010274,0.05796724,0.00624228,0.016162137,-0.017794019,0.0012310046,-0.053739876,-0.0031768482,0.005592924,-0.06574372,-0.047130827,-0.015103663,0.0394943,0.026887182,0.13484216,0.020911671,-0.014185203,0.039940305,0.008425284,-0.028642572,0.037174176,-0.024081018,-0.025913965,0.008557285,-0.007348256,0.025121864,-0.020843731,-0.042452063,0.008043029,-0.007261424,0.020640997,0.02154986,0.024285285,-0.02171661,0.014640951,0.020955395,0.058030605,-0.019610705,-0.031935003,0.029697923,-0.0036980084,-0.013738481,0.020772895,0.015288624,0.02634696,0.014284257,-0.05826707,-0.044713933,-0.040487535,-0.018519267,-0.0023657763,0.00760529,0.019969394,0.012325059,-0.016065724,-0.019379217,-0.04574965,-0.025145123,-0.00994192,-0.017526226,0.011573309,-0.013818249,0.07306424,0.050982453,-0.010776813,-0.013891897,0.017656676,0.042298023,-0.011293865,-0.07066459,-0.013702382,-0.012192339,0.02894439,-0.017466564,0.005218911,-0.020417238,0.036375273,-0.00021326254,-0.02676918,0.04581812,-0.0009175432,-0.024450028,0.044043273,-0.01923759,0.05488818,-0.005841674,-0.014028429,-0.0068514682,0.060539834,0.054298405,0.0691043,0.039070368,0.05524272,0.004654033,0.0022404369,0.055249456,0.052737407,-0.023065003,0.036759943,-0.0057518687,0.051491965,0.0377562,-0.005057944,0.008260835,-0.036480635,0.025327263,0.038619827,0.052689157,0.0065112147,-0.004719032,-0.02846656,0.030086128,-0.012734157,0.012103969,0.040880974,0.02128404,0.043494683,-0.0034120097,0.05812246,0.025337413,0.039985623,-0.004259656,-0.023457326,-0.08013405,0.02865269,0.0023453066,0.01504137,0.026294671,-0.008238047,-0.0025292214,-0.015486593,0.049828544,-0.03645577,0.04987452,-0.027462037,0.0052444236,-0.0035284946,-0.049915005,0.02046972,0.0023291043,-0.03010307,0.009902417,-0.01665183,-0.0006051575,0.010349167,0.013242734,-0.03310566,0.0009460886,0.0147944335,0.09046431,-0.028488934,0.022558535,0.01880732,0.027599957,0.033087883,-0.06179496,0.008042735,0.04364187,0.022693941,0.004829384,-0.0038611763,-0.00089041115,-0.005678567,0.0076181726,0.0921434,0.06094677,-0.0031316122,-0.004797865,0.03332593,0.023483023,0.001583145,0.00054993853,0.036956552,0.04384495,-0.0031470584,-0.0048835548,-0.051923145,-0.10409988,0.05322108,-0.09653931,0.06160866,0.012573173,-0.0013251982,0.012002762,-0.01110576,0.048423998,0.039997824,0.012596867,0.033201363,0.0011978521,-0.07705667,0.025902681,-0.0020834766,-0.01756803,0.03850402,0.015441748,0.027324012,0.056123085,0.014931328,-0.026420532,0.037690718,0.039887883,0.029983798,0.011124364,-0.005419116,-0.0078071738,0.0031985904,0.0034907947,0.01151008,0.056845423,-0.038760282,0.08829948,-0.045262314,0.027662741,-0.0013703929,-0.08056464,-0.020440249,0.056820475,-0.015140976,0.008727168,-0.009862957,0.060706083,-0.044672843,-0.027517142,-0.017999034,0.040466003,0.020265747,-0.046348102,-0.021330228,-0.034838088,0.025524344,-0.05538961,-0.015196515,-0.038668282,0.06522391,0.030744772,0.037033245,-0.0062472704,0.031771496,-0.031625085,0.036757227,0.027342383,-0.0010454268,-0.0153153045,-0.0015760069,0.04533135,0.039873734,0.009273451,-0.030350113,0.044544473,0.040449783,-0.04602219,0.027268924,0.07523008,0.0025168012,-0.10053906,0.0328509,0.06253797,0.016838498,0.027585294,-0.049332805,-0.020848444,0.041456122,-0.0022157945,-0.028794816,0.0059510972,0.016904123,-0.04297274,0.0032976323,-0.02836377,0.06297562,0.0244768,0.011836784,0.035865836,-0.031116694,-0.022818483,0.027106134,-0.017034827,-0.065825105,0.04223744,-0.06838118,-0.005668044,0.04236344,-0.0011056854,0.026088443,-0.05588833,0.06459306,-0.101595014,0.033857156,0.056544475,-0.01698822,-0.002018542,-0.04089481,0.0019124373,-0.035150636,-0.00048536548,0.019525781,-0.030032275,-0.008516771,-0.0034142637,0.04314203,0.016693143,0.0071125203,0.01093668,-0.026930468,-0.017768364,-0.013433826,-0.049391087,0.00034729103,0.01157347,0.032035723,-0.020851852,0.013219811,0.011470975,-0.010315528,-0.02226566,-0.03017019,-0.002185354,-0.03311837,0.0028398132,0.011881715,-0.05043288,0.016098304,-0.0063905222,-0.020203743,0.004443959,0.026459962,0.06636082,-0.00290341,-0.033743385,-0.07063945,0.009491392,0.046808973,-0.019451024,-0.019240562,-0.032972578,-0.0348972,-0.05186505,0.0398359,0.042131808,0.058937553,0.020615565,0.020225856,0.016187796,-0.0309099,-0.014481149,-0.006513923,-0.0060289726,-0.0449298,-0.03306249,0.0035300564,0.05607659,0.026498618,0.0037400944,-0.0094579,0.009263664,-0.0024401974,0.01876033,0.040835325,0.021716783,-0.005300431,-0.0050014253,-0.0030478905,-0.019123703,0.060977228,-0.008608977,0.008880382,0.013049114,0.022945514,0.018269155,-0.0138937775,-0.003935703,0.005410807,0.013600601,0.004254156,0.050822593,0.059701655,0.058573946,-0.05153986,0.06668655,-0.029256273,-0.03935193,0.038483128,-0.076711304,-0.032610264,0.018457515,-0.0881909,-0.023489388,0.030178923,-0.032386836,-0.048495933,0.005725293,-0.038200796,0.07461983,-0.07878012,0.018697292,0.011762314,0.0021086896,0.084566295,0.019823274,-0.029140446,-0.020837573,-0.02900776,-0.029918442,0.0044389023,0.08279184,2.1237574e-06,-0.0019244121,0.062591076,0.0030395403,0.035349134,-0.025556073,-0.020751648,-0.0050557456,-0.019385647,-0.022930508,-0.008330829,0.02059893,0.021428058,0.016126163,0.023284419,-0.006382538,0.0342595,-0.027322378,0.03008897,0.046553146,-0.0026624934,-0.0403818,-0.036736444,0.054150596,0.028866585,0.021298653,-0.012213172,0.024647104,0.023996942,0.00035678243,-0.032324933,0.020254439,0.00963913,-0.023276353,-0.007734323,0.030900419,0.010057169,-0.039270204,-0.004448113,0.0510084,0.0404863,0.029104477,-0.032503,-0.057076454,-0.027666282,-0.026175424,-0.024096081,0.03902983,0.010897666,-0.0141884405,0.0013637124,-0.0051229666,0.07965567,-0.12271753,-0.026244322,-0.060459167,0.03087958,-0.040871594,-0.008154574,-0.012983047,-0.020249408,-0.019762373,-0.019258047,0.06662061,0.052644268,-0.049500056,0.009456831,-0.026049037,-0.01761397,-0.015321302,-0.017113676,-0.041247323,-0.002144587,0.0048646606,-0.02606511,0.10571937,-0.030583646,-0.0009632298,0.044425055,0.011555989,-0.08542544,-0.02410698,0.038525954,-0.0029445204,-0.0021763847,-0.012585985,0.044610806,0.020875521,-0.04284375,-0.03992834,-0.073941715,0.041847587,-0.0031199597,-0.0041804123,0.027873527,0.029792594,-0.06564589,0.017645625,-0.004575279,-0.023633214,-0.0023049293,0.038247284,0.05727498,0.036696076,0.050031215,-0.064538054,-0.09298789,-0.02768823,-0.03706485,-0.015662255,-0.018815484,0.036917936,0.014851314,-0.012295172,0.011004141,-0.022199593,0.00074584445,0.033499345,0.03596253,-0.022432128,-0.024192369,-0.018381454,-0.009256688,0.037209466,0.011385754,-0.0023554058,0.045522906,0.042315517,-0.056674294,-0.0699801,-0.015752241,-0.006948574,0.0037480704,-0.0068399156,0.015524162,0.037381332,-0.0075457026,0.007170634,0.053349715,0.027305223,-0.022147406,0.008590046,0.070190236,-0.00088106963,0.05624938,-0.001420436,-0.041773714,-0.077485874,-0.0126834735,-0.033552215,-0.05713159,0.034891456,0.017850783,0.023491789,-0.028211668,-0.030255567,-0.017804313,0.028586455,0.025588023,-0.007023342,0.020641197,0.010777036,-0.012584889,-0.059090737,-0.015900014,-0.043614622,0.011290153,-0.015059006,0.042589452,0.026441101,0.010964156,-0.018326996,0.020910135,-0.011550029,-0.0030814838,0.016554072,0.038273547,-0.035338275,0.044864893,0.0032517652,-0.024748128,0.059490126,-0.023581376,0.038273793,-0.015556974,0.026276084,0.0050256955,0.0064386935,0.022203952,-0.039441902,-0.05713572,0.020288961,0.019339135,0.020527478,0.022627547,-0.010364337,-0.050743036,-0.074050024,-0.046173677,-0.032907628,0.07840013,0.03526619,-0.003572606,-0.052836657,0.03562378,-0.046388134,0.043219045,0.040278476,-0.0025033394,0.028737627,0.009726125]	Keywords: transformers, machine learning, neural networks, attention mechanisms, natural language processing, object detection, time series, efficiency, optimization, deep learning\nKey Objects: references, survey paper\nRefers to Images: None\nHypothetical Questions:\n- What are some of the most recent developments in Transformer architecture?\n- How are researchers working to make Transformers more efficient?\n- What are some of the diverse applications of Transformers?\n---\nSummary:\nThis is a list of references, likely from a survey paper on Transformers. It includes a wide range of papers related to Transformer architecture, training, applications (like machine translation, speech synthesis, object detection, time-series forecasting), and techniques for improving efficiency (like early exiting, conditional computation, and memory optimization).  The papers span the years 2016-2021 and cover both theoretical explorations and practical implementations.\nOriginal Text:\n- [68] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Huskinsury. 2019. Revealing the Dark Secrets of BERT. In Proceedings of EMNLP-IJCNLP . 4364-4373. https://doi.org/10.18653/v11/D19-1445\n- [69] Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovici Demoyer, and Herv'e J'egou. 2019. Large Memory Layers with Product Keys. In Proceedings of NeurIPS . 8546-8557. https://proceedings.neurips.cc/paper/2019/ hash/9d8f7d3a3fcb3f5bc47e9b052f01a1f2-Abstract.html\n- [70] Juho Lee, Yoonho Lee, Jungtek Kim, Adam R. Kosorei, Seungjin Choi, and Ye Whye Teh. 2019. Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. In Proceedings of ICML . 3744-3753. http://proceedings.mlr.press/v97/19e1d1h.html\n- [71] Dmitry Lepikhin, Youkjouong Lee, Yuzhanong Xu, Dehao Chen, Orhan Firat, Yanying Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2020. GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding. CoRR abs/2006.16668 (2020). arXiv:2006.16668\nContextualized Text:\nlist of references	{"tags": ["transformers", "references", "survey", "deep learning"], "doc_id": "0dd87b46-7247-4e1c-913e-4f80b38d560c", "summary": "This is a list of references, likely from a survey paper on Transformers. It includes a wide range of papers related to Transformer architecture, training, applications (like machine translation, speech synthesis, object detection, time-series forecasting), and techniques for improving efficiency (like early exiting, conditional computation, and memory optimization).  The papers span the years 2016-2021 and cover both theoretical explorations and practical implementations.", "doc_type": "text", "entities": ["BERT", "GShard", "HIBERT", "Informer", "LazyFormer", "MEMER", "Poolingformer", "R-Transformer", "SETransformer", "Wang", "SET Transformer"], "keywords": ["transformers", "machine learning", "neural networks", "attention mechanisms", "natural language processing", "object detection", "time series", "efficiency", "optimization", "deep learning"], "key_objects": ["references", "survey paper"], "contextual_text": "list of references", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the most recent developments in Transformer architecture?", "How are researchers working to make Transformers more efficient?", "What are some of the diverse applications of Transformers?"]}
cba545b1-a0e4-48f4-86bf-e610776914b5	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.026489783,0.037091423,-0.016010687,-0.007480879,-0.036681637,0.07553049,0.0045868456,0.07489585,0.04645978,-0.04893492,0.0031588804,0.007937517,0.010631356,-0.009447929,-0.04210124,0.050536588,0.022735296,0.054930113,0.027466564,-0.01524409,0.05619976,0.008027374,-0.016462661,-0.022374496,-0.006370195,0.029487984,-0.012438888,-0.03535558,0.01711272,0.03646996,0.018170217,-0.04737579,0.033563886,-0.011887462,0.031998392,0.004767141,0.009318792,0.022956476,0.021298688,-0.02153175,-0.01064786,0.03727979,-0.059047878,-0.0023945894,-0.030009264,-0.074255735,-0.070892274,-0.04268251,0.037783142,-0.013377411,-0.0072351154,0.019038793,-0.026349587,-0.01954958,0.041447565,-0.0030729768,-0.00068438024,-0.0138949575,0.045894783,-0.12651405,-0.034541294,0.038666684,-0.03389028,-0.005650273,0.046477918,-0.07858645,-0.042336658,-0.0133477645,0.00403271,0.11005744,-0.027145717,0.04384052,-0.004698918,-0.051485535,0.119458415,0.035257246,0.0032507088,-0.020462329,-0.057350963,-0.014925878,-0.02635797,0.059112392,-0.06325489,-0.05113169,0.07279686,-0.0017022371,-0.037027527,-0.020834014,0.09307154,-0.014853334,0.02122586,-0.007543482,-0.008976432,0.063281454,-0.026490392,-0.052858748,0.007536705,-0.034767818,-0.018929143,-0.043474436,-0.013404624,-0.040027224,0.065780364,0.070762396,0.003987041,-0.008192451,-0.04672674,-0.022624528,0.008764492,0.005270776,0.0031191662,-0.0074892472,-0.04156818,0.0023856417,-0.012626835,-0.029462,-0.06618206,0.02024975,0.0067179985,0.0062600793,-0.031852502,-0.019495009,0.008988442,0.002903482,0.061299957,0.07990286,0.014532257,-0.015380518,-0.055673305,-0.014872193,-0.010226878,0.011920722,-0.025469435,0.017539447,0.036110103,0.017818907,0.044822797,-0.050746176,-0.0111304335,-0.0005014131,0.018399177,0.0011724095,-0.03400085,0.038111523,-0.01653259,0.0036982337,-0.0646758,-0.02029132,-0.02926804,0.06273561,-0.0022553897,0.012157534,0.0125849685,0.036528137,0.008311193,0.008760657,0.00010889358,-0.09001841,-0.0039952933,0.014521955,-0.05303507,-0.0615784,-0.02865081,0.03252171,0.023837738,0.117649056,-2.1549089e-05,-0.021512536,0.048858352,-0.012745656,-0.03820885,0.013607797,-0.01761358,-0.009885344,0.01061412,-0.027146732,0.007676538,-0.0011740102,-0.028320614,0.016485391,0.018442145,-0.0032894695,0.008277367,0.060897656,-0.021576794,0.0133675635,-0.001144202,0.080873854,-0.018010033,-0.03508327,0.02533297,0.0015465588,0.017684786,0.015998542,0.010185941,0.0010357745,0.033608463,-0.055475887,-0.06986048,-0.022489509,-0.048356023,-0.0013817704,0.03656089,0.01769413,0.011244011,-0.013884081,-0.040262982,-0.02113238,-0.030361619,-0.03057529,0.02923012,-0.027664142,-0.006670962,0.053193953,0.023017585,-0.020980297,-0.03691321,0.032316376,0.015394192,0.0004871711,-0.059258964,-0.0073750177,-0.010937568,0.021505326,-0.03441746,0.017453039,-0.043075196,0.030631172,-0.010611865,-0.012846996,0.059270084,-0.017667206,-0.0014691682,0.01147108,-0.007033973,0.03654755,-0.041857556,-0.0075817094,-0.012915645,0.05122738,0.02395275,0.06627052,0.042853788,0.0691883,-0.0018884793,-0.024761623,0.043892663,0.019691044,-0.02533216,0.021656655,-0.01335293,0.06596964,0.03315617,0.0070317145,0.00676755,-0.046405863,0.039530214,0.0026456427,0.04244409,0.039797645,-0.006479702,0.0027256133,0.025855502,-0.035040513,0.0039610313,0.01977078,0.04178503,0.052911863,0.031221816,0.0060841627,0.039608978,0.046704482,0.007882885,-0.031954836,-0.11947128,-0.01785138,0.027751632,0.030986486,0.025493857,-0.008477836,0.011413587,-0.03599588,0.050331473,-0.0027329037,-0.0063388655,-0.0152616855,-0.0112885665,-0.028236765,-0.056575965,-0.0004029115,0.014443658,-0.043416508,0.0097287465,-0.023529185,0.03173163,0.008876637,0.012892514,-0.030708225,-0.014497259,0.007938234,0.08308943,-0.02276033,0.017212724,-0.012170535,-0.012894216,0.017333342,-0.063835524,0.011872159,0.06252182,-0.0039143083,0.022864949,-0.0032662961,-0.016689131,-0.006184689,0.06384903,0.1026145,0.0674205,-0.011425612,-0.016062729,0.019472437,0.00963939,0.0021137022,0.01675823,0.042832218,0.04021812,-0.003763328,-0.0024218457,-0.027869882,-0.058233384,0.06383053,-0.049092185,0.025242236,0.030132875,0.0034567295,-0.02977414,-0.013289147,0.058849644,-0.001998096,-0.012145007,0.035437934,0.01679019,-0.057660375,-0.021862654,-0.024167594,-0.009893902,0.029626269,0.014014862,0.021865185,0.049651638,-0.0058623385,-0.011646086,0.03160735,0.031409416,0.0068732547,-0.014026165,-0.0065702163,0.002893597,0.01720858,-0.012241512,0.04666138,0.055045053,-0.04566572,0.08361506,-0.024470236,0.009741019,0.0047330093,-0.064970516,-0.059417758,0.04169458,-0.031770073,-0.0068235346,-0.03520005,0.03392635,-0.034358144,-0.040186357,-0.011732687,0.0318393,0.008969788,-0.029157005,-0.021078983,-0.0265771,0.024858221,-0.039226584,-0.033746507,-0.03808103,0.06739164,0.022609543,0.014776795,0.030002452,0.036517255,-0.029024083,0.033295814,0.046096366,-0.024037134,-0.011740527,-0.018414259,0.035631783,0.019657578,-0.0045146854,-0.03718778,0.052324772,0.02368369,-0.035722416,0.022997366,0.062005874,0.048808657,-0.07882788,0.027646564,0.06172853,0.0119362185,0.05865029,-0.034777336,-0.044397824,0.030804139,-0.005927565,-0.04071441,0.0014205497,0.015017565,-0.05436032,-0.02225635,-0.050365113,0.04351116,0.0018976888,0.037464704,0.03318953,-0.029872362,-0.03898222,0.020178925,-0.009528767,-0.03906943,0.018980265,-0.079128206,0.0015199718,0.025554318,0.036934126,0.020058334,-0.051834416,0.07035967,-0.09374344,0.019547852,0.046470195,-0.0009006417,0.010995232,-0.035809033,-0.021976098,-0.020273715,0.029140878,0.04117045,-0.0017217428,-0.001733997,-0.0013125468,0.05259796,0.01833337,-0.0118111735,-0.0059248763,0.004513503,-0.036203593,-0.0010433074,-0.06492804,-0.008411552,-0.039707385,0.04869777,-0.027197134,-0.0032725618,0.020244854,-0.0005148611,-0.050911747,-0.0128390975,-0.02533552,-0.0033850123,0.001700995,0.046114296,-0.055674143,-0.025191605,-0.021934388,-0.010907916,-0.02330026,0.046869364,0.045450024,-0.0002842682,-0.050174195,-0.064492956,-0.0032460957,0.058884773,-0.022275977,-0.0052469317,-0.02909032,-0.04804461,-0.062568255,0.014917794,0.009311779,0.06459039,0.013438299,0.026177945,0.031950496,-0.01596514,-0.0103017455,0.001784102,0.009603763,-0.030084904,-0.04973755,-0.00030259328,0.057023223,-0.02211443,0.024361348,0.019434704,0.011805074,-0.0121015515,0.032656267,0.050861664,0.021343967,0.013058946,0.023019174,0.0064857183,-0.026422424,0.05261822,-0.028320733,0.0068333764,0.011612418,0.014524752,0.008794531,-0.036163,-0.010704048,-0.010998519,0.0357311,-0.040685553,0.042575624,0.040116977,0.04349881,-0.0011044858,0.048506435,-0.007548969,-0.038265698,0.024320228,-0.03135125,-0.019688884,0.015295978,-0.08607216,-0.002093555,0.03849023,-0.036460966,-0.012325798,0.020576848,-0.051325515,0.08511244,-0.05787911,0.05101181,-0.054097846,0.024335532,0.11555351,0.021606345,-0.00046744314,-0.010808929,-0.042929534,-0.046224393,0.006749356,0.0608716,-0.0016523002,-0.027453117,0.06297976,0.006657588,0.057852365,0.022930607,-0.056097567,-0.019225026,-0.033683278,-0.07060121,0.009162008,-0.0061578634,0.027792867,0.014075485,-0.00359503,0.013403353,0.039876234,-0.024057556,-0.01112339,0.012404935,-0.012605038,0.013364953,-0.048845384,0.054976862,0.017891534,0.0047010616,-0.03267911,0.020159803,0.019287813,-0.020636419,-0.020910969,0.0072303023,0.0139783155,-0.015038548,-0.024130592,0.029674504,0.025461271,-0.039931722,-0.010270999,0.03240211,0.045328863,0.011602244,-0.026028872,-0.019045243,-0.018191744,0.003853727,-0.0042828904,0.020530999,0.022070741,0.0057906737,-0.00913577,-0.03644078,0.092630364,-0.07323931,-0.04126904,-0.04386931,0.037561487,-0.057138935,-0.0068445895,-0.0076316437,-0.0214705,-0.021401996,-0.038816378,0.04840399,0.039047796,-0.05696286,0.0055053104,-0.002496782,0.0008047363,0.0027599437,-0.04393487,-0.01056055,0.0039998377,-0.01967549,-0.02114141,0.08979156,-0.03279642,0.0037798604,0.043605052,-0.011765624,-0.072480515,-0.018840903,0.015794331,0.032513987,-0.03033153,0.0084301215,0.025052862,0.025532609,-0.05320038,0.001255773,-0.052328903,0.03874898,-0.025982494,0.000113094095,0.01664547,0.007953302,-0.08868935,0.03940673,-0.042899933,-0.017308142,-0.019888757,0.029097563,0.07259871,0.04723634,0.0827452,-0.055118836,-0.104671985,-0.039257806,-0.038037244,-0.011158345,-0.03006076,0.015348815,-0.0015414748,-0.03099149,0.05031074,0.00014036492,-0.019121148,0.0077531515,0.020857694,-0.022512434,-0.027318362,0.0052669244,-0.0156229725,0.03206221,-0.025761172,-0.020961031,0.049664672,0.044602998,-0.07253915,-0.034209326,-0.012383047,0.014444865,0.013751663,-0.009665499,0.027244313,0.04147306,-0.02220315,0.016268684,0.06573858,0.016304296,-0.043194275,0.0035960374,0.027914857,-0.0035392211,0.03859756,0.017944021,-0.04310675,-0.086568154,0.012538343,-0.019117698,-0.062473983,0.04801511,0.013131235,0.0039096326,-0.0062710512,-0.046877358,-0.0027767648,0.024325961,0.05551371,0.045809884,0.0007710762,0.0075864787,-0.042330083,-0.035449695,-0.017278692,-0.0216983,0.021419004,-0.019053137,0.012833405,0.0197049,0.021537786,0.002606783,0.026486583,0.0063057854,-0.034182645,0.018845452,0.022679316,-0.022617187,0.06756528,-0.018178418,-0.005683103,0.035904896,-0.0016185114,0.019589553,0.0034458686,0.0024717161,0.021502953,-0.0030241243,0.0112316,-0.03171116,-0.08975499,-0.0055018975,0.011822178,0.0029892412,0.01797058,-0.007332783,-0.03274649,-0.032564025,-0.072927676,-0.0074270265,0.07769822,0.04178167,0.022338731,-0.053578988,0.0026010377,-0.022428181,0.021723993,0.032222696,-0.018528713,0.02522074,-0.0070079034]	Keywords: Transformers, NLP, Attention Mechanisms, Pretraining, Vision-Language, References, ACL, arXiv, BERT, Vision-IBERT, denoising, sequence-to-sequence, attention, Vision-Language Models, ACL, arXiv, BERT, Vision-IBERT, denoising, sequence-to-sequence, attention, Vision-Language Models\nKey Objects: VisualIBERT, BERT, Vision-IBERT, Visual Transformers, Sequence to Sequence\nRefers to Images: None\nHypothetical Questions:\n- What is the common theme among these references?\n- Which conference or archive is most frequently cited?\n- What types of models are described in these references?\n- Can you describe the different models mentioned in the references?\n- What are some of the applications of the models described in the references?\n---\nSummary:\nThis is a list of references (numbered 72-75) from a survey on Transformers. The entries include authors, titles, publication venues (ACL, arXiv, etc.), and sometimes URLs or DOIs. The works cover topics like denoising sequence-to-sequence pretraining, attention mechanisms, and vision-language models.\nOriginal Text:\n- [72] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denosing Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehensions. In Proceedings of ACL . 781-7880. https://doi.org/10.18653/v1/2020.acl-main.703\n- [73] Jian Li, Zhaopeng Tu, Baosong Yang, Michael R. Luy, and Tong Zhang. 2018. Multi-Head Attention with Disagreement Regularization. In Proceedings of EMNLP . Brussels, Belgium, 2897-2903. https://doi.org/10.18653/v1/1D-1317\n- [74] Jin Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R. Luy, and Zhaopeng Tao. 2019. Information Aggregation for Multi-Head Attention with Routing-by-Agreement. In Proceedings of HLT-NAACL . 3566-3575. https://doi.org/10. 18653/v1/N19-1359\n- [75] Luiian Harold Li, Mark Yatskar, Da Yin, Cho-Ji Hsuieh, and Kai-Wei Chang. 2019. VisualIBERT: A Simple and Performant Baseline for Vision and Language. arXiv:1908.03575 [cs.CV]\nContextualized Text:\nThis is a list of references (numbered 72-75) from a survey on Transformers. The entries include authors, titles, publication venues (ACL, arXiv, etc.), and sometimes URLs or DOIs. The works cover topics like denoising sequence-to-sequence pretraining, attention mechanisms, and vision-language models.	{"tags": ["references", "transformers", "nlp", "acl", "arxiv", "bert", "vision-language", "attention mechanisms", "survey", "nlp", "transformers"], "doc_id": "cba545b1-a0e4-48f4-86bf-e610776914b5", "summary": "This is a list of references (numbered 72-75) from a survey on Transformers. The entries include authors, titles, publication venues (ACL, arXiv, etc.), and sometimes URLs or DOIs. The works cover topics like denoising sequence-to-sequence pretraining, attention mechanisms, and vision-language models.", "doc_type": "text", "entities": [], "keywords": ["Transformers", "NLP", "Attention Mechanisms", "Pretraining", "Vision-Language", "References", "ACL", "arXiv", "BERT", "Vision-IBERT", "denoising", "sequence-to-sequence", "attention", "Vision-Language Models", "ACL", "arXiv", "BERT", "Vision-IBERT", "denoising", "sequence-to-sequence", "attention", "Vision-Language Models"], "key_objects": ["VisualIBERT", "BERT", "Vision-IBERT", "Visual Transformers", "Sequence to Sequence"], "contextual_text": "This is a list of references (numbered 72-75) from a survey on Transformers. The entries include authors, titles, publication venues (ACL, arXiv, etc.), and sometimes URLs or DOIs. The works cover topics like denoising sequence-to-sequence pretraining, attention mechanisms, and vision-language models.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the common theme among these references?", "Which conference or archive is most frequently cited?", "What types of models are described in these references?", "Can you describe the different models mentioned in the references?", "What are some of the applications of the models described in the references?"]}
a9352f9c-8407-42a1-b76b-e1ab91645e61	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.001632597,0.029044773,-0.00037427674,0.012068431,0.01943575,0.07136897,0.0016905795,0.07190988,0.04890172,-0.013422376,-0.002864171,0.024422564,-0.0033062354,-0.025077667,-0.03570833,0.042254217,0.0074918903,0.06336176,0.015280926,-0.01866057,0.034124747,0.0050858175,-0.016038686,-0.051506616,0.0030437973,0.014995352,0.010592994,-0.037200272,0.021951113,0.03485169,0.02387308,-0.045444883,0.037443187,-0.009908973,0.01950994,0.025719624,0.020043308,0.027464276,0.02214669,-0.02183327,-0.019038564,0.05900057,-0.03199536,-0.008948728,-0.017684942,-0.058871698,-0.06195729,-0.034881473,0.032258615,-0.026584903,-0.015493212,0.020103268,-0.012094119,-0.002352355,0.002129455,-0.031224208,-0.016544288,-0.028877236,0.042689297,-0.09828143,-0.05441356,0.03316349,-0.05599134,0.0043677404,0.028386192,-0.07021083,-0.03594512,0.009328291,0.0037180814,0.11092477,-0.027364727,0.037332185,-0.023225203,-0.04818058,0.09650252,0.044635516,-0.008747024,-0.023769775,-0.06979852,-0.011845576,-0.024130054,0.07175827,-0.03658138,-0.040697012,0.09012113,0.0020138873,-0.043483626,-0.03509092,0.096755594,-0.01574165,0.025752902,-0.01933435,-0.0077107702,0.059194945,-0.021576911,-0.08491981,0.03264841,-0.03847297,-0.0015424961,-0.07116969,-0.034759358,-0.0041856943,0.07135323,0.07481398,0.032698445,-0.012665984,-0.027126087,-0.049567137,-0.002238539,0.018174345,0.018222876,5.8080008e-05,-0.009827338,0.0020794137,-0.021184705,-0.027322723,-0.065547206,-0.006053747,-0.010799273,-0.008838931,0.0058401516,-0.014057665,0.017697632,0.020108446,0.07301942,0.06822527,0.0048566777,0.014514949,-0.02810232,0.0034383433,0.0023132146,-0.009460015,0.0020509704,0.01492615,0.024221173,-0.0006837907,0.073809884,-0.06094425,0.016674576,0.017203318,0.02454905,-0.03561758,-0.014028401,0.041616604,-0.030012563,0.012081924,-0.045835584,0.0050342157,-0.013636423,0.052589133,0.01680603,0.0015628734,0.025733292,0.021005334,0.012449938,-0.010876726,0.0066443146,-0.04718971,-0.0016805164,-0.002778184,-0.054386724,-0.056769516,-0.040488936,0.041737445,0.030460564,0.12547344,0.016185354,0.018828457,0.029507507,0.0047835982,-0.049158085,0.024238972,0.0017158985,-0.027078353,0.0068202433,-0.023414994,0.013822252,0.00088560145,-0.0504998,0.0162367,0.022134667,0.0063539143,-0.014842835,0.06464983,-0.026806207,0.018718917,-0.010447996,0.06759828,-0.016259493,-0.026229665,0.02701365,-0.014744292,-0.01997209,-0.010044842,0.025014801,0.014267935,0.01817738,-0.079773776,-0.035289,-0.0039551607,-0.025273377,0.011765242,0.044772092,0.010454658,0.013752652,-0.03220834,-0.028683422,-0.012364721,-0.049781434,-0.034461763,-0.0024488368,-0.0106488345,-0.011324894,0.05164767,0.048714392,-0.0011216154,-0.034657612,0.025879536,0.02481169,-0.014767535,-0.066775896,0.0012685643,-0.019508058,0.007413361,-0.043159034,-0.0025310267,-0.04909527,0.058843154,0.007510816,-0.041130703,0.069761924,-0.0091353655,-0.018807583,0.032172263,-0.020730877,0.039404575,0.0017108476,-0.0073826476,0.022985876,0.049311314,0.026717504,0.0477742,0.024450704,0.052943595,-0.0041537955,0.00085257425,0.024284102,0.046890385,-0.0064770775,0.012273796,0.002435945,0.04176942,0.05873264,0.0073032123,0.0070366105,-0.03291068,0.026161088,0.042251624,0.047832124,0.01211535,-0.010932045,-0.011303824,0.03272921,-0.011057035,0.01565618,0.008809806,0.026378836,0.04592156,0.0107083,0.0029537885,0.018965224,0.051537663,-0.012070108,-0.031556603,-0.09883379,0.00088536734,-0.00027036312,0.039320298,0.03647462,-0.009955708,-0.0054136696,-0.033655103,0.036818884,-0.024925807,0.012411539,-0.017630633,0.005929484,-0.027947113,-0.08114662,-0.0060648196,-0.0040029474,-0.03447756,0.021863742,-0.02414445,0.018141784,0.010626806,0.018069228,-0.035198126,-0.036422923,0.004164146,0.08751521,-0.03837854,0.014175273,-0.0067977007,0.0012604733,0.0132376,-0.08629945,0.014321776,0.048378304,-0.002451142,0.02550507,-0.021659909,-0.0022235785,0.00019173633,0.038733717,0.09844347,0.051204357,-0.020195592,-0.009732831,0.02440482,0.015887262,0.012568293,-0.00048607952,0.06393183,0.03313291,-0.014243247,0.0022864589,-0.050076142,-0.10938747,0.04041327,-0.0650859,0.058666848,0.025769575,0.023643924,-0.0056752902,-0.0043602395,0.034100153,0.025709905,0.014191547,0.029976709,0.0016413579,-0.06002846,-0.0032091164,-0.0009927257,0.012107627,0.029571531,0.018202957,0.030732,0.039666012,0.0050003277,-0.008441647,0.043535195,0.052841246,0.00568268,-0.0071786423,-0.0060546114,-0.009384347,0.0058031366,0.007555002,0.021033725,0.05964241,-0.04256192,0.07759611,-0.023886213,0.010340988,-0.0032710428,-0.07425084,-0.021628847,0.0639779,-0.02654713,-0.009000887,-0.027882703,0.05073198,-0.023989115,-0.01806844,-0.005258924,0.033470485,0.02055081,-0.06527194,-0.040940925,-0.036685325,0.022597201,-0.055541627,-0.02479222,-0.033836927,0.055148873,0.014792702,0.022247165,0.016976383,0.038516186,-0.03145271,0.010218276,0.03755584,-0.015167787,-0.00693433,-0.007305754,0.030510766,0.025050892,0.021033203,-0.039645396,0.046739664,0.02671891,-0.043819357,0.03323421,0.03309813,0.03319405,-0.06729063,0.014008478,0.06470316,0.019019391,0.036737014,-0.045304917,-0.023550175,0.025101207,-0.023294672,-0.01193718,-0.011381654,0.00065834983,-0.056224052,0.015159219,-0.051158603,0.04424212,-0.005325545,0.044906896,0.041437488,-0.04237589,-0.030148702,0.032101247,-0.009409277,-0.052555766,0.017669134,-0.08744942,-0.026440786,0.05328996,0.02977685,0.026291048,-0.052992683,0.08093428,-0.0943667,0.025686873,0.065659784,-0.03224614,0.0025499642,-0.041863214,-0.009014196,-0.043173566,0.00860878,0.03738635,0.010560123,-0.022045411,-0.0039739492,0.04520513,0.019227142,-0.0043225014,-0.009701435,-0.015986342,-0.0584363,0.011675555,-0.046936776,0.0013618709,-0.009928182,0.060671944,0.0009272484,0.0063785305,0.0180098,-0.007391309,-0.050060842,-0.003543669,-0.017340587,-0.007957177,0.009927115,0.01976515,-0.04274695,-0.007007941,-0.027170505,-0.015969494,-0.03083303,0.054650746,0.05329203,0.006315222,-0.053304933,-0.0424593,0.02189617,0.057474896,-0.025460048,-0.00033139408,-0.031920478,-0.035117164,-0.047538444,0.028870998,0.022964692,0.085232556,0.040697142,0.029201666,0.016394185,-0.02736851,-0.004162392,0.004474631,0.0131134745,-0.058756284,-0.040630918,-0.0014944706,0.059781555,0.0035696959,0.026745219,0.016286975,0.008475434,-0.0038709554,0.023805764,0.047195178,0.022809949,0.012578619,-0.008773314,0.016580323,-0.021588543,0.036560416,-0.015008966,-0.007839205,0.027812524,0.05392956,-0.0043527004,-0.024208294,0.013250077,-0.015515154,0.006633665,-0.011885313,0.039893884,0.041754577,0.053822186,-0.049498715,0.06594958,-0.018821213,-0.026845653,0.015624829,-0.044000767,-0.024549002,0.023027202,-0.08152849,-0.026280161,0.022705907,-0.018748911,-0.0242892,0.009461148,-0.0149976,0.07079199,-0.07772293,0.018236298,-0.034162186,0.00677702,0.10174115,-1.43027555e-05,-0.0077662095,-0.013086202,-0.037861552,-0.017732875,0.018902684,0.07469963,0.0147325685,-0.0044949353,0.07119462,-0.005514276,0.054399177,-0.0015423663,-0.04097703,-0.02076486,-0.01792574,-0.035234466,0.0014146545,-0.02491989,0.010344648,0.0142391175,-0.012545284,0.021165397,0.01522136,-0.031832784,0.016559316,0.026711598,-0.032284755,0.01675428,-0.047265984,0.04015149,0.032172684,0.01266572,-0.031103186,-0.0034417822,0.021967787,-0.005654422,-0.038796004,0.011001911,-0.015961237,-0.014723051,-0.028121443,0.046443608,0.03419601,-0.040844925,-0.013320695,0.030326776,0.025691407,0.026472718,-0.027430613,-0.0242577,-0.025704257,-0.04038452,-0.02065812,0.025947204,-0.011241213,-0.020656347,0.0058582146,-0.011384509,0.084656484,-0.107864864,-0.053133238,-0.0335254,0.020341603,-0.00894576,-0.00498402,0.023610868,0.002220818,-0.017354047,-0.02780338,0.070262365,0.045825765,-0.053391602,0.027277997,0.008255047,-0.019382399,-0.011703621,-0.014464313,-0.035550524,-0.008109398,-0.028420676,-0.017599247,0.09018547,-0.043460134,-0.007092276,0.034104723,-0.012317146,-0.09443571,-0.0056637092,0.029637545,0.026747989,-0.019869473,-0.009285304,0.020233743,0.044955593,-0.048548784,-0.012313726,-0.08268128,0.040347613,-0.014909532,-0.0051012477,0.03504904,0.012511837,-0.06041285,0.038436316,-0.025419239,-0.047282267,-0.0060354653,0.004264033,0.065137,0.046880048,0.056636803,-0.04053598,-0.09570914,-0.019831775,-0.03311197,-0.012470566,-0.042758342,0.024488706,-0.0084369145,-0.021242982,0.0064855497,-0.033514827,-0.007545863,0.02804755,0.026609182,-0.024073651,-0.019796181,-0.042058762,-0.020286888,0.02270537,0.0033356915,-0.0023611041,0.063366674,0.046179675,-0.0860099,-0.045245204,-0.012199425,-0.01594181,0.008244159,-0.012331909,0.028289845,0.014434984,-0.01948411,0.0039984486,0.057113823,0.016147299,-0.016613713,-0.0024985634,0.04026743,-0.0214828,0.044875275,0.0067865774,-0.036799952,-0.08726251,-0.018394502,-0.020810684,-0.06435669,0.03122631,0.03916458,0.008212915,-0.047016036,-0.033280697,-0.0016301748,0.024280738,0.041183375,0.0017031812,0.014347624,0.019095672,-0.028204555,-0.06085596,-0.022885365,-0.046511035,0.025910793,-0.025857676,0.0336997,0.036091376,0.02761703,-0.009054109,0.022613147,-0.008435916,-0.016963791,0.020144986,0.027148772,-0.05528307,0.04472447,-0.007439261,0.016131615,0.06692535,-0.0106229875,0.037197575,-0.005311515,0.02878431,0.007071449,-0.003512157,-0.0048431926,-0.025834497,-0.1000332,-0.005627385,0.029476723,0.022016402,0.020958649,-0.005930814,-0.035683382,-0.05125864,-0.041758575,-0.0127239255,0.07945622,0.033029668,0.025300322,-0.056242857,0.028610047,-0.038141612,0.043617655,0.04432033,-0.026892385,0.021671306,-0.018852262]	Keywords: transformers, neural networks, speech synthesis, vision and language, inference acceleration, unified-modal learning, self-attention, sparse connection, contrastive learning, document modeling, object detection, time-series forecasting, speech enhancement, sequence modeling, positional embeddings, attention mechanism\nKey Objects: references, survey, Transformer architecture, Neural Networks, Self-Attention mechanisms, Attention is All You Need, BERT, Transformer Decoder, Vision and Language Models, Speech Enhancement\nRefers to Images: None\nHypothetical Questions:\n- What are some key applications of Transformer models?\n- How has the Transformer architecture evolved?\n- What are the challenges in implementing Transformer models?\n- What are the limitations of current Transformer models?\n- What are some future directions for research on Transformer models?\n---\nSummary:\nThis is a list of references from a survey on Transformers. It includes diverse areas like Vision and Language understanding, speech synthesis, accelerating inference, and unified-modal learning. The references cover both theoretical advancements in transformer architecture and practical applications across different domains.\nOriginal Text:\n- [75] Luiian Harold Li, Mark Yatskar, Da Yin, Cho-Ji Hsuieh, and Kai-Wei Chang. 2019. VisualIBERT: A Simple and Performant Baseline for Vision and Language. arXiv:1908.03575 [cs.CV]\n- [76] Naihan Li, Shujiue Liu, Yanging Liu, Sheng Zhao, and Ming Liu. 2019. Neural Speech Synthesis with Transformer Network. In Proceedings of ANAIG . 660-6717. https://doi.org/10.1690/aav.3311.3013610760\n- [77] Wei Li, Can Gao, Guoenchong Niu, Xinyan Xiao, Hao Liu, Jiachen Liu, Hua Wu, and Haifeng Wang. 2020. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning. arXiv preprint arXiv:2012.15409 (2020).\n- [78] Xiaoya Li, Yuxian Meng, Mingxin Zhou, Qinghong Han, Fei Wu, and Jiwei Li. 2020. SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection. In Proceedings of NeurIPS . https://proceedings.neurips.cc/paper/2020/ hash/c5c1bda119f4923d744e0ef67d9f4ee-.Abstract.html\nContextualized Text:\nThis section lists references related to the survey on Transformer models. The references span a wide range of topics, reflecting the versatility of Transformer architecture and its widespread use in various machine learning tasks.	{"tags": ["machine learning", "deep learning", "NLP", "computer vision", "survey", "references", "architecture", "algorithms", "applications", "model development", "research", "innovation", "technical documentation"], "doc_id": "a9352f9c-8407-42a1-b76b-e1ab91645e61", "summary": "This is a list of references from a survey on Transformers. It includes diverse areas like Vision and Language understanding, speech synthesis, accelerating inference, and unified-modal learning. The references cover both theoretical advancements in transformer architecture and practical applications across different domains.", "doc_type": "text", "entities": ["UNIMO", "VisualIBERT", "SAC", "Informer", "DeeBERT", "Lite Transformer", "PoolingFormer", "Memer", "LazyFormer", "Hard-Coded Gaussian Attention", "HIBERT", "Wangchunshu Zhou", "Minghang Zheng", "Canwen Xu", "Yuekai Zhao", "Naihan Li", "Ming Liu", "Xiaoya Li", "Jiwei Li", "Wei Li", "Haifeng Wang", "Guoenchong Niu", "Xinyan Xiao", "Jiachen Liu", "Hua Wu", "Yuxian Meng", "Qinghong Han", "Fei Wu", "Yanging Liu", "Sheng Zhao", "Luian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Ji Hsuieh", "Kai-Wei Chang", "Wangchunshu Zhou", "Minghang Zheng", "Canwen Xu", "Yuekai Zhao", "Naihan Li", "Ming Liu", "Xiaoya Li", "Jiwei Li", "Wei Li", "Haifeng Wang", "Guoenchong Niu", "Xinyan Xiao", "Jiachen Liu", "Hua Wu", "Yuxian Meng", "Qinghong Han", "Fei Wu", "Yanging Liu", "Sheng Zhao", "Luian Harold Li", "Mark Yatskar", "Da Yin", "Cho-Ji Hsuieh", "Kai-Wei Chang"], "keywords": ["transformers", "neural networks", "speech synthesis", "vision and language", "inference acceleration", "unified-modal learning", "self-attention", "sparse connection", "contrastive learning", "document modeling", "object detection", "time-series forecasting", "speech enhancement", "sequence modeling", "positional embeddings", "attention mechanism"], "key_objects": ["references", "survey", "Transformer architecture", "Neural Networks", "Self-Attention mechanisms", "Attention is All You Need", "BERT", "Transformer Decoder", "Vision and Language Models", "Speech Enhancement"], "contextual_text": "This section lists references related to the survey on Transformer models. The references span a wide range of topics, reflecting the versatility of Transformer architecture and its widespread use in various machine learning tasks.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some key applications of Transformer models?", "How has the Transformer architecture evolved?", "What are the challenges in implementing Transformer models?", "What are the limitations of current Transformer models?", "What are some future directions for research on Transformer models?"]}
ab6f0556-60f9-48d6-9ef5-aa68030cd59e	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.02031311,0.028903646,0.03862458,-0.018088082,0.01563456,0.06594664,0.0071407952,0.06885096,0.04191069,-0.060281552,-0.0009250363,0.014895588,0.02182166,-0.0019093009,-0.056622896,0.017411148,0.0156059805,0.071158305,0.0020258508,-0.016180428,0.040866744,0.010208754,-0.012237144,-0.039350033,-0.019021181,0.035830636,0.018468974,-0.029165162,0.03450336,0.018796306,0.01807059,-0.04705813,0.00930689,0.008412286,0.016979003,0.040712807,0.014729684,-0.02355646,0.015083427,-0.030975426,-0.04086108,0.033782918,-0.061927795,-0.021058932,0.012050562,-0.04801552,-0.04779141,-0.050930087,0.016266402,-0.020949634,-0.0039595454,0.023511253,-0.039627053,-0.011619639,0.01165403,-0.021919258,-0.024462376,-0.025783682,0.030096356,-0.11684533,-0.06921904,0.034342337,-0.0481437,-0.034062047,0.044715308,-0.057367947,-0.0065628095,4.943298e-06,-0.00012134999,0.10850533,0.0008486401,0.03879805,-0.016627245,-0.026367176,0.101979546,0.038212564,-0.027207803,-0.005211975,-0.054990962,-0.018147338,-0.034578104,0.06296131,-0.037860926,-0.036999788,0.08113172,0.0028780422,-0.04494703,-0.022065043,0.086015716,-0.03549093,0.039026726,-0.0135879265,-0.011861045,0.05323887,0.0032264201,-0.043270677,0.020477813,-0.037274245,0.009673061,-0.055483516,-0.04271348,-0.040684413,0.080640435,0.07770324,0.012230012,-0.01445999,-0.014483698,-0.030109815,-0.009879182,0.018440917,-0.0045117796,-0.0066314144,0.01480798,-0.010248438,-0.017590022,-0.017885858,-0.06382557,0.012585787,0.0032330314,-0.00034855714,0.014092467,-0.004115567,-0.0009938466,-0.02683518,0.034154635,0.059375018,0.010447328,-0.017317213,-0.02177514,-0.005019126,-0.0025772906,0.021888753,0.026986167,0.023065792,0.017336944,-0.002457516,0.054561917,-0.022021258,-0.005495664,0.0016068208,0.002631497,-0.011461829,-0.0364579,0.03562787,-0.03775931,0.023575578,-0.084976085,-0.009779994,-0.014063681,0.04167255,-0.008942836,0.017142462,0.03506426,0.028254658,0.01076364,0.005952151,0.015552157,-0.08726237,0.015958708,-0.01714906,-0.07066792,-0.06380886,-0.034511097,0.030889366,0.05837975,0.12752981,0.01702075,0.0072753467,0.05007926,0.02608466,-0.04453951,0.011383947,-0.009939422,-0.023180416,-0.0041661807,0.0038033957,0.03531739,0.016966,-0.016209813,0.02474188,0.0012017867,0.007399065,-0.03780377,0.049142286,-0.027752955,0.021350585,0.014015054,0.07755998,0.0021375741,-0.01547808,0.037173156,-0.00083135377,-0.013473745,-0.0019259906,0.023315813,0.020314518,0.03844864,-0.07592392,-0.034943644,-0.043646343,0.005095483,0.021734571,0.02334082,-0.00025172567,0.034879472,-0.024581071,-0.032656007,-0.043156736,-0.05295239,-0.033596847,-0.005739891,-0.012837564,-0.0019505548,0.053775825,0.029226365,-0.007437397,0.0099839065,0.014931816,0.04713124,-0.04130502,-0.0734933,0.0036978808,-0.02571763,0.017052958,-0.05558359,0.0011525454,-0.039169785,0.040859498,0.012636003,-0.016753918,0.08431921,-0.02924311,-0.040400397,0.025863063,-0.015315632,0.03506541,-0.015374097,0.008371811,0.005134064,0.049813922,0.053162288,0.051412173,0.017936576,0.08187343,-0.012692903,0.007902603,0.033677604,0.040696066,0.011711619,0.017336087,-0.002522466,0.054446224,0.047769185,0.026128238,-0.009015767,-0.040083334,0.042275254,0.026267152,0.03698293,0.044473354,0.009587814,-0.036653068,0.012150889,-0.025583394,0.007788647,0.015024831,0.030635368,0.045231897,0.0021468718,0.011567246,0.034737755,0.0387028,0.015666606,-0.021973405,-0.0836822,0.012228345,-0.015399657,0.02373005,0.04532389,-0.03382454,0.015187463,0.0022928002,0.02979451,-0.008411746,0.02140588,-0.038302187,-0.014711766,-0.007810972,-0.070593186,-0.0048786704,0.010046369,-0.04590739,0.044094913,-0.019547854,0.0010689483,-0.012077832,0.0069007077,-0.016514657,-0.036204338,-0.009794427,0.102055855,-0.03337374,0.0025948328,0.006696943,0.01229862,0.00011644432,-0.072003044,0.011787405,0.03595718,0.008612341,0.011519758,-0.009668177,-0.0075007454,-0.024105,0.055669963,0.11014284,0.048462357,-0.04932375,0.009085617,0.0020880282,0.0027792272,0.010614499,-0.014113977,0.0012572746,0.028201193,0.0067024184,-0.010597263,-0.031067476,-0.11298608,0.05035533,-0.081447564,0.010631099,0.04927538,0.00070706225,-0.023885198,-0.006322942,0.019760974,0.040669147,0.01303682,0.03942815,0.04012523,-0.07448623,-0.044016343,0.0075826016,-0.012550524,0.028706653,-0.020722505,0.0606299,0.038357526,0.0057658204,-0.0040217643,0.054440357,0.012739988,0.0175542,-0.012497311,-0.022125185,0.01039669,-0.012086583,0.016187238,0.03286814,0.07727419,-0.058684487,0.06758552,-0.024777496,-0.014071879,0.0021014195,-0.07230284,-0.046765976,0.079297364,-0.025599401,-0.027708549,-0.028905002,0.05701667,-0.015183669,-0.043037515,-0.014601402,0.05564778,0.029302677,-0.034433156,-0.047425553,-0.026336271,0.023788527,-0.023614844,-0.011927388,-0.045741744,0.041363545,0.012180613,0.05694604,-0.0018383908,0.071504176,0.00069904304,0.008824072,0.042600706,-0.012244364,-0.012902574,0.018119227,0.029961068,0.03943771,0.019943496,-0.04045192,0.05184814,0.030475304,-0.030710684,0.051115267,0.03224485,0.032481637,-0.06769606,0.043700144,0.057065833,0.037938613,0.03145439,-0.037874956,-0.0506017,0.02521563,-0.019142851,-0.033169176,0.02174909,0.015543943,-0.056986723,-0.0002969204,-0.0731241,0.036953337,-0.011780442,0.03151891,0.00081146235,-0.00822575,-0.03405699,0.030072236,-0.0108645065,-0.033479143,0.01714681,-0.05289315,-0.033793315,0.03401189,0.022278475,0.017222732,-0.04978654,0.048679233,-0.08381603,0.024626762,0.04251826,0.0015283283,0.007886546,-0.03596133,-0.0035702202,0.015545595,0.01417378,0.042701032,-0.022141984,-0.03786111,-0.017041693,0.009829513,0.031369496,0.0030120173,-0.0054816925,-0.05113113,-0.021603595,0.029987117,-0.05004242,0.018773416,-0.0082094595,0.08573799,0.0064548594,-0.011337462,0.0055670566,-0.0005359747,-0.03349773,-0.03902273,-0.015302918,0.023021048,-0.033025768,0.050555628,-0.04412188,0.018927678,6.485592e-05,-0.0055719693,-0.030880084,0.03702049,0.058018584,0.017398577,-0.06319942,-0.0147645185,-0.023625204,0.060922857,-0.019410549,-0.028683057,-0.036465574,-0.053966224,-0.063035056,-0.0012219818,0.014617145,0.05983174,0.03902175,0.03860213,0.020241538,-0.006474469,0.02657737,-0.0057714,-0.014939107,-0.030436844,-0.03839255,0.009380618,0.057954937,0.0025854246,0.005366256,0.0047509777,0.026022812,0.0013988817,0.016031535,0.031295832,0.0026145487,0.027929418,0.0024593503,-0.010855746,-0.025962107,0.020183913,-0.008460361,-0.0176713,0.0043746494,0.04555326,-0.034615576,-0.01567814,0.029677056,-0.017958341,0.033098526,0.0012489428,0.025000481,0.054464284,0.060028955,-0.041116916,0.0663664,-0.0005874946,-0.03994693,0.019063141,-0.087832935,-0.012148453,0.028236873,-0.06586357,-0.014164194,0.037074976,-0.0076852194,-0.020058718,0.014705476,-0.04067546,0.11201408,-0.06490924,0.00043225457,0.009910923,0.034777038,0.09511838,0.022107491,-0.003701783,-0.027521253,-0.024809932,-0.009546896,0.023989018,0.049287226,0.010524663,-0.028431816,0.07495758,0.021261621,0.02493197,-0.030195503,-0.02196699,-0.0054949047,-0.027076272,-0.032610845,0.0016173954,-0.0021334393,-0.009315124,0.007817896,-0.019061416,0.0023213343,0.016538857,-0.014106405,0.0023472782,0.040314913,-0.020622177,0.02564238,-0.036906883,0.05855036,0.030133616,-0.005699051,-0.019564029,0.0065262383,-0.016319856,0.0062195817,-0.041635554,-0.007736693,-0.025968041,0.016003009,-0.027006954,0.06124072,0.00625919,-0.04827406,-0.0010540624,0.050429396,0.025515724,0.016926302,-0.002430791,-0.005815841,-0.03335788,-0.036259897,-0.016633242,-0.009618964,-0.004535939,0.013893921,0.00386398,0.018329488,0.09390879,-0.11191346,-0.033040516,-0.027793156,0.059114225,-0.016794017,-0.002786264,-0.014699268,-0.013479391,-0.017668312,-0.03621805,0.055994745,0.07003967,-0.027113756,0.022301795,-0.007929602,-0.030149838,-0.010999545,-0.021923993,-0.017874647,0.00035131277,-0.021937883,-0.021332983,0.11028529,-0.011157207,-0.01197391,0.0029759083,-0.021010023,-0.09109279,-0.018982744,0.014621353,0.016393462,-0.0045227506,-0.0046777916,0.014163387,0.013629633,-0.03842873,-0.019873481,-0.07685814,0.03373044,-0.030228322,-0.01200003,0.06485393,0.04098442,-0.077036895,0.033746906,-0.052693807,-0.023014005,-8.244145e-05,0.048255567,0.029383196,0.021521324,0.044198446,-0.040735375,-0.10169135,-0.011335568,-0.01832098,-0.0116842035,-0.015265362,0.010226992,0.014886589,-0.009198909,0.006533483,-0.013936762,-0.012022552,0.01477503,0.010418014,-0.00841071,-0.04035369,-0.039336804,-0.006922962,0.022680828,-0.02876143,0.020043274,0.05252608,0.03292328,-0.05238082,-0.01668501,-0.045917984,-0.017615547,0.009120977,0.019406274,0.02156758,0.029595926,-0.021458121,0.008913783,0.06926411,0.019507676,-0.0066208495,-0.0031033806,0.063505955,0.008435176,0.03725165,0.029705763,0.007248411,-0.09912882,0.0008517021,-0.016348032,-0.046947576,0.045336064,0.003755077,-0.0072329477,-0.033354294,-0.033776697,-0.0046719345,0.028718913,0.04463449,-0.003638759,-0.0072557614,0.014090517,-0.0124194585,-0.06419027,-0.014465994,-0.019639678,0.00041241825,-0.029377913,0.028309783,0.028489133,0.03102952,-0.009974118,0.029729934,-0.029261215,-0.026893562,0.024995666,0.050503474,-0.04003916,0.02921566,-0.030606844,-0.010296556,0.06785783,-0.018322702,0.039407734,-0.016235074,0.03577491,0.0024146317,0.0022327232,-0.017098822,0.0012463364,-0.09220817,0.024058277,0.011650131,0.001121041,0.013066135,-0.008887557,-0.04040615,-0.06554649,-0.03726089,0.0038207993,0.044823624,0.0026792998,0.014679377,-0.01569948,0.01912164,-0.023263425,0.04954414,0.013651625,0.003047466,0.0057499567,-0.02153886]	Keywords: Transformer, BERT, Architecture Search, Chinese Language Processing, Named Entity Recognition (NER), Early Exit, Multimodal Pretraining, Differentiable Architecture Search\nKey Objects: FLAT, M6, DARTS\nRefers to Images: None\nHypothetical Questions:\n- What is the primary focus of the papers listed?\n- What are some common applications of the Transformer architecture mentioned?\n- Which papers specifically deal with accelerating Transformer inference?\n---\nSummary:\nThis is a segment of a references section for a survey or paper focused on Transformers. It lists several papers concerning optimization, architecture search, and applications of Transformers, particularly in Chinese language processing and applications like Named Entity Recognition (NER).\nOriginal Text:\n- [79] Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, and Xuanjing Huang. 2021. Accelerating BERT Inference for Sequence Labeling via Early-Exit. arXiv:2105.13878 [cs.CL]\n- [80] Xiaonan Li, Hang Yan, Xipeng Qiu, and Xuanjing Huang. 2020. FLAT: Chinese NER Using Flat-Lattice Transformer. In Proceedings of ACL . 6836-6842. https://doi.org/10.18653/v1/2020.acl-main.61\n- [81] Junyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang Wang, Le Jiang, Xianyan Jia, Jie Zhang, Jianwei Zhang, Xu Zou, Zhikang Xia, Lioadeng Die, Jie Liu, Jinbao Xue, Huiuling Zhou, Jianxin Ma, Jin Yu, Yong Li, Wei Lin, Jingren Zhou, Jie Tang, and Hongxia Yang. 2021. M6: A Chinese Multimodal Pretrainer. arXiv:2103.00823 [cs.CL]\n- [82] Haxionai Liu, Karen Simonyan, and Yiming Yang. 2019. DARTS: Differentiable Architecture Search. In Proceedings of ICLR . https://openreview.net/forum?id=S1eYeH0CxFX\nContextualized Text:\nThis is a list of references, implying a formal document likely a survey or research paper discussing Transformers.	{"tags": ["references", "transformer", "nlp", "cs"], "doc_id": "ab6f0556-60f9-48d6-9ef5-aa68030cd59e", "summary": "This is a segment of a references section for a survey or paper focused on Transformers. It lists several papers concerning optimization, architecture search, and applications of Transformers, particularly in Chinese language processing and applications like Named Entity Recognition (NER).", "doc_type": "text", "entities": ["Xiaonan Li", "Yunfan Shao", "Tianxiang Sun", "Hang Yan", "Xipeng Qiu", "Xuanjing Huang", "Junyang Lin", "Rui Men", "An Yang", "Chang Zhou", "Ming Ding", "Yichang Zhang", "Peng Wang", "Ang Wang", "Le Jiang", "Xianyan Jia", "Jie Zhang", "Jianwei Zhang", "Xu Zou", "Zhikang Xia", "Lioadeng Die", "Jie Liu", "Jinbao Xue", "Huiuling Zhou", "Jianxin Ma", "Jin Yu", "Yong Li", "Wei Lin", "Jingren Zhou", "Jie Tang", "Hongxia Yang", "Haxionai Liu", "Karen Simonyan", "Yiming Yang"], "keywords": ["Transformer", "BERT", "Architecture Search", "Chinese Language Processing", "Named Entity Recognition (NER)", "Early Exit", "Multimodal Pretraining", "Differentiable Architecture Search"], "key_objects": ["FLAT", "M6", "DARTS"], "contextual_text": "This is a list of references, implying a formal document likely a survey or research paper discussing Transformers.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the primary focus of the papers listed?", "What are some common applications of the Transformer architecture mentioned?", "Which papers specifically deal with accelerating Transformer inference?"]}
c9aa440b-6c4e-41c6-85d8-57b547bae646	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.017866949,0.021100022,-0.00678904,0.008412033,-0.002222326,0.06645864,0.012373055,0.048622627,0.033578653,-0.041874457,-0.024023978,0.011838432,0.024453565,-0.01796337,-0.032234054,0.045175303,0.008300597,0.06839676,0.024398273,-0.017764296,0.051510435,0.008927973,0.014419335,-0.052499972,-0.002747889,0.025849156,0.029099794,-0.027724145,-0.0033558335,0.044641666,-2.3821904e-05,-0.02091755,0.037704565,0.01549497,0.008437082,0.031176103,0.0022359826,0.017298035,0.034518808,-0.005975075,-0.027096868,0.05462403,-0.07812987,-0.001962483,0.005559182,-0.08801481,-0.07219385,-0.041327737,0.029373938,-0.023339419,-0.038422268,0.010426137,-0.040629238,0.00030008896,0.01077195,-0.02714495,-0.0101276655,-0.050485138,0.035203144,-0.09811869,-0.034470517,0.046027277,-0.046935536,-0.01813169,0.04458986,-0.08279369,-0.0067431806,0.013455948,0.020186083,0.10953305,-0.019720731,0.023800187,-0.011384377,-0.055583894,0.13392775,0.053003293,-0.018242624,-0.03193103,-0.074151516,-0.0076681552,-0.023991682,0.080890104,-0.056123465,-0.048456226,0.07934861,0.0056588254,-0.025912505,-0.04543301,0.07935298,-0.016984219,0.018280132,-0.024593389,-0.032909602,0.0839125,-0.012789982,-0.08716342,0.009136578,-0.042108692,-0.013797533,-0.047946244,-0.025209043,-0.03636146,0.0807994,0.07219896,0.00804116,-0.018821336,-0.044485256,-0.05009189,0.0073163062,0.003320119,-0.010877806,-0.01049519,-0.015508484,0.023067612,-0.021185482,-0.028292987,-0.05940319,0.0024577465,-0.0066288756,-0.00865435,-0.0204011,-0.015549029,-0.017107064,0.011608531,0.057855885,0.043381117,0.0049470374,-0.0026754232,-0.034771133,-0.01852433,-0.004427294,-0.016519887,0.022400064,0.034111746,0.027336983,0.022513775,0.049603034,-0.020037655,0.012390701,-0.00082698563,0.029269041,-0.025839573,-0.029315043,0.031323403,-0.025587266,0.024392344,-0.035011742,-0.0072008073,-0.014498155,0.05780628,-0.0013641765,-0.01121055,0.03536757,0.013209735,-0.0013768815,0.011833827,0.004114932,-0.06998201,0.012367708,-0.0053487625,-0.05509736,-0.061479665,-0.03871466,0.014167209,0.036823813,0.11847449,-0.0006557685,0.019404566,0.031488396,-0.01920075,-0.0290498,0.02016295,-0.0041784816,-0.015849147,0.020388704,-0.016273124,0.00092066085,0.019736577,-0.033958983,-0.005902464,-0.0029019266,0.015580709,-0.011097365,0.0662047,-0.028504964,0.023499921,0.017332612,0.067077644,-0.011622121,-0.014032472,0.0373783,-0.005726633,0.0060331044,0.011695948,0.024163265,0.0041982164,0.036892056,-0.07460406,-0.04571341,-0.02782488,-0.02056716,0.011755247,0.029652074,0.003689908,0.02170582,-0.04082646,-0.026005248,-0.030144928,-0.04292128,-0.0003543162,-0.010955628,-0.010718784,-0.0015581221,0.031081056,0.02998497,0.0009792566,-0.02522939,0.031107383,0.012555626,-0.0032728517,-0.075359,-0.004845737,-0.012511225,-0.009362422,-0.049469884,0.0049768933,-0.04248936,0.028528143,0.0038716595,-0.0139879435,0.0552404,-0.011134597,-0.009711083,0.035363346,0.001625511,0.03727942,-0.023527125,-0.002717928,0.0072047324,0.043843698,0.034160472,0.044761002,-0.009430238,0.076078534,0.02622972,-0.01818587,0.03230577,0.03900371,-0.017528594,0.02564207,0.014398114,0.041683707,0.07095211,0.015792957,-0.011387552,-0.034832995,0.011821449,0.02591433,0.020141272,0.0075255074,-0.012160541,-0.0076330886,0.023995673,-0.007992012,0.008715121,0.02405162,0.021776212,0.044207208,0.017699735,-0.014102422,0.023994017,0.04519091,0.0045607924,-0.031107308,-0.062781006,0.025756305,-0.0022777957,0.019843023,0.05151458,-0.007933836,-0.011052296,-0.019287638,0.05170444,0.0008425687,0.014278114,-0.023979388,-0.011450471,-0.012923972,-0.08271329,-0.02115136,0.0036866905,-0.060642738,0.036844607,-0.023719914,-0.00015016296,-0.019888198,0.017673748,-0.0050910767,-0.054938264,-0.008219781,0.08567745,-0.015598016,0.0071344594,-0.018625706,-0.012254348,0.0127827795,-0.08595769,0.008394409,0.058585133,-0.0045388,0.034319736,-0.005680914,-0.0086969305,-0.029501576,0.052473236,0.110856116,0.06942072,-0.054338496,0.008186045,0.0062098973,-0.011348464,0.0062229685,0.0025827037,0.031253602,0.02513257,-0.00055347144,-0.009459019,-0.04092741,-0.092996195,0.0491524,-0.042792797,0.04056136,0.038081158,0.009938086,-0.017201954,-0.0014725907,0.02385906,0.049644914,0.011857163,0.011862186,-0.015863368,-0.067289606,0.0039867186,-0.014651655,0.008296517,0.021465639,-0.0054811914,0.024277296,0.047454555,-0.003894186,0.026925433,0.03038795,0.052978475,0.029227009,-0.021683259,-0.004575762,0.011788407,-0.008473062,0.02913991,0.023274913,0.074919954,-0.043123245,0.08953389,-0.027465712,-0.011966354,0.008348859,-0.06873208,-0.04853011,0.05434659,-0.039725658,-0.009698595,-0.038239658,0.070128694,-0.03322778,-0.025187848,0.0059469375,0.033487737,0.019074615,-0.0343707,-0.02468559,0.0033446103,0.014661226,-0.022830859,-0.042155035,-0.02142673,0.04184627,-0.0026976746,0.035465382,0.012948897,0.06416314,-0.02410807,0.022455612,0.043126147,0.024086054,-0.011728747,0.0008823566,0.029789867,0.0010863522,-0.002082859,-0.033593956,0.055200826,0.01963446,-0.033352066,0.039977275,0.05123083,0.020644182,-0.071770504,0.02664883,0.06383015,0.0110980235,0.031257946,-0.041364886,-0.060155526,0.033499233,-0.022686195,-0.015968025,0.010586449,0.02859087,-0.056822777,-0.005952753,-0.07055381,0.038827147,-0.023337273,0.044785902,0.022923606,-0.017918564,-0.021862853,0.03064804,-0.026128136,-0.04048042,0.023523154,-0.07292824,0.015787546,0.052920606,0.05491394,0.005180486,-0.03377791,0.07038955,-0.097795695,0.007455129,0.054119542,-0.023284806,0.021125637,-0.03121039,-0.00069267605,-0.020129273,0.012287961,0.021206465,0.022691106,-0.012557041,-0.013389505,0.023727715,0.013002655,-0.028956627,0.0024996747,0.0028057725,-0.042794246,-0.0012537142,-0.0729322,0.0021208348,-0.015927032,0.04283465,-0.0065948083,0.010590907,0.020268619,-0.012371771,-0.032611646,-0.04037328,-0.028763816,0.02105828,-0.0007796065,0.03886281,-0.061722364,-0.010357406,0.0018259238,-0.020730559,-0.04974794,0.034585826,0.035281878,-0.0115221385,-0.047015883,-0.04272897,-0.0024093497,0.07560425,-0.028253661,0.00015637107,-0.036816664,-0.057122167,-0.047245927,0.0052185864,0.021825844,0.069932796,0.03543013,0.016108653,0.024813045,-0.010425779,-0.005996693,0.0014728949,0.018380512,-0.053108443,-0.028720222,0.008762047,0.0944236,-0.00080397155,-0.022201777,0.0066650594,0.019259453,0.0046794787,0.006886305,0.06498592,0.025832089,0.027234627,0.004031656,0.0069459258,-0.019675566,0.03344144,-0.030957216,0.0057788296,0.04105135,0.05084239,-0.026007261,-0.014956258,-0.0113556525,-0.023469657,0.02254015,-0.016664887,0.05305717,0.051020477,0.050103884,-0.047413804,0.0523926,0.0045216912,-0.032966726,0.020497954,-0.06346456,-0.025486875,0.01594627,-0.062240526,-0.030223848,0.009404066,-0.0075061815,-0.00019527315,-0.0022395453,-0.051059075,0.08972076,-0.04546152,-0.00096550636,-0.025576273,0.032116406,0.11597258,-0.015242634,-0.01399761,0.009895355,-0.014631514,-0.020360263,0.024219228,0.06236103,-0.010657068,-0.01917239,0.08110318,-0.008955631,0.040919542,-0.0027788305,-0.055537146,-0.008953477,-0.057548445,-0.031968527,-0.0042655063,-0.0038345975,0.025165899,0.014099707,-0.012739249,-0.0031028637,0.023404602,-0.052479874,0.022964401,0.03592449,-0.018529018,0.025045687,-0.047294203,0.07055227,0.024707587,-0.0020813849,-0.0333394,0.0070269303,0.046208378,0.012459746,0.00337481,0.022937827,-0.026171802,-0.015753899,-0.0241884,0.03197031,0.03143899,-0.036262892,0.0031488843,0.028225627,0.021047598,-0.003445308,-0.04301424,-0.011216804,-0.036211565,-0.03399052,-0.021879347,0.014092687,0.0010602887,0.01026418,0.01086205,-0.026869662,0.0836272,-0.094141334,-0.03411959,-0.034485847,0.03724797,-0.035727512,-0.030176256,0.04022205,-0.00041910395,-0.044862937,-0.03154961,0.06224707,0.0829912,-0.027432505,0.032598715,-0.030828455,-0.009431623,-0.015050109,-0.036654916,-0.014595407,-0.011202561,-0.029289452,-0.025405787,0.08693889,-0.029437967,0.008086338,0.029731173,-0.0013534571,-0.08598136,-0.019141637,0.021154108,0.027816178,-0.01460162,0.020164633,0.0006973963,0.024226122,-0.05227607,-0.02090866,-0.058802806,0.024913944,-0.012943725,0.008954164,0.041748386,0.007080898,-0.075401366,0.027454097,-0.037701376,-0.055489514,-0.0022475445,0.04922878,0.06009223,0.044685077,0.040209867,-0.036476076,-0.103975005,-0.02263243,-0.027025666,0.0185693,-0.043581564,0.027351564,-0.017805396,-0.013858067,0.011014657,0.003016331,-0.015549561,0.013904594,-0.0018658481,-0.013310032,-0.032346558,-0.03974591,-0.027055549,0.0225278,-0.010000004,0.00015784345,0.044379473,0.033092316,-0.08465269,-0.052107286,-0.028548812,-0.013739235,-0.0058500506,0.011842108,0.03692488,0.03490944,-0.01154372,-0.01739139,0.055822622,0.032446004,-0.012787315,0.004054683,0.03950727,-0.020513227,0.027760647,0.021438627,-0.021433057,-0.09051923,0.0027024029,-0.0022832756,-0.062105633,0.039870616,0.041871578,-0.00041564103,-0.03656497,-0.01892655,-0.01658919,0.017171295,0.051810816,0.035361864,0.001418145,0.028443068,-0.030899497,-0.04684047,-0.018324737,-0.040873896,0.029181711,-0.008587473,0.024029698,0.0075073913,0.030554362,0.03931948,0.049932268,-0.021316921,-0.019414408,0.018726172,0.017904725,-0.039971832,0.04982022,-0.037603807,0.009936643,0.059000958,-0.009589249,0.018057814,-0.013611266,0.018900793,0.0076565533,-0.015445529,0.02221816,-0.019653756,-0.06459204,0.00091982976,0.028021097,0.011887567,0.015997414,-0.008754611,-0.04156291,-0.04717585,-0.047561806,0.00053756096,0.037556976,0.011845163,0.027976586,-0.046127968,0.01407589,-0.035187062,0.031474,0.040975228,0.010699133,0.015721807,-0.05079976]	Keywords: Transformer, Deep Learning, Architecture Search, Training, Text Generation, Position Encoding, ICLR, EMNLP, ACL, ICML, ICASSP, OpenReview, DARTS, Informer, BERT, DETR, LazyFormer, Big Bird, Memer, Poolingformer, SETransformer, Predictive Attention Transformer, Hard-Coded Gaussian Attention, Deformable DETR, Wang, HIBERT, BERT Losses Patience, Informer, LazyFormer, Memer, Poolingformer, SETransformer, Predictive Attention Transformer, Hard-Coded Gaussian Attention, BERT Losses Patience, BERT, DARTS, Big Bird, LazyFormer, Poolingformer, SETransformer, Predictive Attention Transformer, Hard-Coded Gaussian Attention, BERT Losses Patience, BERT, Transformers, Deep Learning, Neural Networks, Language Modeling, Natural Language Processing, Computer Vision, Time-Series Forecasting\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What are some common challenges in training Transformer models?\n- How are Transformer architectures being optimized for specific tasks?\n- What are the latest advancements in position encoding for Transformers?\n---\nSummary:\nThis is a list of references related to Transformer models, covering a wide range of topics including architecture search, training difficulties, text generation, position encoding, and more. The list includes authors, titles, conference/journal publications, and links to online resources like OpenReview.net.\nOriginal Text:\n- [82] Haxionai Liu, Karen Simonyan, and Yiming Yang. 2019. DARTS: Differentiable Architecture Search. In Proceedings of ICLR . https://openreview.net/forum?id=S1eYeH0CxFX  \n- [83] Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. 2020. Understanding the Difficulty of Training Transformers. In Proceedings of EMNLP. 5747-5763. https://doi.org/10.18653/v1/2020.emnlp-main.463\n- [84] Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. 2018. Generating Wikipedia by Summarizing Long Sequences. In Proceedings of ICLR. https://openreview.net/forum?id= Hyg0vbWC-\n- [85] Xuanqing Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, and Cho-Jui Hsieh. 2020. Learning to Encode Position for Transformer with Continuous Dynamical Model. In Proceedings of ICML. 6327-6335. http://proceedings.mlr.press/v119/liu20n.html\nContextualized Text:\nThis list appears to be part of a survey or comprehensive overview of Transformer architectures and related research.	{"tags": ["references", "transformers", "deep learning", "natural language processing", "computer vision", "iclr", "emnlp", "acl", "icml", "icaspp"], "doc_id": "c9aa440b-6c4e-41c6-85d8-57b547bae646", "summary": "This is a list of references related to Transformer models, covering a wide range of topics including architecture search, training difficulties, text generation, position encoding, and more. The list includes authors, titles, conference/journal publications, and links to online resources like OpenReview.net.", "doc_type": "text", "entities": [], "keywords": ["Transformer", "Deep Learning", "Architecture Search", "Training", "Text Generation", "Position Encoding", "ICLR", "EMNLP", "ACL", "ICML", "ICASSP", "OpenReview", "DARTS", "Informer", "BERT", "DETR", "LazyFormer", "Big Bird", "Memer", "Poolingformer", "SETransformer", "Predictive Attention Transformer", "Hard-Coded Gaussian Attention", "Deformable DETR", "Wang", "HIBERT", "BERT Losses Patience", "Informer", "LazyFormer", "Memer", "Poolingformer", "SETransformer", "Predictive Attention Transformer", "Hard-Coded Gaussian Attention", "BERT Losses Patience", "BERT", "DARTS", "Big Bird", "LazyFormer", "Poolingformer", "SETransformer", "Predictive Attention Transformer", "Hard-Coded Gaussian Attention", "BERT Losses Patience", "BERT", "Transformers", "Deep Learning", "Neural Networks", "Language Modeling", "Natural Language Processing", "Computer Vision", "Time-Series Forecasting"], "key_objects": [], "contextual_text": "This list appears to be part of a survey or comprehensive overview of Transformer architectures and related research.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some common challenges in training Transformer models?", "How are Transformer architectures being optimized for specific tasks?", "What are the latest advancements in position encoding for Transformers?"]}
c0fe203e-5520-480f-8c88-0636e5397b94	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.024397437,0.036887936,0.028660038,0.04115904,0.0126155615,0.049850017,0.018597275,0.07227307,0.051315214,-0.028350528,0.003433126,0.041370995,0.013426561,0.009453917,-0.061062936,0.050299346,-0.014170076,0.0647266,0.016927956,-0.04906652,0.023606218,0.032807812,0.016947722,-0.033687208,-0.009540522,0.03874765,0.003106972,-0.016496856,0.017549215,0.009139521,0.013808293,-0.05520138,0.03369113,0.002950212,0.019951144,0.02141992,-0.003108072,-0.011204278,0.036324605,-0.016142627,-0.010235456,0.03533853,-0.07652119,-0.035953723,-0.012139804,-0.048105977,-0.097309515,-0.017824005,0.026723819,-0.019670647,-0.040515307,0.01597271,-0.040680386,-0.03467556,0.027638778,-0.017051674,0.002560968,-0.06093628,0.034127455,-0.0854657,-0.053981353,0.03789949,-0.034461714,-0.03349629,0.035760682,-0.043393824,0.01417031,0.0014353989,0.02552781,0.11619274,-0.026723407,0.02544242,-0.0038828494,-0.021837959,0.1106657,0.03307364,-0.023390047,-0.035678305,-0.052529298,-0.007767274,-0.045206953,0.07856421,-0.060628973,-0.037720203,0.09003737,-0.008395372,-0.0064308634,-0.031354863,0.08002172,-0.014801562,0.028612131,-0.041017354,-0.0011103675,0.044487502,-0.009348986,-0.06755668,0.0038371638,-0.042248886,-0.0013263933,-0.05079786,-0.0034426253,-0.036167335,0.0825768,0.089637265,0.012688488,-0.04034314,-0.015497496,-0.049297173,0.021387365,0.0051346207,-0.025594715,-0.008203634,-0.029144354,0.01706243,-0.0003411121,-0.016215812,-0.03423098,-0.02259005,-0.0013290321,-0.021888847,-0.011965727,-0.0044002547,0.00083842064,0.008846878,0.05594403,0.05793613,-0.013790534,-0.009866445,-0.025975898,-0.00015022172,0.0075619575,-0.012660639,-0.010768808,0.0379225,-0.013477805,0.026438003,0.07364436,-0.023309179,0.016842226,-0.01829711,0.013759599,-0.02714704,0.0010424709,0.034336608,-0.012831927,0.013775385,-0.03470372,0.024015164,0.0053788,0.023494974,4.7754686e-05,-0.014056601,0.033252295,0.01631769,0.006504622,0.000102394115,-0.008329311,-0.07176362,-0.009383046,-0.024372628,-0.07128141,-0.072901845,-0.03647566,0.020491794,0.042950314,0.114023894,0.044525683,0.00437397,0.05660092,0.012398211,-0.06456192,0.008091632,-0.031705882,-0.053702213,-0.008486362,-0.027346939,0.008870719,-0.019161118,-0.026327668,0.02787055,0.0035223612,0.011707468,-0.0062168683,0.053416073,-0.023326019,0.027118703,0.015986012,0.077251524,-0.012764501,-0.025961671,0.018632714,-0.00505242,0.032930702,0.01888404,0.042456962,0.025621725,0.046263404,-0.07736946,-0.03691211,-0.0022599548,-0.02139481,-0.0021829947,0.001821134,-0.0141066285,0.016909469,-0.01932282,-0.0133931115,-0.04779943,-0.021583512,-0.02378477,-0.010513545,-0.013200599,-0.012787082,0.03678094,0.06760588,-0.02350366,-0.011757031,0.0019968043,0.026472602,-0.019243676,-0.07170717,0.0065138657,-0.021483364,0.024268223,-0.020820549,-0.021898663,-0.044764522,0.02582076,0.0054392,-0.04513483,0.03771266,0.0061061047,-0.034840077,0.0006757624,-0.029662607,0.05709427,-0.013699172,-0.007227037,0.021194994,0.04461293,0.029671757,0.0495811,0.02066231,0.067033716,-0.005774092,-0.010247769,0.01389185,0.039725572,-0.032816216,0.028726608,-0.009556189,0.077054724,0.026944906,0.03474743,-0.009749973,-0.045455072,0.023969306,0.02271229,0.011743143,0.039539415,-0.008017401,0.010210247,0.028820347,0.041815918,0.0060477923,0.011957285,0.017628474,0.026130348,0.02616334,0.014789284,0.03320718,0.07771901,0.002048451,-0.02976815,-0.07642078,-0.006033499,0.0028460913,0.029391665,0.06707351,-0.0037231427,-0.007343862,-0.015338625,0.04710212,0.009541153,0.0061138337,-0.034382574,0.0059891255,-0.016184334,-0.102989756,-0.009802111,-0.011335541,-0.04993572,0.020009937,-0.024212725,0.009906199,-0.003370157,0.0045389235,-0.009239223,-0.044758715,0.0021898765,0.07846918,-0.003223751,-0.003474075,-0.019958382,-0.007771073,0.028762441,-0.055571135,0.004552977,0.039855365,0.0073166806,-0.006813562,-0.028576955,-0.013624536,-0.01005767,0.013115909,0.11665199,0.06939816,-0.0493507,-0.0015404474,0.030397486,0.006268385,-0.012132198,0.0029992822,0.015823979,0.020786935,0.0072428873,-0.0111784,-0.03144864,-0.08766096,0.06128529,-0.10033759,0.025465418,0.058828562,-0.011186829,-0.018014865,0.00038352315,0.037663817,0.042815704,-0.0007630813,0.030619867,-0.014862617,-0.06884787,-0.010081769,-0.008452313,0.0032935294,0.022810016,0.010269878,0.030219842,0.044749014,-0.018451124,0.006233191,0.017622853,0.019185962,0.033754565,0.0064343577,-0.019850282,0.015438525,0.004570447,0.010645972,0.05236253,0.062404763,-0.031944156,0.06806012,-0.040987864,0.013144806,0.005909929,-0.06531028,-0.037844002,0.048751786,-0.022321044,-0.00809021,-0.038024154,0.045076385,-0.037617523,-0.01455443,-0.023046292,0.06443002,0.034306757,-0.058260214,-0.014063646,-0.0050082956,-0.0005301586,-0.04702185,-0.022320196,-0.0139880255,0.022073267,0.027714295,0.038109384,-0.0045083705,0.047280714,-0.020961242,0.0016060331,0.0399185,0.008991318,-0.02628956,0.029365119,0.03320408,0.021069368,0.012406137,-0.025945231,0.032192867,0.049523924,-0.027500628,0.034707736,0.04919806,0.03768966,-0.046180643,0.017728068,0.08876774,0.033571083,0.028924048,-0.00013620965,-0.029727893,0.009388673,0.0021430417,0.0041275816,-0.011595421,-0.0029872975,-0.052378003,0.0018784103,-0.07216047,0.04997287,-0.0062358296,0.032884587,0.033976912,-0.015813762,-0.02592642,0.040380023,-0.012778643,-0.029161468,0.014749022,-0.05689755,-0.01516675,0.030617855,0.018328838,0.027374053,-0.06899412,0.046626464,-0.103843756,0.007841104,0.046142504,-0.040303923,0.005504974,-0.038384728,0.0013833536,-0.01800612,0.009986271,0.04194914,-0.019576987,0.0026568035,-0.002377714,0.022312291,0.042510033,-0.018478246,0.0004618717,-0.004482114,-0.033680934,0.028182076,-0.028562661,0.026786027,-0.016731648,0.06647831,0.004914478,-0.00049164693,0.0012862886,-0.025639214,-0.04813818,-0.02374342,-0.03987583,-0.011498646,0.00625125,0.03884635,-0.05894151,-0.009040265,-0.013245853,-0.020988123,-0.016675353,0.07815611,0.04391457,-0.006078429,-0.048354268,-0.022548936,0.002550551,0.044020243,-0.01293716,-4.213683e-05,-0.002707165,-0.041946884,-0.07476346,0.040793292,-0.0067149084,0.059399955,0.046825074,0.019264465,0.017768111,-0.022648577,0.005535711,-0.0029122385,0.0006236043,-0.056109738,-0.04585017,0.01014563,0.04868783,-0.0009816575,-0.014009011,0.019616773,0.0015716928,-0.010405069,-0.019442907,0.050582524,-0.0061690644,0.013761978,-0.016257994,-0.0024170915,-0.02697985,0.03147663,-0.0182448,0.015700601,0.009110564,0.059809137,-0.000688619,0.020438505,-0.007268295,-0.040268112,0.025137074,-0.007937693,0.04343758,0.05365558,0.08477245,-0.0332819,0.05949897,-0.021574197,-0.033412397,0.002514629,-0.056825135,-0.01106266,0.0019917157,-0.06509634,-0.03170658,0.025613757,-0.031437665,-0.02133107,0.0047339927,-0.06274007,0.064474605,-0.056662522,0.040551286,-0.013120148,0.0036193621,0.1016036,0.00314344,-0.027555821,-0.017453501,-0.02454126,-0.018853309,0.028541477,0.06160374,0.0336102,-0.0044463053,0.05959711,-0.030403614,0.015017059,-0.007881646,-0.047605693,-0.009329804,-0.04441483,-0.022575391,0.01637034,-0.023748258,-0.0027565137,-0.01657645,0.0076444317,-0.0051935255,0.005107209,-0.019398814,0.02804745,0.057860985,-0.0053848485,0.06900403,-0.056634035,0.05929785,0.056866433,0.0074234763,-0.03147511,0.020858122,0.0036115323,-0.0076241763,-0.016304709,0.029790675,-0.002872724,-0.023753157,-0.013957807,0.04772848,-0.0029797365,-0.049032174,-0.0057588657,-0.009722914,0.027379293,0.011874843,-0.016958956,-0.024982851,-0.0027546848,-0.04909346,-0.009404447,0.02173718,0.0077419803,0.009946188,-0.0022416739,-0.024011882,0.067885086,-0.12510969,-0.035321273,-0.007176511,0.055712968,0.007989819,0.00017875047,-0.021346133,-0.033233345,-0.03326487,-0.04323026,0.06204141,0.06713238,-0.050953172,0.0019061757,0.0032278828,-0.036532175,-0.0012547311,-0.035248626,-0.024199728,-0.0019793557,-0.03446611,-0.0071500926,0.08034249,-0.014468544,-0.012519499,0.025152529,0.014379842,-0.074215084,-0.038166575,-0.014295365,0.025023455,-0.020500306,-0.0027420316,0.04368944,0.04129291,-0.08202939,-0.015252185,-0.04604353,0.028151114,-0.013792594,0.0024054183,0.07109145,0.015591876,-0.055421803,0.024826126,-0.017564073,-0.019872323,-0.032736357,0.055804193,0.044467486,0.04243715,0.052531485,-0.05049694,-0.08507343,-0.00725121,-0.012509074,-0.015014371,-0.028491886,0.0393633,-0.018021278,-0.03387554,0.009245412,-0.01494043,-0.003905123,0.014524199,0.026035521,0.005050579,-0.037876166,-0.036816165,0.0372117,0.023949808,-0.015486939,0.016001353,0.044356916,0.018801287,-0.06115577,-0.04149846,-0.04249361,-0.036997437,-0.0061247027,-0.013543419,0.03137308,0.03338033,-0.02716078,0.018576456,0.05195016,0.004170867,-0.052014556,0.013207902,0.036658663,-0.009198735,0.04396679,0.0020315128,0.0013253777,-0.11453654,-0.0025310216,-0.0037584442,-0.09609987,0.039836552,0.015865263,-0.013481818,-0.0458271,-0.009078194,-0.0157618,-0.0014402225,0.04884072,0.039463885,0.00034993852,0.015645841,-0.025044259,-0.066998735,-0.012412098,-0.017390495,0.02062395,-0.013523081,0.04241301,0.020938916,-0.0026139342,0.03393688,0.0007321695,-0.039589554,-0.03284216,-0.01037723,0.07646116,-0.042181347,0.04168132,-0.02428125,-0.013164788,0.053965867,-0.026998525,0.040211033,-0.038505316,0.03905118,-0.0047575827,-0.010580541,-0.010458012,-0.016964627,-0.08690077,0.031418506,0.03475375,0.011334557,-0.008611884,-0.00724157,-0.027901022,-0.05915083,-0.023136232,-0.021393815,0.029145868,0.00031253087,0.054079063,-0.020046245,0.0243508,-0.046930455,0.02278437,0.03645974,0.02110761,0.0018431027,-0.023800168]	Keywords: transformers, BERT, vision transformer, long sequences, attention, pretraining, efficiency, architecture, survey, machine learning, natural language processing, computer vision, deep learning, Swin Transformer, Informer, LazyFormer, ROBERTA, Attention is All You Need, hierarchical transformers, lengthy context, document summarization, robustness, dynamic systems, algorithmic efficiency, pooled attention\nKey Objects: hierarchical transformers, pretraining approach, vision transformer, self-attention mechanism, document summarization\nRefers to Images: None\nHypothetical Questions:\n- What are some of the key advancements discussed in this survey?\n- Can you list the papers that focus on handling longer sequences?\n- What are the foundations of the Transformer architecture?\n- How do vision transformers differ from standard Transformers?\n---\nSummary:\nThis is a list of references, likely from a survey paper titled "A Survey of Transformers."  It includes a wide range of publications related to transformer architectures, from foundational works like Attention is All You Need and BERT, to more recent advancements in vision transformers (Swin Transformer), techniques for handling longer sequences (Informer, LazyFormer), and methods for improving efficiency and robustness.  The citations cover topics like hierarchical transformers, pretraining approaches, vision tasks, and the underlying mechanics of transformers.\nOriginal Text:\n- [86] Yang Liu and Mirella Lapata. 2019. Hierarchical Transformers for Multi-Document Summarization. In Proceedings of ACL. Florence, Italy, 5070-5081. https://doi.org/10.18653/v1/P19-1500\n- [87] Yinhan Liu, Myle Ott, Naman Goyal, Jingjie Du, Manad Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. ROBERTA: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.116929 (CSL).\n- [88] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Xixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv:2103.14030 (cs.CV)\n- [89] Yiping Liu, Zhuhoan Li, Di He, Zhiquing Sun, Bin Dong, Tao Qi, Linwei Wang, and Tie-Yan Liu. 2020. Understanding and Improving Transformer from a Multi-Particle Dynamic System Point of View. https://openreview.net/forum? id=SJl102NfW-\nContextualized Text:\nThis is a list of references, likely from a survey paper titled "A Survey of Transformers". It includes a wide range of publications related to transformer architectures, from foundational works like Attention is All You Need and BERT, to more recent advancements in vision transformers (Swin Transformer), techniques for handling longer sequences (Informer, LazyFormer), and methods for improving efficiency and robustness.	{"tags": ["references", "bibliography", "machine-learning", "deep-learning", "transformer-models", "natural-language-processing", "computer-vision"], "doc_id": "c0fe203e-5520-480f-8c88-0636e5397b94", "summary": "This is a list of references, likely from a survey paper titled \\"A Survey of Transformers.\\"  It includes a wide range of publications related to transformer architectures, from foundational works like Attention is All You Need and BERT, to more recent advancements in vision transformers (Swin Transformer), techniques for handling longer sequences (Informer, LazyFormer), and methods for improving efficiency and robustness.  The citations cover topics like hierarchical transformers, pretraining approaches, vision tasks, and the underlying mechanics of transformers.", "doc_type": "text", "entities": ["BERT", "ROBERTA", "Swin Transformer", "Informer", "LazyFormer", "Attention is All You Need", "BERT Losses Patience", "Hierarchical Transformers", "Deformable DETR", "Pooled Attention", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Yiping Liu", "Zhuhoan Li", "Di He", "Zhiquing Sun", "Bin Dong", "Tao Qi", "Linwei Wang", "Tie-Yan Liu", "Yang Liu", "Mirella Lapata", "Yinhan Liu", "Myle Ott", "Naman Goyal", "Jingjie Du", "Manad Joshi", "Danqi Chen", "Omer Levy", "Mike Lewis", "Luke Zettlemoyer", "Veselin Stoyanov", "Ze Liu", "Yutong Lin", "Yue Cao", "Han Hu", "Xixuan Wei", "Zheng Zhang", "Stephen Lin", "Baining Guo", "Wangchunshu Zhou", "Julian Mcauley", "Ke Xu", "Furu Wei"], "keywords": ["transformers", "BERT", "vision transformer", "long sequences", "attention", "pretraining", "efficiency", "architecture", "survey", "machine learning", "natural language processing", "computer vision", "deep learning", "Swin Transformer", "Informer", "LazyFormer", "ROBERTA", "Attention is All You Need", "hierarchical transformers", "lengthy context", "document summarization", "robustness", "dynamic systems", "algorithmic efficiency", "pooled attention"], "key_objects": ["hierarchical transformers", "pretraining approach", "vision transformer", "self-attention mechanism", "document summarization"], "contextual_text": "This is a list of references, likely from a survey paper titled \\"A Survey of Transformers\\". It includes a wide range of publications related to transformer architectures, from foundational works like Attention is All You Need and BERT, to more recent advancements in vision transformers (Swin Transformer), techniques for handling longer sequences (Informer, LazyFormer), and methods for improving efficiency and robustness.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the key advancements discussed in this survey?", "Can you list the papers that focus on handling longer sequences?", "What are the foundations of the Transformer architecture?", "How do vision transformers differ from standard Transformers?"]}
f854eeb0-1785-474a-bc03-591ad7345303	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0011084109,0.020163385,-0.016434012,0.027853616,-0.012338385,0.062809706,0.02667227,0.049793936,0.053091086,-0.024727384,0.00051827857,0.012507749,0.009709329,-0.006647851,-0.039772302,0.026002742,-0.010379044,0.069230825,0.046430945,-0.034969203,0.04405098,0.010291819,0.012544555,-0.030378956,-0.0059094625,0.018041879,0.029643897,-0.03164826,0.008633714,0.03744219,0.012655891,-0.06607102,0.027198577,-0.002439918,0.028452206,0.0011329588,0.006090798,0.008594238,0.021836078,-0.020197824,-0.0045287283,0.03459252,-0.06768089,-0.006519616,-0.013590275,-0.048301317,-0.0660171,-0.028011294,0.029431893,-0.012120742,-0.031437013,0.040769856,-0.030574549,-0.013498192,0.0027517949,-0.03147329,0.013363361,-0.040863723,0.027471913,-0.1236074,-0.024545077,0.028979493,-0.027052293,-0.0052065956,0.03612923,-0.054042738,-0.015624041,-0.016877187,0.035823207,0.12170835,-0.03826295,0.010916271,-0.0075590783,-0.07454801,0.11617794,0.047315024,-0.012698086,-0.011980496,-0.0798853,-0.019276034,-0.032219417,0.07085746,-0.06246326,-0.043814547,0.08447102,0.0021705753,-0.042436615,-0.010127918,0.095863886,-0.028397836,-0.0016301008,-0.032675263,-0.009397106,0.078460746,-0.04609821,-0.052888546,0.023887057,-0.03397983,-0.011437737,-0.079308905,-0.02130242,-0.00034670683,0.08666056,0.09231787,0.011716699,-0.030833576,-0.051560957,-0.04274741,0.01126639,0.013271781,-0.013196241,0.027570525,-0.012451094,0.021523468,-0.0015611409,0.0011597938,-0.06274674,0.029033132,2.9632434e-05,0.0008176401,-0.005803742,0.0006832319,0.013750134,0.011562124,0.07865511,0.03375407,0.030613462,-0.0049627125,-0.04183282,0.012910482,0.01946577,5.41051e-05,-0.012025301,0.00017913332,0.0049812044,0.0058820364,0.060299166,-0.021226112,0.012355407,-0.0017173872,0.027520094,-0.03694291,-0.02426531,0.031406842,-0.019772323,0.011177582,-0.056094892,0.0026649106,-0.02725889,0.05552114,0.025125714,0.0052834684,0.034858122,-0.004749016,0.01051264,0.006851597,0.009247678,-0.05359192,-0.003787348,-0.011731649,-0.04493691,-0.06596511,-0.017731832,0.026559876,0.05721206,0.14494304,-0.005678911,-0.0056901844,0.055885088,0.001640074,-0.04676844,0.007971213,-0.013293035,-0.0233939,0.01340452,-0.00636171,0.014306076,-0.0019070567,-0.032349624,-0.0057832412,0.029797718,0.009959197,-0.01941458,0.04358818,-0.0453092,0.042198956,0.015047783,0.07799036,-0.012008336,-0.03952695,0.016798142,-0.023595408,-0.009042238,-0.008553268,0.033053447,0.008035358,0.03686451,-0.08791224,-0.037173737,-0.046567444,-0.03674527,-0.004981596,0.0028718675,-0.0013951298,0.028300213,-0.027832832,-0.005823237,-0.02752914,-0.031112857,-0.0069576367,-0.0058857403,-0.029026164,-0.01048501,0.05015856,0.016733969,-0.0028728626,-0.028475823,0.033166915,0.014314601,-0.008049378,-0.090540685,-0.0020776577,-0.0214139,0.022041747,-0.03412026,0.014687557,-0.038343158,0.02342967,0.024838924,-0.013557941,0.04229748,-0.010541002,-0.009436383,0.012607345,-0.0031380234,0.04509049,0.0001747324,0.002468933,-0.014248298,0.037253223,0.035204485,0.07027749,0.005186124,0.07908523,0.025289029,-0.0113023445,0.01855105,0.04882892,-0.021804027,0.007927304,-0.007516548,0.06424775,0.05539697,-0.016255489,-0.011506177,-0.04615343,0.01771781,0.040879723,0.022346422,0.0067076706,-0.011558783,-0.020377766,0.040606353,-0.011111111,0.012582854,0.012383287,0.002327459,0.025267564,0.028469639,0.003010217,0.026391696,0.028175235,0.0077000707,-0.015005681,-0.055945918,0.0023184924,0.012313088,0.027353942,0.01744017,-0.015107436,0.002712243,-0.05747504,0.039200276,-0.0079902215,0.032298334,-0.016203713,0.0065111914,-0.030128049,-0.0634189,-0.010156352,0.011784016,-0.04091687,-0.012043743,-0.014091061,0.016632713,-0.008840381,-1.5298549e-05,-0.028664188,-0.015121802,0.009558026,0.094511524,-0.005779463,0.007108602,0.017174732,-0.015583213,0.024631664,-0.08579881,0.042873897,0.0323087,0.0040538986,0.008176819,-0.008828456,0.0021058607,-0.01913688,0.027640615,0.09115563,0.07603693,-0.040043164,-0.0035420612,0.008961484,0.048089456,0.0059170644,-0.0074927146,0.051881295,0.04991389,0.0055600503,0.013389338,-0.030638514,-0.076187186,0.077365465,-0.091663904,0.045276012,0.0509729,0.004492894,-0.00085682515,-0.00866493,0.028412692,0.016064517,0.0035657268,0.01183084,-0.0058223056,-0.07793259,-0.00512341,-0.021197155,-0.027451131,0.023424827,-0.008671299,0.013011725,0.04251626,-0.00013979523,0.021007804,0.03680282,0.04048993,0.028270457,-0.012329398,-0.03137703,-0.0038964425,0.022997951,-0.0022784402,0.02096516,0.056169186,-0.035135172,0.1039713,-0.021426557,0.031691283,0.0006435753,-0.05301213,-0.033806022,0.06206955,0.0044813305,-0.025711264,-0.044235777,0.045723457,-0.03903717,-0.02704934,-0.0039444696,0.037136536,0.042351905,-0.0414075,-0.017005721,-0.020843366,-0.010368815,-0.039118744,-0.03570455,-0.023408722,0.066504404,0.011913651,0.024709336,0.008543294,0.013806349,-0.044214405,-0.007678955,0.05938009,0.002579863,-0.012656794,0.013239406,0.047751162,0.031296548,0.01518308,-0.000635287,0.0511559,0.027510118,-0.0427596,0.02620299,0.062441684,0.041456632,-0.09121795,0.04900415,0.055636767,0.006341827,0.022710498,-0.03460338,-0.03099465,0.014079324,0.0049713105,-0.015475115,0.0010603325,0.025949374,-0.06343861,0.012500313,-0.04751859,0.058570806,0.0074278805,0.027114317,0.03222377,-0.011983144,-0.019903554,0.04788326,-0.014532265,-0.049018238,-0.006075937,-0.083950356,-0.004295953,0.03156203,0.032299034,0.022726577,-0.07660152,0.08207824,-0.081600554,0.011092242,0.05837065,-0.014139377,0.019776572,-0.046394702,-0.010175522,-0.034829404,0.021095565,0.03779902,0.0030000706,-0.019432075,0.007925709,0.019235121,0.03520542,-0.025631998,0.014836943,-0.005227128,-0.03357596,-0.0017997986,-0.05018358,-0.013826903,-0.0035442626,0.05438066,0.014824687,-0.004621755,0.0067399447,0.003944037,-0.055732787,-0.054767247,-0.043207847,0.01693428,0.02153679,0.039635655,-0.04130758,-0.007367834,0.014822158,-0.023686115,-0.023405682,0.035469137,0.044933632,0.0075865495,-0.054481372,-0.034227196,0.019545902,0.05854389,-0.009552073,-0.025483757,-0.017101357,-0.009792626,-0.059323445,0.017947506,0.03694874,0.056566052,0.0233817,0.029940305,0.027407518,-0.036676988,-0.010997736,0.0084640095,0.008306914,-0.038543567,-0.03684024,0.010833708,0.06283174,0.032613117,-0.020242156,-0.0045416974,0.024990194,0.011378828,0.036987513,0.045161266,0.03503904,-0.0100697195,0.011609411,-0.003793616,-0.038456697,0.045664124,-0.03628821,-0.0011702244,0.030394735,0.04650279,-0.012319197,-0.030493101,0.0058667855,-0.035885118,0.00048148367,-0.027278556,0.021891832,0.039325185,0.039689176,-0.053250644,0.06023427,-0.020565676,-0.039545804,0.039610673,-0.06407032,-0.04768667,0.0039429227,-0.09543557,-0.0159814,0.014358371,-0.019562311,-0.023353418,0.005415225,-0.038970727,0.09466406,-0.044044636,0.008026554,-0.0016887105,0.005480626,0.118159756,0.016739517,0.0058991266,-0.021290453,-0.00027163513,-0.025643326,0.038483147,0.07074491,0.0030194793,-0.013412809,0.083176665,0.022071702,0.026903981,-0.014019874,-0.07061632,-0.011603118,-0.03379901,-0.034197465,-0.014918837,-0.027697144,0.048154444,0.015552867,-0.019534072,0.012771906,0.027665662,-0.026355382,0.014370987,0.0329836,-0.0015046967,0.023649788,-0.03959473,0.039427184,0.033544503,0.02868694,-0.022415405,0.017119167,0.038965836,0.006367249,-0.019928863,0.0067654983,-0.033499014,0.005251776,-0.0117611755,0.025132135,0.026258158,-0.028770812,-0.025977947,0.013561088,0.021265231,0.03641895,-0.038957026,-0.023301855,-0.021399386,-0.023558483,-0.034142505,0.011839669,0.015499084,0.012303628,-0.0011206334,-0.03895999,0.07290598,-0.089755744,-0.009988676,-0.040477853,0.028569287,-0.0172313,-0.0035991704,0.026607266,-0.02676215,-0.032835077,-0.037061494,0.04532954,0.027377598,-0.05143181,0.022728309,-0.027985804,-0.015610477,-0.022836786,0.031828187,-0.010058923,-0.011881273,-0.016455848,-0.009964902,0.08041681,-0.044852156,-0.011273215,0.029531041,-0.036040917,-0.07514479,-0.046790197,0.007818894,0.014974321,-0.032481465,0.00013834465,0.03095823,0.02078431,-0.056018356,-0.04235785,-0.07557918,0.044294413,0.00943799,-0.002086598,0.033649616,-0.026233083,-0.065177254,0.01348201,-0.021509046,-0.041636698,0.02513697,0.009600025,0.048497062,0.05389284,0.04671673,-0.055677436,-0.088970795,-0.031827554,-0.040300842,-0.004059545,-0.048557613,0.06367827,-0.005321533,-0.012386646,-0.002313708,-0.016145565,0.0043466096,0.022417542,0.009719204,0.018022416,-0.007144248,-0.015346048,-0.021025028,0.03310112,-0.0114959,-0.016376564,0.021517085,0.026757687,-0.044490024,-0.068543024,-0.037113167,-0.017912608,0.0028024092,-0.005415473,0.03158726,0.043380246,-0.008696852,-0.0057821306,0.06770501,0.022242699,-0.025957964,-0.0029904016,0.027544105,-0.020060914,0.017046161,0.03150908,-0.025051551,-0.11332696,0.0071647363,-0.015551151,-0.06680023,0.04216186,0.0007349612,-0.011233734,-0.047955576,-0.0068819034,0.0047320807,0.04245501,0.045983855,0.022714186,0.004580779,-0.008207274,-0.012756041,-0.05881232,-0.011082188,-0.015595891,0.019143933,-0.020116635,0.029341007,0.009561531,0.033058707,0.01886987,0.03084756,-0.012981374,-0.0013518418,0.025809344,0.039897565,-0.028264694,0.06364411,-0.008751063,0.00799054,0.04955957,-0.014518038,0.030550212,0.003022265,0.016227135,-0.0015810587,0.028510606,0.017606273,-0.046509612,-0.058610324,0.015155091,0.0061915834,0.011595576,0.0100215385,-0.010243557,-0.02604481,-0.036969785,-0.044556987,-0.01433091,0.06182998,0.022588052,0.01958954,-0.07223508,-0.008141938,-0.033713028,0.047271874,0.01791623,0.009202206,-0.0154335145,-0.030392105]	Keywords: transformer, attention, deep learning, neural networks, machine learning, efficient, lightweight, document translation, image processing, normalization, architecture, innovation, survey, review, references, architecture search, nested attention, hierarchical attention, normalization, document level, efficient, image processing, architecture innovation, deep learning models, efficient models, transformer architecture, image transformer, hierarchical attention models, normalization models\nKey Objects: transformer models, deep learning architectures, neural networks, machine learning algorithms, efficient models\nRefers to Images: None\nHypothetical Questions:\n- What are the different types of transformer architectures mentioned in this list?\n- How do these references contribute to the advancement of transformer models?\n- What are some common themes or goals addressed by these references?\n- What specific problems are these modifications and improvements designed to solve?\n---\nSummary:\nThis chunk of text provides a list of references related to transformer models, focusing on efficient and modified architectures. It includes references exploring unified nested attention, lightweight transformer designs, document-level translation, improved normalization, and image transformers. The format is consistent with an academic survey or review paper, listing authors, publication year, title, and where the work was published (e.g., arXiv, ICML, EMNLP). The references cover a range of advancements and modifications to the original transformer architecture, aiming for improvements in efficiency, context handling, and application to different domains like image processing and document translation.\nOriginal Text:\n- [90] Xuezhe Ma, Xiang Kong, Sinoq Wang, Chunoting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. 2021. LunaLinear Unified Nested Attention. arXiv:2106.01540 [cs.LG]\n- [91] Sachin Mehta, Marjan Ghazvinineedi, Srinivasan Iyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2020. DeLight: Very Deep and Light-weight Transformer. arXiv:2008.00623 [cs.LG]\n- [92] Lesly Miculich, Dhananjay Ram, Nikolao Pappas, and James Henderson. 2018. Document-Level Neural Machine Translation with Hierarchical Attention Networks. In Proceedings of EMNLP. Brussels, Belgium, 2947-2954. https: //doi.org/10.18653/v1/1D-13825.19\n- [93] Toan Q. Nguyen and Julian Salazar. 2019. Transformers without Tears: Improving the Normalization of Self-Attention. CoRR abs/1910.05895 (S).\n- [94] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image Transformer. In Proceedings of ICML. 4052-4061. http://proceedings.mlr.press/v80/parmar18a.html\nContextualized Text:\nThis chunk of text provides a list of references related to transformer models, focusing on efficient and modified architectures. It includes references exploring unified nested attention, lightweight transformer designs, document-level translation, improved normalization, and image transformers. The format is consistent with an academic survey or review paper, listing authors, publication year, title, and where the work was published (e.g., arXiv, ICML, EMNLP). The references cover a range of advancements and modifications to the original transformer architecture, aiming for improvements in efficiency, context handling, and application to different domains like image processing and document translation.	{"tags": ["references", "transformers", "deep learning", "machine learning"], "doc_id": "f854eeb0-1785-474a-bc03-591ad7345303", "summary": "This chunk of text provides a list of references related to transformer models, focusing on efficient and modified architectures. It includes references exploring unified nested attention, lightweight transformer designs, document-level translation, improved normalization, and image transformers. The format is consistent with an academic survey or review paper, listing authors, publication year, title, and where the work was published (e.g., arXiv, ICML, EMNLP). The references cover a range of advancements and modifications to the original transformer architecture, aiming for improvements in efficiency, context handling, and application to different domains like image processing and document translation.", "doc_type": "text", "entities": ["LunaLinear Unified Nested Attention", "DeLight", "Document-Level Neural Machine Translation", "Transformers without Tears", "Image Transformer"], "keywords": ["transformer", "attention", "deep learning", "neural networks", "machine learning", "efficient", "lightweight", "document translation", "image processing", "normalization", "architecture", "innovation", "survey", "review", "references", "architecture search", "nested attention", "hierarchical attention", "normalization", "document level", "efficient", "image processing", "architecture innovation", "deep learning models", "efficient models", "transformer architecture", "image transformer", "hierarchical attention models", "normalization models"], "key_objects": ["transformer models", "deep learning architectures", "neural networks", "machine learning algorithms", "efficient models"], "contextual_text": "This chunk of text provides a list of references related to transformer models, focusing on efficient and modified architectures. It includes references exploring unified nested attention, lightweight transformer designs, document-level translation, improved normalization, and image transformers. The format is consistent with an academic survey or review paper, listing authors, publication year, title, and where the work was published (e.g., arXiv, ICML, EMNLP). The references cover a range of advancements and modifications to the original transformer architecture, aiming for improvements in efficiency, context handling, and application to different domains like image processing and document translation.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the different types of transformer architectures mentioned in this list?", "How do these references contribute to the advancement of transformer models?", "What are some common themes or goals addressed by these references?", "What specific problems are these modifications and improvements designed to solve?"]}
9b50c8c0-3675-4df4-bf50-0521aaa00287	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.012067514,0.014181035,0.0033913136,0.01971207,0.016633043,0.080051035,0.017588122,0.079731114,0.031058742,-0.041366145,0.0011975286,0.03519369,0.015313312,-0.027208429,-0.042551145,0.029069876,0.01464171,0.07737693,0.030747397,-0.030087566,0.043012597,-0.013314176,0.008100791,-0.034471236,-0.011549105,0.004761945,0.027217016,-0.058626544,0.015089672,0.015463483,0.0023056618,-0.038946856,0.022562455,0.0037254652,0.0063334876,0.020593273,-0.0023458581,0.004985599,0.0060043163,-0.022747701,-0.0065451236,0.025636079,-0.07794016,-0.01211244,-0.02049959,-0.061900556,-0.08518955,-0.021964876,0.020713318,-0.026086213,-0.021627732,0.016818952,-0.021157844,-0.01400964,0.008319989,-0.018868912,-0.0069081825,-0.048592947,0.03648936,-0.096958175,-0.030621115,0.03390376,-0.071680866,-0.0073398105,0.029653344,-0.06552077,-0.019133298,-0.009340327,0.019512396,0.11144327,-0.015994785,0.03502207,-0.016691564,-0.05320977,0.099221095,0.036680657,-0.030838268,-0.038867537,-0.05429297,-0.009460969,-0.02592116,0.06721905,-0.03295655,-0.06649165,0.07999578,0.0069568926,-0.04018608,-0.01966285,0.07551252,-0.0028463532,0.032903664,0.008632254,-0.0035934844,0.066599414,-0.020516727,-0.068145745,-0.006090243,-0.027662672,-0.0055152676,-0.038975887,-0.03412284,0.0024406968,0.07886818,0.084341176,0.021442521,-0.012453905,-0.043855064,-0.031872895,0.0010397058,0.011747704,0.01857093,0.0029136273,-0.00067697425,0.005416656,-0.022204522,-0.035057075,-0.035764057,0.0033180162,-0.0033022014,-0.00052199845,-0.002725963,-0.01101467,0.010205144,0.013509566,0.04506168,0.064138174,0.013175118,0.0023029386,-0.024024855,-0.019068059,0.016953396,-0.012686369,-0.0023048378,0.021684626,0.015871054,0.015000994,0.048497077,-0.045631483,0.009719737,0.021067835,0.026531894,-0.023742,-0.041627735,0.03941432,-0.033315215,0.0003486253,-0.058001626,0.005311885,-0.024232596,0.041196264,0.004274918,0.010515101,0.028435137,0.012750267,0.010836817,-0.013996818,0.0024074726,-0.053072177,0.022685451,-0.009360193,-0.060105965,-0.048702568,-0.048361372,0.0588193,0.042892184,0.12686148,0.017109007,-0.018267704,0.027604671,0.014056999,-0.033355698,0.018897306,-0.027703071,-0.01842847,0.022207621,-0.01866313,0.033095025,-0.022632124,-0.024120199,-0.029942455,-0.0038415878,-0.0008392747,0.00030063422,0.05201655,-0.020202622,-0.014726103,0.0038519232,0.06872542,0.004193363,-0.037232824,0.030851122,-0.0207606,-0.00812176,0.016821032,0.038672198,0.009772787,0.03614022,-0.054356664,-0.052600328,-0.037869494,-0.008435569,0.013889334,0.038540352,-0.01311519,0.022521045,-0.01981639,-0.019638652,-0.02414026,-0.040617175,-0.027096251,-0.006119226,-0.04413896,-0.022275375,0.07298626,0.024351038,0.013526885,-0.006967688,0.025604041,0.048556052,-0.0021943054,-0.06841166,0.02256661,-0.017303212,0.018499596,-0.04670264,0.026349155,-0.021857537,0.038924396,-0.0049965656,-0.02436908,0.039362315,-6.7548746e-05,-0.024987938,0.032889854,0.0027146442,0.056813106,-0.00092352944,0.0010986279,0.020025328,0.057772957,0.034872994,0.05656554,0.0074564833,0.06372706,0.029549016,0.0042187073,0.036146764,0.04384425,-0.010146397,0.02493886,0.0009348402,0.050925788,0.052178852,-0.00557499,0.0044998815,-0.04142739,0.029134754,0.039430954,0.04252089,-0.0048459903,-0.022531455,-0.028985156,0.021833535,-0.010470966,0.00436158,0.014849484,0.032004256,0.040992152,0.0029919655,0.0013247936,0.014596893,0.044815566,-0.0005872125,-0.02776238,-0.0795086,0.010017446,-0.0055379053,0.03198147,0.032815244,-0.007859472,-0.01326773,-0.017465876,0.060666017,-0.0075885043,0.017740864,-0.010250573,-0.011653991,-0.028267128,-0.096002705,0.007437694,0.010668079,-0.03415347,0.04112428,-0.014382823,0.010315295,0.010119473,0.010382645,-0.028296396,-0.026841538,0.021498542,0.09514196,-0.03435884,0.020930745,-0.015342812,-0.008283431,0.035291485,-0.065968744,0.0072318288,0.05351272,0.019451657,0.028859315,-0.017971555,0.0116999885,-0.00978484,0.02708533,0.07118794,0.074566334,-0.015910005,-0.0014530291,0.019949254,0.006073139,0.008378022,-0.020895699,0.047906443,0.03696358,-0.018809812,-0.009202902,-0.03219409,-0.0940887,0.035211947,-0.090823546,0.03885118,0.03365247,0.007997466,-0.0074995067,-0.013479659,0.035756554,0.04043197,0.019377142,0.011080188,-0.0020587354,-0.05991651,-0.01571254,-0.005222952,-0.024020813,0.029045453,-0.0060859383,0.0138480365,0.044980153,0.014633513,-0.02464469,0.03369976,0.057157896,0.031450283,-0.022310236,-0.0069611548,0.0009339962,0.0035395941,0.010723488,0.024944868,0.061741263,-0.053217307,0.10649704,-0.007294262,0.0020016085,-0.011316469,-0.08477354,-0.039192837,0.05042514,-0.030707018,-0.025337553,-0.031583373,0.06199138,-0.050265547,-0.015656369,-0.00749525,0.02090174,0.030248763,-0.058101233,-0.01939165,-0.025027523,0.02967742,-0.0523824,-0.03465962,-0.0283796,0.047797322,0.0017100429,0.051620178,-0.009513904,0.03393258,-0.034261398,0.0190627,0.038675908,-0.00787392,0.001280487,0.011745902,0.034249566,0.028253531,0.023994645,-0.031535804,0.038156606,0.0314477,-0.042251557,0.064876616,0.055448137,0.024166578,-0.086986125,0.031635806,0.0710892,0.025258321,0.03609643,-0.032848608,-0.033513308,0.023508731,-0.024181107,-0.016626533,-0.016380623,0.024823178,-0.046563808,0.024649916,-0.056879282,0.046255235,0.0069512553,0.04252203,0.03157697,-0.00631759,-0.027958022,0.018846188,-0.02459612,-0.036219046,0.013440111,-0.083172455,-0.034990862,0.0443821,0.043502264,0.026466552,-0.0843843,0.06336865,-0.085849546,0.009578631,0.05702234,-0.016273202,-0.016193183,-0.001879736,-0.021266144,-0.0058827125,-0.011121151,0.012996167,-0.0072441814,-0.019468762,0.00011474194,0.03573388,0.004318346,-0.0199229,-0.007840736,-0.04084867,-0.025992863,0.00035189756,-0.06561092,0.006011605,-0.000695169,0.07208423,-0.006069868,0.004861016,0.017568212,-0.0031055394,-0.024691014,-0.024877504,-0.035881933,0.013530478,0.016509773,0.039069947,-0.04274015,-0.004350712,0.0046545328,-0.0071176486,-0.013718704,0.049009286,0.06658239,-0.0014571641,-0.04783259,-0.036745165,0.0065292646,0.047112674,-0.026196437,-0.017373532,-0.031192617,-0.019588042,-0.058971122,0.026352715,0.012511978,0.07067286,0.02762336,0.025809323,0.028774006,-0.024906259,-0.015207394,-0.002796726,0.021938723,-0.046979077,-0.039788328,0.012159776,0.044570927,0.025468389,-0.0154036395,-0.004210014,0.025806848,0.020144114,0.015562587,0.03016926,0.02337355,0.004124749,-0.006713912,-0.0072967303,-0.013899901,0.035353504,-0.029601924,-0.02323734,0.024277808,0.042393696,-0.006921438,-0.016491951,0.0035819279,-0.026413618,-0.0056331665,-0.016918695,0.056648813,0.06128743,0.05336794,-0.043508407,0.074039176,-0.047282886,-0.03976987,0.018118296,-0.0671332,-0.020605093,0.00091898686,-0.066329226,-0.0064845216,0.042190004,-0.016261458,-0.008549518,0.013787884,-0.017121706,0.09040066,-0.076742515,0.038232032,-0.006408446,0.014819849,0.12262845,0.030657738,-0.019291053,-0.01560816,-0.025728704,-0.042991992,0.008226513,0.056727625,-0.014607229,-0.026228735,0.05918944,0.010122774,0.035765003,-0.0027060842,-0.054845728,-0.04362385,-0.04255684,-0.019964263,-0.0034363088,-0.0107158795,0.021103077,0.012816664,-0.008974658,0.010706734,0.024049038,-0.023318931,0.030066539,0.028548317,-0.028370216,0.035514124,-0.039838936,0.048126068,0.02908373,-0.012306031,-0.0078087724,-0.011766245,0.023460845,-0.000928747,-0.015789233,-0.0011596519,-0.010196504,-0.02743946,-0.021819208,0.034867622,0.017845796,-0.032297086,-0.0068015135,0.012510425,0.060748674,0.003692712,-0.0109545775,-0.030177565,-0.031228911,-0.01230515,-0.013978851,0.018411212,0.0016906313,0.005463633,0.014567437,-0.021818262,0.0868819,-0.12665078,-0.0437995,-0.03807486,0.041036665,-0.012511824,-0.0015181535,0.020298302,-0.02755474,-0.020491937,-0.027803432,0.058276568,0.045040295,-0.045225322,0.039553817,0.00253832,-0.013321905,-0.023269806,-0.010611897,-0.0074153347,-0.0031706383,-0.012616697,-0.017861703,0.09128056,-0.044413883,0.006479608,0.04824335,-0.02001654,-0.08848039,-0.036795083,0.015621808,0.008179137,-0.0029024235,-0.006537934,0.018161517,0.0444629,-0.04589128,-0.048513893,-0.06827304,0.042828858,-0.035036374,-0.0021617652,0.029431218,0.017632362,-0.056916412,0.013233476,-0.020876046,-0.035318073,0.017048636,0.018777654,0.06454859,0.052720066,0.059070684,-0.05327299,-0.10524166,-0.014714041,-0.048301138,-0.014851028,-0.052115083,0.055700075,-0.0063284677,-0.0032797668,-0.011527115,-0.0038896655,0.0002933811,0.008695959,0.00135659,-0.0059484337,-0.015417102,-0.032093816,-0.0069628544,0.015162671,-0.00022714058,-0.01191793,0.044414267,0.045199297,-0.062381897,-0.060013864,-0.018119374,-0.008675027,-0.00015246692,0.013562798,0.016011316,0.01436853,-0.013335916,0.010075608,0.075377695,0.016639264,-0.009152595,-0.016464781,0.05159855,-0.0043734265,0.037795756,-0.0002470319,-0.034413792,-0.094325736,0.0058454596,-0.0121089155,-0.06039305,0.030952945,0.02214278,0.00020107251,-0.0034157527,-0.026296537,-0.021628337,0.005750825,0.06478987,0.005749541,0.017282838,0.00942651,-0.00728224,-0.049317755,-0.020408146,-0.048483957,0.020850824,-0.022677077,0.0217071,0.0014138832,0.027550098,-0.006840241,0.060504634,-0.0061946935,-0.0028557475,0.06306698,0.04030848,-0.047831096,0.07762959,-0.015559843,-0.010471172,0.062133532,-0.0044752145,0.04072561,-0.0035607705,0.010553965,0.0070408937,-0.0044707367,0.010439254,-0.027667722,-0.09450754,0.014361964,0.031079473,0.02206987,0.025629632,-0.021984315,-0.042271066,-0.05492697,-0.022644348,-0.030866018,0.07693659,-0.00067868095,-0.0063706,-0.06158756,0.006147922,-0.05000507,0.048119552,0.028266083,-0.010494216,0.007714441,-0.009553204]	Keywords: Transformers, Neural Networks, Machine Learning, Deep Learning, Attention Mechanisms, Speech Recognition, Visual Reasoning, Time-Series Forecasting, Efficiency, Architecture, NLP, Computer Vision, arXiv, ICLR, AAAI, EMNLP, Interspeech, ICASSP, ACL, ICCV, NeurIPS, Findings of EMNLP, ICML, Cognitive Computation, IEEE Trans. Neural Networks Learn. Syst.\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What are some of the key architectural modifications of the Transformer model discussed in these references?\n- How do the optimization techniques mentioned aim to improve the efficiency of Transformers?\n- What are some of the most diverse applications of Transformers explored in this collection of references?\n- Which references are primarily focused on architectural innovations?\n- Which ones primarily deal with efficiency improvements?\n- Which references are mostly about applying Transformers to a specific task, such as speech recognition?\n---\nSummary:\nThis is a list of references, likely from a survey or review paper on Transformers. The entries include authors, year, title, and publication details (conference or arXiv preprint). The topics covered range from variations on the Transformer architecture (e.g., LazyFormer, Deformable DETR), to techniques for improving efficiency (e.g., BERT Losses Patience, Informer), and applications in various domains like speech recognition, visual reasoning, and time-series forecasting.  The formatting is consistent, with a mix of conference proceedings and arXiv preprints.\nOriginal Text:\n- [95] Hao Peng, Nikolao Pappas, Danjani Yogati, Roy Schwartz, Norah Smith, and Jimgeng Kong. 2021. Random Feature Attention. In Proceedings of ICLR. https://openreview.net/forum?id=QTkTDVfRBFB\n- [96] Ethan Perez, Florian Strub, Hardam de Vries, Vincent Dumoulin, and Aaron C. Couville. 2018. FILM: Visual Reasoning with a General Conditioning Layer. In Proceedings of AAAI. 3942-3951. https://www.aaaai.org/ocs/index.php/AAAI/ AAAI/paper/view/16528\n- [97] Ngoc-Quan Pham, Thai-Son Nguyen, Jan Niehues, Markus Mller, and Alex Wabel. 2019. Very Deep Self-Attention Networks for End-to-End Speech Recognition. In Proceedings of Interspeech. 66-70. https://doi.org/10.21437/ Interspeech.2019-2072\n- [98] Jonathan Pilault, Amine Elhattami, and Christopher Pal. 2021. Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Patterns &amp; Less Data. In Proceedings of ICLR. https://openreview.net/ forum?id=1de1dbHzAMF\nContextualized Text:\nThis list comprises references to Transformer models and related works, covering architectural innovations, optimization strategies, and diverse applications.  The diverse authorship suggests a broad community exploring and building upon the original Transformer architecture. The sheer volume of entries (over 170) indicates the significant ongoing research in this area.	{"tags": [], "doc_id": "9b50c8c0-3675-4df4-bf50-0521aaa00287", "summary": "This is a list of references, likely from a survey or review paper on Transformers. The entries include authors, year, title, and publication details (conference or arXiv preprint). The topics covered range from variations on the Transformer architecture (e.g., LazyFormer, Deformable DETR), to techniques for improving efficiency (e.g., BERT Losses Patience, Informer), and applications in various domains like speech recognition, visual reasoning, and time-series forecasting.  The formatting is consistent, with a mix of conference proceedings and arXiv preprints.", "doc_type": "text", "entities": [], "keywords": ["Transformers", "Neural Networks", "Machine Learning", "Deep Learning", "Attention Mechanisms", "Speech Recognition", "Visual Reasoning", "Time-Series Forecasting", "Efficiency", "Architecture", "NLP", "Computer Vision", "arXiv", "ICLR", "AAAI", "EMNLP", "Interspeech", "ICASSP", "ACL", "ICCV", "NeurIPS", "Findings of EMNLP", "ICML", "Cognitive Computation", "IEEE Trans. Neural Networks Learn. Syst."], "key_objects": [], "contextual_text": "This list comprises references to Transformer models and related works, covering architectural innovations, optimization strategies, and diverse applications.  The diverse authorship suggests a broad community exploring and building upon the original Transformer architecture. The sheer volume of entries (over 170) indicates the significant ongoing research in this area.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the key architectural modifications of the Transformer model discussed in these references?", "How do the optimization techniques mentioned aim to improve the efficiency of Transformers?", "What are some of the most diverse applications of Transformers explored in this collection of references?", "Which references are primarily focused on architectural innovations?", "Which ones primarily deal with efficiency improvements?", "Which references are mostly about applying Transformers to a specific task, such as speech recognition?"]}
512108dd-b21b-4fe5-b68b-5339c757f23a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.015115051,0.025039183,0.017328897,0.03140527,-0.011375974,0.07528716,0.01965953,0.075321384,0.032319173,-0.04674802,-0.016499095,0.014454753,0.019643879,0.00305934,-0.0240549,0.012177706,0.0026426252,0.05204331,0.0081060035,-0.025125023,0.06364548,-0.0157934,-0.028842418,-0.0054363846,-0.0024793884,0.057446565,0.012738079,-0.030626345,0.019048117,0.018259488,0.025680222,-0.04409505,0.0058277836,0.017589055,0.018646985,-0.011220363,0.00986801,-0.009471661,0.012869292,-0.013267473,-0.01278328,0.058121193,-0.058271054,-0.0023966827,0.0104146,-0.056935452,-0.10737761,-0.03917928,0.031027788,-0.03639991,-0.026264329,-0.004045526,-0.028417187,-0.014462876,0.0039619496,-0.008951379,0.009530537,-0.057511594,0.029216073,-0.09321415,-0.02860429,0.041318,-0.05350993,-0.010372756,0.061755653,-0.062215276,-0.015916714,-0.0025537915,0.019600611,0.112489335,-0.02487551,0.05307732,-0.0023064364,-0.06280194,0.11990287,0.045842715,-0.0010604047,-0.022333154,-0.06321879,-0.0070036943,-0.023439363,0.067843,-0.039931845,-0.04079164,0.086515084,-0.019059116,-0.013552887,0.020450462,0.06063879,-0.012061737,0.011597692,-0.005501428,-0.001817631,0.05493423,-0.014150991,-0.07244004,-0.008709348,-0.044365957,0.00056684855,-0.026261738,-0.03673738,-0.024211165,0.10211864,0.10438787,0.020936085,-0.01404127,-0.025268745,-0.014890837,0.0055907364,0.0053342553,-0.006154567,0.01781827,-0.0097558275,-0.022130769,-0.035990976,-0.010986477,-0.06384592,0.005861614,0.00763578,0.0052082352,-0.0003458043,0.009869176,0.011411825,0.0018238684,0.08075718,0.057514284,0.012479366,0.008138695,-0.041181084,-0.008220785,-0.005509798,-0.020354863,-0.042653523,0.051824633,-0.011225476,0.0035162664,0.051153556,-0.02769239,0.008384148,0.015171323,0.011601678,-0.0018586875,-0.042576514,0.002950474,-0.029107826,-0.00080568524,-0.04989182,-0.0073671974,-0.031211961,0.05955156,-0.012680197,0.00708549,0.041053288,0.008952177,-0.0006432919,-0.013360868,-0.015248057,-0.066078015,-0.010409318,-0.02095152,-0.06084274,-0.050510854,-0.026595661,0.055998918,0.054227382,0.13239539,0.00014218272,-0.005459991,0.042404577,0.029862763,-0.028383745,0.014201617,-0.024179455,-0.047362853,0.019689439,-0.003476038,0.008880401,-0.02980096,-0.04034555,0.014881881,0.021200724,0.027264372,-0.014505073,0.040713016,-0.05915457,0.013306325,0.02618819,0.072529316,-0.019936576,-0.019792933,0.019235231,-0.0071877483,-0.005771493,0.035115976,0.036609255,0.02335025,0.053705744,-0.07045989,-0.06731497,-0.04869969,-0.011813949,0.002758023,-0.0030868642,-0.0075233383,0.032506227,-0.01648926,0.0003715644,-0.038336135,-0.052904274,-0.002487258,0.012040408,-0.0080117155,-0.018039282,0.04825891,0.027986953,-0.022082435,-0.031940088,0.020054704,0.03980066,-0.012216176,-0.051430505,0.006225041,0.00938636,0.03945474,-0.053206675,0.019440033,-0.012041508,0.018154755,-0.024543718,-0.016903631,0.038809318,0.016827265,-0.041908585,0.038375422,-0.025314614,0.035584893,0.006280204,0.0075223045,-0.015978128,0.06172721,0.036930166,0.0655079,-0.016942114,0.055148207,-0.00041314284,-0.0012265715,0.032266036,0.028771216,-0.017002389,0.006742076,-0.024221355,0.032653466,0.05483132,-0.0065198345,-0.0044177566,-0.04842399,0.03254895,0.01924652,0.011784754,0.024279779,-0.021780834,0.0039021592,0.021471862,0.008394592,0.015213772,0.03776787,0.027125392,0.048992537,0.0117239645,0.05063545,0.043104928,0.05755158,0.0037057514,-0.012867499,-0.061155185,-0.010660003,-0.011285549,-0.006619383,0.063606724,-0.022866655,-0.0038878608,-0.00041630672,0.059128594,0.00618846,-0.02857869,-0.016877664,0.00017152753,0.0060916445,-0.05020362,-0.0028703935,0.020910578,-0.04995756,0.030605162,0.00083369954,0.017259039,0.00044266455,0.016045528,-0.034095462,-0.023917368,0.011162082,0.06244351,-0.02443189,0.035866782,0.018456135,0.00028872213,0.027709935,-0.03369047,0.014178228,0.05408832,-0.007054,0.015390307,-0.031392556,-0.023336403,-0.0022146879,0.026952893,0.10281371,0.060136463,-0.040361356,0.007973853,-0.025431482,0.021929089,-0.022843082,0.0082391165,0.0372368,0.03291031,-0.027532054,0.009694177,-0.06660825,-0.061567806,0.052661322,-0.08416248,0.03745501,0.03908726,0.004762068,-0.008631758,0.008390223,0.05162726,0.028319009,-0.009630638,0.04589804,-1.44841715e-05,-0.06414683,-0.003292609,-0.016432794,-0.027031196,0.028808812,-0.009260915,-0.012375289,0.04972713,0.005931742,0.008523667,0.018702023,0.021716228,0.046912815,0.03210273,0.002270115,-0.010658471,0.017006606,0.013456446,0.01557237,0.068751484,-0.07758456,0.089126326,-0.03220102,-0.0027724125,0.007822585,-0.07961168,-0.017265351,0.08121396,0.007623241,0.0036865913,-0.036033485,0.053328834,-0.059030354,-0.014367311,0.0020815146,0.028905453,0.01237972,-0.060369026,-0.017889798,-0.027421162,0.035856955,-0.041679185,-0.038854644,-0.016539266,0.04688884,0.020092882,0.04605931,-0.003801196,0.043153085,-0.026178619,0.009068007,0.058198582,-0.019476445,-0.02683461,-0.0014055247,0.026350219,0.038076673,0.009295764,-0.023517624,0.04129957,0.03327624,-0.025400056,0.027458092,0.035452746,0.003287989,-0.050226737,0.041519888,0.06669507,0.032215662,0.0353215,0.0003974808,-0.043302193,0.024513958,-0.019788133,-0.03319924,-0.006215131,0.010882413,-0.034942187,-0.0007601513,-0.05845686,0.050377358,0.0042672004,0.038294964,0.0367215,-0.03104849,-0.028843442,0.0386895,-0.031572297,-0.05149463,0.0028307792,-0.079040706,0.0029883084,0.030017888,0.045422614,0.021665731,-0.07005337,0.066400774,-0.07283962,0.012605435,0.05987375,-0.03110785,-0.009976885,-0.030719306,-0.02287811,-0.018923303,0.028551372,0.019681897,-0.032445826,-0.02454833,0.018149924,0.036260538,0.025146386,-0.02946633,0.01654076,-0.030846993,-0.028789015,-0.0149343405,-0.055139992,0.053980645,-0.014313707,0.05830934,0.004912656,-0.0058097844,-0.0064994292,-0.037952296,-0.057905246,-0.022709245,-0.010089168,-0.023152487,0.00092657027,0.040595073,-0.033953622,0.012235769,-0.006437932,0.017553618,-0.016230678,0.055734716,0.045399763,-0.02215795,-0.04857219,-0.020456139,-0.0018998141,0.048542444,0.02271196,-0.00040559392,-0.009000778,-0.03242167,-0.04764486,0.024041979,0.0033706627,0.0586044,3.7382633e-05,0.023886677,0.02404807,-0.013184084,-0.0058707856,-0.0103078,0.012805213,-0.048448436,-0.064685754,0.024448628,0.05431432,0.025646765,-0.008744588,0.010706793,0.033923473,0.009878389,0.012225955,0.031335972,0.002756438,-0.0059730397,0.009947266,-0.015245341,-0.02508058,0.04317094,-0.009819742,0.029262668,0.018957358,0.060259275,0.003271049,-0.025721075,0.008728046,-0.013098005,0.014467076,-0.005001873,0.05464211,0.05249269,0.052884225,-0.048760023,0.026693072,-0.045515437,-0.06133403,0.014977629,-0.081711866,-0.022673102,-0.0031960583,-0.052641843,-0.00820165,0.01546413,-0.0075527695,-0.019674832,0.016713686,-0.07012884,0.050181363,-0.06827072,0.03876262,0.0051212925,0.013258148,0.1272589,0.01917117,-0.042301573,-0.015491695,-0.025815053,-0.036144592,0.0051810797,0.057015054,0.010790294,-0.006650846,0.07426327,-0.0129399,0.024938669,-0.014786977,-0.055341866,-0.051476873,-0.046335854,-0.053656343,0.0054758326,0.015932033,0.017322112,-0.008108876,-0.0030329651,0.0007023941,0.019416742,-0.01717225,0.035849016,0.036153123,-0.0038071792,0.029436659,-0.025977364,0.03912872,0.03415021,0.0016051304,-0.022588063,0.006575261,0.024141844,-0.020534558,-0.023772111,0.015597028,0.017479546,-0.020096682,-0.012880103,0.06362514,-0.011081361,-0.02186412,0.005510134,0.032139715,0.027500996,-0.0076342174,-0.017181648,-0.021612091,-0.032227308,-0.022828337,-0.015394604,0.03393168,0.0038195655,-0.0033548325,0.010872073,-0.03968438,0.08722031,-0.1043941,-0.03661776,-0.053357247,0.03587898,-0.031143084,-0.014919342,-0.024556804,-0.00886463,-0.023672936,-0.018218739,0.059815653,0.044979464,-0.05027998,0.008522009,-0.029469853,-0.008880861,-0.03017855,-0.030214123,0.00043597014,-0.00178746,0.008970606,-0.01616048,0.10438348,-0.023918716,-0.007382362,0.03594655,0.0065511866,-0.07855867,-0.048223503,0.027903704,-0.017410446,-0.0015179232,0.028480953,0.021512603,0.033132702,-0.04990664,-0.025560739,-0.037913613,0.03152815,-0.024309596,0.011279262,0.026599757,0.005452639,-0.054423865,-0.0036791002,-0.031445153,-0.02972332,0.016921839,0.029325979,0.06514259,0.025680276,0.047556445,-0.074580334,-0.097880125,-0.026516877,-0.026821753,-0.012542325,-0.051763903,0.061019577,0.010599913,-0.01829833,0.021455314,-0.02101776,0.0070511308,0.0014588581,0.023868605,-0.032998558,-0.0069575906,-0.052375786,0.011261063,0.026603226,0.01352992,-0.004662202,0.08891186,0.03009066,-0.052773938,-0.0375219,-0.0430876,-0.011860375,0.009168399,-0.0050603384,0.044687446,0.034589972,-0.020536333,0.014836166,0.057653684,0.017311202,-0.0396905,-0.007919559,0.018225241,0.019506685,0.03887132,0.0067973062,-0.030765427,-0.120760076,0.008247336,-0.00032099514,-0.07482564,0.027315335,0.021714782,-0.0006335822,-0.024277538,-0.025763461,-0.024240939,0.014793617,0.028732656,0.018489525,-0.009933291,0.005550708,-0.018040804,-0.05887218,-0.0036417784,-0.021266662,0.007358099,-0.00669524,0.005518315,0.045195416,0.0061311396,0.004757049,0.03213226,-0.011121437,-0.025019448,-0.0032937292,0.051021885,-0.047007345,0.04132411,-0.008699622,0.006243434,0.061367437,-0.007804166,0.0045170356,0.0017815303,-0.003960187,0.004116775,-0.010536679,0.01304002,-0.043469094,-0.061699543,0.032574005,0.028385574,0.0197477,0.018709622,-0.028516108,-0.05296979,-0.058188137,-0.038001474,-0.013976586,0.06045264,0.041681714,0.033684224,-0.052199706,0.017106408,-0.043732785,0.036434457,0.06344962,-0.004781495,0.009736517,-0.0040713735]	Keywords: Transformers, NLP, Machine Learning, Deep Learning, Attention Mechanism, Pre-training, Architectures, Optimization, ACL, NeurIPS, ICML, ICLR, BERT, GPT, Long-Range Sequences, Efficiency, Applications, Survey, References, Reference List, List of References, Transformer Models, Neural Networks, Deep Learning Architectures, Natural Language Processing, Sequence Modeling, Computational Linguistics, Transformer Dissection, Poolingformer, LazyFormer, Hi-Transformer, SETransformer, Informer, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Pay Less Attention, End-to-End Object Detection, BERT Losses Patience, Hormalization, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers, Wangchunshu Zhou, Deformable DETR, Compressive Transformers\nKey Objects: Papers, References, Transformer Models, Deep Learning Architectures, Natural Language Processing, Sequence Modeling, Computational Linguistics, Survey, List of References, Reference List, Deep Learning, Machine Learning, NLP, Transformer Models, Attention Mechanism, Architectures, Pre-training, Optimization, Applications, ACL, NeurIPS, ICML, ICLR, BERT, GPT, Long-Range Sequences, Efficiency\nRefers to Images: None\nHypothetical Questions:\n- What are the key architectures discussed in these references?\n- What are the primary conferences cited for Transformer research?\n- What are some techniques for optimizing Transformer models?\n- What are the limitations of standard Transformer models that these papers address?\n- What are some practical applications of Transformer models that are discussed in these references?\n---\nSummary:\nThis is a list of references related to Transformers, likely from a survey or overview of the topic.  The list includes a variety of papers covering improvements, architectures, pre-training techniques, and applications of Transformers.  Many papers are from leading conferences like ACL, NeurIPS, ICML, and ICLR. The list contains a mix of fundamental papers (like the original Attention is All You Need) and more recent works proposing architectural modifications and optimizations.\nOriginal Text:\n- [99] Ofir Press, Noah A. Smith, and Omer Levy. 2020. Improving Transformer Models by Reordering their Sublayers. In Proceedings of ACL. Online, 2996-3005. https://doi.org/10.18653/v1/2020.acl-main.270\n- [100] Xipeng Qiu, TianXiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. 2020. Pre-trained Models for Natural Language Processing: A Survey. SCIENCE CHINA Technological Sciences 63, 10 (2020), 1872-1897. https://doi.org/10.1007/s11431-020-1647-3\n- [101] Alec Radford, Karthik Naranishan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. (2018).\n- [102] Alec Radford, Jen Wu, Rewon Child, David Luan, Dario Amodel, and Iiya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).\n- [103] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chlohe Hillier, and Timothy P. Lillicrap. 2020. Compressive Transformers for Long-Range Sequence Modelling. In Proceedings of ICLR. https://openreview.net/forum?id= SyikKSYDH\nContextualized Text:\nThis is a list of references related to Transformers, likely from a survey or overview of the topic. The list includes a variety of papers covering improvements, architectures, pre-training techniques, and applications of Transformers. Many papers are from leading conferences like ACL, NeurIPS, ICML, and ICLR. The list contains a mix of fundamental papers (like the original Attention is All You Need) and more recent works proposing architectural modifications and optimizations.	{"tags": ["Deep Learning", "Natural Language Processing", "AI", "Machine Learning", "Transformer", "Survey", "Reference List"], "doc_id": "512108dd-b21b-4fe5-b68b-5339c757f23a", "summary": "This is a list of references related to Transformers, likely from a survey or overview of the topic.  The list includes a variety of papers covering improvements, architectures, pre-training techniques, and applications of Transformers.  Many papers are from leading conferences like ACL, NeurIPS, ICML, and ICLR. The list contains a mix of fundamental papers (like the original Attention is All You Need) and more recent works proposing architectural modifications and optimizations.", "doc_type": "text", "entities": ["Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Ofir Press", "Noah A. Smith", "Omer Levy", "Xipeng Qiu", "TianXiang Sun", "Yige Xu", "Yunfan Shao", "Ning Dai", "Xuanjing Huang", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap", "Wangchunshu Zhou", "Canwen Xu", "Tao Ge", "Julian Mcauley", "Ke Xu", "Furu Wei", "Alec Radford", "Karthik Naranishan", "Tim Salimans", "Ilya Sutskever", "Jen Wu", "Rewon Child", "David Luan", "Dario Amodel", "Jack W. Rae", "Anna Potapenko", "Siddhant M. Jayakumar", "Chlohe Hillier", "Timothy P. Lillicrap"], "keywords": ["Transformers", "NLP", "Machine Learning", "Deep Learning", "Attention Mechanism", "Pre-training", "Architectures", "Optimization", "ACL", "NeurIPS", "ICML", "ICLR", "BERT", "GPT", "Long-Range Sequences", "Efficiency", "Applications", "Survey", "References", "Reference List", "List of References", "Transformer Models", "Neural Networks", "Deep Learning Architectures", "Natural Language Processing", "Sequence Modeling", "Computational Linguistics", "Transformer Dissection", "Poolingformer", "LazyFormer", "Hi-Transformer", "SETransformer", "Informer", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Pay Less Attention", "End-to-End Object Detection", "BERT Losses Patience", "Hormalization", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers", "Wangchunshu Zhou", "Deformable DETR", "Compressive Transformers"], "key_objects": ["Papers", "References", "Transformer Models", "Deep Learning Architectures", "Natural Language Processing", "Sequence Modeling", "Computational Linguistics", "Survey", "List of References", "Reference List", "Deep Learning", "Machine Learning", "NLP", "Transformer Models", "Attention Mechanism", "Architectures", "Pre-training", "Optimization", "Applications", "ACL", "NeurIPS", "ICML", "ICLR", "BERT", "GPT", "Long-Range Sequences", "Efficiency"], "contextual_text": "This is a list of references related to Transformers, likely from a survey or overview of the topic. The list includes a variety of papers covering improvements, architectures, pre-training techniques, and applications of Transformers. Many papers are from leading conferences like ACL, NeurIPS, ICML, and ICLR. The list contains a mix of fundamental papers (like the original Attention is All You Need) and more recent works proposing architectural modifications and optimizations.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the key architectures discussed in these references?", "What are the primary conferences cited for Transformer research?", "What are some techniques for optimizing Transformer models?", "What are the limitations of standard Transformer models that these papers address?", "What are some practical applications of Transformer models that are discussed in these references?"]}
10ca98db-23c8-4045-a18c-4ef4833cfb9c	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.012672761,0.018611155,-0.0022432255,0.03883136,0.010926102,0.07061418,0.018376088,0.06982212,0.035787158,-0.040541098,0.0049079587,0.030689767,0.015054551,-0.021710278,-0.038147353,0.053038172,-0.004867056,0.06712287,0.020928463,-0.0036068659,0.060837183,0.01909468,-0.014787139,-0.031835623,0.0062142117,0.031361606,0.0003020974,-0.036294412,0.011679711,0.027468754,0.012082861,-0.043920457,0.018157704,-0.0066653066,0.024449853,0.017452331,0.005645951,-0.015878797,0.03277873,-0.018762695,-0.004913061,0.0565693,-0.066758394,-0.03180012,0.012208312,-0.07705639,-0.082809925,-0.037620474,0.01924411,-0.03436386,-0.0492514,0.032421455,-0.034534328,-0.037766524,0.006896259,-0.03385237,-0.026334783,-0.04182718,0.024764111,-0.11537775,-0.022529634,0.038180206,-0.036686514,-0.030383775,0.049796175,-0.07144661,-0.033070866,-0.014639571,0.007651797,0.106260255,-0.039975222,0.009050389,-0.003784007,-0.0655634,0.10482018,0.038110785,-0.008073524,-0.01308799,-0.09902933,-0.008251976,-0.03657225,0.08093225,-0.0383392,-0.03817274,0.07731924,-0.013714898,-0.028717311,-0.00982833,0.080390155,-0.044306498,0.019737188,-0.01383659,-0.0056500025,0.055788882,-0.023736145,-0.0636147,-0.0032275598,-0.03391544,-0.020126833,-0.041845873,-0.044556912,0.020584699,0.0834931,0.09494364,0.023592444,-0.016387537,-0.024681851,-0.0406121,0.007449651,0.0067495196,0.018212644,0.0002562077,-0.025724558,0.00671166,-0.008076605,-0.028893068,-0.059179638,-0.0062152115,0.0029228646,0.024737017,0.0047467356,-0.010923016,0.017676217,0.0019852372,0.08087062,0.044064406,-0.009785872,0.0051585734,-0.048526485,-0.0014025574,0.0013627806,-0.021881446,-0.016407693,0.022923134,0.02164055,0.022587996,0.04069609,-0.032567807,0.0005597538,0.002887657,0.031764563,-0.0366845,-0.018019566,0.04682717,-0.03235913,0.014178282,-0.058419548,0.008304652,-0.0009773081,0.03394216,0.0033109707,0.028368084,0.048410166,0.030225525,0.022558417,0.018642776,0.0056223236,-0.050724983,-0.030522179,0.012323695,-0.057479467,-0.03630185,-0.039499924,0.031063542,0.0444826,0.13083915,0.018750077,0.016127337,0.0313957,0.022358768,-0.02235087,0.02328554,-0.011367819,-0.028639967,0.017008044,-0.0011869482,0.019557836,0.008353457,-0.013845961,0.0059509207,-0.0021249412,0.013338032,0.0013766016,0.040128816,-0.030240575,0.0137312,0.0080432575,0.07378608,-0.017060753,-0.04415135,0.011881212,0.0025928607,-0.03188471,-0.003675584,0.03620614,0.0161485,0.027856985,-0.06581083,-0.036894396,-0.0243005,0.0072865356,0.011588021,-0.0058972035,0.015043369,0.029554833,-0.021736354,-0.025865316,-0.047271132,-0.052122403,-0.00905382,0.008770859,-0.048613,-0.026762696,0.06871881,0.04548873,-0.0124085,-0.015349818,0.030754257,0.0034379675,0.008778687,-0.046058245,0.0021532378,-0.0065127695,0.0097889975,-0.02585756,-0.015436472,-0.026482563,0.04120548,-0.0022647022,-0.03561746,0.019655317,0.005054794,-0.02073047,0.016934782,-0.016488517,0.06344488,-0.014319252,0.00270915,-0.022517765,0.06299644,0.017958615,0.05692217,0.04059112,0.06633522,-0.018314745,0.0031077745,0.058128044,0.0301608,-0.0016508809,0.037504476,0.040114652,0.01777727,0.05370934,0.0022064333,0.04001385,-0.016472938,0.003496056,0.051054597,0.029678846,0.012372239,-0.003638633,0.0043194704,0.028877592,0.001484169,0.014508546,0.014195841,0.041178614,0.039987452,0.007911692,-0.0038462002,0.030981433,0.03697957,0.007920133,-0.04201999,-0.070645586,0.0059154914,0.001991636,0.018524878,0.064166926,-0.034429267,-0.013422327,0.0067906524,0.057752278,-0.004398099,0.021971684,0.002222279,-0.024000688,-0.018372746,-0.095688425,0.009197937,0.02648114,-0.019531634,0.02745695,-0.0001453959,0.0066178166,0.0111744935,0.002288401,-0.02811626,-0.03232922,0.0074725514,0.09118024,-0.010886033,0.034319412,0.028664049,-4.731877e-05,0.03893386,-0.08460524,-0.0042742616,0.042694222,0.025672289,0.030052938,-0.020176614,-0.0062106843,-0.01475581,0.04209422,0.08122549,0.049785946,-0.024985079,-0.016414305,0.020975634,0.023750104,0.012811188,3.871589e-05,0.059582,0.041374482,-0.012807071,0.009384336,-0.032349147,-0.08189913,0.045390487,-0.058085833,0.07237769,0.033950303,0.014516993,-0.03464913,-0.011116581,0.040818058,0.029482419,-0.007259026,0.04335571,0.03172783,-0.066627875,0.0060911826,0.009456729,0.011540516,0.035750266,0.0141998585,-0.0005487977,0.052783113,0.007986251,-0.015194703,0.047210623,0.044574857,0.040341195,-0.005231191,0.008915973,0.006937671,0.022508867,0.02248651,0.021498883,0.058976296,-0.037353873,0.0842482,-0.041687638,-0.008734944,-0.0070961565,-0.073848546,-0.040379323,0.036734194,-0.033272427,0.0035874585,-0.037613068,0.055765044,-0.043414995,-0.002979998,-0.029330898,0.027998256,0.01779425,-0.04534689,-0.008777605,-0.017095305,0.022260243,-0.06290343,-0.00070067786,-0.022235725,0.036454882,0.022965763,0.03204684,0.011776577,0.07394397,-0.02242857,0.020592334,0.038730796,-0.012063242,-0.019883374,-0.014216151,0.0017828346,0.017853994,0.009241332,-0.03531713,0.0070210486,0.0496148,-0.03983642,0.03825813,0.059176683,-0.010146729,-0.07653586,0.015480105,0.08000212,0.008212125,0.028431732,-0.037807312,-0.031861134,0.048876394,-0.012013526,-0.028670348,-0.012783824,0.016015807,-0.053104986,0.010746738,-0.03982483,0.04896823,0.037152924,0.027155679,0.06931779,-0.027730305,-0.022780078,0.028742695,-0.012137588,-0.06886964,0.010451027,-0.07402661,-0.007163156,0.04694761,0.015161822,0.033133846,-0.036056895,0.05825293,-0.10616498,0.024866564,0.068261474,-0.016442703,0.0016737613,-0.034700986,-0.008477324,-0.025952473,-0.0068610883,0.022736067,-0.01178244,-0.023762885,-0.0026580952,0.028607871,0.023931779,-0.013272525,0.006547616,-0.0042282124,-0.030241653,0.01005205,-0.053099588,0.02204024,-0.0071076187,0.059488136,0.0015005894,-0.026536508,0.016380373,-0.030738857,-0.021842334,-0.01860969,-0.018972246,-0.005219313,0.00401477,0.03931527,-0.07390667,-0.0042982353,-0.014918428,-0.011214545,-0.00752387,0.061282746,0.041169584,0.0026705705,-0.0666647,-0.06859596,0.002650106,0.05259214,-0.0348255,-0.004575515,-0.010850682,-0.0068380996,-0.08072845,0.024967004,0.025466876,0.04523346,0.037382584,0.015869966,0.030908857,-0.029987464,-0.023658201,0.00798815,0.030596105,-0.033648316,-0.07077036,0.0012332305,0.05215627,0.014425316,-0.0030200954,0.040142655,0.0120850215,0.0008757169,0.017921468,0.052918445,0.01890601,-0.022938566,-0.016752316,0.019647555,0.015556907,0.023822345,-0.028194966,-0.0040551266,0.0073394235,0.0381428,0.023474652,-0.032913297,0.0077627334,-0.006859883,0.012396637,0.0006908138,0.03364589,0.049435835,0.044806167,-0.042914677,0.067653276,-0.021613082,-0.053388625,0.02215188,-0.04768324,-0.032768175,0.0122446,-0.06880892,-0.008243794,0.021576036,-0.018216949,0.0054824837,0.046169076,-0.0519082,0.10596869,-0.067424245,0.037755746,0.0012145373,-0.0006333685,0.12643206,0.04681292,-0.02039308,-0.0136331925,0.008750369,-0.0358758,0.017976798,0.07572923,0.02443655,-0.017336914,0.06288697,0.013755588,0.039369248,-0.02616345,-0.034363497,-0.0004712473,-0.013126456,-0.032079205,-0.0038665917,-0.017794402,0.02235502,0.012000052,-0.012194488,0.012495762,0.03378168,-0.028548934,0.028636554,0.0072142086,-0.0092298435,0.012004507,-0.008933136,0.0702152,0.038277287,-0.010936436,-0.010003856,0.008193219,0.022866827,-0.010600894,-0.022685664,0.039286118,0.0061955103,-0.02591308,-0.00027477046,0.07293568,0.014066342,-0.04216698,0.0064148544,0.011592075,0.036272094,0.042955752,-0.023000741,-0.01599793,-0.035864826,-0.026818937,-0.025359303,0.0337873,-0.0011050986,-0.007619891,0.012187841,-0.048036862,0.083316304,-0.083947346,-0.0221112,-0.020924477,0.037000373,-0.03946569,-0.027354686,0.023841707,-0.017062206,-0.021192335,-0.010822752,0.06641271,0.047138143,-0.043859135,0.035511572,-0.0012065022,-0.02550819,-0.002949631,-0.04373484,-0.007909846,-0.007790901,0.0036332482,-0.012954114,0.091151655,-0.004678406,0.012105117,0.032958187,0.0004984139,-0.0597491,-0.02328117,0.023918742,0.0068622255,-0.0064933253,0.0015079618,0.01589174,0.029220585,-0.05885739,0.002629696,-0.051391486,0.023724388,-0.021080602,0.0066455947,0.017636377,0.018733814,-0.0720636,0.0149891265,-0.02619071,-0.034179747,0.00054292846,0.029245956,0.07886714,0.023051446,0.05793491,-0.060613368,-0.08966879,-0.011426814,-0.056256365,-0.012012967,-0.045372374,0.039816756,-0.006857573,-0.018729655,0.016160255,-0.006321957,-0.02098674,-0.0017156441,0.01623626,0.0030719158,-0.007951057,-0.021259883,-0.0027887993,0.024467962,0.0132173,0.013884564,0.06630106,0.05539079,-0.055403944,-0.054178532,-0.020037342,-0.00030792793,0.0014253657,0.010683568,0.04101548,0.029068548,-0.004730132,0.0163128,0.06488457,0.021636723,0.0018239935,-0.043169357,0.047186732,-0.012070954,0.05086991,0.0015243272,-0.033338908,-0.10130826,0.025367903,-0.019362142,-0.055511132,0.030708423,0.030993398,0.0029342226,-0.0010625966,-0.047088467,-0.0026421603,0.014502613,0.0263297,-0.00071203476,0.0045707575,0.036981933,-0.0113271745,-0.06038672,-0.017917713,-0.021693932,0.03123604,-0.004980516,0.030381517,0.03834596,0.012640254,0.020621618,0.00938879,0.0014848515,-0.007348761,0.021835212,0.0347782,-0.030372148,0.05139358,0.015709177,0.00013793922,0.07588469,-0.018846776,0.04427077,-0.009193741,0.014944846,0.02334194,0.013468965,0.0047687967,-0.014422731,-0.06990298,0.011947775,0.038913976,0.017243324,0.04608092,-0.016825587,-0.038956225,-0.04938195,-0.034686755,-0.017054558,0.056338083,0.023079585,0.017597672,-0.04490971,0.028616795,-0.033086836,0.0331789,0.04133862,-0.0132764485,0.026078867,0.009673003]	Keywords: transformers, references, machine learning, deep learning, natural language processing, computer vision, transfer learning, kernel methods, activation functions, text-to-image generation, arxiv, neurips, iclr, acl, emnlp, transformers, deep learning, machine learning, natural language processing, computer vision, neural networks, architecture search, attention mechanisms, sequence modeling, representation learning, text generation\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What are the primary topics covered by the referenced papers?\n- What conferences and preprint servers are the papers sourced from?\n- Can you provide a brief overview of the scope of research presented in these references?\n---\nSummary:\nThis is a list of references from a survey on Transformers. The list spans a wide range of related topics including transfer learning, kernel methods, activation function search, text-to-image generation, and more. The references include preprints on arXiv and proceedings from major conferences like NeurIPS, ICLR, ACL, and EMNLP.\nOriginal Text:\n- [104] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Shanar Nag, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv:1910.10683 [cs.LG]\n- [105] Ali Rahimi and Benjamin Recht. 2007. Random Features for Large-Scale Kernel Machines. In Proceedings of NeurIPS. 1177-1184. https://proceedings.neurips.cc/paper/2007/hash/013a006f03dbc5392efebf8f18da7f55\\_Abstract.html\n- [106] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. 2018. Searching for Activation Functions. In Proceedings of ICLR. https://openreview.net/forum?id=Hkqu2EkPF  \n- [107] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-Shot Text-to-Image Generation. arXiv:2102.12092 [cs.CV]\nContextualized Text:\nThis is a list of references from a survey on Transformers. The list spans a wide range of related topics including transfer learning, kernel methods, activation function search, text-to-image generation, and more. The references include preprints on arXiv and proceedings from major conferences like NeurIPS, ICLR, ACL, and EMNLP.	{"tags": [], "doc_id": "10ca98db-23c8-4045-a18c-4ef4833cfb9c", "summary": "This is a list of references from a survey on Transformers. The list spans a wide range of related topics including transfer learning, kernel methods, activation function search, text-to-image generation, and more. The references include preprints on arXiv and proceedings from major conferences like NeurIPS, ICLR, ACL, and EMNLP.", "doc_type": "text", "entities": [], "keywords": ["transformers", "references", "machine learning", "deep learning", "natural language processing", "computer vision", "transfer learning", "kernel methods", "activation functions", "text-to-image generation", "arxiv", "neurips", "iclr", "acl", "emnlp", "transformers", "deep learning", "machine learning", "natural language processing", "computer vision", "neural networks", "architecture search", "attention mechanisms", "sequence modeling", "representation learning", "text generation"], "key_objects": [], "contextual_text": "This is a list of references from a survey on Transformers. The list spans a wide range of related topics including transfer learning, kernel methods, activation function search, text-to-image generation, and more. The references include preprints on arXiv and proceedings from major conferences like NeurIPS, ICLR, ACL, and EMNLP.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the primary topics covered by the referenced papers?", "What conferences and preprint servers are the papers sourced from?", "Can you provide a brief overview of the scope of research presented in these references?"]}
f19d848d-4e81-4a00-98af-474c6984e510	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.009823436,0.00353487,0.007148639,0.019399766,-0.0038907095,0.025902523,0.05606395,0.061656132,0.01699607,-0.0408832,0.008391069,0.010440456,-0.008924714,-0.021070894,-0.02929171,0.007944062,0.012775494,0.07329683,0.0211784,-0.048322752,0.026538558,-0.0007415855,0.0060979803,-0.024455965,0.023291126,0.034291487,0.049647518,-0.02278967,0.0047640307,-0.023384469,-0.005129146,-0.064346105,0.051159523,0.011629978,0.030180326,-0.038381506,-0.007255314,-0.076744415,0.015946772,0.036640517,0.014293351,0.03202426,-0.047741324,-0.03932852,0.010668943,-0.044847906,-0.08144165,-0.041531883,0.0450447,-0.0026145398,0.026205858,0.039066855,0.009651597,-0.01718913,-0.020788755,-5.2696778e-05,-0.0009273445,-0.04045341,0.009340367,-0.09717774,-0.018683614,0.037956502,-0.02445887,0.0028473823,0.036926817,-0.009891883,-0.009481385,0.010348952,0.06568766,0.11711609,-0.019901188,0.007901498,0.0013650964,-0.087757066,0.12572552,0.056726776,-0.008294441,-0.034794427,-0.044758797,0.005835937,0.005154983,0.10209309,-0.035457417,-0.01807516,0.080409914,-0.016280597,-0.020183638,0.019866876,0.03761844,-0.0147227915,0.017135698,-0.0084044095,-0.022341743,0.07222426,-0.019314734,-0.06544975,0.008658239,-0.015948663,-0.0021300276,-0.05994504,0.009244502,0.0027312757,0.07065223,0.12542781,0.029873624,-0.02723884,0.00028978658,-0.0019374258,-0.0024437383,0.01445626,0.0068124686,0.018327625,-0.0078085246,-0.017156744,-0.030814055,-0.010136601,-0.057300374,0.027984813,-0.01976694,0.012745321,0.016021961,-0.0050966404,-0.00296335,0.027374076,0.05766489,0.07380471,0.039947174,-0.025141977,-0.01052262,0.00035548702,0.03077096,0.0010759115,-0.03345255,0.021832516,-0.056977086,0.008613416,0.07065405,-0.029893527,0.0062583718,-0.0038119394,-0.00094485027,-0.024638785,-0.018752087,0.006535199,0.006241814,-0.023790117,-0.059873264,-0.020514881,-0.031825155,0.06139534,0.03263123,-0.027584925,0.015317095,-0.0071804663,-0.0149321975,0.0053232475,-0.024390321,-0.050074898,-0.0070992666,-0.028070167,-0.03753912,-0.061801437,-0.0065605766,0.05947733,0.0728336,0.11323426,-0.010335365,-0.007602433,0.06623599,0.004411158,-0.0125768855,0.007836503,-0.04018026,-0.029566703,-0.002770473,0.0013318014,0.025747113,-0.034818437,-0.013953688,-0.0031493744,-0.002733867,0.04401726,-0.011770985,-0.00881147,-0.031013707,0.031278674,0.022604547,0.076049626,-0.020796286,-0.0227779,0.024012752,-0.05696861,0.003818457,-6.229e-05,0.048589237,0.035668787,0.057761915,-0.05322734,-0.016274514,-0.043542102,0.0060559288,-0.021304494,-0.003014586,-0.010804736,0.023986995,-0.030220231,0.02680661,-0.061120085,-0.023268804,0.021389198,-0.010569908,0.0019397077,-0.028270544,0.04998928,0.01913664,0.01182619,-0.028289787,0.03889319,0.055409916,0.0057046707,-0.030762602,0.0068323733,-0.008056342,0.043121893,-0.019326119,0.032333493,-0.010474155,0.019997064,0.00929807,0.010031029,0.06288942,0.008028766,-0.014963364,0.05042555,-0.035045102,0.048542704,-0.0036456059,-0.0010189064,-0.03594105,0.03140406,0.03838151,0.09610207,-0.025554787,0.07299931,0.027299026,-0.0011495878,0.016591588,0.028678989,0.014374068,0.0159518,0.0058138645,0.03121981,0.06069496,-0.028109232,-0.0036408787,0.0005148484,-0.005208681,0.011211523,0.04816339,-0.035962842,-0.004878854,-0.022336844,0.005258411,-0.009963732,-0.0020352725,-0.0005171232,0.0038941049,0.01643688,-0.018501857,0.010702516,0.00894212,0.034650024,0.03127712,-0.04158857,-0.007006548,0.033341583,0.041776303,-0.015317262,0.017543655,-0.026163427,-0.014613488,-0.024008092,0.024527915,0.01688999,0.011860175,-0.008232051,0.019218046,-0.017155774,-0.0545839,-0.0070083784,-0.007239271,-0.03066909,0.027276509,-0.041670978,0.046250127,-0.003310975,0.0048487294,-0.049388085,-0.035678897,-0.011848672,0.09336471,0.018672217,0.024490809,0.036130328,0.021978805,3.0398742e-05,-0.055350203,0.030346597,0.055063978,0.01076969,0.0005073401,-0.003151941,-0.034874182,-0.016459685,0.013539327,0.084130496,0.08206362,-0.028078467,-0.00074676273,0.03699041,0.03825823,-0.00024470003,0.0011469373,0.040808223,0.02339977,0.030550992,0.0075450847,-0.020532722,-0.08897063,0.117921494,-0.09331097,0.03956111,0.05772884,0.004565837,-0.023542568,0.016609197,-0.034841113,0.019074587,0.010999605,-0.017877443,-0.00869207,-0.059856925,-0.01656866,-0.02086141,0.005411122,0.06337654,-0.03452097,-0.021153707,0.036604777,-0.020186843,0.022229536,0.014031452,0.016510218,0.06965122,0.011865224,0.016782684,0.012115256,0.026336815,0.012059369,0.007429852,0.07956575,-0.09789029,0.10707441,-0.013821682,-0.005363224,-0.005200328,-0.06458923,-0.0076647303,0.09175928,0.025721468,-0.017144226,-0.030180385,0.0616526,-0.017784027,-0.024150772,0.050492436,0.042687256,0.027701257,-0.018052088,-0.0016379247,-0.0049019284,0.0034487252,-0.008922705,-0.019423714,-0.0059495736,-0.0021852956,-0.00023050094,0.07242125,-0.048409823,0.07519097,-0.05417338,0.041313,0.015993888,0.0070566377,-0.039876502,-0.0007825505,0.003345747,0.016985882,0.013880181,-0.022478031,0.041969366,0.02004301,-0.03874104,0.016344938,0.04840524,0.0421576,-0.08869856,0.030110922,0.022555325,0.03720633,0.018457955,-0.03171667,-0.022402532,0.005486597,0.026593182,-0.013346544,0.0051665865,0.010648567,-0.043471143,0.023461381,-0.061528657,0.049595613,0.034868665,0.017033547,0.0026854614,-0.014832137,-0.031078245,0.024816634,-0.03136401,-0.036610227,0.048766088,-0.031890817,-0.043075066,0.010966442,0.0043125106,-0.0050684735,-0.067846775,0.0903727,-0.04504323,0.00014356339,0.060255762,-0.053703025,0.016254934,-0.017690921,-0.008710078,-0.025093129,0.004884787,0.035035767,-0.019040992,0.019228954,0.008855371,0.021479882,-0.013442344,-0.052761085,0.0072513097,-0.023278594,0.003915668,-0.0151179405,-0.027815085,0.025583414,0.0041206186,0.02162008,0.0113516925,-0.026196832,0.007744839,-0.0033227499,-0.032303844,-0.03020874,-0.04301723,0.012177303,-0.0036163938,0.046068434,-0.035031237,0.039542302,0.004593793,0.0055751805,0.008678303,0.043529328,0.058061793,0.039288357,-0.039078683,-0.021533335,-0.0045873635,0.032216143,-0.006122891,-0.012405973,0.010343075,0.012947488,-0.054804098,0.0597768,0.0026898684,0.072827384,-0.005457019,-0.0031198643,0.018051706,-0.032105893,-0.008641102,-0.008421957,0.052285984,-0.0063388725,-0.04120558,0.008627478,0.04202904,0.048810296,-0.025659667,-0.006674934,0.026134072,-0.0057373433,0.020756966,0.052012466,0.020312028,-0.03332412,0.006538732,-0.034743622,-0.012697119,0.028955463,0.0075844424,0.0026483154,0.035134457,0.06763143,-0.01692934,-0.010006306,0.027694015,-0.011555498,-0.025520049,-0.0038806028,0.018123195,0.07133987,0.05953711,-0.04930636,0.052039377,-0.06575782,-0.031816334,0.026901182,-0.08122256,-0.039670598,0.003795368,-0.049812365,0.019672511,-0.03313079,-0.030441117,-0.04828876,0.022591954,-0.06868404,0.06233483,-0.044781547,0.020072764,-0.01269554,0.023767175,0.1126716,0.03573262,-0.018905517,-0.03669609,-0.027909128,-0.042708285,-0.0010290014,0.07273511,-0.004717782,0.011561448,0.053939223,0.014505087,0.024068372,-0.022969881,-0.0672796,-0.019882288,-0.017700845,0.007592223,-0.008563388,-0.011026077,0.025895966,-0.027419591,0.014153823,0.005615483,-0.011941276,-0.017320544,0.023769408,0.009395931,-0.0052040326,0.024627956,-0.018922037,0.041936047,0.0031678963,0.060016602,-0.032247208,-0.0049433503,0.052168082,-0.012780812,0.038694948,0.020635441,-0.0019314932,-0.050421026,-0.032635354,0.046329014,-0.042790145,0.02315095,-0.022458863,0.0135470675,0.021426408,0.02426989,-0.042475175,-0.047385797,-0.03743706,-0.024471978,-0.042110957,-0.009812994,0.0038388362,0.026499415,0.008939226,-0.021790927,0.03301787,-0.060754225,-0.014830814,-0.024989137,0.055963647,-0.004657775,-0.0339352,0.0021355231,-0.0059338533,-0.016288873,-0.027761584,0.051608354,0.032049503,-0.03621262,0.044611506,-0.028626082,-0.0014111383,-0.015461055,-0.008013578,-0.015584429,-0.020863505,-0.006330983,0.0014931774,0.09697101,-0.053121768,0.0059077498,-0.0035711613,-0.046646122,-0.07708536,-0.013511921,0.015050458,0.033622123,-0.014661069,-0.0009645887,0.022280749,0.02652061,-0.058553565,-0.046855662,-0.09219098,0.03532796,0.026278932,0.038742643,-0.0048550214,-0.01942972,-0.039807845,-0.0073935273,-0.023960818,-0.03073379,0.042308815,0.03992939,0.05819086,0.06817906,0.06172025,-0.044789534,-0.054026578,-0.026540563,-0.02088709,-0.0048632855,-0.046276364,0.034523986,-0.0044312347,-0.031859934,-0.010459784,-0.0018172276,-0.012005981,-0.007531487,-0.008815127,-0.012828213,0.0029031045,-0.021699335,-0.0082499925,0.01289889,0.025981588,-0.0302141,0.05848421,0.019485006,-0.041491877,-0.049999982,-0.052638043,-0.014018498,0.013169075,0.0039197872,0.00949532,0.027949274,-0.023308184,0.024495162,0.05641214,0.05795754,-0.014130663,-0.03595774,0.019839665,-0.0033874926,0.047348496,-0.0059910202,-0.041794132,-0.09866095,0.017557185,-0.003717359,-0.055749867,0.053881217,-0.033495434,-0.029701488,-0.033452302,0.012172882,0.010705936,0.033920716,0.009312649,0.00147805,0.0058677113,-0.02109803,0.043113194,-0.057678528,-0.0032183817,-0.04220654,0.019976217,-0.012624997,0.0032618803,0.025355073,0.014839438,0.036254257,0.023231896,0.025077937,0.010794922,0.0066803223,0.027456379,-0.017918376,0.047453687,0.0005744311,0.01664482,0.09365635,-0.024003945,0.030503279,-0.017617468,-0.0046936884,0.0049342476,0.012056611,-0.03238012,-0.04853135,-0.03518201,0.013863474,0.034504708,0.018084077,0.013086707,-0.013238739,-0.047599904,-0.045781963,-0.003622498,-0.050232008,0.0737391,-0.030397667,0.045470282,-0.061225295,0.020416634,-0.024748662,0.065132506,-0.02211657,0.014033185,-0.017083721,-0.012609656]	Keywords: sparse transformers, efficient attention, linear transformers, dynamic routing, weight memory, hash layers, routing transformers, capsule networks, transformer efficiency, attention mechanism, content-based attention, weight memory system, transformer architecture, attention system, linear transformers, hash layers, routing transformers, capsule networks, transformer efficiency, transformer architecture, sparse transformer, dynamic routing, linear transformers, memory system, attention\nKey Objects: Hash Layers, Routing Transformers, Capsule Networks, Linear Transformers, Sparse Models, Dynamic Routing, Weight Memory System, Attention Mechanism, Transformer Architecture, Transformer Efficiency, Attention System, Transformer Model, Weight Memory, Attention\nRefers to Images: None\nHypothetical Questions:\n- What are some approaches to make transformers more efficient?\n- How do hash layers contribute to sparse models?\n- What is the role of dynamic routing in capsule networks?\n- How do linear transformers function as weight memory systems?\n- What are routing transformers and how do they improve attention?\n- What is the benefit of using hash layers?\n- What are the advantages of linear transformers?\n- How does dynamic routing relate to capsule networks?\n---\nSummary:\nThis section focuses on methods to improve transformer efficiency through sparsity and alternative attention mechanisms. It highlights techniques like Hash Layers for sparse models (Roller et al., 1910101493) and Routing Transformers for efficient content-based attention (Roy et al., 3416919915).  Dynamic routing in capsule networks (Sabou et al., 2000011152) and the exploration of linear transformers as fast weight memory systems (Schlag et al., 2000012341) are also included.\nOriginal Text:\n- [110] Stephen Roller, Sainbayuk Shatbaatar, Arthur Szlam, and Jason Weston. 2021. Hash Layers For Large Sparse Models. arXiv:2106.04426 [cs.LG]\n- [111] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient Content-Based Sparse Attention with Routing Transformers. arXiv:2003.05997 [cs.LG]\n- [112] Sara Sabou, Nicholas Frost, and Geoffrey E. Hinton. 2017. Dynamic Routing Between Capsules. In Proceedings of NeurIPS . 3856-3866. https://proceedings.neurips.cc/paper/2017/hash/c2ad8fa47bbef282bbadb8de5374b894-Abstract. html\n- [113] Imanol Schlag, Kazuki Irie, and Jrgen Schmidhuber. 2021. Linear Transformers Are Secretly Fast Weight Memory Systems. CoRR abs/2102.11174 (2021). arXiv:2102.11174\nContextualized Text:\nThis section focuses on methods to improve transformer efficiency through sparsity and alternative attention mechanisms. It highlights techniques like Hash Layers for sparse models (Roller et al., 1910101493) and Routing Transformers for efficient content-based attention (Roy et al., 3416919915).  Dynamic routing in capsule networks (Sabou et al., 2000011152) and the exploration of linear transformers as fast weight memory systems (Schlag et al., 2000012341) are also included.	{"tags": ["transformers", "attention", "sparsity", "efficiency", "dynamic routing", "linear transformers", "sparse models", "architecture", "techniques"], "doc_id": "f19d848d-4e81-4a00-98af-474c6984e510", "summary": "This section focuses on methods to improve transformer efficiency through sparsity and alternative attention mechanisms. It highlights techniques like Hash Layers for sparse models (Roller et al., 1910101493) and Routing Transformers for efficient content-based attention (Roy et al., 3416919915).  Dynamic routing in capsule networks (Sabou et al., 2000011152) and the exploration of linear transformers as fast weight memory systems (Schlag et al., 2000012341) are also included.", "doc_type": "text", "entities": [], "keywords": ["sparse transformers", "efficient attention", "linear transformers", "dynamic routing", "weight memory", "hash layers", "routing transformers", "capsule networks", "transformer efficiency", "attention mechanism", "content-based attention", "weight memory system", "transformer architecture", "attention system", "linear transformers", "hash layers", "routing transformers", "capsule networks", "transformer efficiency", "transformer architecture", "sparse transformer", "dynamic routing", "linear transformers", "memory system", "attention"], "key_objects": ["Hash Layers", "Routing Transformers", "Capsule Networks", "Linear Transformers", "Sparse Models", "Dynamic Routing", "Weight Memory System", "Attention Mechanism", "Transformer Architecture", "Transformer Efficiency", "Attention System", "Transformer Model", "Weight Memory", "Attention"], "contextual_text": "This section focuses on methods to improve transformer efficiency through sparsity and alternative attention mechanisms. It highlights techniques like Hash Layers for sparse models (Roller et al., 1910101493) and Routing Transformers for efficient content-based attention (Roy et al., 3416919915).  Dynamic routing in capsule networks (Sabou et al., 2000011152) and the exploration of linear transformers as fast weight memory systems (Schlag et al., 2000012341) are also included.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some approaches to make transformers more efficient?", "How do hash layers contribute to sparse models?", "What is the role of dynamic routing in capsule networks?", "How do linear transformers function as weight memory systems?", "What are routing transformers and how do they improve attention?", "What is the benefit of using hash layers?", "What are the advantages of linear transformers?", "How does dynamic routing relate to capsule networks?"]}
6605d0a7-4e1b-4e6e-9102-543bfbddddb9	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0037396257,0.0053742058,-0.02900306,0.026080359,-0.0023732765,0.056964386,0.0043647275,0.077673495,0.044155587,-0.02849018,0.0077850763,0.023783494,-0.008795029,0.0028711269,-0.022160944,0.057345908,0.023954427,0.070587836,0.013463259,-0.026216455,0.040087976,0.012861224,0.027256897,-0.028180659,-0.012920996,0.0057919854,0.024588427,-0.028898004,0.004156595,0.041099392,0.035856336,-0.05252981,0.040615994,0.00076791825,0.036705803,0.009761647,-0.010284696,0.025073804,0.050611734,-0.031187164,-0.009255233,0.04504655,-0.070147656,0.008155927,-0.008685742,-0.07882788,-0.087004706,-0.019196942,0.050593194,-0.030206287,-0.026194744,0.018592639,-0.026459834,-0.030338634,0.006353235,-0.02344065,-0.018665703,-0.059010737,0.044422235,-0.09027444,-0.033519827,0.014037488,-0.04033865,-0.022288255,0.032329857,-0.049835134,-0.02170403,-0.012877392,0.024323037,0.1281061,-0.021623965,-0.010621432,-0.021991111,-0.06704175,0.12162929,0.029887963,0.0037302992,0.004137891,-0.085755385,-0.0050315997,-0.030593509,0.0829972,-0.049299005,-0.045812234,0.09054034,-0.004885684,-0.023275845,-0.007209516,0.07212019,-0.026535679,0.015972193,-0.01608738,0.0033599485,0.07059274,-0.0133981975,-0.06612467,0.015657933,-0.028876137,-0.0137676485,-0.052852478,-0.021431021,-0.012113288,0.06455387,0.1040443,0.034136742,-0.0471241,-0.03267082,-0.060067598,0.0022429188,0.021709474,-0.0025423004,0.012841992,-0.009348946,0.013844221,-0.017231818,-0.01231398,-0.051959403,0.012068699,-0.012951408,0.02232851,0.00585193,-0.009140242,0.0059151226,0.036412448,0.0887967,0.049418226,0.019902445,-0.005146158,-0.027053641,0.012163632,-0.016855204,-0.024487058,-0.029573405,0.011358666,-0.010053602,0.030230243,0.06950956,-0.054873854,0.016390605,0.0028480452,0.010506461,-0.04654568,-0.022002717,0.051137954,-0.025237609,0.021851884,-0.018829593,0.007472083,-0.015298902,0.041001577,0.0006325766,-0.028752537,0.044684045,0.0029816139,0.0052600205,0.0026715258,0.01066949,-0.045452114,0.016988028,-0.006263643,-0.045459274,-0.06871351,-0.025171062,0.044449776,0.021450257,0.12823172,-0.0042669056,0.017055469,0.0518303,-0.0019450727,-0.027949195,-0.005876375,-0.029175548,-0.032940708,0.022488037,0.0050451364,0.016700732,0.0035282373,-0.03292284,-0.011853881,0.0074954866,0.016638763,0.0007876321,0.028460028,-0.026661163,0.034177084,0.028829115,0.08568299,-0.037121765,-0.03963878,0.024596633,0.0031460773,-0.009128232,-0.0025084596,0.03793378,0.021381864,0.044920254,-0.08946355,-0.045558136,-0.029523045,-0.016271196,-0.001276648,0.011261774,-0.0030605975,0.044805635,-0.026781106,-0.022079932,-0.052833635,-0.0025676403,-0.00721072,-0.013361451,-0.03838619,-0.0012485019,0.052265983,0.043324053,-0.009434171,-0.026730128,0.032648794,-0.0025215938,-0.008337611,-0.06593945,-0.0102468105,-0.02408321,0.036666367,-0.058508217,0.008710065,-0.04393449,0.024369756,0.01876799,-0.033779074,0.035295103,0.006701914,-0.016423594,0.015761647,-0.017951494,0.048894607,-0.042834125,-0.017695107,-0.0050596343,0.05906101,0.02334208,0.071387805,-0.006472349,0.04837209,0.03323665,-0.0038677102,0.025840618,0.059828937,-0.03200539,0.017031234,0.008074948,0.049395986,0.0497868,-0.012580178,0.01471574,-0.034778602,0.012192366,0.039981976,0.038542595,0.0038253856,0.0047158957,-0.025460215,0.027448207,-0.0010116692,0.0070175636,0.030272178,0.020735689,0.0468472,0.029302666,0.007654459,0.03376083,0.04530818,0.0039214217,-0.023414193,-0.07489196,0.016009942,0.00651382,0.021640623,0.061260745,-0.0026592587,-0.026364706,-0.046089068,0.040886976,0.006351234,0.018463984,-0.028410796,-0.0034861409,-0.06661125,-0.07733792,-0.015594113,0.009260883,-0.047688745,0.019529963,-0.021733936,0.038263362,-0.0099112345,-0.0011822697,-0.029442996,-0.04563273,0.00010363033,0.12012374,-0.0059567476,0.033153024,-0.0024530285,-0.0064000417,0.03176265,-0.06847974,0.0145242745,0.045547202,0.01913919,0.005123231,-0.022271404,-0.006417051,-0.02052351,0.038395464,0.120692894,0.04477305,-0.023125215,-0.028381808,0.019582868,0.016843725,0.0050970083,0.0028018388,0.051593613,0.037045665,0.0067229685,-0.007491961,-0.0418767,-0.0639407,0.064583324,-0.0667301,0.057394236,0.046643116,0.014860644,-0.006677231,-0.017681753,0.031168438,0.04743896,-0.0024401043,0.004801568,-0.023442205,-0.04270183,0.016608337,-0.010499385,0.0062487945,0.026741927,0.008730847,0.03369366,0.034326386,0.0048208344,0.012176063,0.010550593,0.04416784,0.019834986,-0.03178351,-0.021499861,-0.009134151,0.021337025,0.0035707052,0.01173413,0.045698196,-0.04071907,0.070659176,-0.03132887,0.018036474,-0.0044871136,-0.051678598,-0.011650552,0.05418817,-0.030031541,-0.0006458436,-0.036278915,0.053696696,-0.022655303,-0.03308819,0.0032624297,0.021436451,0.013601091,-0.0647309,-0.002766837,-0.021585044,0.011808491,-0.058933534,-0.03740933,-0.015721396,0.0507582,0.007816053,0.031241998,-0.008454907,0.035870314,-0.04039838,-0.0052054645,0.018898552,-0.012322521,0.0052802917,0.0018667758,0.026312541,0.022440994,-0.004183197,-0.011278142,0.03745278,0.007068696,-0.042755507,0.018665072,0.061753776,0.016376097,-0.10767218,0.021857366,0.06007229,0.024619682,0.03977692,-0.033919953,-0.008816305,0.028396748,0.007503009,-0.00878594,-0.028006522,0.022058444,-0.070368186,0.014582366,-0.04244186,0.056144472,0.0018285178,0.023881936,0.033785477,-0.03210351,-0.022092229,0.02800394,0.015239015,-0.04179866,0.016267216,-0.073645286,0.011991747,0.04880698,0.018754983,0.052368037,-0.079821005,0.07615732,-0.06326207,-0.0002265828,0.059416827,-0.04499428,0.015656311,-0.06870574,-0.012068858,-0.026077937,0.008554645,0.027712468,-0.0023008082,-0.009381116,0.009978297,0.02493244,0.0022331243,-0.016172556,-0.0041066813,0.0031146607,-0.031361092,-0.01372231,-0.057159677,-0.009036689,0.0048084715,0.035995603,0.019224318,-0.015949149,0.008559676,-0.018339548,-0.016591966,-0.034765664,-0.048604283,-0.0055039125,0.0047355145,0.016461734,-0.037611067,0.02303554,-0.007506583,-0.0063238763,-0.047161598,0.04678673,0.038684502,0.01148324,-0.046080433,-0.06181705,0.017904412,0.04416519,-0.042233527,-0.009616444,-0.029805835,-0.023733133,-0.07009772,0.008737563,0.01280989,0.06615966,0.009842578,0.017560983,0.014877084,-0.048649088,0.018572481,0.012301542,0.03612479,-0.03266304,-0.049642634,-0.0103345765,0.06716345,0.040644076,-0.016638376,-0.011722965,0.010534994,0.0120894015,0.013019257,0.06903824,0.04001564,-0.009262223,-0.011747781,-0.008636848,-0.024981681,0.0268951,-0.022203589,0.016411625,0.010282801,0.044765145,-0.029725196,-0.011951508,0.0057161963,-0.016571403,0.0031632944,-0.016997295,0.028822243,0.051538747,0.060718197,-0.02873952,0.062016588,-0.044033512,-0.028146304,0.03863563,-0.032392714,-0.055774506,0.008915555,-0.09344399,-0.030406306,0.006889406,-0.01057883,-0.0130556775,0.015124731,-0.040314455,0.098665416,-0.038515504,0.03850512,-0.015019585,-0.0024307768,0.085825294,0.018334152,0.009124193,-0.00081588386,-0.0018776588,-0.056706585,0.030843547,0.05169197,-0.0065978053,0.02236072,0.05844978,0.00061204116,0.028921977,0.003362727,-0.05214122,-0.0257607,-0.047516968,-0.01755096,0.0058968794,-0.028169189,-0.0071265367,-0.003492945,0.014489428,0.008957335,0.0047426587,-0.023136446,0.04518423,0.033751033,-0.017597867,0.020776885,-0.041581817,0.043792967,0.044268776,0.022265915,-0.0053940667,0.009324669,0.04188292,-0.00046747693,0.005700754,0.0129251145,-0.007735769,-0.033085115,-0.04274501,0.024481451,0.0041794297,-0.011625232,0.0025728876,0.02365554,0.04094199,0.039526973,-0.04628961,-0.010739724,0.011772723,-0.021057203,-0.03676934,0.0062846774,-0.016344754,-0.011800403,0.0049145273,-0.021936482,0.076667584,-0.0886255,-0.028692525,-0.042200055,0.018384945,-0.010908186,-0.032989528,0.007698168,-0.031281088,-0.007764955,-0.007981462,0.05805594,0.048121467,-0.06438636,0.012057951,-0.03210265,0.00096619205,-0.015587897,-0.006393907,-0.058928825,-0.008629042,-0.004336859,0.00039580752,0.08427414,-0.015891913,-0.019129343,0.027731635,-0.025461249,-0.055759672,-0.023869993,0.03350105,0.007585102,-0.005648069,-0.033879865,0.010924396,0.038612258,-0.06692766,-0.011429964,-0.057519555,0.02064652,-0.019775249,0.0056978064,0.025979958,0.016996646,-0.07652961,0.016366843,-0.008731506,-0.05482988,-0.018833216,0.06527165,0.04597809,0.061563168,0.05659182,-0.038289774,-0.084597185,-0.034675736,-0.031082623,0.008282498,-0.043304205,0.052975677,-0.02671563,-0.009773563,-0.01111477,-0.019655367,-0.005299847,0.025579946,0.011203123,-0.024897933,-0.028530678,-0.019900495,-0.010755739,0.018579064,0.008184793,-0.015321175,0.049408548,0.03948752,-0.06676389,-0.053740993,-0.04693245,-0.019630166,0.0042334064,-0.0055744937,0.047513828,-0.004631019,-0.011112542,-0.011215408,0.046554245,0.03158758,-0.014822964,-0.015208817,0.042484198,-0.022580648,0.04251514,-0.0036303268,-0.012337527,-0.122344695,0.02749929,0.0009927435,-0.04548913,0.038058165,0.035167158,-0.021321433,-0.04837133,-0.027637849,0.021053664,0.015591246,0.057341512,0.0029194036,0.03787562,0.025595361,-0.016852222,-0.045314282,-0.0071898545,-0.035352547,0.013349404,-0.019118706,0.0052951234,-0.026563566,0.019170698,0.027360959,0.03020066,0.0063774753,-0.03647266,0.03834469,0.04258904,-0.025291778,0.05990232,-0.006121702,0.016704945,0.04502436,-0.022064334,0.034263663,-0.0069294525,0.01043508,0.015293098,0.017484909,-0.0048305304,-0.028647745,-0.06053316,0.04436244,0.024416098,0.032989502,0.04340129,-0.01875606,-0.026143849,-0.06315212,-0.020002738,-0.025760632,0.05878417,0.003570426,0.035295226,-0.0639479,0.028023936,-0.022797681,0.028341666,0.029083988,-0.006753383,-0.013018998,-0.00488856]	Keywords: transformers, attention, natural language processing, computer vision, machine learning, architecture, efficiency, long sequences, object detection, chemistry, position embeddings, sparse attention, decoding, weight memory systems, relative position representations, fast transformer decoding, molecular prediction, video retrieval, transformer dissection, lazyformer, linear transformers, memory-efficient transformer architecture search, self-attention with lazy update, molecular transformer, relative position embeddings, deformable DETR, fast transformer decoding, informer, molecular prediction, relative position embeddings, video retrieval, deformable DETR, molecular transformer, fast transformer decoding, self-attention with lazy update, deformable DETR, molecular transformer, fast transformer decoding, video retrieval, self-attention with lazy update, relative position embeddings, molecular transformer, fast transformer decoding, lazyformer, relative position embeddings, self-attention with relative position representations, weight memory systems, lazyformer, deformable transformer, molecular prediction, relative position embedding, video retrieval, fast transformer decoding, molecular transformer, sparse attention, deformable transformer\nKey Objects: Transformer Architecture, Self-Attention Mechanism, Weight Memory Systems, Relative Position Representations, Lazyformer, Sparse Attention, Molecular Prediction, Video Retrieval\nRefers to Images: None\nHypothetical Questions:\n- What are some of the key architectural advancements related to Transformers?\n- How have researchers improved the efficiency of Transformer models?\n- What are some of the specialized applications where Transformers are being utilized?\n- Can you provide examples of how Transformers are being used in computer vision and natural language processing?\n- What are some recent trends in Transformer research?\n---\nSummary:\nThis section lists references for a survey on Transformers. The references span various applications including natural language processing, computer vision, and even chemistry, demonstrating the broad applicability of Transformer models. They cover advancements in architecture, efficiency, and specialized use cases like long-sequence modeling, object detection, and molecular prediction. The listed papers use a range of techniques such as relative position embeddings, weight memory systems, sparse attention, and techniques for efficient decoding.\nOriginal Text:\n- [113] Imanol Schlag, Kazuki Irie, and Jrgen Schmidhuber. 2021. Linear Transformers Are Secretly Fast Weight Memory Systems. CoRR abs/2102.11174 (2021). arXiv:2102.11174\n- [114] Philippe Schwaller, Teodoro Laino, Thophile Gaudin, Peter Bolgar, Christopher A. Hunter, Costas Bekas, and Alpha A. Lee. 2019. Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction. ACS Central Science 5, 9 (2019), 1572-1583. https://doi.org/10.1021/acsenci.9b00576\n- [115] Jie Shao, Xin Wen, Bingchen Zhao, and Xiangyang Xue. 2021. Temporal Context Aggregation for Video Retrieval With Contrastive Learning. In Proceedings of WAC. 3268-3278.\n- [116] Peter Shaw, Jakob Uszkorei, and Ashish Vaswani. 2018. Self-Attention with Relative Position Representations. In Proceedings of HLT-NACL. New Orleans, Louisiana, 464-468. https://doi.org/10.18653/v1/N18-2074\n- [117] Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You Need. CoRR abs/1911.02150 (2019). arXiv:1911.02150\nContextualized Text:\nThis is a curated list of references focused on various aspects of Transformer models, encompassing architectural advancements, efficiency improvements, and specialized applications across diverse fields. It showcases the evolving landscape of Transformer technology and its impact on both fundamental research and practical implementations.	{"tags": ["reference list", "bibliography", "transformer models", "deep learning", "survey"], "doc_id": "6605d0a7-4e1b-4e6e-9102-543bfbddddb9", "summary": "This section lists references for a survey on Transformers. The references span various applications including natural language processing, computer vision, and even chemistry, demonstrating the broad applicability of Transformer models. They cover advancements in architecture, efficiency, and specialized use cases like long-sequence modeling, object detection, and molecular prediction. The listed papers use a range of techniques such as relative position embeddings, weight memory systems, sparse attention, and techniques for efficient decoding.", "doc_type": "text", "entities": ["Imanol Schlag", "Kazuki Irie", "Jrgen Schmidhuber", "Philippe Schwaller", "Teodoro Laino", "Thophile Gaudin", "Peter Bolgar", "Christopher A. Hunter", "Costas Bekas", "Alpha A. Lee", "Jie Shao", "Xin Wen", "Bingchen Zhao", "Xiangyang Xue", "Peter Shaw", "Jakob Uszkoreit", "Ashish Vaswani", "Noam Shazeer", "Transformer", "ACS Central Science", "WAC", "HLT-NACL", "CoRR", "IEEE", "ICASSP", "AMTA", "AAAI", "Deformable DETR", "Informert", "LazyFormer", "Molecular Transformer", "Self-Attention with Lazy Update", "Fast Transformer Decoding", "Transformer Dissection", "Molecular Prediction", "Self-Attention with Relative Position Representations", "Weight Memory Systems", "Sparse Attention", "Video Retrieval", "Relative Position Embedding", "Molecular Transformer", "Fast Transformer Decoding", "Lazyformer"], "keywords": ["transformers", "attention", "natural language processing", "computer vision", "machine learning", "architecture", "efficiency", "long sequences", "object detection", "chemistry", "position embeddings", "sparse attention", "decoding", "weight memory systems", "relative position representations", "fast transformer decoding", "molecular prediction", "video retrieval", "transformer dissection", "lazyformer", "linear transformers", "memory-efficient transformer architecture search", "self-attention with lazy update", "molecular transformer", "relative position embeddings", "deformable DETR", "fast transformer decoding", "informer", "molecular prediction", "relative position embeddings", "video retrieval", "deformable DETR", "molecular transformer", "fast transformer decoding", "self-attention with lazy update", "deformable DETR", "molecular transformer", "fast transformer decoding", "video retrieval", "self-attention with lazy update", "relative position embeddings", "molecular transformer", "fast transformer decoding", "lazyformer", "relative position embeddings", "self-attention with relative position representations", "weight memory systems", "lazyformer", "deformable transformer", "molecular prediction", "relative position embedding", "video retrieval", "fast transformer decoding", "molecular transformer", "sparse attention", "deformable transformer"], "key_objects": ["Transformer Architecture", "Self-Attention Mechanism", "Weight Memory Systems", "Relative Position Representations", "Lazyformer", "Sparse Attention", "Molecular Prediction", "Video Retrieval"], "contextual_text": "This is a curated list of references focused on various aspects of Transformer models, encompassing architectural advancements, efficiency improvements, and specialized applications across diverse fields. It showcases the evolving landscape of Transformer technology and its impact on both fundamental research and practical implementations.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some of the key architectural advancements related to Transformers?", "How have researchers improved the efficiency of Transformer models?", "What are some of the specialized applications where Transformers are being utilized?", "Can you provide examples of how Transformers are being used in computer vision and natural language processing?", "What are some recent trends in Transformer research?"]}
17ec3aba-8ee6-43b8-bd1d-17e4b719352a	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.03712372,0.0031523947,0.017313259,0.046742007,-0.03970499,0.042967305,0.008683725,0.07243686,-0.001626146,-0.030437034,-0.003336195,0.022210643,-0.015752602,-0.042921968,-0.045981985,0.0618163,0.021072315,0.06028742,0.0144861005,-0.012351212,0.05691248,0.0029180471,0.03769212,-0.024174886,-0.028529737,0.02678924,0.0468416,0.0024070155,0.03402407,0.029123241,0.024481544,-0.044932537,0.052089725,0.032426845,0.040206466,0.021159813,0.014550625,-0.024490027,0.028686518,0.01435768,-0.00453862,0.02122439,-0.04872794,0.009710511,0.022667786,-0.07013018,-0.109908335,-0.045466747,0.016274435,-0.028887011,-0.061008617,0.029651977,0.011808503,-0.0440381,0.012580294,0.013182674,0.0021884837,-0.05165319,0.010658961,-0.113639034,-0.02658811,0.010998605,-0.026236324,-0.009555832,0.03604004,-0.0360063,0.015509102,0.018045804,0.023637574,0.13319246,0.0077860886,0.006356729,0.013866184,-0.07463908,0.13379769,0.053216293,-0.008863963,-0.019947046,-0.07282233,-0.03383984,-0.016299142,0.08907422,-0.050220586,-0.08874155,0.08156945,0.01509227,-0.04686475,-0.023575496,0.067508936,-0.030569606,0.010581449,0.008239547,-0.028547127,0.038435902,0.015077212,-0.09588179,0.024208605,-0.019607367,-0.014600426,-0.028069561,-0.010901509,-0.00756178,0.09080736,0.12638676,-0.0166279,-0.0011857358,-0.019468179,-0.01767892,-0.011344239,-0.0009855035,0.0115166195,0.015832225,-0.031139266,-0.00059291587,-0.031381864,0.002686311,-0.04730935,0.022577662,0.021544244,0.0070502553,0.0023564273,0.0050380067,-0.03154862,-0.013504758,0.053505443,0.05859068,0.0033900675,0.015616737,-0.040463373,-0.00441325,0.028585063,-0.0015014124,-0.0017334246,0.013366933,-0.04023408,0.0043914085,0.036469836,-0.035886656,0.0018106569,0.04257593,-0.00969238,-0.023948193,-0.046600185,0.040329695,-0.013463807,0.023879731,-0.029330648,-0.010415143,-0.014401975,0.07603204,0.036293946,-0.023722854,0.05597225,-0.0017153794,0.00023607588,0.019972686,-0.0069209356,-0.033540882,0.024385726,-0.001047789,-0.03530682,-0.067232825,-0.052744903,0.0495671,0.06312251,0.1042208,0.009928452,-0.005579535,0.04418747,-0.009897988,-0.031296775,-0.011061327,-0.015954293,-0.066916004,0.043378092,-0.02891256,-0.0082242815,-0.03905792,0.0030876098,-0.009377961,0.012129785,0.030661296,-0.026845396,0.049517516,-0.020639876,0.021105753,0.024535593,0.094112195,-0.041666098,-0.033970512,0.014810293,0.001673649,-0.024887264,-0.010780137,0.076660424,0.04137263,0.049329422,-0.04428823,-0.037932396,-0.07403837,0.019896273,0.033810224,0.012766403,-0.004130372,0.024750024,-0.033494793,-0.03936181,-0.020049755,-0.039693367,-0.02326165,0.029744083,-0.015715454,0.009803617,0.06142186,0.027910857,-0.011245016,-0.033913966,0.023428462,0.0408611,-0.010971249,-0.042818557,-0.018893428,0.0044196644,-0.009648034,-0.044737175,0.010922984,-0.015099366,0.042772677,-0.02047739,0.0028906995,0.022860521,0.047075335,-0.033086296,0.02799228,-0.03837522,0.06359067,-0.015217979,0.066194974,-0.038967416,0.057453465,0.053089954,0.039241936,-0.0073128347,0.073332004,0.043487348,-0.022299832,0.03426555,0.022930946,0.00067937054,-0.020628987,-0.02114509,0.040240783,0.041382432,-0.015464441,0.013835279,-0.010640106,0.004386001,0.044312168,0.04512438,0.043512024,-0.022641305,-0.06657432,-0.026698964,0.013778915,-0.020355705,0.003287557,0.0122929765,0.0065732175,-0.022494774,-0.013147756,0.03446939,0.036771268,0.031922124,-0.034293644,-0.054776512,0.035699505,0.0013024358,-0.00845739,0.05071565,-0.054367263,-0.004822914,-0.05059428,0.038069833,0.011604275,0.008552815,-0.016357183,0.015108626,0.011453216,-0.08185217,0.01728147,-0.00090375164,-0.053597525,0.006127325,-0.023262667,-0.026945272,0.001252878,-0.0027511164,-0.027620317,-0.06771086,-0.004260881,0.09760182,-0.015243231,0.029902197,0.041778367,-0.026452579,0.023818051,-0.06740151,0.043577094,0.037108988,0.016338456,0.0057286574,0.0042098393,-0.009177147,-0.035526678,0.0260826,0.08948067,0.061369758,-0.0568391,-0.004836257,0.024687396,0.010066205,0.01189355,0.007993304,0.028431421,0.036074683,-0.023124684,0.022540225,-0.02831599,-0.059658714,0.08192707,-0.047522306,0.028509127,0.061977707,0.02866878,-0.0069976267,-0.00830387,0.019236306,0.015871204,-0.004291584,-0.0038209965,-0.03226132,-0.005099854,-0.052908313,-0.007881875,0.012455152,0.029531807,-0.002265312,-0.0066806762,0.025890792,-0.0043153143,0.009086671,0.030504337,0.047684874,0.030951995,0.020068867,-0.0069458913,0.00505993,0.0013689023,0.033206776,0.04025157,0.037947167,-0.09148908,0.039526697,-0.019032193,0.00040101158,-0.030401923,-0.054243915,0.018340193,0.0794732,-0.027688107,-0.010499577,-0.0027179879,0.067278184,-0.008939542,-0.030439539,0.0065213027,0.029493883,0.035056118,-0.030719124,-0.01301557,0.0023078977,0.018648779,-0.02603502,-0.029022891,0.00076960225,0.03489851,0.018819671,0.0640512,0.013298069,0.061233897,-0.05763313,0.008600258,0.03777931,-0.027783256,-0.051114213,0.013603233,0.017763857,0.024231462,0.0037132239,-0.047437914,0.000919553,0.029812269,-0.051310387,0.06033495,0.05130448,0.06121602,-0.06532688,0.03344336,0.036783256,0.030029975,0.026839588,-0.038066726,-0.049262118,0.0129949935,0.006100969,-0.020752348,-0.018275518,0.053612188,-0.04615031,0.0062553333,-0.09331003,0.051367354,0.01490955,0.036291417,0.00094540365,-0.00251465,-0.036488514,0.05316745,0.004260138,-0.019506175,0.010271411,-0.060953118,0.015856575,0.031154722,-0.011795033,0.010089039,-0.039278127,0.057347175,-0.05692098,0.05734485,0.040894423,-0.025794491,-0.013107377,-0.026643852,-0.028613452,0.020454556,0.041076783,0.016329337,-0.015550623,-0.025040226,0.005815759,0.011786292,0.0075909793,-0.01921979,0.0036928987,-0.021069592,-0.010048646,-0.0009424702,-0.04906625,0.019258657,0.017027153,0.062128507,0.02164136,0.0002681607,0.009137149,-0.0350734,-0.068647765,-0.03048244,-0.0361497,0.008110678,0.008501059,0.0331513,-0.024684902,0.033031438,0.0045576636,-0.025783902,-0.03548056,0.016523914,0.026308915,0.039659776,-0.056168437,-0.0654059,-0.028915297,0.05382004,-0.01251063,-0.022725478,-0.013151901,-0.0072905486,-0.037397046,0.028515562,0.014356192,0.046476405,0.044301026,0.00095553434,0.027609164,-0.021740546,0.010931408,0.022134542,-0.00046342908,-0.023236737,-0.022502147,-0.0061057797,0.062253006,0.043059744,-0.02054846,0.009626583,-0.0019180066,0.02540736,-0.003180995,0.03890478,0.013237422,-0.01812793,0.031056857,0.0027822123,-0.036801245,-0.0055259313,-0.0063670473,0.00016175413,0.010129627,0.055478305,-0.033770315,-0.0023771888,0.0007744097,-0.00483177,-0.012463814,-0.0045815986,0.04397744,-0.0008508993,0.019443246,-0.044346813,0.04946665,-0.04298399,-0.037588246,0.030605158,-0.058681395,-0.04445813,0.022625491,-0.032202847,0.0016023256,-0.0059327423,0.038656168,-0.018431714,0.031107424,-0.04562146,0.07464345,-0.024479924,0.0034672387,0.012764689,0.027893063,0.108663455,0.025025466,0.017234793,0.0047724023,-0.02820511,-0.021067565,0.011694604,0.07505669,0.0032796527,0.023330826,0.076870166,-0.0027395813,0.011344102,-0.0051838113,-0.11945277,-0.02347758,-0.04829784,-0.01614025,-0.004466685,-0.010725377,0.007515312,-0.0027729678,0.004754707,-0.024912495,0.021510242,-0.020604322,0.0278594,0.032607686,-0.008795049,0.007219917,-0.006181904,0.017190153,0.01867648,0.020273462,-0.011590935,0.004553485,0.05269769,-0.03435379,-0.0055535086,-0.008829105,-0.017936133,-0.05545446,-0.041906115,0.024199318,-0.008455343,-0.01805285,-0.020553112,-0.015872039,0.055721104,0.06578288,0.00307981,-0.042912003,-0.006724138,-0.064454064,-0.009017655,0.006615108,0.020669842,-0.012239571,0.020769872,-0.039252784,0.07092132,-0.08390332,0.014799881,-0.028159788,0.010905544,0.002102431,-0.06890214,0.020281611,-0.022137055,0.008898615,-0.022908095,0.04408376,0.06434019,-0.03201454,0.0108750295,-0.024625383,-0.014045478,-0.045824625,-0.042357113,-0.04746892,-0.0043310006,-0.01291215,-0.002470215,0.08812978,0.0054117856,-0.020958554,0.020073632,-0.051623285,-0.07081923,-0.033496935,0.013783017,0.042874698,-0.008430153,0.010593027,-0.012235124,0.006042503,-0.04184696,0.009658508,-0.060542665,0.041263424,-0.006933393,-0.0165419,-0.019926993,-0.04080406,-0.05302944,0.013622207,-0.029605545,0.010884994,-0.03807263,-0.0036510853,0.032767754,0.049859095,0.04816281,-0.00572911,-0.054197386,-0.014909899,-0.021397784,-0.016912416,-0.059334625,0.031206677,-0.017944638,-0.009325327,-0.0016343043,0.006259529,-0.012316952,-0.008648134,-0.023328856,0.020433165,-0.03915594,0.0027639417,-0.0034273698,0.0135452235,0.00016125856,0.03027818,0.07073302,0.041848246,-0.07661643,-0.01965942,-0.046192057,-0.006245137,-0.004946695,0.001032202,0.021057548,-0.0003927563,0.0064842887,0.0033827056,0.09818244,0.04211181,-0.025530346,-0.015698338,0.026644392,0.023104053,0.0174547,0.001823402,-0.026046338,-0.10360278,0.022902185,-0.012251299,-0.06696381,0.01810546,-0.0026434674,-0.025483903,-0.024089126,-0.023568293,0.0028621207,0.0022915043,0.035457794,-0.017242495,-0.0062411656,0.026628848,0.025636757,-0.068979315,0.014373242,-0.02504995,0.0031996847,-0.039055273,0.0064433278,-0.021445954,-0.022397028,0.0158032,0.012598186,0.0015247213,0.004902444,0.023684302,0.033020157,0.004211977,0.034238663,-0.012706509,0.0020054944,0.057097707,0.027572611,0.048708122,0.0028908013,0.034636486,0.021976382,0.0007229554,-0.010774569,-0.03655367,-0.08508282,0.012325312,0.008839478,0.004004615,0.02279157,-0.025609778,-0.037367493,-0.0051527256,0.0057497546,-0.029836046,0.012721497,0.0021438845,0.039188087,-0.042721935,0.011301129,-0.019087434,0.012700337,0.021736579,0.029136492,-0.025610443,0.009411221]	Keywords: Noam Shazeer, Transformer, Fast Transformer Decoding, GLU Invariants, Talking-Heads Attention, Sparse Mixture-of-Experts, Efficiency, Robustness, Large Datasets, Optimization, Neural Networks, Deep Learning, ICLR, ICML, Neural Networks, Deep Learning, ICLR, ICML, Optimization, Transformer Models, Decoder, Architecture, Attention Mechanism, Performance Enhancement, Large Scale Models, Deep Learning Applications, Algorithm Development, Computational Efficiency, Architectural Improvements, Neural Network Optimization, Performance Tuning, Transformer Architecture, Attention Head, Sparse Representation, Model Complexity, Data Efficiency, Efficiency, Architectural Improvements, Deep Learning, Computational Efficiency, Neural Network Design, Algorithm, Optimization, Decoder, Transformer Architecture, Sparse Representation, Computational Efficiency, Attention Mechanism, Fast Inference, Scalability, Algorithm Development, Neural Network Optimization, Transformer Models, Large Language Models, Computational Optimization, Neural Network Algorithm, Deep Learning Method, Model Efficiency, Computational Performance, Deep Learning, Large Models, Efficient Computing, Architecture, Learning\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What is the main focus of this section?\n- Who is Noam Shazeer and what is his contribution to Transformer models?\n- What is 'Fast Transformer Decoding' and why is it important?\n- What does 'GLU Invariants' aim to achieve?\n- What are sparsely-gated Mixture-of-Experts layers and what role do they play in this context?\n---\nSummary:\nThis section of the text focuses on the work of Noam Shazeer and his contributions to improving Transformer models. It highlights several key advancements, including 'Fast Transformer Decoding,' which proposes using a single write-head for efficiency, and 'GLU Invariants,' which aims to enhance the Transformer's performance.  The section also mentions 'Talking-Heads Attention' and 'Outrageously Large Neural Networks', specifically detailing the use of sparsely-gated Mixture-of-Experts layers.  Essentially, it showcases a progression of research focused on making Transformers faster, more robust, and capable of handling larger datasets.\nOriginal Text:\n- [117] Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You Need. CoRR abs/1911.02150 (2019). arXiv:1911.02150\n- [118] Noam Shazeer. 2020. GLU Invariants Improve Transformer. arXiv:2002.05202 [cs.LG]\n- [119] Noam Shazeer, Zhenzhong Lan, Youlong Cheng, Nan Ding, and Le Hou. 2020. Talking-Heads Attention. CoRR abs/2003.02436 (2020). arXiv:2003.02436\n- [120] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc V. Le, Geoffrey E. Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks: The Sparsely-Gated Mixture of-Experts Layer. In Proceedings of ICLR . https://openreview.net/forum?id=bickdMq\n- [121] Sheng Shen, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer. 2020. PowerNorm: Rethinking Batch Normalization in Transformers. In Proceedings of ICML . 8741-8751. http://proceedings.mlr.press/v119sen.2hen0e1.html\nContextualized Text:\nThis section of the text focuses on the work of Noam Shazeer and his contributions to improving Transformer models. It highlights several key advancements, including 'Fast Transformer Decoding,' which proposes using a single write-head for efficiency, and 'GLU Invariants,' which aims to enhance the Transformer's performance.  The section also mentions 'Talking-Heads Attention' and 'Outrageously Large Neural Networks', specifically detailing the use of sparsely-gated Mixture-of-Experts layers.  Essentially, it showcases a progression of research focused on making Transformers faster, more robust, and capable of handling larger datasets.	{"tags": ["Deep Learning", "Transformers", "Optimization", "Algorithms", "Efficiency", "Architecture"], "doc_id": "17ec3aba-8ee6-43b8-bd1d-17e4b719352a", "summary": "This section of the text focuses on the work of Noam Shazeer and his contributions to improving Transformer models. It highlights several key advancements, including 'Fast Transformer Decoding,' which proposes using a single write-head for efficiency, and 'GLU Invariants,' which aims to enhance the Transformer's performance.  The section also mentions 'Talking-Heads Attention' and 'Outrageously Large Neural Networks', specifically detailing the use of sparsely-gated Mixture-of-Experts layers.  Essentially, it showcases a progression of research focused on making Transformers faster, more robust, and capable of handling larger datasets.", "doc_type": "text", "entities": [], "keywords": ["Noam Shazeer", "Transformer", "Fast Transformer Decoding", "GLU Invariants", "Talking-Heads Attention", "Sparse Mixture-of-Experts", "Efficiency", "Robustness", "Large Datasets", "Optimization", "Neural Networks", "Deep Learning", "ICLR", "ICML", "Neural Networks", "Deep Learning", "ICLR", "ICML", "Optimization", "Transformer Models", "Decoder", "Architecture", "Attention Mechanism", "Performance Enhancement", "Large Scale Models", "Deep Learning Applications", "Algorithm Development", "Computational Efficiency", "Architectural Improvements", "Neural Network Optimization", "Performance Tuning", "Transformer Architecture", "Attention Head", "Sparse Representation", "Model Complexity", "Data Efficiency", "Efficiency", "Architectural Improvements", "Deep Learning", "Computational Efficiency", "Neural Network Design", "Algorithm", "Optimization", "Decoder", "Transformer Architecture", "Sparse Representation", "Computational Efficiency", "Attention Mechanism", "Fast Inference", "Scalability", "Algorithm Development", "Neural Network Optimization", "Transformer Models", "Large Language Models", "Computational Optimization", "Neural Network Algorithm", "Deep Learning Method", "Model Efficiency", "Computational Performance", "Deep Learning", "Large Models", "Efficient Computing", "Architecture", "Learning"], "key_objects": [], "contextual_text": "This section of the text focuses on the work of Noam Shazeer and his contributions to improving Transformer models. It highlights several key advancements, including 'Fast Transformer Decoding,' which proposes using a single write-head for efficiency, and 'GLU Invariants,' which aims to enhance the Transformer's performance.  The section also mentions 'Talking-Heads Attention' and 'Outrageously Large Neural Networks', specifically detailing the use of sparsely-gated Mixture-of-Experts layers.  Essentially, it showcases a progression of research focused on making Transformers faster, more robust, and capable of handling larger datasets.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the main focus of this section?", "Who is Noam Shazeer and what is his contribution to Transformer models?", "What is 'Fast Transformer Decoding' and why is it important?", "What does 'GLU Invariants' aim to achieve?", "What are sparsely-gated Mixture-of-Experts layers and what role do they play in this context?"]}
aaeae236-b61e-4747-8254-4f0560797729	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.009639617,0.015084971,0.01674611,0.022682121,0.003611111,0.067541085,0.041005213,0.017831584,0.0212426,-0.027468663,0.02535706,0.0013156999,0.027929517,-0.009453087,-0.01455403,0.030988054,0.0075403163,0.0953725,0.014125226,-0.031532697,0.02933388,-0.007003823,-0.008084552,-0.024685496,0.024626728,0.031846233,0.044068858,-0.024681339,-0.005183826,0.0010732214,0.004286113,-0.012937446,-0.00093508646,0.004212605,0.045219418,-0.0006047622,0.020400718,0.028768219,0.0069315755,-0.00013032104,-0.028796278,0.0509381,-0.057211433,-0.023503628,0.008733768,-0.064668916,-0.1101712,-0.020156344,0.046603262,-0.011913699,-0.028163094,-0.014749543,-0.025791353,-0.01807668,0.030586204,0.0023496558,-0.02789264,-0.023834301,0.039227527,-0.09556123,-0.038221706,0.044260137,-0.047134962,-0.015090978,0.037853483,-0.03153443,-0.010731379,-0.008078283,0.013380931,0.11430088,-0.022268102,0.049143292,0.008586338,-0.050450075,0.12143526,0.052783895,-0.015944125,-0.010388885,-0.06631428,-0.0011071849,-0.02943288,0.07131539,-0.033099554,-0.054216698,0.09411992,0.0025531175,-0.039711162,-0.00018274426,0.06713875,-0.0232507,-0.019916655,-0.011932031,-0.017604863,0.06933319,-0.014687626,-0.057014324,-0.008929461,-0.05720542,-0.017515954,-0.02960971,-0.024086913,-0.01707134,0.072612226,0.078840844,0.024185475,-0.011087631,-0.011030524,-0.033240378,0.010919015,-0.019834502,0.017922767,0.020930225,0.007589638,0.004694863,-0.04253377,0.003308469,-0.05557414,0.011808832,0.024422359,-0.00025309555,-0.026566662,-0.0030959093,0.015055377,0.0075234105,0.064327896,0.017493092,0.009641714,-0.012818862,-0.037760712,0.025256753,0.016932452,-0.037898213,-0.011331104,0.04845293,-0.011244984,0.017683575,0.047369976,-0.03458933,0.005744177,0.008029924,0.029510647,-0.053526245,-0.035191126,0.006325935,-0.031556893,0.013865179,-0.05890298,-0.0035133096,-0.014036597,0.06415558,0.020594051,-0.0014634754,0.050964266,0.003892805,-0.018669495,-0.00033774815,-0.012034102,-0.0724787,-0.014666402,0.005251402,-0.06909588,-0.07472363,-0.024361067,0.05179403,0.04142606,0.11881649,0.024480212,0.0018638435,0.043302763,0.021635914,-0.021300882,0.015251089,-0.004337174,-0.04531154,-0.0020194675,0.028200144,0.010237188,-0.04312008,-0.011783895,0.033059146,-0.0017751004,0.009034106,0.003865575,0.010544236,-0.041775852,0.012170685,0.016482437,0.10249167,-0.019403484,-0.017328545,0.025519654,0.015625766,0.0012988016,0.008488387,0.049572036,0.009746118,0.041885134,-0.08515027,-0.06030872,-0.035850197,0.020678557,0.0063260724,0.023763947,-0.0036763689,0.040665377,-0.016103161,-0.008743698,-0.044703025,-0.025108017,-0.004141616,-0.017498784,0.00784958,0.020319968,0.032352287,0.013898368,0.0072291438,-0.021782665,0.04480544,0.020401256,0.007878886,-0.097504586,-0.019466888,0.021834094,0.0011740534,-0.05559349,0.021172754,-0.032770187,0.022719646,-0.023639409,-0.022367569,0.037763767,-1.8442599e-05,-0.019281156,0.034676883,-0.034076434,0.059102926,-0.010409727,0.020794509,0.005948154,0.037995715,0.034294274,0.03730419,-0.027842283,0.07002183,0.016591722,-0.017414378,0.016091572,0.04006271,-0.020203022,0.002657174,-0.0068391506,0.025116641,0.057148274,-0.015698884,-0.0004051155,-0.038655512,0.00811065,0.014160976,-0.0005180843,-0.027720952,-0.021108577,0.029968742,0.04091536,0.012125933,0.020673126,0.018410886,0.02111765,0.010225126,0.015047162,0.016575886,0.027096512,0.06051856,-0.016493892,-0.044825505,-0.05164498,0.023969434,-0.006134821,-0.0085137375,0.058661588,-0.031947333,-0.020702325,0.0028279214,0.08853231,-0.003446263,-0.003338229,-0.039012052,0.010476716,-0.025088185,-0.07101328,0.010618391,0.009769177,-0.060132075,0.073029436,-0.0065120473,-0.028492143,-0.0019394797,-0.001748483,-0.010922995,-0.025172204,0.024846647,0.065764815,-0.027733827,0.038117234,-0.016887588,-0.0038910408,0.037020136,-0.06992844,0.013188558,0.03050712,0.0065545654,0.011763052,0.0057483264,-0.022925917,-0.030001532,0.013801942,0.06664112,0.045267873,-0.023913413,0.012259616,0.02177474,0.03356011,0.016777866,-0.0025394657,0.034414835,0.007389965,-0.049898084,0.008608483,-0.025695194,-0.098794796,0.06392031,-0.07181347,0.039442334,0.042864967,-0.00689577,-0.0034168274,-0.02320198,0.035594765,0.050849486,-0.01896171,0.031079754,-0.009781277,-0.08271094,0.004304285,-0.007501864,-0.030140182,0.0007632178,-0.013660249,0.008024176,0.041595213,-0.0029735125,-0.006684391,0.034029793,0.041137565,0.010541002,-0.01609113,-0.008446992,-0.0060699517,-0.0101364255,0.019244082,0.030961249,0.05620733,-0.038766544,0.11051019,-0.001135057,0.0309793,-0.021715239,-0.06625817,-0.0305431,0.060254082,-0.02084775,-0.0037178297,-0.013837104,0.046874963,-0.0614653,-0.008579418,0.01587306,0.03124159,0.035232432,-0.053309433,0.00024127192,-0.026235277,0.019437885,-0.039565712,-0.032391723,-0.014634691,0.05291168,0.012931785,0.06281864,0.015808275,0.06443301,-0.042305335,0.018344158,0.040132176,-0.0036968114,-0.0081680585,0.024051517,0.030907772,0.034411494,0.0024951862,-0.012015937,0.010002614,0.0078096287,-0.034603033,0.04077516,0.040053774,0.057106562,-0.080661416,0.050053537,0.07138028,0.03541261,0.018137632,-0.043932676,-0.068499185,0.011470133,-0.027820276,-0.0039442196,-0.013939509,0.015018683,-0.04177028,-0.02067393,-0.040953547,0.055883303,0.016780643,0.030509096,0.004886702,-0.014136102,-0.027522033,0.022610994,-0.020162577,-0.044678003,-0.017644677,-0.085884444,-0.004854067,0.07658767,0.0278854,0.026119415,-0.053926785,0.070344634,-0.08787275,-0.00013541982,0.015836958,-0.035112377,0.044534404,-0.027601281,-0.0005899832,0.0011405709,0.018344978,0.008045282,-0.005996852,-0.009410424,0.010684751,0.04700601,0.027525336,-0.01696286,0.0058183256,-0.020680638,-0.04518899,-0.028613336,-0.061485417,0.0066068266,-0.011922718,0.027718475,0.012439868,-0.0055731656,-0.01080618,-0.02432531,-0.0312853,-0.043443657,-0.0051668887,-0.017042194,0.025881074,0.035314947,-0.04550469,-0.007973435,0.008569138,-0.034201268,-0.0055071586,0.037999697,0.014471313,-0.006921782,-0.029042691,-0.065059826,-0.0039632283,0.053054206,-0.029959869,-0.018353852,0.003876811,-0.03376393,-0.075194865,0.019984502,-0.005119254,0.06394641,0.0041034245,-0.0066818665,0.011054392,0.02240932,0.0055564935,0.028223956,0.024257753,-0.030694459,-0.058811788,0.0038868142,0.056446675,0.02565778,-0.00016822461,0.0070117586,0.016876904,0.0063010366,-0.0027197367,0.025051095,0.021505712,0.015465325,-0.018451221,-0.015450956,-0.010241121,0.058030225,-0.018573456,0.0022096012,0.022694847,0.04999976,-0.038124725,-0.03327581,-0.025375605,-0.027897924,0.02401991,-0.0147121,0.028022539,0.05423231,0.07923026,-0.06695889,0.019889066,-0.021153038,-0.040343016,0.01809848,-0.071296565,-0.00695427,0.050196853,-0.07549704,-0.033857636,0.019605575,-0.018868526,-0.011043195,-0.009472833,-0.0187042,0.070724614,-0.06461901,0.031296916,-0.011715066,0.013313922,0.10804405,0.016961752,-0.055743247,-0.010574551,-0.035677284,-0.046377618,0.02981504,0.03335914,-0.0056014606,0.022763068,0.077361174,-0.023945605,0.03485638,0.0012574454,-0.04801334,-0.02208494,-0.053832255,-0.04011748,-0.0069854204,0.0051310067,0.025770467,-0.013742449,-0.0030934447,-0.0056279832,0.023353096,-0.021565657,0.034633327,0.032856856,0.018517766,0.028375283,-0.040568486,0.040724326,0.009360746,0.001394253,0.0001760746,-0.019002927,0.0276883,0.024065865,-0.0014751396,-0.014652027,-0.0054032262,-0.040136974,0.0038307551,0.026665343,0.017906591,-0.014795744,0.0030268724,0.0071772733,0.03808123,0.0073581496,-0.031140545,-0.055167716,-0.018944768,-0.05603764,-0.032971956,0.025615767,-0.03608323,-0.0012109239,-0.013325029,-0.006840209,0.06568218,-0.0846511,0.006948955,-0.060898688,0.042249005,-0.006169112,-0.009200785,-0.008047717,-0.037410334,-0.02659202,-0.042968158,0.04214357,0.081782386,-0.0369386,0.029767301,-0.018550197,0.0060087903,-0.03460782,0.010180309,-0.028668072,-0.012854649,-0.009548819,-0.020100152,0.09360675,-0.026471227,-0.005107083,0.042452518,-0.0021527659,-0.07921726,-0.018365702,0.01124767,-0.005149067,0.005405686,0.0076581435,0.028047998,0.055824913,-0.0636439,-0.03586953,-0.07382024,0.019862581,0.0017550512,0.023024896,0.024746327,-0.034488294,-0.055907138,0.02983944,-0.018872377,-0.0380451,-0.010925138,0.03219676,0.054291815,0.067500144,0.045625746,-0.059588935,-0.10049282,-0.03385473,-0.04590521,-0.02741085,-0.039685816,0.022048183,-0.022762248,-0.007447628,0.003425507,-0.013354788,0.01114872,0.020854834,-0.023361746,0.0037598473,-0.041956868,-0.025107108,0.0021224015,0.033618324,-0.026019346,0.022275982,0.028124068,0.041823745,-0.0714883,-0.029036121,-0.028017769,-0.03700065,-0.01756001,-0.02942917,0.034391418,0.05388301,-0.00028339127,-0.007978882,0.059626,0.029843587,-0.033501115,-0.01333233,-0.0030709382,-0.00923622,0.013923973,0.026231052,-0.026700962,-0.13483202,0.007564639,-0.013394836,-0.07537645,0.04222621,-0.008107745,-0.016013555,-0.032976255,-0.012608204,-0.0146586485,-0.0027958807,0.03655463,0.00018096273,0.021527596,0.022186691,-0.0040692026,-0.032887332,-0.024941124,-0.017405726,-0.015602541,-0.028652016,0.036543038,0.0056986525,0.0037660887,-0.00095916237,0.06372225,-0.029720768,0.0057137827,0.00237286,0.044756997,-0.010309661,0.06331206,0.0025244774,-0.017689768,0.07622145,0.0023036643,0.04634105,-0.016284445,0.01191412,0.0008036726,1.6412416e-05,0.034126926,-0.06495998,-0.041361406,0.025321582,0.04957746,0.018938947,0.030884353,0.003293764,-0.03510384,-0.038643327,-0.014704694,-0.0035893712,0.07373836,0.040093474,-0.009762187,-0.062612414,0.01836627,-0.013966447,0.05448288,0.028051756,-0.011468014,0.011743611,-0.015011056]	Keywords: transformer models, large language models, architectural innovations, efficiency, long sequences, Megatron-LM, Evolved Transformer, RoFormer, VL-BERT, Adaptive Attention Span, long-range dependencies, visual-linguistic representations, training techniques, architectural design, transformer optimization, sequence modeling, natural language processing, machine learning, deep learning, computer vision\nKey Objects: training algorithms, deep learning models, large-scale training, sequence processing, computational resources, architecture design, optimization techniques, neural networks, computational techniques, large-scale language models, transformer architecture, neural architecture search, sequence optimization, sequence representation, large datasets, machine learning, neural networks, sequence modeling, deep learning, optimization, computer vision\nRefers to Images: None\nHypothetical Questions:\n- What are the main challenges in training large-scale language models?\n- How do the Evolved Transformer, RoFormer, and VL-BERT contribute to the advancement of transformer models?\n- What techniques are employed to improve the efficiency and handle long sequences in transformer models?\n- What is the significance of Adaptive Attention Span in the context of transformer models?\n- How do these references collectively shape the landscape of transformer model research?\n- What are the common themes or trends emerging from these references?\n- What are the potential future directions for research in this field?\n- What are the practical implications of these advancements?\n- How can these technologies be applied to real-world problems?\n- What are the limitations of these approaches?\n---\nSummary:\nThis section presents a curated selection of references focusing on advancements in transformer models, including approaches for training large-scale language models (Megatron-LM), architectural innovations (Evolved Transformer, RoFormer, VL-BERT), and techniques for improving efficiency and handling long sequences (Adaptive Attention Span).\nOriginal Text:\n- [122] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGreys, Jared Casper, and Bryan Catanzaro. 2020. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism. arXiv:1909.08053 [cs.CL]\n- [123] David R. So, Quoc V. Le, and Chen Liang. 2019. The Evolved Transformer In Proceedings of ICLM . 587-5886. http://proceedings.mlr.press/97/so19a.html\n- [124] Jianlin Su, Yu Lu, Sheng Feng, Pan Bo Wen, and Yunfeng Liu. 2021. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv:2104.09864\n- [125] Weije Su, Zihou Zhu, Yue Cao, Bin Li, Lee Liu, Wei Fu, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In Proceedings of ICLR . https://openreview.net/forum?id=SyGXpaEvHV\n- [126] Sainbayuk Sbuhakatar, Eduard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive Attention Span in Transformers. In Proceedings of ACL . Florence, Italy, 331-335. https://doi.org/10.18653/v1/P19-1032\nContextualized Text:\nThis section presents a curated selection of references focusing on advancements in transformer models, including approaches for training large-scale language models (Megatron-LM), architectural innovations (Evolved Transformer, RoFormer, VL-BERT), and techniques for improving efficiency and handling long sequences (Adaptive Attention Span).	{"tags": ["transformer models", "training techniques", "architecture design", "optimization techniques", "neural architecture search", "deep learning", "computer vision", "architecture optimization", "machine learning", "sequence modeling", "large language models", "computational complexity", "transformer architecture", "neural network", "optimization", "computational resources", "large-scale training", "computer vision", "sequence modeling", "optimization techniques", "architecture design", "optimization algorithms", "transformer architecture", "machine learning", "architecture design", "neural networks", "optimization techniques", "neural networks", "computational complexity", "optimization techniques", "deep learning", "architecture optimization", "training resources", "optimization algorithms", "machine learning", "deep learning models", "sequence processing", "model design", "deep learning", "sequence learning", "neural networks", "computational resources"], "doc_id": "aaeae236-b61e-4747-8254-4f0560797729", "summary": "This section presents a curated selection of references focusing on advancements in transformer models, including approaches for training large-scale language models (Megatron-LM), architectural innovations (Evolved Transformer, RoFormer, VL-BERT), and techniques for improving efficiency and handling long sequences (Adaptive Attention Span).", "doc_type": "text", "entities": ["Megatron-LM", "Evolved Transformer", "RoFormer", "VL-BERT", "Adaptive Attention Span", "natural language processing", "machine learning", "deep learning", "computer vision", "transformer models", "large language models", "architectural innovations", "efficiency", "long sequences", "sequence modeling", "transformer optimization", "transformer design", "training techniques", "visual-linguistic representations", "natural language processing", "deep learning", "computer vision", "sequence modeling", "transformer optimization", "transformer design", "training techniques", "visual-linguistic representations", "machine learning", "deep learning", "computer vision", "sequence modeling", "transformer optimization", "transformer design", "training techniques", "visual-linguistic representations", "computer vision", "deep learning", "natural language processing", "machine learning", "computer vision", "sequence modeling", "transformer optimization", "transformer design", "training techniques", "visual-linguistic representations", "machine learning", "computer vision", "deep learning", "natural language processing", "computer vision", "machine learning", "natural language processing", "computer vision", "deep learning", "training techniques", "transformer optimization", "transformer design", "sequence modeling", "visual-linguistic representations", "architecture", "model optimization", "sequence representation", "model architecture", "deep learning models", "large models", "model architecture", "deep learning", "sequence modeling", "large models", "sequence optimization", "architecture", "sequence representations", "architecture", "deep learning models", "optimization", "architecture", "computer vision", "machine learning", "model optimization", "sequence processing", "computer vision", "deep learning models", "architecture", "computational resources", "sequence processing", "architecture", "sequence processing", "large datasets", "architecture", "model size", "large-scale", "architecture", "training", "optimization", "deep learning", "computational resources", "scaling", "sequence learning", "training resources", "large-scale language models", "computational complexity", "large-scale training", "model scaling", "sequence optimization", "transformer architecture", "training algorithms", "architecture design", "neural network", "architecture", "computational resources", "machine learning", "computer vision", "optimization techniques", "model design", "neural architecture search", "neural networks", "computational techniques"], "keywords": ["transformer models", "large language models", "architectural innovations", "efficiency", "long sequences", "Megatron-LM", "Evolved Transformer", "RoFormer", "VL-BERT", "Adaptive Attention Span", "long-range dependencies", "visual-linguistic representations", "training techniques", "architectural design", "transformer optimization", "sequence modeling", "natural language processing", "machine learning", "deep learning", "computer vision"], "key_objects": ["training algorithms", "deep learning models", "large-scale training", "sequence processing", "computational resources", "architecture design", "optimization techniques", "neural networks", "computational techniques", "large-scale language models", "transformer architecture", "neural architecture search", "sequence optimization", "sequence representation", "large datasets", "machine learning", "neural networks", "sequence modeling", "deep learning", "optimization", "computer vision"], "contextual_text": "This section presents a curated selection of references focusing on advancements in transformer models, including approaches for training large-scale language models (Megatron-LM), architectural innovations (Evolved Transformer, RoFormer, VL-BERT), and techniques for improving efficiency and handling long sequences (Adaptive Attention Span).", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the main challenges in training large-scale language models?", "How do the Evolved Transformer, RoFormer, and VL-BERT contribute to the advancement of transformer models?", "What techniques are employed to improve the efficiency and handle long sequences in transformer models?", "What is the significance of Adaptive Attention Span in the context of transformer models?", "How do these references collectively shape the landscape of transformer model research?", "What are the common themes or trends emerging from these references?", "What are the potential future directions for research in this field?", "What are the practical implications of these advancements?", "How can these technologies be applied to real-world problems?", "What are the limitations of these approaches?"]}
1f958bf1-d130-4c3d-acc4-e5a5cf3e8c1d	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.005242745,0.049865305,-0.0048621697,0.027034609,0.009483417,0.08039167,0.01807811,0.07135029,0.042472314,-0.03804529,-0.03171781,0.014996025,0.0024850143,0.005846069,-0.04225381,0.010257797,0.01187615,0.053845454,0.03449398,-0.023354625,0.023515519,-0.003551491,0.000117467425,-0.04918729,0.00040600944,-0.0017442557,-0.00026805923,-0.03712021,0.022988612,0.004787505,0.012075678,-0.044858437,0.026985034,-0.0051935273,0.014999204,0.008165556,0.0041637486,0.005683797,0.02290747,-0.025328932,0.014391318,0.028856274,-0.08097728,-0.00029123278,0.013575374,-0.055871464,-0.078091174,-0.048310805,0.023143867,-0.028275825,-0.019965285,0.013540841,-0.024571514,-0.012225538,0.010938233,-0.035478853,0.015587729,-0.044542503,0.024296448,-0.11563935,-0.017206892,0.017759765,-0.038415994,-0.008168713,0.03714003,-0.057980686,-0.0129049625,0.010828423,0.018388538,0.114080064,-0.028602898,0.040535867,-0.018708259,-0.046491943,0.09466652,0.029874839,-0.033016276,-0.0063693644,-0.060339957,-0.027125716,-0.040748727,0.049250282,-0.054702424,-0.047933318,0.07214049,-0.015017654,-0.02404866,-0.0321734,0.07992864,-0.0401072,-0.012257772,-0.026039282,0.011544634,0.05839854,-0.01414963,-0.08206703,0.0016648468,-0.039795496,-0.007015699,-0.068542704,-0.025825795,0.018245082,0.06768935,0.08926311,0.013209402,-0.026320482,-0.023100685,-0.037662175,0.011356731,0.020071981,0.0077269613,0.028362177,-0.03857686,-0.00027058384,-0.040358193,-0.020281542,-0.033327397,0.013925691,0.0037440613,-0.0043755863,0.017921293,-0.017250463,-0.006218506,0.039801195,0.06085735,0.05644298,0.009482444,0.0055257212,-0.023026288,0.0021429802,-0.008135495,-0.009989553,0.015492536,0.023555215,0.0013390637,0.033930205,0.08490833,-0.0639952,0.0027601866,0.010839688,0.04631099,-0.012832879,-0.02113362,0.016618544,-0.03050423,0.009946397,-0.033102468,0.021021437,-0.041375995,0.07749485,0.0063417368,0.016016165,0.042095132,0.012168975,0.011786324,0.007251038,0.018463029,-0.054412067,0.004670152,-0.0026328634,-0.031213952,-0.062113702,-0.04241137,0.046734966,0.048309483,0.14548339,0.019705357,-0.01539009,0.048595753,0.030053362,-0.050434858,0.021471178,-0.013369382,-0.030205913,0.013773058,-0.021085918,0.010362094,0.0005271155,-0.030794691,-0.004738739,0.027044112,0.014178375,-0.010883582,0.04807747,-0.03520469,0.027031723,0.013143916,0.0550706,-0.037117932,-0.023458216,0.020190256,-0.027968923,-0.023711527,0.009451985,0.030603614,0.023117948,0.020643955,-0.08192124,-0.03550982,-0.027034428,-0.036620155,0.009336884,0.029643854,-0.006035784,0.024572922,-0.03524638,-0.024311272,-0.035314262,-0.019590748,-0.01930329,-0.017692436,-0.009211837,-0.023956949,0.05820729,0.0140755735,0.0022175023,-0.01593614,0.026375905,0.020065647,-0.0382155,-0.04222972,-0.01718872,-0.032485705,0.014368353,-0.032670997,0.013106169,-0.024360016,0.037061777,-0.0073346663,-0.017707672,0.052871007,0.0018916028,-0.05201705,0.037209343,-0.016306875,0.06484985,-0.002089776,0.008979515,0.008169714,0.053012885,0.03623797,0.053899582,-0.008353248,0.06107245,0.004389358,0.005927239,0.05082738,0.054929487,-0.0098842,-0.00014324723,-0.013160792,0.056546137,0.056260623,0.0053682844,0.006193125,-0.038262423,0.02145616,0.03576005,0.022730414,-0.0057670223,-0.012049197,-0.019635512,0.03945318,-0.006939758,0.015051438,-0.013664721,0.01811091,0.04346096,0.0043789097,0.0069479416,0.021772658,0.036923036,0.013469266,-0.031150717,-0.052502334,0.0015047358,-0.0035376376,0.027436174,0.04791677,-0.014118342,-0.01817276,-0.021278284,0.060082544,-0.015170366,0.017205581,-0.030517086,0.017526802,-0.027318526,-0.0781022,-0.0081227645,0.0002477092,-0.048736606,0.021358296,-0.0076044975,0.011465178,0.0015883354,0.0128400605,-0.00909944,-0.02557105,-0.0144407,0.08676496,-0.015356421,0.0014742439,0.010821073,0.00463855,0.018856674,-0.048880547,-0.004155814,0.08269302,-0.015939124,0.0018198551,-0.038663346,-0.01840797,0.0049498254,0.03181547,0.08197991,0.04934917,-0.03473674,0.00094242685,0.028964572,0.005983215,0.0042191083,-0.014356477,0.07536429,0.03933606,-0.024067618,-0.0028665247,-0.05167081,-0.093888536,0.049425654,-0.07920541,0.030179705,0.020478895,0.041549303,0.008173598,-0.002065635,0.035404317,0.022030119,0.012152674,0.037676644,-0.01900011,-0.06663737,0.000446976,-0.0011960552,0.015243607,-0.0038633947,0.0035137373,0.03183738,0.022175606,-0.010653058,0.000370137,0.039463885,0.05216154,0.016282054,-6.5437953e-06,-0.0055170017,-0.014449456,-0.008315494,0.0002868736,0.0018601946,0.076673985,-0.046640147,0.092247084,-0.0011803729,0.007980565,-0.024351822,-0.085428804,-0.041006748,0.056156512,-0.015863664,-0.029820548,-0.04029871,0.07331067,-0.04708903,-0.033478003,-0.0057093105,0.014645368,0.031411424,-0.038113102,-0.01509257,-0.016589327,0.015936442,-0.045698915,-0.026232569,-0.04416724,0.06801915,0.017488476,0.046251636,0.009979532,0.029945277,-0.035188038,-0.0030489622,0.020661723,-0.0017926752,-0.022117186,-0.0070263552,0.032938626,0.049830314,-0.0012144833,-0.033463117,0.059104156,0.040328626,-0.04662054,0.03235128,0.06086332,0.010473745,-0.0611846,0.019482818,0.06695236,0.025957631,0.03483283,-0.052310072,-0.043664724,0.014472531,0.00072709756,-0.0073156687,-0.016229453,0.025162505,-0.047945887,0.0289528,-0.06448638,0.03801643,0.0025030617,0.051532865,0.027701832,-0.022931749,-0.027631048,0.044437755,0.01127018,-0.033769283,0.0054151625,-0.10185166,-0.012546752,0.049863245,0.025980998,0.017464854,-0.06875603,0.061862167,-0.11749031,0.012967467,0.06560455,-0.025327627,-0.018358268,-0.015626105,0.012084522,-0.03322417,-0.013369023,0.044464994,-0.004519605,-0.015850827,0.017162265,0.02027176,-0.0070120203,-0.025309367,-0.00045840946,-0.023124618,-0.041590568,0.005716504,-0.057693932,0.023269847,-0.016938448,0.0452447,-0.021696366,-0.0060673635,0.023511346,0.0032288767,-0.027140783,-0.031671774,-0.015915947,-0.008354229,0.00062635733,0.006787012,-0.056382418,0.025541393,-0.009297768,-0.005408745,-0.036615368,0.066314235,0.062221743,-0.032245457,-0.048180792,-0.04499389,-0.0041711065,0.06124813,0.01440445,0.007875467,-0.02927959,-0.021952393,-0.055901047,0.0033745316,0.043367345,0.062176462,0.012069837,0.042879514,0.042572603,-0.035402846,-0.013230254,-0.014748229,0.023437995,-0.035321027,-0.027448216,0.0075106733,0.04736775,0.004448795,-0.018545028,0.0019865658,0.026033692,-0.008170983,0.027336773,0.051988095,0.028303457,-0.0032848103,-0.004155408,-0.017942669,-0.038165025,0.035282478,-0.03441674,-0.017691927,0.018576909,0.048872523,-0.016035756,-0.021755675,-0.0015711832,-0.011292211,-0.022790385,-0.008746533,0.049440563,0.0351293,0.033130977,-0.039463453,0.057110332,-0.03678365,-0.039012786,0.015294931,-0.047886558,-0.033332486,-0.010822047,-0.067322426,-0.0058471374,0.0028095325,-0.014481035,0.01585464,-0.0100417305,-0.038897697,0.08662013,-0.072668195,0.015736269,-0.01388287,0.041023195,0.119993694,0.012145994,0.0030932908,-0.025386497,-0.015480833,-0.025813581,0.021103881,0.069319285,0.015358441,-0.017847348,0.04899911,0.00041128212,0.04101136,-0.014442106,-0.05143962,-0.033332445,-0.026695913,-0.023332106,0.016833564,0.0027851097,0.008508097,0.016856626,0.0047842623,0.0151055055,0.0378623,-0.012068556,0.041805964,0.037958726,-0.026440028,0.012503737,-0.037314475,0.05811691,0.046421364,0.028260134,-0.015046856,0.0016405596,0.025842043,0.0077095325,-0.017655123,0.0149377715,-0.029016778,-0.011011202,-0.024289284,0.042151798,0.021829046,-0.040799208,-0.02028401,0.015567603,0.03371166,0.03374249,0.0021922363,-0.020857364,-0.017788172,-0.019690093,-0.03283068,0.0016977635,0.0091207065,-0.009075738,-0.005859901,-0.037801072,0.07063443,-0.09415069,-0.029585097,-0.027158292,0.05208278,-0.0042578303,0.004040591,0.0021822134,-0.025863668,-0.024857761,-0.034084678,0.06376334,0.03118958,-0.021016112,0.0154304635,0.007790707,-0.01574774,-0.020030137,0.00026688026,-0.02644903,-0.0007944083,-0.014420216,-0.025032828,0.10029913,-0.05167853,0.027978396,0.04079595,-0.032641347,-0.07872194,-0.0057534515,0.032037817,0.024081916,-0.014316149,0.016426764,0.02512509,0.016422743,-0.06449833,-0.028399093,-0.08610126,0.042276412,-0.026869588,-0.01260018,0.009701452,-0.013662524,-0.06661099,0.016670046,-0.02145582,-0.024942786,-0.0043276297,0.009299979,0.088187546,0.04585829,0.05184043,-0.056375653,-0.104519404,-0.026495887,-0.048087753,-0.012603831,-0.05489423,0.057502404,-0.019974263,0.0008664039,0.018348927,-0.015249572,-0.022103343,-0.015495415,0.015739283,-0.01530625,-0.0077565555,-0.0093002,-0.011047553,0.037559506,0.0101370355,-0.0082170395,0.053792924,0.059568767,-0.057590682,-0.05829802,-0.022814425,-0.024073116,0.0017643082,-0.006062721,0.026528746,0.010250746,-0.012663237,-0.0023747128,0.05857507,0.011959657,-0.029402087,-0.031329583,0.037794616,-0.01421983,0.043620266,0.0043684053,-0.042766582,-0.10229023,0.0016447221,-0.026345316,-0.055301152,0.047969215,0.019061254,0.010861407,-0.024375672,-0.024124254,0.007419944,0.046817873,0.049914777,0.009579726,0.014215201,0.019861305,-0.0053728037,-0.07487814,-0.0072020176,-0.047503017,0.038166102,-0.030142881,0.025884982,0.0432757,0.02965195,-0.0061922525,0.029098038,0.009454485,-0.008350996,0.027882064,0.03824612,-0.061761677,0.044015363,-0.015637271,0.018129338,0.052998852,-0.015237085,0.022207523,0.00831509,0.02059201,0.011879406,0.0036199067,-0.011251142,-0.031322625,-0.10215941,0.01152965,0.011445463,0.017385347,0.015938347,0.014695814,-0.03906927,-0.025440983,-0.014118794,-0.0013768806,0.071771674,0.027041951,0.0077573922,-0.053451106,0.0018880855,-0.02831497,0.0436869,0.023833878,-0.014222055,0.007639455,-0.024389729]	Keywords: transformers, neural networks, attention mechanisms, machine learning, deep learning, wave net, synthesizer, sparse sinkhorn attention, transformer dissection, machine translation, object detection, time series forecasting\nKey Objects: Synthesizer: Rethinking Self-Attention in Transformer Models, Sparse Sinkhorn Attention: A method for improving efficiency., Transformer Dissection: Provides unified understanding of attention., WaveNet: Generative Model for Raw Audio.\nRefers to Images: None\nHypothetical Questions:\n- What is the purpose of the papers listed?\n- What are the key innovations described in these papers?\n- How do these references contribute to the understanding of Transformer architecture?\n- What are common themes explored across these publications?\n- What problem are these papers trying to address?\n---\nSummary:\nThis is a list of references related to Transformers, a type of neural network architecture. The references cover various aspects including modifications, improvements, and understandings of the original Transformer model. The list includes papers on Synthesizer, Sparse Sinkhorn Attention, Transformer Dissection, WaveNet, and more.\nOriginal Text:\n- [131] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Zheeng. 2020. Synthesizer: Rethinking Self-Attention in Transformer Models. CoRR abs/2005.00743 (2020). arXiv:2005.00743  \n- [132] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse Sinkhorn Attention. In Proceedings of ICML . 9438-9447. http://proceedings.mlr.press/v119/tay20a.html\n- [133] Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Transformer Dissection: An Unified Understanding for Transformer's Attention via the Lens of Kernel. In Proceedings of EMNLP-IJCNLP. Hong Kong, China, 4344-4353. https://doi.org/10.18653/v1/ID9-1443\n- [134] A'aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew W. Senior, and Koray Kavukcukoglu. 2016. WaveNet: A Generative Model for Raw Audio. In Proceedings of ISCA . 125. http://www.isca-speech.org/archive/SSW\\_2016/abstracts/ssw9\\_DS-4\\_van\\_den\\_Oord.html\nContextualized Text:\nThis section of text presents a curated list of references related to Transformer models. It outlines various papers and their contributions to the field. The references are ordered numerically, and each includes the authors, year, title, and publication details. Each reference builds upon or provides deeper insights into the principles and workings of the Transformer architecture.	{"tags": ["references", "transformers", "machine learning"], "doc_id": "1f958bf1-d130-4c3d-acc4-e5a5cf3e8c1d", "summary": "This is a list of references related to Transformers, a type of neural network architecture. The references cover various aspects including modifications, improvements, and understandings of the original Transformer model. The list includes papers on Synthesizer, Sparse Sinkhorn Attention, Transformer Dissection, WaveNet, and more.", "doc_type": "text", "entities": ["Synthesizer", "Sparse Sinkhorn Attention", "Transformer Dissection", "WaveNet"], "keywords": ["transformers", "neural networks", "attention mechanisms", "machine learning", "deep learning", "wave net", "synthesizer", "sparse sinkhorn attention", "transformer dissection", "machine translation", "object detection", "time series forecasting"], "key_objects": ["Synthesizer: Rethinking Self-Attention in Transformer Models", "Sparse Sinkhorn Attention: A method for improving efficiency.", "Transformer Dissection: Provides unified understanding of attention.", "WaveNet: Generative Model for Raw Audio."], "contextual_text": "This section of text presents a curated list of references related to Transformer models. It outlines various papers and their contributions to the field. The references are ordered numerically, and each includes the authors, year, title, and publication details. Each reference builds upon or provides deeper insights into the principles and workings of the Transformer architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the purpose of the papers listed?", "What are the key innovations described in these papers?", "How do these references contribute to the understanding of Transformer architecture?", "What are common themes explored across these publications?", "What problem are these papers trying to address?"]}
b92d8f6d-e391-4be2-975f-32b5fd320619	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.001475001,0.038665112,0.008828963,0.044461366,0.008713242,0.06223693,0.013428538,0.055869,0.062366642,-0.02176194,-0.005987934,0.012149469,0.012907809,-0.011219959,-0.032362606,0.050657436,-0.013963823,0.062172286,0.04702087,-0.023145366,0.049833722,0.009176892,-0.0052799326,-0.03623937,-0.004173641,0.010364739,0.020032816,-0.03392954,0.015977008,0.02584552,0.03911734,-0.052505147,0.026082318,-0.015243265,0.026001515,0.0063953595,-0.0024098794,0.017041618,0.040454406,-0.02349017,-0.004526019,0.04333953,-0.082566716,0.025364215,-0.0054634786,-0.07102912,-0.07207962,-0.035306085,0.014308627,-0.013125613,-0.018861488,0.008915639,-0.03686701,-0.043057635,0.02093106,-0.013916223,-0.008292747,-0.0504682,0.037180547,-0.09704268,-0.01629183,0.036583655,-0.03340798,0.0017103903,0.028916225,-0.060656782,-0.025779225,-0.023354186,0.020765971,0.11935926,-0.02089461,0.0025272449,-0.027616492,-0.07083297,0.12706311,0.023902524,0.010789312,-0.011752164,-0.06810103,-0.010052966,-0.006594768,0.07729573,-0.06887282,-0.06097764,0.07352828,-0.008336188,-0.017366555,-0.0025799864,0.079616666,-0.008580534,0.02577508,-0.018660098,-0.015450763,0.07308795,-0.04123549,-0.07948758,0.00674345,-0.03936854,-0.019408299,-0.06623197,-0.0139462445,-0.014659962,0.070577316,0.09597059,0.024680072,-0.040334743,-0.020402467,-0.052766867,0.005700504,-0.0010952132,0.009188557,0.00040246666,-0.028482787,0.011994621,-0.013245407,0.0037252775,-0.04323812,0.0032154247,0.002068689,-0.003260724,-0.012310671,-0.026857067,0.007968022,0.023406075,0.063292466,0.046530586,0.015336506,-0.0028747504,-0.04469987,0.010458023,0.008554563,0.006476586,-0.022477645,0.024016568,0.011484009,0.0041689062,0.06345199,-0.044290222,0.0053672865,0.009384178,0.025225867,-0.02965512,-0.032948814,0.030366415,-0.027684718,-0.0038729415,-0.032063767,-0.0013689616,-0.012383299,0.046247803,-0.013616509,-0.007667418,0.029555546,-0.009226756,0.0021757726,0.0005747217,0.005963521,-0.047142133,0.009165597,0.010690197,-0.048650622,-0.0500628,-0.028621789,0.04218286,0.038120117,0.14831042,0.022371728,-0.0036821489,0.036686894,0.035280876,-0.01810653,0.019030936,-0.016020581,-0.045281913,0.014182988,-0.013517898,0.017896406,-0.028516302,-0.03631659,-0.004112543,0.011864519,0.026043776,0.000308242,0.04732553,-0.044639874,0.028684787,0.013232626,0.08492359,-0.01715736,-0.030020306,0.012669945,-0.017181609,-0.006070647,0.0024463302,0.024102144,0.02011193,0.041777354,-0.11375779,-0.056750283,-0.034016605,-0.023406504,0.015004646,-0.0015294374,-0.011405855,0.014538625,-0.029307611,-0.013462246,-0.03941777,-0.03463897,-0.0018792163,-0.0023043638,-0.015812708,0.006592652,0.050659325,0.034631874,-0.032571293,-0.028105896,0.017306784,0.020109989,-0.009486789,-0.058653675,0.004420946,-0.0044247545,0.03560268,-0.02964672,0.010640434,-0.03263023,0.024351135,-0.016070923,-0.023975011,0.027000364,0.0025708224,-0.009099382,0.016548645,-0.0066848155,0.054016832,0.0032836264,-0.020643378,-0.012285601,0.06641897,0.029009983,0.094272934,0.027437992,0.06184928,0.01960219,-0.01983107,0.036532715,0.048817497,-0.047125783,0.006818997,0.014061462,0.048985563,0.03775956,-0.007756131,0.023389159,-0.044815455,0.022475507,0.017494934,0.023929276,-0.0020992197,0.009297542,-0.018925156,0.019467901,-0.0050158105,0.012111367,0.035662375,0.005111488,0.039302032,0.011396792,0.010561805,0.03353396,0.044054195,-0.004654499,-0.0255943,-0.083542384,0.011071142,0.02263381,0.002123322,0.05105888,0.0014455176,-0.009896996,-0.036456145,0.040498096,-0.0071118823,0.0115592815,-0.030192679,-0.00045859796,-0.023550231,-0.06870182,-0.013535021,-0.0077975444,-0.048306186,-0.005727517,-0.027699426,0.043288983,0.0012430797,0.0071690967,-0.028304609,-0.034310617,0.009753014,0.08003411,-0.010137572,0.037164673,-0.0071966434,0.007869604,0.024694312,-0.062124964,0.034707624,0.054280363,0.010515718,0.014617499,-0.021659495,-0.013825025,-0.040965885,0.04096259,0.0992327,0.07015795,-0.022594256,-0.013458206,0.0029246795,0.043321203,-0.0104138795,0.011077522,0.0378649,0.03918373,-0.0032006034,0.0075931647,-0.0458809,-0.055542096,0.057511568,-0.082680024,0.049422584,0.0318439,0.0027790533,-0.008423695,-0.013389866,0.037369814,0.037598714,-0.014424295,0.02734187,-0.024175651,-0.06406548,0.032220915,-0.023121445,-0.0059694755,0.025428485,-0.0047918167,0.004243586,0.03560597,0.0125444075,-0.0015421701,0.022974133,0.036679603,0.026182463,-0.015876643,-0.0060611963,-0.00047488557,0.02864816,-0.00029237568,0.025056392,0.049158204,-0.039664917,0.086052604,-0.044295624,0.026344197,-0.015328022,-0.07884067,-0.026452437,0.05978639,-0.008622803,-0.0040613525,-0.028648553,0.045823146,-0.04095755,-0.021325955,0.0114563685,0.010035759,0.017786527,-0.04687626,-0.008260633,-0.029586107,0.027193908,-0.05629847,-0.045601,-0.018395273,0.036125537,0.011834802,0.018263873,0.0008996697,0.025473535,-0.039975002,0.00065603916,0.02750307,0.005243694,-0.017524567,0.010586034,0.020753795,0.039444704,0.0053271675,-0.024824169,0.03857758,0.011811252,-0.04120359,0.030102529,0.06945858,0.006848997,-0.08350632,0.027580466,0.060983047,0.014087383,0.02446674,-0.039905053,-0.012430344,0.032530572,-0.00280175,-0.029892625,-0.0061197774,0.033656426,-0.058046013,0.016456438,-0.0543439,0.06829419,0.010210365,0.039089825,0.04391269,-0.057418544,-0.009896434,0.0459784,-0.007237724,-0.04431346,-0.007052453,-0.09275025,-0.00052892894,0.041564476,0.015860854,0.045248307,-0.07201006,0.08479869,-0.08292581,0.015274806,0.057737194,-0.0430469,0.0055432315,-0.03127473,0.0019758826,-0.02534837,0.02442246,0.03806819,-0.0037317346,0.003139585,0.010722277,0.03548414,0.008992653,-0.026068639,0.0047026314,0.008580942,-0.059194002,-0.027114008,-0.05138544,0.009044919,0.004143893,0.02508234,0.017475862,0.0034719831,-0.0012167617,-0.026492923,-0.037300248,-0.035091873,-0.04260674,-0.0044804215,0.014865407,0.034164086,-0.047574364,0.017004734,-0.0062149954,-0.026248526,-0.02841524,0.064555354,0.045984764,-0.007094224,-0.05134585,-0.049851,0.002925821,0.06303892,-0.026631331,-0.00456134,-0.020546295,-0.016466297,-0.07409319,0.02978892,0.02420425,0.074274525,0.021981828,0.00091771,0.03381234,-0.023395304,0.00070562813,0.00025010074,0.011986456,-0.030370187,-0.028746372,0.020554053,0.05573381,0.012070852,-0.016154494,0.023700876,0.015976325,-0.0036503999,0.015409073,0.05445327,0.02181794,0.0026677025,0.0043931743,0.0083815595,-0.026671771,0.04125414,-0.020603469,0.016077623,0.00966479,0.045145195,-0.0167424,0.0025792841,-0.007291599,-0.014762631,-0.0028508164,-0.0005815199,0.04461508,0.03153172,0.037309453,-0.058150414,0.043425262,-0.0387705,-0.04724327,0.030546907,-0.051901437,-0.0488284,-0.011648216,-0.075438686,-0.017248608,0.012667704,-0.00830528,-0.020542255,0.009427698,-0.037357874,0.08119741,-0.06302206,0.04337649,-0.012190583,0.00977762,0.096393995,0.023954894,0.009985546,-0.0240024,-0.018295579,-0.04655744,0.02002946,0.083752945,0.014401296,0.011088328,0.08083968,-0.006121698,0.03161829,-0.012273006,-0.069272496,-0.02714627,-0.025295064,-0.031431176,0.015852667,-0.006146546,0.027738338,0.0051786182,-0.017291998,0.014317154,0.028586302,-0.0076005342,0.0279242,0.04402115,-0.007466812,0.041432038,-0.025595998,0.039521344,0.04969141,0.013188077,-0.041161608,0.026523175,0.04617059,0.0017068706,-0.01864103,-0.0054625846,0.008369459,-0.013566265,-0.040245768,0.02398626,-0.023520919,-0.012407207,0.013384832,0.01755057,0.043738164,0.029260777,-0.041503,-0.020701686,-0.0031536294,-0.018000865,-0.030726463,0.039760232,-0.0011952836,-0.003068663,0.013358887,-0.048725516,0.08355202,-0.116318606,-0.0036928437,-0.04390652,0.036491938,-0.036154337,-0.017985495,0.018798452,-0.035525624,-0.020235926,-0.010336378,0.052568078,0.053106263,-0.03572909,0.014828565,-0.024433458,-0.0012882417,0.007236928,-0.027603077,-0.010565943,0.0035378612,-0.016500419,-0.009225099,0.09427674,-0.030270725,-0.017695932,0.04582958,-0.014988833,-0.040538628,-0.046263322,0.020601612,-0.00087942334,-0.018488262,-0.0032083672,0.025620623,0.035354204,-0.06905207,-0.0149450535,-0.051733773,0.03185818,-0.026149286,0.0106890965,0.016584614,-0.019252129,-0.06546888,0.0010207721,-0.020518692,-0.045261994,0.01227704,0.028295482,0.05487475,0.03751677,0.05014992,-0.053979233,-0.08690713,-0.0373967,-0.027883915,0.0029265068,-0.051942255,0.0554671,-0.009951556,-0.01970998,-0.013844493,-0.013182006,-0.0184449,0.024058828,0.025774928,-0.009740573,-0.024256574,-0.008071934,0.013928719,0.017335808,-0.0038467203,-0.018173944,0.059641227,0.04957824,-0.06627675,-0.053948928,-0.016547821,-0.0095529,0.0048601665,0.002003643,0.042277545,0.010742154,-0.022109905,0.026591731,0.08440584,0.019062674,-0.007908023,-0.0075488845,0.018601608,-0.026730465,0.042729363,-0.01669417,-0.027664237,-0.1190747,0.018888624,-0.01374202,-0.06564745,0.05634767,0.036063578,0.006468346,-0.045791563,-0.03449912,0.012346368,0.02944404,0.055129714,0.015332962,0.02636735,0.026156068,-0.027815728,-0.07740583,0.002910149,-0.0262813,0.027879557,-0.013413402,0.025879279,0.0035395704,0.0031066036,0.021056436,0.020461487,-0.0061878976,-0.0036808338,0.028840568,0.040173057,-0.02177279,0.07335014,-0.011592163,0.013835536,0.06120017,-0.001475117,0.030485399,-0.006277485,0.004912334,0.0010999045,-0.0040795086,-0.0071904766,-0.03342363,-0.06376226,0.016008034,0.007109381,0.017122408,0.046046004,-0.008099073,-0.03349605,-0.024223644,-0.030393796,-0.030492201,0.055899385,0.011291567,0.023936976,-0.06792663,0.013065698,-0.044244014,0.039612353,0.025768848,-0.018289935,0.012645821,-0.008395468]	Keywords: transformers, attention mechanism, neural networks, machine translation, representation learning, architectural improvements, arxiv, conference proceedings, machine learning, deep learning, attention is all you need, lazyformer, poolingformer, tensor2tensor, fast transformers, clustered attention, representation learning with contrastive predictive coding, deep learning, machine learning, attention, transformers, representation learning, machine translation, neural networks, attention mechanism, architectural improvements, arxiv, conference proceedings, machine learning, deep learning, attention is all you need, lazyformer, poolingformer, tensor2tensor, fast transformers, clustered attention, representation learning with contrastive predictive coding, deep learning, machine learning, attention, transformers, representation learning, machine translation, neural networks, attention mechanism, architectural improvements, arxiv, conference proceedings\nKey Objects: references, architectures, attention, neural networks\nRefers to Images: None\nHypothetical Questions:\n- What are the foundational works related to transformers?\n- What types of architectural improvements are being explored in transformer models?\n- Where can I find more information about LazyFormer and Poolingformer?\n- What's the general purpose of this list of references?\n---\nSummary:\nThis is a list of references related to Transformers, likely from a survey or academic paper. The references span a wide range of topics including representation learning, neural machine translation, attention mechanisms, and various architectural improvements. They cover foundational works like 'Attention is All You Need' and more recent advancements like LazyFormer and Poolingformer. The list includes arXiv preprints and published conference proceedings, showcasing the active research area of Transformer models.\nOriginal Text:\n- [135] A'aron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018). arXiv:1807.03748\n- [136] Ashish Vaswani, Samy Bengio, Eugene Brevo, Francois Chollet, Aidan Gomez, Stephan Gouws, Llion Jones, ukasz Kaiser, Nal Kalchbrenner, Nikari Parmar, Sayepan Isnao, Shazeer and Jakob Uszkoreit. 2018. Tensor2Tensor for Neural Machine Translation. In Proceedings of AMTA . 193-199. https://www.acweb.org/anthology/W18-1819\n- [137] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of NeurIPS . 5998-6008. https://proceedings.neurips.cc/ paper/2017/hash/3f5eee234547dfe01bd5cf301c485a4a-Assat.html\n- [138] Apoory Vays, Angelas Katharopoulos, and Franois Fleuret. 2020. Fast Transformers with Clustered Attention. arXiv:2007.04825 [cs.LG]\nContextualized Text:\nThis is a list of references related to Transformers, likely from a survey or academic paper. The references span a wide range of topics including representation learning, neural machine translation, and various architectural improvements. They cover foundational works like 'Attention is All You Need' and more recent advancements like LazyFormer and Poolingformer. The list includes arXiv preprints and published conference proceedings, showcasing the active research area of Transformer models.	{"tags": ["transformers", "survey", "references", "deep learning", "machine learning"], "doc_id": "b92d8f6d-e391-4be2-975f-32b5fd320619", "summary": "This is a list of references related to Transformers, likely from a survey or academic paper. The references span a wide range of topics including representation learning, neural machine translation, attention mechanisms, and various architectural improvements. They cover foundational works like 'Attention is All You Need' and more recent advancements like LazyFormer and Poolingformer. The list includes arXiv preprints and published conference proceedings, showcasing the active research area of Transformer models.", "doc_type": "text", "entities": [], "keywords": ["transformers", "attention mechanism", "neural networks", "machine translation", "representation learning", "architectural improvements", "arxiv", "conference proceedings", "machine learning", "deep learning", "attention is all you need", "lazyformer", "poolingformer", "tensor2tensor", "fast transformers", "clustered attention", "representation learning with contrastive predictive coding", "deep learning", "machine learning", "attention", "transformers", "representation learning", "machine translation", "neural networks", "attention mechanism", "architectural improvements", "arxiv", "conference proceedings", "machine learning", "deep learning", "attention is all you need", "lazyformer", "poolingformer", "tensor2tensor", "fast transformers", "clustered attention", "representation learning with contrastive predictive coding", "deep learning", "machine learning", "attention", "transformers", "representation learning", "machine translation", "neural networks", "attention mechanism", "architectural improvements", "arxiv", "conference proceedings"], "key_objects": ["references", "architectures", "attention", "neural networks"], "contextual_text": "This is a list of references related to Transformers, likely from a survey or academic paper. The references span a wide range of topics including representation learning, neural machine translation, and various architectural improvements. They cover foundational works like 'Attention is All You Need' and more recent advancements like LazyFormer and Poolingformer. The list includes arXiv preprints and published conference proceedings, showcasing the active research area of Transformer models.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the foundational works related to transformers?", "What types of architectural improvements are being explored in transformer models?", "Where can I find more information about LazyFormer and Poolingformer?", "What's the general purpose of this list of references?"]}
fe800edb-39bd-4bf5-bd34-c68e87a182d2	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0031637705,0.009200124,-0.03484592,0.032744713,-0.00061894977,0.07937459,0.014732112,0.066561975,0.0480336,-0.032352846,0.00670999,0.014757675,0.004203548,-0.022299219,-0.03849681,0.05295581,-0.005748556,0.0645327,0.041906886,-0.030113334,0.05965141,0.003843525,-0.0032988791,-0.031348232,-0.013400421,0.012473386,-0.002055484,-0.032635704,0.0040909667,0.040469047,0.03257976,-0.04634317,0.04187306,0.004542382,0.026145145,0.011266014,-0.013796827,0.015878994,0.030743092,-0.028298471,-0.012822056,0.04679531,-0.070803784,0.008927799,-0.00907664,-0.07154008,-0.05180423,-0.048501093,0.042130604,-0.022308454,-0.023148691,0.019544693,-0.021827642,-0.024374103,0.00074038084,-0.0137267085,0.0051589543,-0.06004968,0.039217427,-0.097867616,-0.026356488,0.023819886,-0.04222264,-0.013122349,0.035369284,-0.058965202,-0.013264963,-0.017659558,0.012418144,0.12679274,-0.027472978,0.0023083936,-0.024623683,-0.06798914,0.11431561,0.06078349,0.013731766,-0.008078308,-0.093454726,-0.00545767,-0.035598036,0.07032418,-0.06587555,-0.03844855,0.07612673,0.00254675,-0.046258014,-0.028288556,0.06163793,-0.009176977,0.025490193,-0.029786944,-0.011603671,0.057513617,-0.03569146,-0.047336776,0.004284577,-0.0170952,-0.0010463694,-0.064990245,-0.023080524,-0.017617388,0.06481457,0.08719737,0.014357757,-0.0011512393,-0.020479236,-0.043101087,-0.008821563,0.00979899,-0.009765048,0.005046944,-0.027587911,-0.008201406,-0.014296224,-0.021942176,-0.0576854,0.004436868,-0.008172211,0.0054996083,0.0062055723,-0.031534478,0.017460255,-0.0011736586,0.08344242,0.044585176,0.010248477,-0.010109366,-0.05015187,0.01267572,-0.009843701,-0.00031366892,-0.0047652596,0.002047548,0.01873933,0.019575572,0.05201944,-0.048933025,0.019725088,0.009188666,0.02508262,-0.021934856,-0.030240517,0.01889401,-0.015752995,0.0026394725,-0.03434621,-0.010788494,-0.014978766,0.04235667,-0.010523643,-0.00057087233,0.029984491,0.00055235904,0.012537378,0.002337737,0.008322059,-0.057169124,0.016845841,-0.004705084,-0.050938558,-0.057033043,-0.014995575,0.029749252,0.055839106,0.14999689,0.0056636585,-0.0063930666,0.03166483,-0.0009772998,-0.039118066,0.023851484,-0.020977069,-0.019682381,0.007973658,-0.01663172,0.008058487,-0.007906101,-0.03331935,0.0058102277,0.019196184,0.013896283,-0.0039421706,0.050817057,-0.027475594,0.020993588,0.027514331,0.074382566,-0.031073315,-0.03479297,0.042998575,0.0043398044,-0.017636452,-0.005192392,0.011290081,0.017621221,0.029050648,-0.08989612,-0.065745085,-0.029812474,-0.0274101,0.017744908,0.013673765,-0.007633028,0.017923113,-0.017385049,-0.024800383,-0.033174824,-0.031547055,-0.0057936874,-0.015875798,-0.016479034,-0.0069306246,0.062465064,0.04099128,-0.022159562,-0.024448765,0.011071657,0.019041266,0.0040211948,-0.0569291,0.0062000593,-0.0036116,0.030398067,-0.03867074,0.01304548,-0.051770627,0.0302599,0.019221624,-0.028969629,0.031142397,0.006308162,0.0005973299,0.029316142,-0.009809894,0.04204173,-0.016078915,-0.021407943,-0.025450047,0.04206564,0.037489627,0.07898722,0.022715958,0.06968192,0.021752588,-0.01082151,0.032745708,0.03984678,-0.029023131,0.030660553,0.005854328,0.045977753,0.062547676,-0.007529014,0.004843393,-0.036365688,0.022948897,0.02589906,0.04660766,-0.0006336589,0.005841577,-0.015573084,0.03230435,-0.008994045,0.026417142,0.00928115,0.01707806,0.027198004,0.020099996,0.017834788,0.023569454,0.033920284,0.0030232195,-0.024948066,-0.081546575,0.009022099,-0.010317317,0.021968743,0.0398362,-0.015967822,-0.017555121,-0.042577196,0.046668194,-0.010141928,0.035029896,-0.026428428,-0.004147348,-0.03466928,-0.07326607,-0.01342959,0.01506722,-0.042780135,0.021600911,-0.024841625,0.012294806,0.00036757515,0.00494355,-0.028874513,-0.031788945,0.011955208,0.0785607,-0.035057373,0.0040276097,-0.007185038,0.011546938,0.0333654,-0.07704646,0.017139724,0.040760938,0.019024624,0.005806941,-0.012156926,-0.018265324,-0.04693236,0.028258394,0.099510245,0.07893728,-0.015992677,-0.0069245473,0.010919472,0.0354637,0.0035222785,-0.0024494654,0.035089806,0.039583977,0.017092943,-0.008559689,-0.04208623,-0.07905341,0.061255354,-0.06409997,0.038293675,0.031043041,0.0036393215,-0.013720625,-0.02962094,0.040847454,0.0090100365,0.015620674,0.013592835,-0.012932804,-0.07073245,0.017770205,-0.0044786143,0.0012514272,0.045145266,-0.0041093514,0.03219319,0.037667178,0.011448828,-0.00020470802,0.027998663,0.036951035,0.03635783,-0.018531276,-0.006198479,0.009854417,0.010791229,0.005848921,0.036399152,0.06604616,-0.044959113,0.09394508,-0.007865218,0.012976069,-0.008209235,-0.077484965,-0.035877712,0.08116423,-0.024563486,0.0025540185,-0.03167223,0.051579066,-0.033659723,-0.029142544,0.0042494917,0.006036462,-0.004512056,-0.066250145,-0.019617416,-0.03920927,0.009547703,-0.05391085,-0.042174753,-0.02291373,0.052484054,0.025943456,0.029034747,-0.0008944326,0.046831056,-0.03653731,0.0068582585,0.03227837,-0.010059865,-0.011255646,-0.0048539913,0.058353182,0.0114942845,0.022130543,-0.029494563,0.048702214,0.010411538,-0.054601036,0.013083643,0.059233338,0.028376656,-0.10121065,0.0349859,0.05210971,0.0017169955,0.033611607,-0.061527606,-0.015497058,0.054970194,-0.014954854,-0.03843164,-0.006116765,0.0020883519,-0.06546609,0.014287186,-0.03826685,0.060903315,0.029428897,0.038639322,0.031058447,-0.014529549,-0.031884525,0.047313415,0.001177265,-0.05683349,0.015719881,-0.08074585,-0.00070095336,0.04562382,0.012018441,0.04108973,-0.06721634,0.08815937,-0.087205395,0.00068157795,0.046061568,-0.034470867,0.0056155967,-0.042323697,-0.0021150243,-0.031690445,0.01458831,0.044453554,-0.0059176073,-0.013020043,0.005799375,0.051035844,0.027667828,-0.012177421,-0.00092565396,-0.0019397615,-0.03297502,-0.025690239,-0.0578564,0.0062057623,-0.0039685513,0.04657077,0.01608752,-0.0074277064,-0.014740609,-0.01201126,-0.049694322,-0.043508887,-0.037522454,0.008481204,0.004884604,0.04022398,-0.045618072,0.024467168,-0.007633641,-0.01841078,-0.018644737,0.051368296,0.06136239,0.007867448,-0.05080616,-0.04373909,0.02280779,0.06151898,-0.011919304,-0.01989249,-0.02118713,-0.01236479,-0.05147088,0.019122029,0.022734974,0.07384419,0.008698011,0.0063662697,0.01565385,-0.029750131,0.0062552756,0.0012660284,0.015682042,-0.028948918,-0.04921665,0.019644866,0.047356922,0.02532228,0.00056805165,0.006830555,0.007923397,-0.0100222025,0.036824793,0.034060746,0.015965724,0.008588379,-0.0240207,-0.0040856022,-0.017758125,0.030785985,-0.03485752,0.015159616,0.0333596,0.04016051,-0.012871285,-0.025444265,-0.0201743,-0.014560813,0.016791059,0.012909299,0.031946525,0.063309275,0.020910596,-0.032285903,0.06537374,-0.036976967,-0.031717442,0.02920302,-0.07615266,-0.04394617,-0.006239073,-0.07850642,-0.0005565276,0.007834095,-0.023482842,-0.03562699,0.0037525326,-0.034300566,0.079726495,-0.065605626,0.012446818,-0.012563671,0.0073422333,0.1146731,0.015750526,-0.001800058,-0.0158167,-0.029676141,-0.061303057,0.037710167,0.07503989,-0.0025893215,-0.0063937106,0.07328211,-0.0005610719,0.031597495,0.0057296474,-0.05415911,-0.019786289,-0.019394098,-0.030764267,0.020018855,-0.0154517,-0.00021696177,0.0017248753,-0.00789366,0.01912757,0.014599436,-0.0063206563,0.029872261,0.04406231,-0.010159777,0.027713407,-0.044109374,0.059388526,0.028972037,0.044554513,-0.031619795,0.014881101,0.037041623,0.016550887,0.0012554743,-0.005615536,-0.0012779223,-0.026576992,-0.009676855,0.032950554,0.0063531063,-0.027813923,0.0033371414,0.018396331,0.03926765,0.01772698,-0.044765707,-0.030779619,-0.0042813835,-0.009355229,-0.03100828,0.02173365,0.0041789897,0.002139317,0.007381325,-0.025954098,0.09841875,-0.10374315,-0.033066563,-0.050574534,0.026332747,-0.039580576,-0.016276408,0.02722214,-0.030873239,-0.016879443,-0.029703237,0.07151482,0.03408581,-0.044705734,0.020008238,-0.012308113,-0.0053286855,-0.009881407,0.007065833,-0.0040428056,-0.005344594,-0.019412423,-0.014610683,0.09907874,-0.015242146,0.010834283,0.04787146,-0.014446632,-0.07828571,-0.0335209,0.025969002,0.021835346,-0.020420717,-0.014164808,0.022344667,0.0018290704,-0.05205253,-0.024469838,-0.048778035,0.032756854,-0.01021161,0.010639964,0.020550976,-0.013085512,-0.0705688,0.012708054,0.006195131,-0.06481604,-0.015162729,0.047146775,0.04882124,0.062962264,0.052758463,-0.05279768,-0.08632249,-0.029986529,-0.01869494,-0.016199423,-0.046905633,0.052250054,-0.03309429,-0.015329917,0.014659607,-0.017315919,0.0046700095,0.03380212,0.01864555,-0.0075305183,-0.036190208,-0.016868923,-0.028386816,0.026558004,0.013125637,-0.021290958,0.042053435,0.02853553,-0.06630637,-0.05652862,-0.045626473,-0.018057952,-0.0002510183,-0.010579007,0.023969745,0.02530126,-0.027855365,0.007754751,0.06821486,0.039814528,-0.021293676,0.0048131277,0.04459297,-0.02925488,0.04299413,0.02070945,-0.015378348,-0.10147895,-0.006052968,-0.020072276,-0.059744284,0.032998394,0.034909863,0.0003742945,-0.049217578,-0.030037845,0.008797156,0.023996718,0.05928807,0.031828616,0.0062114187,-0.005171811,-0.017046614,-0.061932668,-0.01082619,-0.043084167,0.0075403526,-0.019689592,0.025072847,0.009481726,0.013948388,0.020342212,0.012387016,-0.0111319795,-0.019839322,0.040438224,0.047276497,-0.025499187,0.05981323,-0.00875965,-0.0034240526,0.05107175,-0.011702353,0.021611067,-0.008458628,0.008381432,0.004692172,0.032222882,-0.00842447,-0.030507138,-0.07528712,0.016265111,0.010919904,0.023986477,0.034916118,-0.004085893,-0.031438433,-0.03753747,-0.039229255,-0.012424149,0.04864144,0.019470422,0.02232785,-0.053237446,0.022525823,-0.0152155515,0.04450258,0.0053907055,-0.005491399,-0.0025449467,-0.018118197]	Keywords: transformers, attention mechanisms, position embeddings, efficient inference, machine translation, document modeling, object detection, arXiv, NLP, deep learning, BERT, attention, linear complexity, self-attention, fast transformers, speech synthesis, time-series forecasting, long sequences, embeddings, encoder, decoder, pre-training, document level, speech enhancement, object detection, clustering, deformable DETR, BERT losses, LazyFormer, Informer, Linterform, Wang-Transformer, Deep Transformer Models, Encoding, Learning Deep Models, Encoder-Decoder, clustering attention\nKey Objects: references, transformer models, NLP research, deep learning advancements, efficient architectures, sequence modeling\nRefers to Images: None\nHypothetical Questions:\n- What are some common approaches to making transformers more efficient?\n- How are transformers being adapted for tasks beyond traditional NLP?\n- What role does arXiv play in the dissemination of transformer research?\n- What are the limitations of the original transformer architecture that researchers are trying to address?\n- How are position embeddings implemented in transformer models?\n---\nSummary:\nThis is a list of references related to transformers, spanning various improvements, variations, and applications. The references cover topics like attention mechanisms, position embeddings, efficient inference, and adaptations for specific tasks such as machine translation, document modeling, and object detection. The format includes author(s), publication year, title, source (e.g., arXiv, proceedings of a conference), and sometimes URL or page numbers. A significant portion of the references utilize arXiv as a pre-print server.\nOriginal Text:\n- [138] Apoory Vays, Angelas Katharopoulos, and Franois Fleuret. 2020. Fast Transformers with Clustered Attention. arXiv:2007.04825 [cs.LG]\n- [139] Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qu Jun, Ali Zakob Grue Simonsen. [n.d.]d. On Position Embeddings in BERT, url = https://openreview.net/forum?id=onvoXA9FxMw, year = 2021. In Proceedings of ICLR .\n- [140] Benyou Wang, Donghao Zhao, Christina Lioma, Quichli Li, Peng Zhang, and Jakob Grue Simonsen. 2020. Encoding word order in complex embeddings. In Proceedings of ICLR . https://openreview.net/forum?id=Hke-WTVtr\n- [141] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning Deep Transformer Models for Machine Translation. In Proceedings of ACL . 1810-1822. https://doi.org/10.18653/v1/p19-1176\n- [142] Sinong Wang, Belinda Z. Li, Madian Khabas, Han Fang, and Hao Ma. 2020. Linterform: Self-Attention with Linear Complexity. arXiv:2006.04768 [cs.LG]\nContextualized Text:\nThis list appears to be extracted from the bibliography of a survey paper on transformer models. It showcases a comprehensive overview of research efforts aimed at improving or adapting these powerful architectures for a variety of tasks. The diversity of the references indicates the rapid evolution of the field and the ongoing pursuit of more efficient and effective transformer-based solutions.	{"tags": ["research", "references", "transformers", "NLP", "deep learning", "AI", "machine learning", "architecture", "sequence models", "long sequences", "fast inference", "encoder-decoder architectures"], "doc_id": "fe800edb-39bd-4bf5-bd34-c68e87a182d2", "summary": "This is a list of references related to transformers, spanning various improvements, variations, and applications. The references cover topics like attention mechanisms, position embeddings, efficient inference, and adaptations for specific tasks such as machine translation, document modeling, and object detection. The format includes author(s), publication year, title, source (e.g., arXiv, proceedings of a conference), and sometimes URL or page numbers. A significant portion of the references utilize arXiv as a pre-print server.", "doc_type": "text", "entities": ["BERT", "BERT Losses", "LazyFormer", "Informer", "Linterform", "Wang-Transformer", "BERT Losses", "Transformer", "Encoder", "Decoder", "Self-Attention", "Document Model", "Speech Synthesis", "Machine Translation", "Object Detection", "Speech Enhancement", "Time-Series Forecasting", "Deep Learning", "NLP", "arXiv", "Position Embeddings", "Attention Mechanisms", "Fast Transformers", "Speech Enhancement Transformer", "Deep Transformer Models", "Wang-Transformer", "LazyFormer", "Informer", "Linterform"], "keywords": ["transformers", "attention mechanisms", "position embeddings", "efficient inference", "machine translation", "document modeling", "object detection", "arXiv", "NLP", "deep learning", "BERT", "attention", "linear complexity", "self-attention", "fast transformers", "speech synthesis", "time-series forecasting", "long sequences", "embeddings", "encoder", "decoder", "pre-training", "document level", "speech enhancement", "object detection", "clustering", "deformable DETR", "BERT losses", "LazyFormer", "Informer", "Linterform", "Wang-Transformer", "Deep Transformer Models", "Encoding", "Learning Deep Models", "Encoder-Decoder", "clustering attention"], "key_objects": ["references", "transformer models", "NLP research", "deep learning advancements", "efficient architectures", "sequence modeling"], "contextual_text": "This list appears to be extracted from the bibliography of a survey paper on transformer models. It showcases a comprehensive overview of research efforts aimed at improving or adapting these powerful architectures for a variety of tasks. The diversity of the references indicates the rapid evolution of the field and the ongoing pursuit of more efficient and effective transformer-based solutions.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some common approaches to making transformers more efficient?", "How are transformers being adapted for tasks beyond traditional NLP?", "What role does arXiv play in the dissemination of transformer research?", "What are the limitations of the original transformer architecture that researchers are trying to address?", "How are position embeddings implemented in transformer models?"]}
b40e0b18-a963-4348-afaf-f3c4f5d67a4c	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.010749572,0.005323637,0.0063042254,0.02642003,0.0030409924,0.056106437,0.0069038873,0.056609843,0.015501265,-0.03918127,0.0018616525,0.048138406,0.016320417,3.8197817e-05,-0.04592127,0.029091837,0.021105489,0.061214175,0.046339512,-0.02762821,0.039261047,-0.004308721,0.013185714,-0.046396013,-0.0014079261,-0.0017631981,0.019549537,-0.030098261,0.0011455116,0.030446189,0.028459208,-0.05631683,0.027169663,-0.004250102,0.012149837,0.011009174,-0.017903307,-0.0052128653,0.0014516294,-0.025915077,-0.02080948,0.035506174,-0.09513584,0.0012078661,0.0027774102,-0.050854392,-0.063096136,-0.045507774,0.02561961,-0.022525141,-0.027598644,0.015793,-0.038696498,-0.010376508,0.009867964,-0.02017973,0.008718499,-0.033939824,0.04142536,-0.10360777,-0.0390075,-0.00037051536,-0.029765544,0.0006505637,0.011204277,-0.052875552,0.015105737,0.0046509486,0.015591658,0.10547209,-0.014130863,0.0040021273,-0.022488927,-0.048308663,0.092010185,0.0488853,-0.030493494,-0.024489993,-0.0842815,-0.011939232,-0.04879723,0.067129664,-0.07710228,-0.03817078,0.06388789,-0.00012279462,-0.045794215,-0.023113856,0.071844436,-0.038576413,0.031707384,-0.008314204,-0.0026486306,0.04731837,-0.038549658,-0.058682613,-0.0137654,-0.032407425,0.011896373,-0.07171495,-0.0033744602,-0.015736552,0.06981603,0.07833311,0.017589275,-0.036867086,-0.027606951,-0.04056086,0.028722562,0.028771896,-0.015579597,-0.002846631,-0.016453283,-0.0069452506,-0.018792622,-0.017847128,-0.056576908,0.015576148,0.013927674,0.011802892,0.01945267,-0.023782568,-0.0076366304,0.031551793,0.07602624,0.05575568,0.031255573,0.014685441,-0.021243542,-0.0022154965,0.005414409,0.0058341757,0.013891126,0.02025436,0.02440306,0.025316395,0.08228207,0.008333962,0.0070832213,0.013798415,0.029699896,-0.02132717,-0.010414548,0.028911587,-0.020095544,0.008047817,-0.033686727,0.012776733,-0.02766098,0.067893334,0.027202634,-0.011814252,0.040643245,-0.0024233083,0.0032625825,0.0110493135,0.030444333,-0.052564416,0.023118716,-0.017260566,-0.035291888,-0.061578862,-0.025294159,0.030639898,0.04903401,0.12859792,0.0061337827,-0.010345283,0.042633884,-0.019661061,-0.047401164,0.012897675,-0.008326896,-0.017170025,0.01119794,-0.0419856,0.02433267,-0.0029413106,-0.03363796,-0.018907826,0.009077057,0.0039105173,-0.022366066,0.052495103,-0.010128631,0.0067948215,0.024817187,0.07888679,-0.027695993,-0.034280367,0.027394155,-0.020969959,-0.025937296,-0.0018986232,-6.452969e-05,0.018795298,0.023112692,-0.111951865,-0.04559383,-0.040968932,-0.024507603,0.005992082,0.024077883,-0.012873131,0.028269911,-0.03030543,-0.007541326,-0.024469048,-0.04256152,-0.021072038,-0.013973815,-0.029782807,-0.02493226,0.077807166,0.020557748,0.00316327,0.0041118027,0.019970788,0.047494743,-0.0052644094,-0.053906206,0.007125851,-0.019500671,0.026063757,-0.025935521,0.03405456,-0.031021208,0.028134532,-0.0011514046,-0.043948427,0.054213118,-0.00038421442,-0.031132014,0.023686204,0.0036766657,0.02720804,0.0043879035,-0.0067635374,-0.0076903235,0.0418664,0.05493417,0.069500625,0.010137138,0.07083064,-0.0005906823,0.0068702917,0.03774891,0.04277803,0.0055367136,0.034117505,-0.007253811,0.051095378,0.062302392,0.026285999,-0.0014773842,-0.064184576,0.03406283,0.034734365,0.02785492,-0.032678913,-0.015098651,-0.00507712,0.054976825,-0.010262048,0.025906503,0.003777298,-0.0021406885,0.024076832,0.0033911034,-0.015138794,0.029915784,0.01756636,0.012024523,-0.031737622,-0.08368656,0.02741981,-0.0059446213,0.0180392,0.04327003,-0.017744165,-0.009974284,-0.029099239,0.0459937,-0.0052939933,0.04523136,-0.025663204,-0.0043644323,-0.024698494,-0.08617007,-0.037225645,0.015164334,-0.04324527,0.035480313,-0.016749958,-0.008051498,0.005101379,0.011257881,-0.013913573,-0.006499066,0.009010654,0.10733827,-0.032149196,-0.0045521013,0.003257003,-0.004134586,0.013695562,-0.059472647,-0.015413061,0.03426522,-0.00029044945,-0.001584135,-0.020916136,0.0025079062,-0.023666942,0.026838664,0.08484839,0.071659535,-0.05487108,-0.005369238,0.0131932,0.013366465,-0.012893161,-0.02493392,0.04892577,0.03736908,-0.011050751,-0.0021957618,-0.021447476,-0.08993521,0.06731767,-0.07895054,0.034160495,0.036997844,0.027702173,0.00094439223,-0.017067844,0.024578433,0.03607037,0.017868644,0.010822033,-0.0242408,-0.083368875,-0.008171688,0.0075108884,-0.005298493,0.038632132,-0.00477915,0.047554195,0.05392723,0.004737598,0.024431277,0.020483837,0.021179928,0.044303026,-0.027122648,-0.0012866431,0.0100198025,-0.009083077,0.0018877542,0.026639093,0.05315478,-0.041094158,0.06540277,-0.0042240396,0.0038452626,0.0037230994,-0.07427334,-0.037595525,0.043127038,-0.0048694653,-0.024211744,-0.026066601,0.05788458,-0.046336178,-0.045114495,0.0032371238,0.038638413,0.030670462,-0.044105288,-0.021920932,-0.02159388,-9.665207e-05,-0.051669378,-0.028401494,-0.021027848,0.046942487,0.009639586,0.04983026,-0.020517245,0.048309356,-0.025884097,0.021471268,0.033845942,0.025546338,-0.028759196,0.0021000574,0.054955754,0.016909912,0.004720354,-0.045145754,0.06438645,0.028475333,-0.046714272,0.031283043,0.04393274,0.02325704,-0.098538116,0.02163274,0.05762678,0.030991253,0.013681015,-0.04190196,-0.06283231,0.033620454,0.021710018,-0.01784454,-0.007929162,0.023306178,-0.047449198,0.01762303,-0.032088593,0.049062226,-0.012285965,0.05706258,0.013161673,-0.02955257,-0.027424524,0.03329072,-0.0042875865,-0.04166551,0.009634132,-0.060750697,-0.020697013,0.044169635,0.05158751,0.018229254,-0.0771272,0.07004274,-0.10702813,0.0036623748,0.02467894,-0.03258062,0.0075208796,-0.045440935,0.014688837,-0.04163436,-0.003053892,0.051612735,-0.013267708,-0.0065412414,0.008007065,0.04468125,0.032922998,-0.020514546,0.010386237,-0.029231923,-0.019231642,-0.003252667,-0.066715725,-0.003232953,-0.020843938,0.0423466,0.009293223,0.0015855823,-0.0033162602,-0.016166275,-0.010019266,-0.045181923,-0.011127772,0.0076133204,0.00025806975,0.034180608,-0.049631145,0.03225587,0.011175502,-0.033909705,-0.020912526,0.05151793,0.060293145,0.0013632055,-0.08213227,-0.028325679,0.005383513,0.06722758,-0.01789897,-0.006568935,-0.041166037,-0.018516086,-0.0407658,0.030998794,0.03462166,0.064650975,0.038399816,0.0024181886,0.049036764,-0.04756424,0.012725381,-0.0041729817,0.007722509,-0.048066974,-0.026155211,0.008790914,0.08438561,0.013848513,-0.015956,-0.0035130973,-0.0029061593,-0.0061479397,0.025515003,0.06439511,0.021222288,0.0131315,-0.011160345,-0.016455594,-0.0545189,0.031664878,-0.028809972,0.004916909,0.047620073,0.064786114,-0.029400256,-0.014863891,0.01051485,-0.0295543,0.011424124,0.0010903215,0.050997883,0.039698895,0.041621186,-0.05420493,0.06939568,-0.015000014,-0.022560762,0.032135937,-0.07709418,-0.042399805,0.011426376,-0.05512388,0.012132656,0.009781265,-0.017912548,-0.029225804,0.020945493,-0.025343439,0.06815527,-0.07315305,-0.006951213,-0.025363393,0.026739972,0.116257384,0.017699547,-0.013911534,-0.03473074,-0.014454353,-0.02605491,0.0482986,0.07481443,0.0155161405,-0.02774491,0.059324272,-0.026021592,0.04378824,-0.023035098,-0.05899197,-0.02108194,-0.027019426,-0.025478305,0.0014127311,-0.004513391,0.022647124,0.013460979,0.014895615,0.01440282,-0.00801155,0.010482764,0.025318746,0.0415575,-0.031152999,0.011443087,-0.016967287,0.039826702,0.018890593,0.030705819,0.0033096233,0.016630458,0.0213463,0.013064822,0.0080974,0.013856188,-0.028990319,-0.018654436,-0.01831586,0.01012202,0.02407411,-0.021231338,-0.02178208,0.008828206,0.033828177,0.03072885,-0.016026678,-0.03599284,-0.047000997,-0.018609047,-0.03936982,-0.03274738,0.03808988,0.014252127,-0.018225953,-0.03475561,0.09228212,-0.091541156,-0.030204467,-0.031993687,0.05034364,-0.016929204,-0.011734358,0.0055860057,-0.031244837,-0.022503639,-0.049612865,0.051873673,0.049954046,-0.027603194,0.029178135,0.011092726,-0.016011631,-0.024409842,0.0011915301,5.0039257e-06,-0.013474892,-0.037081964,-0.018748121,0.082492955,-0.01620424,0.012420296,0.04278693,-0.00834255,-0.0903031,-0.009418372,0.014674499,0.047506385,-0.03411412,0.018319871,0.009650221,0.01766349,-0.027083546,-0.021084685,-0.06733829,0.026895177,-0.023321325,-0.016004058,0.046583157,-0.0053005633,-0.06973885,0.009156515,0.003817335,-0.016204832,-0.00509612,0.019082718,0.06342311,0.065333605,0.049925473,-0.033913065,-0.08514807,0.007930108,-0.03916153,-0.012231716,-0.050733835,0.059311207,0.013008034,0.0022215287,-0.014321459,-0.019554961,-0.010960327,0.0029312798,0.010271945,0.015343638,-0.042475913,-0.011786946,-0.021955656,0.010336037,-0.010218247,-0.009473102,0.032515742,0.049026866,-0.061335493,-0.059262224,-0.03209658,-0.02544048,-0.010014742,0.0150287505,0.009557179,-0.0005706724,-0.0067589935,-0.005300234,0.05817038,0.028955221,-0.007826603,0.0014274872,0.07276782,-0.018686345,0.042615503,0.008307004,-0.013482631,-0.10505011,0.0040597077,-0.022505978,-0.057301536,0.037554573,0.0029626102,-0.0006103924,-0.029334236,-0.03041683,-0.00020156104,0.019403905,0.044473086,0.04060337,0.0024504839,0.012672634,-0.006949256,-0.059373032,-0.025387071,-0.036647454,-0.0062162173,-0.029643435,0.031183824,0.014517706,0.021488639,0.015222607,0.026983947,-0.023720525,0.004092234,0.04599444,0.051284887,-0.032296903,0.05044932,-0.04059174,-0.021398345,0.05246588,-0.009167584,0.03003251,-0.009404814,0.023748364,0.0016895297,0.0031634362,-0.018497292,-0.046367813,-0.10844027,0.015854485,0.020324133,0.017999835,-0.0031256813,0.028208833,-0.04220257,-0.040978927,0.00036431546,-0.0020709631,0.034470346,0.015050117,0.021945082,-0.051834606,0.018910963,-0.046807475,0.038265288,-0.007409077,0.01621999,0.030846952,-0.027879937]	Keywords: Transformer, Attention Mechanism, Efficiency, Long Sequence Modeling, Hierarchical Modeling, Convolution, Recurrent Neural Network, Preprint, arXiv, Document Modeling\nKey Objects: Linterform, Predictive Attention Transformer, R-Transformer, Hi-Transformer\nRefers to Images: None\nHypothetical Questions:\n- What are some common areas of improvement for Transformer models?\n- Why are many of these publications preprints on arXiv?\n- What is the significance of the modifications described in these publications?\n---\nSummary:\nThis is a list of references related to Transformer models and their various modifications and improvements. The references cover a range of topics including efficiency, long sequence modeling, attention mechanisms, and hierarchical document modeling.  Many of these entries are preprints on arXiv, indicating ongoing research and development in this area.  The list showcases the active exploration of Transformer architecture to address limitations and enhance performance.\nOriginal Text:\n- [142] Sinong Wang, Belinda Z. Li, Madian Khabas, Han Fang, and Hao Ma. 2020. Linterform: Self-Attention with Linear Complexity. arXiv:2006.04768 [cs.LG]\n- [143] Yujing Wang, Yaming Yang, Jiang Bang, Maiang Zhang, Jing Bai, Yu Jing, Ce Zhang, and Yunhai Tong. 2021. Predictive Attention Transformer: Improving Transformer with Attention Map Prediction. https://openreview.net/ forum?id=YQVjbPnC9\n- [144] Zhwei Wang, Yao Ma, Zitao Liu, and Jiang Tang. 2019. R-Transformer: Recurrent Neural Network Enhanced Transformer. CoRR abs/1907.05572 (2019). arXiv:1907.05572\n- [145] Chuhan Wu, Fangzhao Wu, Tao Qi, and Fongyeng Huang. 2021. Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling. arXiv:2106.01040 [cs.CL]\n- [146] Felix Wu, Angela Fan, Alexei Vaevski, Yann N. Dauphin, and Michael Auli. 2019. Pay Less Attention with Lightweight and Dynamic Convolutions. In Proceedings of ICLR . https://openreview.net/forum?id=skVhH09TX\nContextualized Text:\nThis section provides a list of research publications focused on Transformer models and their variations. The focus is on improvements and modifications to the original Transformer architecture. Many of the publications are preprints on arXiv, indicating active research and development.	{"tags": ["machine learning", "natural language processing", "deep learning"], "doc_id": "b40e0b18-a963-4348-afaf-f3c4f5d67a4c", "summary": "This is a list of references related to Transformer models and their various modifications and improvements. The references cover a range of topics including efficiency, long sequence modeling, attention mechanisms, and hierarchical document modeling.  Many of these entries are preprints on arXiv, indicating ongoing research and development in this area.  The list showcases the active exploration of Transformer architecture to address limitations and enhance performance.", "doc_type": "text", "entities": ["Sinong Wang", "Belinda Z. Li", "Madian Khabas", "Han Fang", "Hao Ma", "Yujing Wang", "Yaming Yang", "Jiang Bang", "Maiang Zhang", "Jing Bai", "Yu Jing", "Ce Zhang", "Yunhai Tong", "Zhwei Wang", "Yao Ma", "Zitao Liu", "Jiang Tang", "Chuhan Wu", "Fangzhao Wu", "Tao Qi", "Fongyeng Huang", "Felix Wu", "Angela Fan", "Alexei Vaevski", "Yann N. Dauphin", "Michael Auli"], "keywords": ["Transformer", "Attention Mechanism", "Efficiency", "Long Sequence Modeling", "Hierarchical Modeling", "Convolution", "Recurrent Neural Network", "Preprint", "arXiv", "Document Modeling"], "key_objects": ["Linterform", "Predictive Attention Transformer", "R-Transformer", "Hi-Transformer"], "contextual_text": "This section provides a list of research publications focused on Transformer models and their variations. The focus is on improvements and modifications to the original Transformer architecture. Many of the publications are preprints on arXiv, indicating active research and development.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some common areas of improvement for Transformer models?", "Why are many of these publications preprints on arXiv?", "What is the significance of the modifications described in these publications?"]}
7dfb7faa-1102-4f8c-8658-9b812d85dda2	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0030246726,-0.005101673,-0.013180232,-0.0014184344,-0.010382916,0.0678521,0.008842652,0.062481094,0.040927634,-0.042206947,0.002670138,0.011409854,0.018450763,-0.0003878399,-0.049205855,0.05010576,0.016882356,0.039634664,0.033884067,0.004392778,0.046197385,0.03199408,-0.018133493,-0.021041682,-0.0028514457,0.037729133,0.0021226802,-0.030616922,0.021779468,0.024390427,0.0047722496,-0.059278894,0.01590679,-0.007969247,0.025002923,-0.004655923,-0.005465488,0.012659726,0.031550005,-0.024724878,0.021938924,0.03411327,-0.058743518,-0.03019272,0.02109273,-0.06737688,-0.067741014,-0.02910207,0.03290851,-0.03991408,-0.010629693,0.028137641,-0.04709876,-0.037480842,0.0027808435,0.0012634468,-0.001089575,-0.019398034,0.031830896,-0.10840258,-0.054206375,0.042372238,-0.025824731,-0.036753517,0.0039754226,-0.08763842,-0.008325768,0.014142879,0.008754851,0.10291451,-0.008057425,0.03449656,-0.019813297,-0.04520463,0.103920214,0.06015912,-0.016584063,-0.02725588,-0.06808025,-0.0050858757,-0.013218436,0.06705766,-0.0444738,-0.047865167,0.07451989,-0.015217649,-0.06801462,-0.016243286,0.11389266,-0.02433183,0.036861148,-0.028586887,-0.0017758479,0.07523236,-0.02260737,-0.086278014,0.019172918,-0.02736665,-0.0027224852,-0.061174355,-0.009683928,-0.035749834,0.06372379,0.09124552,0.02868561,-0.015573236,-0.048159186,-0.032641426,-0.00060940534,0.018301126,-0.009997715,-0.01188953,0.0045590857,0.0104624415,-0.0135918595,-0.017067287,-0.060509298,0.015319969,-0.011121572,-0.0011264696,0.036430582,-0.011256292,0.006299551,0.019218119,0.051265206,0.049379617,-0.023426402,-0.01752911,-0.05638539,-0.011935607,-0.0103673525,0.014554323,-0.018749936,0.032200623,0.03769023,0.017940665,0.039540246,-0.038682368,-0.010991397,0.0054368456,0.027085677,-0.0083558215,-0.021163518,0.030187726,-0.02692978,0.025626585,-0.069011115,0.01485873,-0.029776247,0.0496168,0.036547374,0.021506896,0.0120498575,0.037838105,0.03353867,0.006004014,0.010532176,-0.08483851,0.011118652,-0.015017815,-0.046624336,-0.043478213,-0.017019084,0.04603967,0.037090763,0.15940903,0.0049573,-0.024665713,0.038115088,-0.0030632864,-0.030998973,0.009154343,-0.028102523,-0.032228716,0.014801636,-0.010324501,0.039175395,0.008246143,-0.042901345,0.012004748,0.029272601,0.008375977,-0.0038640876,0.051614925,0.0061103227,0.03324059,-0.021230716,0.0726996,-0.023368511,-0.04203263,0.008002602,-0.013828605,-0.01122127,-0.0067622093,0.026707783,0.0057905787,0.013142456,-0.06781644,-0.017615736,-0.039030813,-0.011504562,0.009568494,0.0007706197,0.026818631,0.023027655,-0.009002434,-0.038294338,-0.01793548,-0.026100859,-0.017058486,0.0070381695,-0.021215372,-0.024392147,0.03418302,0.029581025,0.011152705,-0.014811626,0.025782933,0.020161616,-0.034554962,-0.06888667,-0.0063028047,-0.023564097,0.019766668,-0.043192472,-0.011604193,-0.020271912,0.028617054,0.021436408,-0.044812143,0.06295012,-0.023280317,-0.027162848,0.007931722,-0.018113606,0.033814386,0.00965169,-0.0070128716,0.024010252,0.058372147,0.029265517,0.06405536,0.022415802,0.074372835,-0.0030256605,-0.004826124,0.026467158,0.034342986,-0.0057749855,0.030732207,0.022124847,0.060668904,0.03359722,0.030106386,0.016761089,-0.024290456,0.04046431,0.022245029,0.039551746,0.029931342,-0.008819584,-0.028762342,0.029827213,-0.0011155631,0.013529295,0.018166967,0.007348393,0.055631593,0.003235384,-0.0019110161,0.022628598,0.04303523,0.006835398,-0.045215096,-0.12257177,0.00057154294,0.028708648,0.01220077,0.032853577,-0.020140676,-0.008973572,-0.035444826,0.018654073,-0.020056315,0.022434363,-0.032671154,-0.02277003,-0.011524078,-0.06527677,-0.027945472,-0.0011660552,-0.04278381,0.029022677,-0.0016372808,-0.012937906,-4.947151e-05,0.015826924,-0.03582082,-0.010345637,0.010691337,0.09939289,-0.01831312,-0.0043623913,-0.010947189,0.013294967,0.03928521,-0.087171786,0.013424984,0.062238332,0.019789245,0.0045321626,-0.02765645,0.023234248,-0.023246862,0.04540913,0.08345675,0.06374252,-0.008711275,-0.0018106684,0.06261574,0.014348943,-0.021774406,0.00705224,0.047783017,0.030186838,0.013842181,-0.010338792,-0.029398631,-0.08477093,0.07254382,-0.06264705,0.042071003,0.037644163,0.0054336726,-0.004552327,-0.03367727,0.05356009,0.03108947,0.01948776,0.022158993,0.020626843,-0.09711103,0.005436281,0.021242777,-0.012530285,0.010005179,-0.0047511593,0.019871764,0.016913733,0.0073861615,0.017119404,0.008541569,0.038771186,0.007048118,0.004070959,-0.015533868,0.03539023,-0.017512197,0.014067359,0.04515953,0.041915026,-0.032767996,0.09800341,-0.040273868,-0.0025844316,-0.0052959346,-0.05203435,-0.045498457,0.077631064,-0.0059163333,-0.018037386,-0.029276904,0.054042496,-0.053388607,-0.024410045,-0.021780927,0.046748616,0.025299584,-0.052231073,-0.05652237,-0.013870375,0.0035790785,-0.036322463,-0.023585085,-0.022696726,0.0531291,0.023699854,0.031945,-0.030081056,0.03790577,0.0048802355,0.05122285,0.017592054,0.0011219882,-0.0018499899,-0.009451697,0.018968744,0.035946395,0.014946087,-0.024603695,0.029529955,0.007628169,-0.04799718,0.03273011,0.05692516,0.023665858,-0.081701025,0.041913934,0.066039644,-0.0278254,0.05533965,-0.04096514,-0.0040484685,0.05363003,-0.01640628,-0.028351896,-0.00048116627,-0.011893488,-0.04139817,0.016939916,-0.02115606,0.038688656,-0.011225184,0.052088153,0.02417613,0.00030060514,-0.011842826,0.033327147,-0.0026197196,-0.041717995,0.020303454,-0.06672149,-0.011757791,0.018132295,0.021066519,0.019318677,-0.039096598,0.06654083,-0.0917384,0.0025099602,0.02561642,-0.01915966,0.0046985117,-0.036702115,-0.01951389,-0.036839187,-0.008767892,0.048413742,0.0111745,-0.008845349,-0.009010903,0.032008994,0.005560672,0.00041524376,-0.0046213684,-0.004625825,-0.03990253,0.013666186,-0.069218665,-0.026339535,0.0041371393,0.072568774,-0.0023796924,-0.023363458,-0.009870326,-0.020470237,-0.033709038,-0.03575919,-0.049601607,0.008112373,-0.02902204,0.035327908,-0.060142133,0.01709991,-0.02606752,-0.013632584,-0.024109567,0.054580722,0.05466584,-0.008017868,-0.052932642,-0.058394425,0.018039115,0.08167247,-0.017595975,0.0027069044,-0.011159932,-0.03502471,-0.020433428,0.009195572,0.01686625,0.05953607,0.03518943,0.019440962,0.03902298,-0.025964752,0.014681588,-0.0019143218,0.0025397793,-0.05310588,-0.020732922,0.0070757396,0.058016267,0.00047600066,0.025157327,0.0067545613,-0.008963967,0.013097132,0.025872149,0.047984347,0.019197108,-0.006162152,0.010467671,-0.012858023,-0.008073829,0.050779775,0.0057490827,-0.012780855,0.00023753961,0.019628612,-0.010708706,-0.029295722,0.00028683172,-0.024038564,0.0063907905,0.0017818377,0.031297915,0.05003485,0.042319767,-0.033073008,0.050783657,0.016944515,-0.036793258,0.039979465,-0.055134047,-0.029214961,-0.0056605362,-0.062456906,-0.018415311,0.02939571,-0.0107502,-0.02671284,0.04063814,-0.045305423,0.107552275,-0.056165356,0.022963112,-0.00039111974,0.032672998,0.08918081,0.00908327,-0.0013328737,-0.012860229,0.002684627,0.00013243439,0.016858729,0.07516197,0.03150129,-0.008520528,0.09149476,0.017798303,0.032744136,0.013784269,-0.010719749,-0.0076975,-0.05458416,-0.03978621,-0.008604383,-0.0059314473,0.029358428,0.025598843,-0.014512066,-0.0035428163,0.02848511,-0.018819496,0.00018294215,0.032021888,-0.02846527,0.008219236,-0.04145487,0.067295514,0.028276034,0.02879216,-0.036528293,0.02367644,0.010345362,-0.007508542,-0.020269968,0.008124621,-0.036304597,-0.002993358,-0.01421504,0.032191887,0.0027583106,-0.0627414,-0.018015329,0.025992703,0.05424632,0.022746766,-0.034424927,-0.023567801,-0.000190932,-0.048182018,-0.014470341,0.012445195,0.019493856,-0.01969124,0.023476204,-0.033677995,0.088898286,-0.07551843,-0.029270103,-0.015133906,0.053785905,-0.05752639,-0.029353969,-0.005459242,-0.026784139,-0.020501504,-0.0075823725,0.0547977,0.064274825,-0.040479198,0.012356073,-0.002988811,-0.00031289636,-0.0030542514,-0.044791777,-0.022565257,-0.001451091,-0.032503653,-0.020814251,0.09795822,-0.034040876,0.0037602612,0.047486164,0.00080517825,-0.070097536,-0.0022749228,0.0046122554,0.0153783895,-0.02987365,0.027906174,0.051349856,0.03736423,-0.03937696,-0.009612986,-0.066469505,0.028248109,-0.025605833,-0.034522586,0.02045846,0.03236347,-0.06835607,0.044139065,-0.022722403,-0.012198674,-0.020734789,0.031309523,0.057587642,0.01824771,0.06805773,-0.0658924,-0.089314714,-0.014787722,-0.011209578,0.0058874493,-0.035200458,0.028154464,-0.0103945555,-0.030722978,0.038270574,-0.032454267,-0.019925974,0.013635071,0.033378344,-0.002530702,-0.020813575,0.0050815325,0.0018054073,0.029231131,-0.017091606,0.009117873,0.04203831,0.041043214,-0.063997686,-0.038634405,-0.021655671,0.0014932682,-0.0029235473,-0.0010092101,0.021432001,0.030346582,-0.02459933,0.016680442,0.06391746,0.0067862137,-0.0068505476,-0.019578021,0.057481233,-0.0063014044,0.035851818,0.0026291597,-0.027800327,-0.08957427,-0.015462478,-0.016590063,-0.066903524,0.020093074,0.007837401,0.008609588,-0.031958606,-0.03451911,-0.027428385,0.03858422,0.035656903,0.015228704,0.013707576,0.0005306304,-0.027461601,-0.019808732,-0.034300108,-0.025882568,0.035533376,-0.055113226,0.0558436,0.012145742,0.028792629,0.012378633,0.016162924,-0.02711216,0.0028251116,0.034804955,0.022402914,-0.019785987,0.046750307,-0.023185732,0.023206482,0.05654718,-0.0023519169,0.03742889,-0.03222357,0.009117891,0.002912112,0.016710725,-0.014342436,-0.0180695,-0.07972489,-0.0025074717,0.0136371255,0.027839297,0.014397676,-0.0031757597,-0.047594063,-0.06284959,-0.051059883,0.009193475,0.071256064,0.030786969,0.048288103,-0.025558356,0.010606217,-0.033543322,0.043766893,0.020642737,-0.015785595,-0.0051546865,-0.0347553]	Keywords: transformers, references, memory augmented, graph neural networks, early exiting, BERT, inference\nKey Objects: \nRefers to Images: None\nHypothetical Questions:\n- What is the main focus of this document?\n- What are some of the topics covered in these references?\n- What is Dynamic Early Exiting for BERT inference?\n- Can you describe the purpose of these references in context?\n---\nSummary:\nThis is a list of references (147-150) from a survey on Transformers. The references cover topics like memory-augmented transformers, lightweight transformers with long-short range attention, a comprehensive survey on graph neural networks, and dynamic early exiting for BERT inference.\nOriginal Text:\n- [147] Qingyang Wu, Zhenzhong Lian, Jing Gu, and Zhou Yu. 2020. Memer: The Memory-Augmented Transformer. arXiv:2010.06891 [cs.CL]\n- [148] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song Han. 2020. Lite Transformer with Long-Short Range Attention. In Proceedings of ICLR . https://openreview.net/forum?id=ByMEPIHKPH\n- [149] Zonghan Wu, Shiran Fuji, Wenchen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. 2021. A Comprehensive Survey on Graph Neural Networks. IEEE Trans. Neural Networks Learn. Syst. 32, 1 (2021), 4-24. https://doi.org/10. 1109/TNNLS.2020.2978386\n- [150] Ji Xin, Raphael Tang, Jaegun Lee, Yaoi Yang, and Jimmy Lin. 2020. DeeBERT: Dynamic Early Exting for Accelerating BERT Inference. In Proceedings of ACL . 2246-2251. https://doi.org/10.18653/v1/2020.acl-main.204\nContextualized Text:\nThis is a list of references (147-150) from a survey on Transformers. The references cover topics like memory-augmented transformers, lightweight transformers with long-short range attention, a comprehensive survey on graph neural networks, and dynamic early exiting for BERT inference.	{"tags": ["transformers", "references"], "doc_id": "7dfb7faa-1102-4f8c-8658-9b812d85dda2", "summary": "This is a list of references (147-150) from a survey on Transformers. The references cover topics like memory-augmented transformers, lightweight transformers with long-short range attention, a comprehensive survey on graph neural networks, and dynamic early exiting for BERT inference.", "doc_type": "text", "entities": [], "keywords": ["transformers", "references", "memory augmented", "graph neural networks", "early exiting", "BERT", "inference"], "key_objects": [], "contextual_text": "This is a list of references (147-150) from a survey on Transformers. The references cover topics like memory-augmented transformers, lightweight transformers with long-short range attention, a comprehensive survey on graph neural networks, and dynamic early exiting for BERT inference.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the main focus of this document?", "What are some of the topics covered in these references?", "What is Dynamic Early Exiting for BERT inference?", "Can you describe the purpose of these references in context?"]}
235f8460-d034-4388-97b2-62a0d252b6d7	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.008756057,0.021053368,-0.010571575,0.0300863,0.017091101,0.07317122,0.015132646,0.054423254,0.05112976,-0.0259593,-0.029188117,0.036598206,0.017806325,-0.02770789,-0.03307786,0.059629366,0.009491344,0.058541372,0.039073873,-0.024305752,0.03244293,0.025293846,0.0029894619,-0.03072318,-0.021050895,0.027612893,0.007016634,-0.022463448,0.043966994,0.032073915,0.014268543,-0.06977073,0.017886885,0.00109047,0.0028468072,0.033553243,0.011689455,0.015125049,0.015473682,0.0042408677,-0.0043689837,0.052548997,-0.0632373,-0.0051873727,-0.007171226,-0.059730846,-0.06388386,-0.048790947,0.042575307,-0.01620809,-0.037307955,0.02752264,-0.026269134,-0.035752654,0.0037569965,-0.018143946,-0.020343462,-0.04749583,0.036907047,-0.107971095,-0.04082103,0.044348765,-0.039525334,-0.025075123,0.026402738,-0.07118131,-0.025655285,0.017466303,0.0027547446,0.10195423,-0.01688433,0.029675256,-0.022317335,-0.021965258,0.09145423,0.041532185,-0.020666512,-0.015989421,-0.06847213,-0.02498433,-0.027329262,0.051669534,-0.039648507,-0.023957452,0.07130458,-0.006566973,-0.042642422,-0.042236775,0.06946429,-0.023597332,0.010034546,-0.021793628,-0.008329453,0.069121465,-0.02122308,-0.08487944,0.011734061,-0.05329229,0.011316724,-0.04748966,-0.011834202,0.0011767633,0.05683603,0.07620908,0.02443894,-0.015121711,-0.051944956,-0.045136478,-0.00204645,0.0003806827,-0.01686961,-0.0059141237,-0.00012293409,-0.0042201933,-0.01781889,-0.024714882,-0.04221013,-0.0035151131,0.0025880944,0.006199448,-0.012186087,-0.00635534,-0.0013486196,0.010260684,0.082807615,0.07200146,-0.0034542843,0.009026714,-0.038331643,0.0041525257,0.010904178,-0.009193661,-0.017922735,0.04442194,0.014641191,0.0017157261,0.080928005,-0.043334242,0.016110124,-0.0046622013,0.04106832,-0.005368259,-0.01389998,0.036049888,-0.04204236,-0.021619001,-0.049743354,0.005155858,-0.014994665,0.019332314,0.030176753,0.034307003,0.031856563,0.045243226,0.004276072,0.0033816658,-0.0014024556,-0.09397749,0.015773077,0.032801215,-0.04682072,-0.050896473,-0.0209915,0.020025693,0.030103484,0.114877425,0.02489684,0.006120039,0.04258662,-0.0070647234,-0.046689954,0.03225485,-0.022467023,-0.0068434603,0.026298542,-0.028099393,0.01477723,-0.011956099,-0.034072015,0.0020512713,0.013383491,-0.0053350595,-0.017958058,0.047108874,-0.018281247,0.009844061,0.005080891,0.07442463,-0.029452125,-0.017035393,0.05570662,-0.032186374,-0.008223737,-0.0014603714,0.023015557,0.007587225,0.0151512865,-0.07782368,-0.054513346,-0.005094334,-0.031735033,0.002863927,0.046036206,-0.0012112537,0.033236247,-0.023133997,-0.031976398,-0.027707709,-0.018398743,-0.005420754,0.0013657953,-0.027473528,0.0015594136,0.06586708,0.02457464,-0.02459813,-0.01787644,0.025274934,0.02222805,-0.010196846,-0.0704983,0.027876655,-0.01130146,0.013284045,-0.032110922,-0.010747779,-0.03439472,0.028537946,-0.0030365482,-0.028040653,0.07556269,0.011003718,-0.03196844,0.00803693,-0.020729335,0.043490965,-0.020229897,-0.0042791385,0.017702261,0.05010643,0.033439152,0.04698238,0.02338516,0.05617934,-0.006232388,-0.01321212,0.047063712,0.030565372,-0.0047649965,0.048281927,0.0062987003,0.05123173,0.04661923,0.025657395,0.0005542654,-0.031981822,0.017516378,0.026483847,0.050639424,0.027092326,-0.0021488622,-0.012056951,0.015863061,0.00048851664,0.012946386,0.011088595,0.015654594,0.04249409,0.019261206,-0.005679418,0.024846515,0.0673166,0.0009004924,-0.025123889,-0.09422521,0.015477224,-0.012933329,0.013877086,0.026572695,-0.014582996,-0.007149691,-0.031060724,0.045276996,-0.014726471,0.023926118,-0.02495008,-0.026851876,-0.035748947,-0.09994626,-0.009507649,-0.021391975,-0.04784727,0.0229236,-0.011687445,0.0034296399,0.005962085,0.024921946,-0.03595852,-0.018611318,-0.011359315,0.07702495,-0.014610436,0.015620287,-0.002554972,-0.0021373082,0.032249466,-0.05375216,0.0052095093,0.04698612,-0.005989375,0.010687863,-0.027729882,-0.017334072,0.0023401706,0.03581607,0.0977952,0.076544315,-0.042625174,-0.01526828,0.030910114,0.006460213,-0.002332684,0.006639546,0.0649329,0.049881764,0.0097931605,-0.018460138,-0.037717965,-0.07960227,0.060924396,-0.056536023,0.050834324,0.020339644,0.010601087,-0.015090058,0.0045518773,0.031031147,0.029827707,0.02101659,0.008064892,0.027343271,-0.07111508,-0.00519532,0.017518062,0.01036433,0.0322751,0.00047660538,0.03134041,0.05699274,0.0159983,0.0137032475,0.013385468,0.036183022,0.02290458,-0.008538046,-0.005982268,0.02000904,-0.0071660657,0.021207357,0.022313215,0.08395038,-0.035739683,0.083042465,-0.02330265,0.004929278,0.0069433893,-0.06719772,-0.04478718,0.04190071,-0.018808896,-0.001816154,-0.05969008,0.073712945,-0.036628604,-0.035550434,-0.029403394,0.05851644,0.03375408,-0.031480316,-0.033645708,-0.023608467,0.010225269,-0.042239573,-0.017170709,-0.02934463,0.04068189,0.017349007,0.025796602,0.003576173,0.036880214,-0.018717546,0.006829122,0.053399332,-0.010375033,-0.018127933,-0.015794689,0.04633164,0.019562159,0.0032764247,-0.05278859,0.04982877,0.028981289,-0.036573805,0.04073642,0.044666152,0.024727462,-0.06905686,0.012558917,0.049123622,-0.010332436,0.025056724,-0.026054269,-0.050182074,0.02472787,-0.01870028,-0.02846345,-0.010449695,0.013902413,-0.05142796,0.0024122864,-0.05221934,0.051648363,-0.004195486,0.0481339,0.036214568,-0.020598922,-0.002643189,0.04274703,0.0054911138,-0.027950695,0.027602788,-0.06444788,-0.013386167,0.026896937,0.030076733,0.010117147,-0.059001107,0.06428756,-0.10590243,0.008179068,0.057679765,-0.03290571,0.0099706715,-0.029421566,0.00066044275,-0.030567683,0.0029834113,0.048831873,-0.016329883,-0.017039578,-0.020112034,0.033353753,0.018268665,-0.0005062679,-0.011993359,-0.019358097,-0.027312316,0.026143247,-0.05003828,-0.00048724585,-0.025820674,0.038609255,-0.004088062,-0.0011547492,0.012222098,0.0021774978,-0.034338083,-0.019892199,-0.01940348,0.027403522,0.0042137844,0.039355755,-0.054657828,0.020919928,-0.0045172703,-0.0069895107,-0.031037666,0.06471489,0.05709204,-0.009906443,-0.065049864,-0.064548574,0.0025952505,0.061959926,-0.019479405,0.007895278,-0.027689671,-0.058757033,-0.04198692,0.019796189,0.044921607,0.06406552,0.030771364,0.045393664,0.03252736,-0.032893226,-0.016669597,0.017662788,-0.0043549477,-0.04208956,-0.031218871,0.005182371,0.07400485,-0.015130733,0.0013585036,0.0067433203,0.001395182,0.0100448355,0.011093339,0.06581418,0.021025904,0.018524002,-0.012949533,0.012554014,-0.025268592,0.048728105,-0.0330259,-0.0067929286,0.017668866,0.024062365,0.00291585,-0.020863412,0.011149956,-0.0246248,0.0038819206,-0.0074550123,0.033438317,0.043095745,0.03709285,-0.03839395,0.08250408,-0.012849122,-0.031751353,0.011676345,-0.058561254,-0.017706994,0.03117933,-0.08264571,-0.03543256,0.032696143,-0.031458043,-0.018761525,0.012172502,-0.046076305,0.10776816,-0.063417554,0.022781877,-0.031596117,0.0061074407,0.12484622,0.0002007877,-0.004630714,-0.04344846,-0.01598038,-0.008108876,0.02516406,0.086661376,0.0075493134,-0.022677904,0.06417719,-0.0014283591,0.049073827,0.0046091015,-0.052869357,-0.00957049,-0.033973485,-0.03216625,-0.0041332953,-0.036343973,0.023513105,0.019725248,-0.0050538275,0.0076748473,0.02600658,-0.036760047,0.022923242,0.025700236,-0.019521054,0.010105913,-0.025576208,0.06546313,0.036763363,-0.013033489,-0.015723813,0.038423203,0.029334752,-0.014419952,-0.023002299,0.021983251,0.0038589595,-0.03244232,-0.013447009,0.0554185,0.040024634,-0.05453601,-0.0042240126,0.0109727355,0.030564472,0.042887885,-0.0068124356,-0.03092693,-0.03429077,-0.04225816,-0.022550853,0.019682096,0.014614483,-0.011787301,-0.0012998964,-0.019241715,0.07474126,-0.080413684,-0.033860683,-0.028162336,0.03305822,-0.020455848,-0.002371084,-0.0055528423,-0.00744072,-0.01354069,-0.029081859,0.042744126,0.07525399,-0.037814025,0.023930954,-0.0134938015,-0.022160072,-0.014856198,-0.021854823,-0.0360651,-0.0026285928,-0.024573572,-0.027543902,0.09480798,-0.033934504,0.0064170496,0.05624567,0.008181932,-0.07711337,-0.0034072846,0.025120633,-0.007501865,-0.014840762,0.0015853111,0.015392086,0.046110354,-0.04092535,-0.029668013,-0.07964314,0.024455618,-0.005901139,-0.035402317,0.03296261,0.03248602,-0.07672481,0.0058259293,-0.026189763,-0.032009404,-0.012981986,0.014686776,0.0819852,0.047091186,0.060589477,-0.05559873,-0.088240676,-0.028186968,-0.021289712,0.0052775084,-0.037287954,0.040334143,-0.005103142,-0.031474266,0.015515695,-0.03688787,-0.02478152,0.011019632,0.010880339,-0.019412488,-0.02502464,-0.014795049,5.018556e-05,0.02788025,-0.010749699,0.0042168056,0.054907374,0.03136712,-0.06626859,-0.04734415,-0.01654413,-0.011703702,0.0035126274,0.016496582,0.012219685,0.0032816075,-0.015293971,0.004641765,0.06879076,0.0010218521,-0.03353115,0.002845488,0.04102457,-0.034159675,0.03999486,0.008029971,-0.0038321265,-0.10071517,0.0014033299,-0.023590513,-0.06598946,0.0420018,0.030755583,0.02143275,-0.032782134,-0.03379451,-0.029408436,0.0007060909,0.047859445,0.038493812,0.012071841,0.018384906,-0.024879199,-0.060211077,-0.01918174,-0.03958758,0.03349599,-0.0068270336,0.03819893,0.016938988,0.022418352,-0.0011698281,0.017834958,-0.027460137,-0.040555514,0.041730493,0.0222246,-0.03952322,0.046930354,-0.00969824,0.021643229,0.05420563,-0.021125343,0.043192275,-0.0067841746,0.015832981,0.00707933,-0.008341164,-0.0049616485,-0.009650777,-0.10415751,0.021401782,0.024028009,0.0031098227,0.037204742,0.0010523421,-0.053716276,-0.07140786,-0.038786005,0.006478886,0.03707056,0.01909862,0.031414513,-0.037881132,0.02896109,-0.016103212,0.04672758,0.016265387,-0.007419631,0.025386175,-0.02395158]	Keywords: Transformers, Machine Learning, Deep Learning, Attention Mechanisms, Layer Normalization, Efficient Computation, Object Detection, Speech Synthesis, Time Series Forecasting, Natural Language Processing\nKey Objects: ICML, NeurIPS, arXiv\nRefers to Images: None\nHypothetical Questions:\n- What are some key challenges in Transformer architecture that these references address?\n- How do these references contribute to the ongoing evolution of Transformer models?\n- Can you identify any common themes or approaches across these references?\n---\nSummary:\nThis is a list of references for a survey on Transformers. It includes a variety of works related to Transformer architecture, training, and applications such as machine translation, object detection, speech synthesis, and more. The references cover topics like layer normalization, efficient attention mechanisms, memory-efficient designs, and specialized applications.\nOriginal Text:\n- [151] Rubin Xiong, Tunchang Tan, Di He, Kai Zhheng, Sujin Zhu, and Zhijian Zhu. 2020. Wang, and Tie-Yan Liu. 2020. On Layer Normalization in the Transformer Architecture. In Proceedings of ICML . 10524-10533. http://proceedings.mlr.press/v119/xiong20.html.\n- [152] Yunyang Xiong, Zhanpeng Zeng, Rudrasch Krabatorby, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. 2021. Nystromfer: A Nystrom-based Algorithm for Approximating Self-Attention. (2021).\n- [153] Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. 2019. Understanding and Improving Layer Normalization. In Proceedings of NeurIPS . 4383-4393. https://proceedings.neurips.cc/paper/2019/hash/ 2f4e03d7772a472717006e5d16728784-Abstract.html\n- [154] Hang Yan, Bocao Deng, Xiaoran Li, and Xipeng Qiu. 2019. TENER: Adapting transformer encoder for named entity recognition. arXiv preprint arXiv:1911.04474 (2019).\nContextualized Text:\nThis section provides a consolidated list of references supporting the discussion of Transformers. Each entry includes author names, publication year, title, and location/format (e.g., arXiv preprint, conference proceedings).	{"tags": ["reference", "list", "transformer", "survey"], "doc_id": "235f8460-d034-4388-97b2-62a0d252b6d7", "summary": "This is a list of references for a survey on Transformers. It includes a variety of works related to Transformer architecture, training, and applications such as machine translation, object detection, speech synthesis, and more. The references cover topics like layer normalization, efficient attention mechanisms, memory-efficient designs, and specialized applications.", "doc_type": "text", "entities": ["Rubin Xiong", "Tunchang Tan", "Di He", "Kai Zhheng", "Sujin Zhu", "Zhijian Zhu", "Yunyang Xiong", "Zhanpeng Zeng", "Rudrasch Krabatorby", "Mingxing Tan", "Glenn Fung", "Yin Li", "Vikas Singh", "Jingjing Xu", "Xu Sun", "Zhiyuan Zhang", "Guangxiang Zhao", "Junyang Lin", "Hang Yan", "Bocao Deng", "Xiaoran Li", "Xipeng Qiu"], "keywords": ["Transformers", "Machine Learning", "Deep Learning", "Attention Mechanisms", "Layer Normalization", "Efficient Computation", "Object Detection", "Speech Synthesis", "Time Series Forecasting", "Natural Language Processing"], "key_objects": ["ICML", "NeurIPS", "arXiv"], "contextual_text": "This section provides a consolidated list of references supporting the discussion of Transformers. Each entry includes author names, publication year, title, and location/format (e.g., arXiv preprint, conference proceedings).", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some key challenges in Transformer architecture that these references address?", "How do these references contribute to the ongoing evolution of Transformer models?", "Can you identify any common themes or approaches across these references?"]}
ac3b0166-7c6e-4219-b2f7-5176c4a7f1e5	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.028539946,0.021233687,0.0020863786,0.026064446,0.0061781574,0.077285305,0.029520432,0.07457467,0.02679916,-0.02691957,-0.025228286,0.030227555,0.033076886,-0.0032331913,-0.052399546,0.03162963,0.010863278,0.07548041,0.0109053515,-0.0047244537,0.057285354,0.02529673,0.013193327,-0.043844752,-0.019685995,0.016961893,-0.0058489135,-0.058491338,0.048664093,0.039471027,0.021085715,-0.08442175,0.018878432,0.009626963,-0.009118908,0.027157702,0.0114885485,-0.019913822,0.017271243,-0.0006392857,-0.01700082,0.04055576,-0.081530385,0.0015177933,-0.023789596,-0.057219144,-0.069270514,-0.047970973,0.029384341,-0.025285775,-0.021970239,0.028707206,-0.025132803,-0.008405287,0.009271853,-0.017548284,-0.015211669,-0.023250613,0.034373485,-0.107575655,-0.041271854,0.012782028,-0.044341348,-0.025494963,0.014778134,-0.07886619,-0.02624384,-0.0015793521,0.0036255172,0.106628,-0.020586397,0.021293165,0.0033581904,-0.015843818,0.09791701,0.03916795,-0.0062793465,-0.030721543,-0.06474216,-0.009722664,-0.03905486,0.06337881,-0.029785866,-0.018937316,0.07203545,0.005144889,-0.05675422,-0.032273658,0.0936548,-0.011669524,0.028783614,-0.0074464264,-0.01753283,0.061700407,0.003394754,-0.049219027,0.012460429,-0.04213172,0.021017784,-0.03170489,-0.031980805,-0.0062825205,0.064240165,0.06710431,0.009455867,-0.008647073,-0.039610345,-0.022470465,0.010837082,0.019697774,0.006421575,-0.0056407885,-0.003389381,0.01329259,-0.02452119,-0.03077501,-0.031011585,0.028510507,0.022357866,0.0046421466,-0.038249083,-0.021673735,-0.010520757,0.03362805,0.056062393,0.073084414,-0.009528449,0.004483638,-0.036497694,-0.019597502,0.01263086,0.009425703,0.00875697,0.020469775,0.02306051,0.017282438,0.06619538,-0.051820666,0.0015795043,0.0062087267,0.0345952,-0.016651388,-0.0071434085,0.04163312,-0.05050011,0.006032756,-0.068112575,-0.0067932615,-0.022357358,0.0450582,0.019351108,0.0050393157,0.024288654,0.029358847,0.01802043,0.013481444,0.017051227,-0.09442487,0.02943346,0.0024075129,-0.023455052,-0.060257565,-0.040141836,0.0313052,0.043031063,0.107811876,0.010570996,-0.013091852,0.0358405,0.0045118337,-0.056246813,0.025179584,-0.009630214,-0.015789706,0.019180227,-0.03156326,0.016865276,0.01607376,-0.020205336,0.008919865,0.0035357175,0.005065535,-0.014472479,0.065750435,-0.016530216,0.023424303,-0.0041595562,0.076434106,-0.017268278,-0.022957081,0.04344678,-0.009588462,-0.021886053,-0.024801562,0.017857669,0.006458138,0.029997371,-0.06971509,-0.051668927,-0.0042752745,-0.041499812,0.013174459,0.03941282,0.014053157,0.024286402,-0.034797974,-0.024292143,-0.023425376,-0.052825514,-0.017496515,0.023609797,-0.022719802,-0.033689074,0.05653149,0.0375573,-0.025056347,-0.017412372,0.0055323346,0.045872416,-0.015004787,-0.058596916,0.001328069,-0.038402956,0.017032579,-0.055768587,-0.00606953,-0.048173632,0.035871692,0.0038676618,-0.022234857,0.0726741,0.009049141,-0.028050799,0.0047675865,-0.034842227,0.031285066,-0.014237403,0.0018945243,0.011870816,0.051280335,0.030235512,0.06263636,0.0102353925,0.06706766,-0.004320493,-0.0089856265,0.021344338,0.010566494,0.0059607103,0.023737269,-0.007147069,0.06838951,0.05539428,0.048727572,0.03225369,-0.03507158,0.052406423,0.018747242,0.050857443,0.030814178,0.004976636,0.005363534,0.006660366,-0.0066286097,0.0035678449,0.012569572,-0.0025444443,0.033402044,0.023804497,-0.0245921,0.04949495,0.039603904,0.0067963195,-0.033291224,-0.07206221,-0.0033305706,0.022426914,0.042458616,0.014967593,-0.022977049,-0.00015967945,-0.012183202,0.049782943,-0.010715109,0.0039411196,-0.015366861,-0.022085136,-0.028269012,-0.069340274,-0.0103143705,-0.014184339,-0.039038647,0.019409044,-0.026000405,-0.00992817,0.010681692,0.0049716625,-0.04873841,-0.03255362,0.00073384243,0.10140535,-0.022000123,0.019371573,0.00031428857,-0.022048326,0.010694451,-0.07476537,-0.0044674897,0.06639201,0.015975704,0.00088994595,-0.015287196,-0.002037402,-9.9415614e-05,0.056673672,0.084256545,0.063602865,-0.050683346,-0.016045466,0.033618752,0.0129041895,0.024292603,0.0050081257,0.050556786,0.056567565,-0.0074722916,-0.014041556,-0.054466885,-0.08722236,0.032009482,-0.06990097,0.021488179,0.049320813,-0.0019395255,-0.026969988,0.0069703013,0.058303922,0.024947282,0.00633118,0.024608035,0.018052677,-0.07856142,-0.001786042,-0.0067675305,0.0018597431,0.034124933,-0.006476498,0.04975368,0.020763937,-0.007960634,0.00020600743,0.035120696,0.01007022,0.019427488,-0.02277715,-0.032231085,0.0072673117,-0.0062340065,0.0013630012,0.023663754,0.062202815,-0.03082853,0.08836324,-0.032937203,-0.01713112,0.009281743,-0.07719307,-0.0428022,0.07838729,-0.018182052,-0.030798316,-0.054949716,0.047848053,-0.033271674,-0.04120443,0.000715789,0.07370756,0.041261915,-0.039503705,-0.053569052,-0.00399657,0.0055585965,-0.036934715,-0.018377122,-0.026429404,0.045531992,0.020264022,0.01807779,-0.016512174,0.04169444,-0.010119983,0.0066018896,0.033116054,-0.016858803,0.005730526,0.015133005,0.027585045,0.009926538,0.016808398,-0.064671025,0.025132414,0.0017061923,-0.042784274,0.021567665,0.055030093,0.037961476,-0.062154662,0.016630378,0.060497534,0.0088939145,0.021020088,-0.030486187,-0.0407513,0.040669233,-0.011070771,-0.0020936208,-0.011650125,0.0009497681,-0.06287636,0.017458338,-0.051161624,0.047642723,-0.0050870087,0.05595714,0.04045585,-0.01893493,-0.03560723,0.020099739,-0.01684682,-0.054664567,0.0024787828,-0.05347609,-0.027788885,0.018017743,0.032959595,0.02169841,-0.026980236,0.068800986,-0.08731511,0.016550735,0.06402709,-0.029332291,0.02059578,-0.008741456,-0.0025606197,-0.01360944,-0.010676869,0.061714776,-0.010670645,-0.021526206,-0.029712686,0.04910719,-0.007909396,0.008977274,-0.005754786,-0.024166679,-0.036889132,0.025651496,-0.045117762,-0.017454693,-0.02491269,0.05104561,-0.0039505824,-0.011241491,0.0035817923,0.010697118,-0.042141907,-0.004919113,-0.009468609,0.011952418,-0.007679703,0.023719227,-0.05374737,-0.026499663,-0.0005081295,-0.0078010303,-0.042114437,0.08071783,0.044214144,0.023698436,-0.055104304,-0.04826742,-0.012795513,0.07188222,-0.017688388,-0.015776584,-0.033477053,-0.06890582,-0.06818784,0.02320405,0.037905928,0.05580564,0.051322,0.022095751,0.016620742,-0.016367627,0.0062048123,0.0140212905,-0.005202383,-0.04100369,-0.02953425,-0.0020107948,0.04228807,-0.013320944,0.021040399,0.028237995,0.022385878,-0.0038667491,0.01637945,0.048644412,0.024064383,0.013392984,-0.020964613,0.024640748,-0.017459132,0.01406864,-0.020429336,0.010841679,0.0017668495,0.023278065,-0.014353239,-0.041940123,-0.015175515,-0.045591597,0.008675454,-0.018257963,0.024724463,0.043749813,0.0400436,-0.028848069,0.06069824,-0.009400284,-0.025016813,0.0071400357,-0.029729242,-0.039551556,0.024032788,-0.09085379,-0.012366045,0.012571902,-0.024027834,-0.008853578,0.039302032,-0.053487062,0.11309291,-0.055471778,0.009293451,-0.02884628,0.013168579,0.11679117,-0.002129111,0.011218827,-0.033086434,-0.046618383,-0.014762468,0.019868735,0.061653648,0.034374032,-0.051567387,0.06393802,0.015739316,0.036688898,0.016513908,-0.0464374,-0.010781354,-0.035180517,-0.054371174,0.025461143,-0.034699306,0.008508838,0.031544436,-0.020045236,0.023766695,0.010073962,-0.00030014053,-0.014957022,-0.0010775814,-0.008502834,0.035496123,-0.04290366,0.048581466,0.032714803,-0.029290382,-0.026323622,0.0451654,0.0017507384,-0.019334067,-0.035418563,-0.0033269047,-0.026952704,-0.004397041,0.005184311,0.034172107,0.04764203,-0.052225508,0.0026777405,0.006078705,0.02443236,0.034135982,-0.022937873,-0.022480175,-0.025185633,-0.056253202,-0.0151347555,0.009863436,0.020678334,0.0024643282,0.0027481767,-0.017723892,0.085061096,-0.076877505,-0.03715652,-0.026043436,0.04387774,-0.021446913,-0.0114774825,-0.0060104197,-0.022482077,-0.022694983,-0.03672522,0.055514395,0.044476543,-0.05038433,0.03646479,-0.012975187,-0.014017768,-0.016538562,-0.05043921,0.008956815,-0.013690873,-0.0037799429,-0.018636161,0.08487446,-0.010029493,0.014198331,0.03725914,-0.022663504,-0.0837118,-0.010670776,0.0044906233,0.020302493,-0.027926458,0.0063003064,0.016568653,0.02344747,-0.043198545,-0.011400877,-0.06969448,0.016167939,-0.009622953,-0.03683896,0.046437006,0.01367441,-0.08120234,0.05476063,-0.046314187,-0.029363804,-0.030135725,0.01644348,0.072855234,0.040947888,0.049088176,-0.007826095,-0.09788163,-0.011795155,-0.013651553,-0.03066964,-0.03564774,0.04701391,0.0092163235,-0.037564125,0.0061722617,-0.026734017,-0.0153431855,0.03736707,0.010702621,-0.02371661,-0.025756897,-0.009254803,-0.029768962,0.02425666,-0.016780246,0.0025596884,0.061170295,0.026684612,-0.07941034,-0.051205665,0.008957388,0.004574999,0.024223706,0.020058552,0.022099636,0.042771414,-0.02262865,0.017545559,0.051499132,0.016532011,-0.021119734,0.013158725,0.041306466,-0.009950476,0.02977579,0.0120835295,-0.017916398,-0.07723834,0.014391465,-0.009607636,-0.07731235,0.026491363,0.006839234,-0.0076043517,-0.04706949,0.0020319484,0.0030720124,0.0028628672,0.031943563,0.02660046,0.014818459,0.004997088,-0.017558469,-0.03919663,-0.0025504907,-0.016216224,0.036233675,-0.037097815,0.060098793,0.033796344,0.024738729,0.008822862,0.010258579,-0.024218109,-0.042844772,0.036988717,0.053265564,-0.047816046,0.041439764,-0.0068192612,0.004874936,0.06140311,-0.006471439,0.03636928,-0.00939602,0.016709555,0.02490369,0.0025210038,-0.010211479,0.010402771,-0.10061845,-0.0072822706,0.020637028,0.013186647,0.030203102,0.0036245147,-0.03546407,-0.044561766,-0.044633076,-0.0072896383,0.05239827,-0.00047352244,0.030017862,-0.025370123,0.026968645,-0.04672198,0.07001311,0.0071369605,-0.0124142105,0.006496287,-0.027080305]	Keywords: Transformers, Named Entity Recognition, Localness, Decoder, Sparse Expert Models, Self-Attention, Neural Networks, Machine Learning\nKey Objects: TENER, BERT, EMNLP, Findings of EMNLP\nRefers to Images: None\nHypothetical Questions:\n- What are the various adaptations of Transformers mentioned in this list?\n- Which researchers are frequently cited in these references?\n- What conferences or workshops are associated with these works?\n- What are the key applications or tasks for which these Transformer models are adapted?\n- How do these works contribute to the broader understanding of Transformer architectures?\n---\nSummary:\nThis is a list of references related to a survey on Transformers. The references span various aspects of Transformers including adaptations for specific tasks like named entity recognition, modeling localness, understanding sub-layer functionalities, and exploring sparse expert models.\nOriginal Text:\n- [154] Hang Yan, Bocao Deng, Xiaoran Li, and Xipeng Qiu. 2019. TENER: Adapting transformer encoder for named entity recognition. arXiv preprint arXiv:1911.04474 (2019).\n- [155] An Yang, Junyang Lin, Rui Men, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Jiamang Wang, Yong Li, Di Zhang, Wei Lin, Lin Qu, Jingren Zhou, and Hongxia Yang. 2021. Exploring Sparse Expert Models and Beyond.  \n- arXiv:2105.15082 [cs.LG]\n- [156] Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang. 2018. Modeling Localness for Self-Attention Networks. In Proceedings of EMNLP . Brussels, Belgium, 4449-4458. https://doi.org/10.18653/v1/D181475\n- [157] Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee, and Zhaopeng Tu. 2020. On the Sub-layer Functionalities of Transformer Decoder. In Findings of EMNLP . Online, 4799-4811. https://doi.org/10.18653/v1/2020. findings-emnlp.432\nContextualized Text:\nThis is a list of references related to a survey on Transformers. The references span various aspects of Transformers including adaptations for specific tasks like named entity recognition, modeling localness, understanding sub-layer functionalities, and exploring sparse expert models.	{"tags": ["references", "survey", "transformers"], "doc_id": "ac3b0166-7c6e-4219-b2f7-5176c4a7f1e5", "summary": "This is a list of references related to a survey on Transformers. The references span various aspects of Transformers including adaptations for specific tasks like named entity recognition, modeling localness, understanding sub-layer functionalities, and exploring sparse expert models.", "doc_type": "text", "entities": ["Hang Yan", "Bocao Deng", "Xiaoran Li", "Xipeng Qiu", "An Yang", "Junyang Lin", "Rui Men", "Chang Zhou", "Le Jiang", "Xianyan Jia", "Ang Wang", "Jie Zhang", "Jiamang Wang", "Yong Li", "Di Zhang", "Wei Lin", "Lin Qu", "Jingren Zhou", "Hongxia Yang", "Baosong Yang", "Zhaopeng Tu", "Derek F. Wong", "Fandong Meng", "Lidia S. Chao", "Tong Zhang", "Yilin Yang", "Longyue Wang", "Shuming Shi", "Prasad Tadepalli", "Stefan Lee", "Zhaopeng Tu"], "keywords": ["Transformers", "Named Entity Recognition", "Localness", "Decoder", "Sparse Expert Models", "Self-Attention", "Neural Networks", "Machine Learning"], "key_objects": ["TENER", "BERT", "EMNLP", "Findings of EMNLP"], "contextual_text": "This is a list of references related to a survey on Transformers. The references span various aspects of Transformers including adaptations for specific tasks like named entity recognition, modeling localness, understanding sub-layer functionalities, and exploring sparse expert models.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the various adaptations of Transformers mentioned in this list?", "Which researchers are frequently cited in these references?", "What conferences or workshops are associated with these works?", "What are the key applications or tasks for which these Transformer models are adapted?", "How do these works contribute to the broader understanding of Transformer architectures?"]}
c635ca93-4d03-4be4-9c86-fd0604ea2e2c	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0077125328,0.017591469,0.01073855,0.014417047,-0.005741868,0.069764234,0.009951322,0.056089636,0.023006491,-0.04814712,-0.020650838,0.018375374,0.011773855,-0.023794608,-0.056956757,0.029283702,0.042875532,0.077157766,0.029462004,-0.006996532,0.035566054,0.0062158876,-0.0076778824,-0.035006456,0.0052769217,0.0021574283,-0.0018646994,-0.02536641,0.003785389,0.04717789,0.008702474,-0.04946525,0.013660419,-0.0013573202,0.034875937,-0.0025993816,-0.0019098206,-0.015800472,0.003556175,-0.019082109,-0.013675856,0.04954923,-0.08479813,-0.012711013,0.014869033,-0.06439209,-0.05783059,-0.016797863,0.03118995,-0.013867755,-0.0020541141,0.01731064,-0.027364723,-0.026821082,0.016797597,-0.021142196,0.019089283,-0.016347041,0.03993569,-0.104613006,-0.032416284,0.026642583,-0.03426931,0.013264329,-0.005511875,-0.043136213,-0.020063117,-0.019598976,0.015157185,0.100642,-0.029513448,0.030254295,-0.03342032,-0.044078313,0.117061295,0.04362186,-0.0274832,-0.029680742,-0.08463343,-0.008602844,-0.049394485,0.06695044,-0.069817975,-0.045444895,0.07893612,0.029318705,-0.049744517,-0.003027292,0.07060045,-0.014721477,0.004659113,-0.02913721,0.0064251577,0.06420562,-0.030441977,-0.060982835,-0.002828506,-0.045387603,-0.014845756,-0.060311697,-0.0008158894,0.013117151,0.08551357,0.10155059,0.012992968,-0.03278302,-0.020610634,-0.028557181,0.020324264,0.020913245,-0.008919856,0.021259721,-0.0043264073,-0.009759976,-0.012007303,-0.018419463,-0.062306758,0.020790318,-0.007484708,-0.0043016905,-0.021934614,-0.032623187,-0.00638204,0.039673854,0.07599244,0.057916485,0.0060614017,-0.0018418798,-0.067917846,0.0035478484,-0.0053247446,0.011983005,-0.028522903,0.018662684,0.020621821,0.013726127,0.07914917,-0.046225738,-0.00039899335,0.013204276,0.015185889,-0.028802555,-0.052494578,0.025981653,-0.014098266,-0.0029762317,-0.070401944,0.018055681,-0.0498872,0.03431354,0.018571261,0.003054643,0.03721286,-0.0027965035,-0.01857837,0.015294552,0.0019813706,-0.06316682,0.023770645,-0.013912024,-0.030920051,-0.06684144,-0.03424014,0.059739355,0.03147039,0.14051534,-0.005869998,0.00078896305,0.052835822,0.019140372,-0.047606815,0.005533683,0.0017640255,-0.031825863,0.0009544396,-0.02643869,0.032056063,-0.013986604,-0.018004706,-0.03464472,0.023329623,0.017129496,-0.019745346,0.054885942,-0.044203237,0.024868887,-0.0012453555,0.10109946,-0.027700877,-0.03975048,0.021298915,-0.013194774,-0.011146508,-0.0139751565,0.014862274,0.018301697,0.03585227,-0.06860027,-0.053995874,-0.056213457,-0.018977026,0.0017254972,0.025310352,-0.019788943,0.0053398134,-0.024881726,-0.041175928,-0.027367834,-0.03625406,-0.012204863,-0.032634303,-0.025681946,-0.004440519,0.072521456,0.023551857,0.008688529,-0.020964164,0.04697545,0.044514943,-0.01830609,-0.064114146,0.019691784,-0.033289995,0.028646246,-0.034338363,0.035773035,-0.023647822,0.0314747,-0.003908311,-0.018192904,0.042014908,-0.0078824945,-0.0139054945,0.037688844,-0.019271703,0.021702094,0.020038618,-0.00010512881,-0.022816874,0.035438314,0.03873383,0.05976197,0.022779195,0.084870815,0.015668016,-0.0057340194,0.014343051,0.038173538,0.0042816103,0.030125983,-0.01506499,0.054968584,0.054701883,0.011423857,-0.0030760027,-0.055553567,0.03405338,0.04003058,0.034853976,-0.018539406,-0.010060273,-0.02062522,0.019219726,-0.0129702175,0.03361733,-0.0033783503,0.028834198,0.016865019,0.007386022,-0.02007347,0.035309132,0.01995602,0.0013339726,-0.020449854,-0.099301785,-0.0012097482,0.037952755,0.024779813,0.034150798,-0.035749376,-0.020420745,-0.05537961,0.022836022,-0.0032382293,0.029244926,-0.026519641,-0.023888022,-0.030253768,-0.09371584,-0.022003312,-0.0041397396,-0.030556694,0.023663288,-0.021557683,-0.0056053987,-0.011757951,0.0112946555,-0.023062797,-0.008244306,0.008449798,0.0914259,-0.04281896,0.0060405964,0.017141208,-0.00448607,0.024047613,-0.072351165,0.006040132,0.045176756,0.016873749,-0.009634275,-0.020511253,-9.67859e-05,-0.012088396,0.06025927,0.074457884,0.07146784,-0.0073137805,0.008840494,0.014099746,0.0332813,0.026640534,-0.022760876,0.052466996,0.02893664,-0.04204027,0.022040106,-0.019511653,-0.095957555,0.050047606,-0.089902505,0.050897494,0.04965054,-0.018628312,0.008957212,-0.0051105167,0.029419763,0.024343232,-0.009214224,0.021875449,-0.039824057,-0.073728554,-0.00038157983,0.0036359406,-0.002248189,0.017195517,-0.0044271136,0.02354809,0.039569817,0.0013328359,-0.004453693,0.017848637,0.043518182,-0.008650636,-0.012062185,0.013502878,0.0019643835,0.006332873,0.0049078404,0.021551803,0.053090893,-0.04172365,0.08271893,-0.005198069,0.013390963,-0.018738948,-0.06288751,-0.023563588,0.054987103,-0.004633403,-0.022726418,-0.017291347,0.055799644,-0.058875907,-0.020768145,0.016876418,0.04391972,0.033782735,-0.05322586,-0.046272747,-0.007163612,-0.0094988765,-0.056394286,-0.07742624,-0.023044268,0.05541354,0.013346736,0.036288127,-0.0051407083,0.030139241,-0.01708429,0.0235091,0.020665936,0.016519392,0.00012101047,-0.003033544,0.053784918,0.02621042,0.017529126,-0.036007695,0.034246124,0.03321026,-0.045689628,0.024502715,0.04719941,0.031365927,-0.10714241,0.0479829,0.057798672,0.025295045,0.020373099,-0.047853127,-0.034114435,0.020279089,-0.0061494457,-0.0035106153,-0.0071524307,0.016273648,-0.037912063,0.0071047503,-0.041983645,0.05486577,-0.003418522,0.0679908,0.009151869,-0.031328324,-0.036241166,0.027299445,0.0038889232,-0.032227833,0.007027704,-0.05996682,0.005453161,0.056818075,0.03721566,0.035033733,-0.058796823,0.06874304,-0.09156306,-0.0021464704,0.009097336,-0.04380498,0.0011328463,-0.031650674,-0.021499766,-0.04453038,0.011173367,0.0458426,0.00084531575,0.0037699873,-0.0014208625,0.06533205,0.018158808,-0.0039210585,-0.0023680571,-0.03596941,-0.030573465,-0.012913061,-0.040677834,-0.020097112,0.00034286178,0.017469907,0.017645476,0.0006333231,-0.004226737,-0.007221681,-0.0088412,-0.03367016,-0.013481515,0.021888968,0.023181738,0.021243082,-0.055384304,0.015904438,-0.0037978946,-0.017582929,-0.022739287,0.046243872,0.04625755,0.013231107,-0.06995091,-0.02673915,0.0030779336,0.0708379,-0.015419898,-0.011570505,-0.008604606,-0.020967178,-0.03088557,0.030648524,0.025985155,0.041243948,0.030635215,-0.008365492,0.011975088,-0.042846244,0.01210404,-0.004934255,0.025105612,-0.042429052,-0.016193045,0.010458982,0.039125513,0.018524047,-0.010665004,-0.00025598198,0.005300717,0.016405052,0.03796368,0.056356322,-0.0048043444,-0.0046293726,-0.0145691605,-0.0071447087,-0.019676682,0.03363784,-0.037187986,-0.0019953095,0.011891134,0.064526305,-0.014050211,-0.02491718,0.0011851266,-0.04116106,-0.0007306852,-0.0073648407,0.038838454,0.042147815,0.043702926,-0.07488752,0.029755028,-0.044350468,-0.030002803,0.04059444,-0.061409872,-0.028170273,-0.01025209,-0.08204968,-0.010304705,0.008796493,-0.031178093,-0.012502623,0.028958997,-0.023888722,0.08754346,-0.0687499,0.010354215,-0.022387717,0.00029628625,0.121509425,0.03889158,-0.01419492,-0.025965994,-0.0373821,-0.035406414,0.025126085,0.07008967,0.0067552305,-0.034514,0.05996397,-0.012787059,0.023960419,-0.002232967,-0.05533891,-0.009725226,-0.020535829,-0.028292404,0.0026536488,-0.033347428,0.016566314,0.0017612953,0.00950112,0.009966435,-0.0035454088,0.0068401387,0.03670723,0.042903204,-0.0065745804,0.027981872,-0.025958678,0.050653093,0.008716289,0.042087704,-0.0058583915,0.030262964,0.016304392,0.007962658,-0.026020918,-0.02355416,-0.03174262,-0.040242795,-0.009885583,0.005632687,0.021797717,-0.018209567,-0.042260934,0.0133626275,0.03308999,0.0074878996,-0.008443724,-0.043779824,-0.030114518,-0.0056808707,-0.034282163,0.008837729,-0.009476291,-0.0037047921,0.002054386,0.0016409789,0.07531086,-0.09061915,-0.025898913,-0.044115286,0.04009937,-0.016535949,-0.0038510992,0.03998475,-0.04426898,-0.021393215,-0.035994127,0.08248771,0.038119048,-0.02249226,0.022118721,0.0010306174,0.0067514703,0.009285803,-0.011941699,0.023874067,-0.013377363,-0.0110841,-0.0111539345,0.06925588,0.0007023039,0.007162117,0.011753878,-0.01234076,-0.06040852,0.0013897708,0.038821977,0.028019827,-0.06201136,-0.0013108727,0.02965307,0.038021434,-0.041233957,-0.030060701,-0.051970486,0.028486738,-0.020613173,-0.01836552,0.028315365,-0.034186568,-0.060034614,0.030789442,-0.04595423,-0.016678309,-0.006184489,0.011993891,0.053988427,0.0386616,0.04756294,-0.049614463,-0.088554226,-0.022527553,-0.07375183,0.0051180995,-0.031787183,0.07276722,0.0077745994,-0.020682387,-0.022000978,-0.00691612,-0.008447171,-0.007869566,0.008398083,0.01675678,-0.022609591,-0.023625864,-0.024892008,0.005832582,-0.014325903,-0.013944036,0.059113786,0.040649842,-0.07281067,-0.06625993,-0.01920408,-0.0307743,-0.007466016,0.006760667,0.018788183,0.021778418,-0.015647393,-0.0008129205,0.09495822,0.033082258,-0.0074618207,-0.016358268,0.026320267,0.0003506437,0.023820234,-0.0012894089,-0.03191965,-0.128223,0.0032469805,-0.0017833587,-0.03326318,0.024454484,0.019836446,0.00039249184,-0.029947713,-0.028786525,-0.00021700855,0.013002549,0.057630427,0.0076774783,9.14478e-05,0.0016951156,-0.023511274,-0.058354016,-0.029755503,-0.015981274,0.0019566007,-0.04224422,0.029806325,0.037661005,0.017840799,0.0015162326,0.03199054,-0.026207367,0.012082182,0.04624894,0.043515015,-0.023046993,0.052970443,-0.03916169,-0.011809429,0.03574316,-0.008704832,0.024855351,-0.0018644702,0.036637984,0.0010633644,0.0009903555,-0.005233581,-0.04266847,-0.08482318,-0.0065849083,0.013736472,0.0046253656,0.023991182,0.004106024,-0.039388288,-0.026968308,0.0037353397,-0.014322352,0.052992675,0.016848905,0.0094764,-0.06781837,0.005065255,-0.030643845,0.06055266,0.016378738,0.0071083554,0.0023976485,-0.020739159]	Keywords: Transformer, attention, long-range context, efficiency, speech enhancement, binary partitioning, lazy update, recurrence, Gaussian attention\nKey Objects: Transformer architecture, attention mechanisms, long sequence modelling, speech processing\nRefers to Images: None\nHypothetical Questions:\n- What are the key modifications made to the original Transformer architecture in these references?\n- What are the limitations of the original Transformer that these references attempt to address?\n- How do these references contribute to the broader field of deep learning and natural language processing?\n- Can you explain the concept of 'lazy update' as applied to attention mechanisms?\n---\nSummary:\nThis section provides a list of references related to Transformer models, focusing on variations and improvements to the original architecture. The entries cover topics like binary partitioning for long-range context, lazy updates for attention, recurrence for improved efficiency, hard-coded Gaussian attention, speech enhancement transformers, and more. The list includes preprints on arXiv and published works from various conferences and journals.\nOriginal Text:\n- [158] Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. 2019. BP-Transformer: Modelling Long-Range Context via Binary Partitioning. arXiv:1911.04070 [cs.CL]\n- [159] Chengxuan Ying, Guolin Ke, Di He, Tie Yi-Nan. 2021. LazyFormer: SelfAttention with Lazy Update. CoRR abs/2120.12702 (2021). arXiv:2102.12702\n- [160] Davis Yoshida, Alsherson Ettinger, and Kevin Gimpel. 2020. Adding Recurrence to Retrained Transformers for Improved Efficiency and Context Size. CoRR abs/2008.07022 (2020). arXiv:2008.07027\n- [161] Weiqu You, Simeng Sun, and Mohit Iyyer. 2020. Hard-Coded Gaussian Attention for Neural Machine Translation. In Proceedings of ACL . Online, 689-7700. https://doi.org/10.18653/v1/2020-acl-main.687\n- [162] Weiwei Yu, Jian Zhou, HuaBing Wang, and Liang Tao. 2021. SETransformer: Speech Enhancement Transformer. Cognitive Computation (02 2021). https://doi.org/10.1052/S-0298-0172-\nContextualized Text:\nThis is a continuation of a list of references likely discussing advancements in Transformer models, emphasizing modifications aimed at improving their efficiency, context handling, or application in specific domains like speech processing.	{"tags": ["deep learning", "natural language processing", "computer vision", "speech processing", "references"], "doc_id": "c635ca93-4d03-4be4-9c86-fd0604ea2e2c", "summary": "This section provides a list of references related to Transformer models, focusing on variations and improvements to the original architecture. The entries cover topics like binary partitioning for long-range context, lazy updates for attention, recurrence for improved efficiency, hard-coded Gaussian attention, speech enhancement transformers, and more. The list includes preprints on arXiv and published works from various conferences and journals.", "doc_type": "text", "entities": ["Transformer", "BP-Transformer", "LazyFormer", "hard-coded Gaussian attention", "SETransformer"], "keywords": ["Transformer", "attention", "long-range context", "efficiency", "speech enhancement", "binary partitioning", "lazy update", "recurrence", "Gaussian attention"], "key_objects": ["Transformer architecture", "attention mechanisms", "long sequence modelling", "speech processing"], "contextual_text": "This is a continuation of a list of references likely discussing advancements in Transformer models, emphasizing modifications aimed at improving their efficiency, context handling, or application in specific domains like speech processing.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the key modifications made to the original Transformer architecture in these references?", "What are the limitations of the original Transformer that these references attempt to address?", "How do these references contribute to the broader field of deep learning and natural language processing?", "Can you explain the concept of 'lazy update' as applied to attention mechanisms?"]}
f7b6331f-65c5-4e55-aa5a-42797a4426b1	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.015551536,0.010102897,0.021327276,0.046884995,-0.008134956,0.062925085,0.010024771,0.057855025,0.01645836,-0.038437255,0.00735946,0.02860812,0.004311909,-0.006518154,-0.04402694,0.03315499,0.010867169,0.057168886,0.021216111,-0.025650723,0.029686118,-0.015162545,-0.008430446,-0.0300785,-0.039027046,0.025277749,0.012753797,-0.028710166,0.010403659,0.01754317,0.03765391,-0.0474525,0.015412328,-0.008490702,-0.0002945968,0.0012919396,0.011162062,-0.0029202176,0.023993278,0.000620016,-0.014364881,0.026679555,-0.07028307,-0.01878932,-0.0031876503,-0.045850113,-0.06584754,-0.02828061,0.036960773,-0.0014126087,-0.038763855,0.01457599,-0.010416712,-0.012545145,0.021750558,-0.022187598,0.017501563,-0.0363305,0.029360574,-0.09244417,-0.046612732,0.018142363,-0.051806044,-0.007851485,0.036171407,-0.03276567,-0.0057459595,-0.012391733,0.015957076,0.117512085,-0.0105573,0.012253895,-0.0011567357,-0.061565455,0.11803734,0.04554222,-0.014852101,-0.047514122,-0.060926095,-0.011952085,-0.03877628,0.09048448,-0.058772653,-0.04735373,0.10351238,-0.012454946,-0.014129233,-0.01589699,0.06527996,-0.03005117,0.014006799,-0.015227236,-0.001573097,0.05136361,-0.015970865,-0.09716551,-0.012712449,-0.030559717,0.019371504,-0.055814385,-0.03547924,-0.014364356,0.092916384,0.10143861,-0.006054521,-0.019023715,-0.00750289,-0.05033586,0.012305709,-0.004440711,0.01305658,0.011880819,-0.018518556,-0.01480256,-0.027274467,-0.021255886,-0.038718406,-0.0024724433,-0.0059522376,-0.025987249,-0.011650287,-0.011136436,-0.0220185,0.018138258,0.081001915,0.055011336,0.029219318,0.01032318,-0.036615986,0.0021378763,-0.0073053627,-0.004190892,-0.023598282,0.02355358,-0.027901458,0.014778187,0.062201783,-0.030712081,0.011971147,0.020602688,0.00991488,-0.0013826793,-0.024718164,0.013807565,0.0043831286,0.0033900214,-0.039288532,0.010045809,-0.015050291,0.052588966,0.0034044941,-0.013313376,0.014287734,-0.0027191907,-0.005767556,0.0062990296,0.014220887,-0.09308954,0.0077798944,-0.017464915,-0.02760026,-0.062060248,-0.030526215,0.058060635,0.055969052,0.1353102,0.012159715,-0.009105689,0.05128378,0.015776914,-0.04737725,0.00017488305,-0.020816913,-0.042975787,0.009615337,-0.041657824,0.012236705,-0.023706883,-0.037602812,-0.012900657,0.022571525,0.013646276,-0.012466862,0.07981629,-0.023930702,0.016963808,0.010495952,0.09285498,-0.025895678,-0.033492465,0.026153691,-0.016229367,0.0010346817,-6.054047e-05,0.030716924,0.027335145,0.054898653,-0.079301104,-0.060616814,-0.038409222,-6.727173e-05,-0.0019543834,0.017559066,-0.0105775595,0.030291326,-0.011259753,-0.012407177,-0.019524882,-0.035558686,-0.011698401,0.021875715,-0.024681969,-0.024955703,0.07475398,0.038749557,0.0077452287,-0.019423721,0.017159052,0.03758619,-0.011762622,-0.07141526,0.038938135,-0.00846761,0.029606275,-0.0312788,0.025156317,-0.034058366,0.022566028,0.020554181,-0.036744054,0.045633107,-0.0063109384,-0.03219457,0.033157725,-0.011997178,0.06371082,0.006202073,0.0027100418,-0.00026528095,0.059178803,0.046490755,0.038138535,-0.0076110377,0.06206408,-0.0013027161,-0.0041261464,-0.0034212966,0.021325924,-0.017125798,0.006306955,-0.026657617,0.04387506,0.05614333,-0.00161246,-0.023230024,-0.054854017,0.034905326,0.049216688,0.014087847,0.012873792,-0.0035725466,-0.026013318,-0.00505973,-0.003083876,0.017299123,-0.0071625044,0.030464979,0.023591964,0.0075357296,-0.0033342275,0.025515875,0.060591523,-0.010403956,-0.0372792,-0.06392808,-0.002993463,0.0007422702,-0.0061389627,0.032836486,-0.018343054,0.0056748865,-0.041843615,0.04575508,0.0106182275,0.025861325,-0.021199401,0.015449102,-0.016766284,-0.091147,-0.008932222,0.0108338315,-0.043702405,0.01895242,-0.011600514,0.005533105,-0.003814775,0.009510635,-0.034657735,-0.030007392,0.005602696,0.07014957,-0.030833375,0.015662538,0.014587183,-0.020101301,0.03882876,-0.051262554,0.017707588,0.038697332,-0.014745322,0.0076580076,-0.012448913,0.00042244894,-0.010042583,0.033191275,0.08462502,0.043313712,-0.04654475,-0.003321457,-0.0012425898,0.0063375607,0.002201626,0.0025763996,0.023905065,0.03838174,0.0053457133,0.004191939,-0.034518603,-0.074912585,0.070099406,-0.10052401,0.027656257,0.055172037,0.024971804,-0.013531783,0.021158824,0.046886925,0.025539063,0.008934117,0.020159075,-0.012331857,-0.054291405,-0.0026652687,-0.0026056035,-0.013943554,0.057809144,0.0007700515,0.026086578,0.040769547,-0.01807479,0.0032345802,0.019996068,0.0075391475,0.037273966,0.019675085,0.0051849196,0.0056234947,0.008848224,0.014052935,0.04931387,0.07097234,-0.058341056,0.075749315,-0.016081546,-0.013447938,0.012286024,-0.08119468,-0.038030785,0.060768306,-0.026611254,2.3546081e-05,-0.036119916,0.0625579,-0.044794008,-0.03422994,0.017064897,0.037278622,0.014848671,-0.04881941,0.0078082834,-0.02440968,0.0181902,-0.027215846,-0.03286364,-0.019577421,0.03013772,-0.008714499,0.04624573,0.008769242,0.05784134,-0.027157739,0.045164835,0.0531107,-0.01144103,-0.022740534,0.01458428,0.025212092,0.024415685,0.016859617,-0.022712914,0.05790502,0.032580573,-0.027468065,0.0148722315,0.03599499,0.047538646,-0.0860535,0.030965135,0.06508352,0.023672767,0.025012188,-0.021162635,-0.040573537,0.026363794,-0.009798324,-0.020117637,-0.003898815,0.023896793,-0.036659148,0.005004832,-0.048062444,0.03657927,0.006778616,0.040291183,0.034231763,-0.0070764204,-0.026240729,0.028321747,-0.016359897,-0.051927112,-0.005682652,-0.08044079,-0.022150408,0.039584104,0.034748696,0.02800856,-0.06408009,0.05588923,-0.08928183,0.011204033,0.04459734,-0.05303544,0.010179938,-0.026781958,-0.010709914,-0.033136245,0.008837286,0.05645861,-0.021300642,-0.017901298,-0.0020687738,0.028586093,0.025239248,-0.022537634,0.005582488,-0.046160143,-0.050217375,0.0034821434,-0.058877442,0.032159686,0.00018993828,0.06021638,-0.0039541437,-0.0010734828,-0.002014068,-0.027223546,-0.052583717,-0.029934358,-0.033275418,-0.010128186,0.019185262,0.028188322,-0.049506415,0.020335866,-0.017350437,-0.02866194,-0.012685894,0.06640491,0.062181726,-0.010246718,-0.040809292,-0.009989,-0.002508462,0.052126817,0.024471927,-0.0127089415,-0.036031645,-0.027347494,-0.05278816,0.03338543,0.016943073,0.07202226,0.012868389,0.013994012,0.021371787,-0.027954893,-0.009225761,-0.007916186,0.032185826,-0.06430361,-0.053541087,0.020250512,0.047582358,0.018217463,-0.014513634,0.009914989,-0.0053475047,0.00063831615,0.017930094,0.05621721,0.005745661,0.009642062,-0.0103341695,-0.005520664,-0.044150475,0.045286357,-0.016007489,0.0040340913,0.022531155,0.065504536,-0.016258968,-0.0134334825,-0.009030023,-0.014019547,0.018305453,-0.020438805,0.05748476,0.04269661,0.023658672,-0.054308984,0.06830063,-0.05763102,-0.054690488,0.012763102,-0.05335457,-0.04147193,0.015900876,-0.05401784,-0.009445345,0.025292568,-0.012712183,-0.008992762,0.045637097,-0.042650074,0.060518853,-0.0564192,0.020762963,-0.023918515,0.015420482,0.110914715,0.000600303,-0.016124915,-0.019568102,-0.016534593,-0.041056503,0.016863564,0.04637944,0.03365747,-0.011894654,0.08186319,-0.025603268,0.048920434,-0.0056251837,-0.058535185,-0.027464123,-0.05482139,-0.02820763,0.004608324,-0.012584297,-0.012519182,0.00040820427,-0.01422146,-0.0016752075,0.016782815,-0.0034472859,0.03696491,0.044896666,0.0018043834,0.05983459,-0.036370266,0.033489186,0.03181999,0.016405597,-0.018055864,0.008680416,0.014490231,-0.0028265684,-0.02068464,-0.0074194786,0.004653714,-0.029525101,-0.02312146,0.039159324,-0.005691941,-0.020156916,-0.03683492,-0.0031474973,0.04173314,0.012664873,0.017659947,-0.015267816,-0.003730953,-0.018517703,-0.03130468,0.00012349575,0.010947456,0.0029442145,0.0028912586,-0.0067858384,0.0745076,-0.11529039,-0.039556637,-0.04818323,0.034843907,-0.0026365956,-0.00030944427,-0.003877891,-0.01857597,-0.02203909,-0.048936006,0.06356684,0.0477656,-0.028129242,0.017233929,0.01954818,-0.016550833,-0.00089961087,-0.018952023,0.009917953,-0.00556032,-0.031562112,-0.0019971023,0.072219685,-0.023838278,0.0115391975,0.030845895,-0.014908001,-0.09689083,-0.041483626,-0.002090968,0.020869594,-0.035804555,0.029348975,0.0012189137,0.051680204,-0.03960256,-0.019189775,-0.030803513,0.037081778,-0.018009676,0.004002489,0.033923075,-0.018050049,-0.043010894,-0.011446179,-0.012839836,-0.023073766,0.014194539,0.025842318,0.063778065,0.06017605,0.07191261,-0.038610652,-0.113741525,-0.024582112,-0.019716285,-0.008898852,-0.06650152,0.054478765,-0.026790231,-0.029660963,0.0077982983,-0.027884807,-0.0011979089,0.0077534188,-0.010274879,0.002755656,-0.004032587,-0.041542966,-0.021499658,0.0017469972,-0.009568808,-0.026737297,0.056706015,0.024284942,-0.06528805,-0.07163627,-0.050118916,-0.028213942,0.0040070456,0.002695459,0.041778523,0.02164863,-0.038556524,0.013938656,0.069317564,0.022431552,-0.024932068,-0.006198399,0.043669075,0.0027032273,0.046031825,0.0073399926,0.009302827,-0.12884067,-0.0037282498,0.0025442978,-0.086432,0.061437484,0.011988377,-0.011250704,-0.036677375,-0.022075394,-0.018911662,0.026490832,0.020305224,0.010785784,-0.007729901,-7.077141e-05,-0.02450833,-0.055448305,-0.02697562,-0.029342638,0.0146984225,-0.030097822,-0.0011477929,0.038856316,0.0013377356,0.013928604,0.01121625,-0.0119561,-0.0131210415,0.019278612,0.044484865,-0.031139826,0.045146905,-0.010143851,-0.002548364,0.061720174,0.002876844,0.02460527,-0.008175044,0.011909411,0.015760498,-0.004005586,-0.0017133951,-0.040989526,-0.1134339,0.01977783,0.0334374,0.019640561,0.013455036,-0.020569326,-0.032316465,-0.026682528,-0.015328671,-0.009788922,0.051273882,0.009484955,0.043624684,-0.045594037,0.010405607,-0.030936146,0.06309844,0.03365904,0.018482711,0.009324646,-0.02001696]	Keywords: Transformer, Attention Mechanism, Efficiency, Long Sequences, Speech Enhancement, Pooling, Big Bird, Average Attention, SETransformer, Poolingformer, Big Bird, references, survey, ACL, EMNLP, ICLR, NeurIPS, ICASSP, Cognitive Computation, arXiv, preprints, conference proceedings, Long Document Modeling, Average Attention Network, Speech Enhancement Transformer, Pooling Attention, Longer Sequences, Neural Machine Translation, Object Detection, Memory-Efficient Transformer Search\nKey Objects: Transformer, Attention Mechanism, Long Sequences, Big Bird, Poolingformer, SETransformer, Average Attention Network, Average Attention, Neural Transformer, Average Attention Network, Poolingformer, Long Document Modeling, Longer Sequences, Speech Enhancement Transformer, Neural Machine Translation, Object Detection, Memory-Efficient Transformer Search, ACL, EMNLP, NeurIPS, ICLR, ICASSP, Cognitive Computation, arXiv, conference proceedings, ICLR, preprints, Average Attention Network, Long Document Modeling with Pooling Attention, Memory-Efficient, Big Bird, SETransformer, Average Attention Network, Neural Transformer\nRefers to Images: None\nHypothetical Questions:\n- What are some common methods to improve the efficiency of Transformers?\n- How do Transformers handle longer sequences?\n- What are some applications of Transformers beyond natural language processing?\n- What is an arXiv preprint?\n- What are conference proceedings?\n---\nSummary:\nThis section lists references related to a survey of Transformers, covering a broad range of improvements and variations on the original Transformer architecture. The references span techniques to improve efficiency, handle longer sequences, focus on specific tasks like speech enhancement, and explore different attention mechanisms. The list includes arXiv preprints (indicating ongoing research) and published conference proceedings (indicating peer-reviewed work).\nOriginal Text:\n- [162] Weiwei Yu, Jian Zhou, HuaBing Wang, and Liang Tao. 2021. SETransformer: Speech Enhancement Transformer. Cognitive Computation (02 2021). https://doi.org/10.1052/S-0298-0172-\n- [163] Manzil Zaeher, Guru Gurughesin, Avina Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Armmer Ahmed. 2002. Big Bird: Transformers for Longer Sequences. arXiv:2007.14062 [cs.LG]\n- [164] Biao Zhang, Deyi Xiong, and Jinsong Su. 2018. Accelerating Neural Transformer via an Average Attention Network. In Proceedings of ACL . Melbourne, Australia, 1789-1798. https://doi.org/10.18653/v1/P18-1166\n- [165] Hang Zhang, Yeuyn Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Poolingformer: Long Document Modeling with Pooling Attention. arXiv:2105.04371\nContextualized Text:\nThis section lists references related to a survey of Transformers, covering a broad range of improvements and variations on the original Transformer architecture. The references span techniques to improve efficiency, handle longer sequences, focus on specific tasks like speech enhancement, and explore different attention mechanisms. The list includes arXiv preprints (indicating ongoing research) and published conference proceedings (indicating peer-reviewed work).	{"tags": ["transformers", "references", "survey", "efficiency", "long sequences", "speech enhancement", "pooling", "average attention network", "long document modeling", "memory-efficient", "arxiv", "conference proceedings"], "doc_id": "f7b6331f-65c5-4e55-aa5a-42797a4426b1", "summary": "This section lists references related to a survey of Transformers, covering a broad range of improvements and variations on the original Transformer architecture. The references span techniques to improve efficiency, handle longer sequences, focus on specific tasks like speech enhancement, and explore different attention mechanisms. The list includes arXiv preprints (indicating ongoing research) and published conference proceedings (indicating peer-reviewed work).", "doc_type": "text", "entities": [], "keywords": ["Transformer", "Attention Mechanism", "Efficiency", "Long Sequences", "Speech Enhancement", "Pooling", "Big Bird", "Average Attention", "SETransformer", "Poolingformer", "Big Bird", "references", "survey", "ACL", "EMNLP", "ICLR", "NeurIPS", "ICASSP", "Cognitive Computation", "arXiv", "preprints", "conference proceedings", "Long Document Modeling", "Average Attention Network", "Speech Enhancement Transformer", "Pooling Attention", "Longer Sequences", "Neural Machine Translation", "Object Detection", "Memory-Efficient Transformer Search"], "key_objects": ["Transformer", "Attention Mechanism", "Long Sequences", "Big Bird", "Poolingformer", "SETransformer", "Average Attention Network", "Average Attention", "Neural Transformer", "Average Attention Network", "Poolingformer", "Long Document Modeling", "Longer Sequences", "Speech Enhancement Transformer", "Neural Machine Translation", "Object Detection", "Memory-Efficient Transformer Search", "ACL", "EMNLP", "NeurIPS", "ICLR", "ICASSP", "Cognitive Computation", "arXiv", "conference proceedings", "ICLR", "preprints", "Average Attention Network", "Long Document Modeling with Pooling Attention", "Memory-Efficient", "Big Bird", "SETransformer", "Average Attention Network", "Neural Transformer"], "contextual_text": "This section lists references related to a survey of Transformers, covering a broad range of improvements and variations on the original Transformer architecture. The references span techniques to improve efficiency, handle longer sequences, focus on specific tasks like speech enhancement, and explore different attention mechanisms. The list includes arXiv preprints (indicating ongoing research) and published conference proceedings (indicating peer-reviewed work).", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some common methods to improve the efficiency of Transformers?", "How do Transformers handle longer sequences?", "What are some applications of Transformers beyond natural language processing?", "What is an arXiv preprint?", "What are conference proceedings?"]}
5dbcfb77-daf1-4f0b-b2b5-f89ccec52ae4	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.028257655,0.010992278,0.023681354,0.027600689,-0.0040141204,0.025317641,-0.0011476483,0.06605875,0.020618886,-0.051744208,-0.014844504,0.02237248,0.019237414,0.011366303,-0.049118888,0.028629644,0.03699452,0.05568524,-0.008116368,-0.027557876,0.038522735,0.0043081776,0.018871311,-0.02945043,0.012732754,0.045722973,0.013931958,-0.012164122,0.01749729,0.020887138,-0.0031714719,-0.065562375,0.027839167,0.008200416,0.028975654,-0.012306915,0.015365984,-0.02678576,0.015532886,-0.00935647,-0.0040791156,0.055856206,-0.06989637,-0.02173997,0.024178775,-0.040101517,-0.09164729,0.0006531289,0.043044817,-0.035006102,-0.028391557,0.01272272,-0.054040167,-0.005273825,0.018446526,-0.028723052,-0.013839206,-0.05069552,0.030365868,-0.08448957,-0.055935107,0.058801074,-0.048730094,-0.0441534,0.052171107,-0.025684405,0.0015890256,0.00780943,0.009472756,0.11616219,-0.029723907,0.026332194,0.016210508,-0.051839847,0.105106436,0.055710405,-0.02114439,-0.028230378,-0.0561768,-0.005537014,-0.04239813,0.10597482,-0.04348205,-0.008372061,0.09722899,-0.020120328,-0.013941206,-0.051963944,0.057825845,-0.025325654,0.006803784,-0.031863306,-0.019058013,0.047003746,-0.013162314,-0.08884478,-0.025536114,-0.03450956,-0.0053629274,-0.07448974,-0.021795955,-0.027000723,0.07869214,0.0974136,0.01164684,-0.036321927,-0.013432588,-0.033754785,0.011495125,0.022419145,-0.02884343,0.0020205781,-0.017296659,0.051353235,-0.021413958,-0.04568327,-0.025298797,0.012737141,-0.0035755166,-0.02546465,-0.0074067344,0.015107932,-0.025841787,0.0032992114,0.061355,0.04170865,0.041638747,-0.009267596,-0.035112947,0.005802926,-0.010571542,0.0070098676,-0.010295567,0.023154078,-0.022198172,0.00673849,0.06658043,-0.0012159236,-0.024836652,-0.023156734,0.004505023,-0.004526683,-0.0145503795,0.04970332,0.028509513,0.011301106,-0.073564515,0.031369817,-0.020489216,0.055767287,0.006857288,0.025073165,0.023783173,0.010436317,-0.012075541,0.000781457,-0.014407086,-0.05582306,-0.012046558,-0.01949288,-0.052106455,-0.05386214,0.0030123892,0.0518499,0.027355557,0.11465418,0.004938236,0.014760194,0.056611814,-0.0027938006,-0.09650692,0.0020824573,-0.035036508,-0.042500876,-0.013865966,0.002540145,0.016288228,-0.02107811,-0.029214088,0.0048778956,0.005981858,0.015895033,-0.022711214,0.038695656,-0.00255575,0.004890987,-0.0047662365,0.10560839,-0.016286535,-0.006407759,0.026071139,-0.011430107,0.02307029,-0.0056669465,0.033448048,0.02475171,0.052223466,-0.069188744,-0.037841085,-0.023257518,-0.00997379,-0.0118649,-0.01806628,-0.006868356,0.041346088,-0.0113146305,-0.03457222,-0.005580627,-0.040063903,0.0056798663,-0.0039304392,0.004401925,-0.006690943,0.04824085,0.054657143,0.013290594,-0.0022219664,0.022677077,0.03323864,-0.019395165,-0.08099525,0.0128915,-0.012491131,0.0642412,-0.035571955,0.00051093247,-0.026977351,-0.007899483,0.044383418,-0.047735978,0.059741594,-0.013697993,-0.04641382,-0.0005591811,-0.016780006,0.03464908,-0.00016992335,0.012341042,0.015677944,0.03993926,0.052163396,0.029881138,-0.03443734,0.08415909,-0.002225292,-0.013667345,0.02149169,0.034916807,-0.007819977,0.014289786,-0.009421074,0.072954156,0.06739329,0.044476748,0.0005581996,-0.058174904,0.031220129,0.032696262,0.0026622347,0.054285623,0.009700639,0.00047216594,0.002250838,0.03478806,0.021558378,0.022922225,0.008374663,0.037810754,0.02099623,-0.011837103,0.008433091,0.062056936,0.0010598137,-0.019330958,-0.013992416,-0.0048530395,0.03157412,0.011154847,0.07107616,-0.027353296,-0.019840881,0.0022943506,0.026111621,0.009485842,0.030495057,-0.049153734,-0.002994281,-0.021564739,-0.047853794,0.0031018166,-0.0035879377,-0.02185781,0.038971014,-0.015503134,-0.015275145,0.03495792,0.0058066063,-0.013693938,-0.019941542,0.010848356,0.0830543,-0.05521271,0.012374514,0.017152332,-0.0053528477,0.021386113,-0.053584013,0.040203523,0.033650193,0.003896665,-0.008135215,-0.015662072,-0.014770594,-0.009440562,0.017258639,0.072978616,0.052769866,-0.022868471,0.015947668,0.010366708,0.023188056,0.007383458,-0.0069637047,-0.0069077937,0.035240747,-0.042617403,0.023725163,-0.014459017,-0.07885992,0.05880337,-0.08271563,0.0062596574,0.0667399,-0.0015289771,-0.042832255,-0.0071037533,0.04183491,0.008685973,-0.01449226,0.017034657,0.0033269788,-0.06460011,-0.025041223,-0.009688438,-0.0005954047,0.02827331,-0.029509954,0.031957477,0.02958697,-0.03493062,0.00431535,0.025950976,0.0016836888,0.029075122,0.018048992,-0.04645491,-0.003694167,0.009212726,0.014121728,0.048993263,0.063382186,-0.069728024,0.07786727,-0.016269432,-0.012893248,0.021154525,-0.067727946,-0.050954536,0.05002587,-0.00033282675,-0.017852675,-0.04517294,0.062243037,-0.06151508,-0.030641083,-0.0132158585,0.059287567,0.06876071,-0.05058734,-0.004401404,0.013371122,0.007309633,-0.035400413,-0.017256113,-0.001303156,0.0138663445,0.014292638,0.06434483,-0.011084672,0.05412012,-0.0026605614,0.06977035,0.021138355,-0.013062373,-0.023900663,0.01318188,0.0064077186,0.021733303,0.020395305,-0.02225101,0.019720899,0.046994977,-0.02805973,0.027701506,0.05578347,0.045493275,-0.075115986,0.06814445,0.06329359,0.020314401,0.04886778,0.0020945324,-0.02428656,0.024927758,-0.005099504,-0.00224556,-0.018859543,0.0004265252,-0.04984119,0.021953955,-0.07586192,0.042992175,0.009217964,0.012219944,0.014846012,-0.00026250596,-0.026508216,0.015039335,0.005048043,-0.048132874,0.009477952,-0.027823003,-0.022031693,0.0029785282,0.04239979,0.0004935643,-0.07563771,0.042579502,-0.06950234,0.017521577,0.039214578,-0.005892574,0.010900391,-0.05703647,-0.013447617,-0.0067446306,0.0074692443,0.04473955,-0.03226556,-0.016529117,-0.007429516,0.022602407,0.021892697,-0.014810078,0.025194155,-0.019217117,-0.044292532,0.014593801,-0.014049425,0.018733572,-0.02000193,0.019885587,0.0029401854,-0.0084817,-0.004468336,-0.0078121955,-0.027251739,-0.008733835,-0.02981883,-0.021386918,-0.008798039,0.052574348,-0.027463255,0.04807835,-0.008701133,0.007169026,-0.032003377,0.06526868,0.047732774,0.0048312554,-0.013013436,-0.03662493,-0.01896359,0.05816098,0.023846304,-0.026089197,-0.0089978445,-0.019459022,-0.05172544,0.031618405,-0.008491222,0.04734224,0.045218505,0.020272164,0.03082489,-0.006188543,0.014785029,-0.007504792,0.009128417,-0.06623124,-0.015582366,0.015031751,0.029940806,0.021294672,0.00096856727,-0.0037189992,-0.0015134047,-0.016135752,-0.016195508,0.085440576,-0.009079911,-0.0009717147,-0.018544368,-0.023091312,-0.052890252,0.021896608,-0.014077568,0.019144015,0.027557591,0.05572016,0.025660373,-0.022510832,0.015341868,-0.037445094,0.039759953,-0.025416937,0.0393773,0.04466425,0.062277164,-0.03787422,0.043571673,-0.024817148,-0.039989643,0.00703431,-0.059161026,-0.037295554,0.015424894,-0.07370838,-0.004677884,0.029508462,-0.037126385,-0.028908148,0.02463513,-0.0629125,0.078830674,-0.082276404,0.012269703,-0.007858217,0.023825753,0.1014007,-0.0035258944,-0.026418097,-0.028156424,-0.04319771,0.003917481,0.022538958,0.039146554,0.033724975,0.0046500815,0.07150314,-0.007324946,0.012610481,-0.014520399,-0.05533564,-0.0136666745,-0.06423391,-0.04172314,0.0048859622,-0.038499624,0.0010768061,-0.01280069,-0.012122146,-0.0082109375,0.019056205,-0.015992053,0.045673892,0.05847134,-0.005068556,0.061051387,-0.03520861,0.04595587,0.02553756,0.031349827,-0.006157705,0.010985083,-0.0054119914,-0.012758141,0.0057877186,-0.011452461,0.012440764,-0.015766043,-0.014756803,0.063709736,-0.025508266,-0.021274421,-0.02044864,-0.01481399,0.017282872,0.020742144,-0.027731325,-0.034676302,-0.027019992,-0.03544904,-0.008728066,-0.017316766,0.010753564,0.011831134,0.00042824156,-0.02037591,0.07291024,-0.10148303,-0.031347267,-0.031343225,0.05836089,-0.002867445,0.0053899097,-0.030689018,-0.010886108,-0.0198842,-0.05751662,0.059725873,0.077258885,-0.049787775,-0.011922,-0.0034474086,-0.019538904,0.042215407,-0.004662667,-0.011304443,0.003401143,-0.03550719,-0.020756379,0.09507194,-0.018422544,0.010390264,-0.002492667,0.012151625,-0.10584114,-0.0055151535,-0.0037117747,0.013614992,-0.02844426,0.030192774,0.012214598,0.037353963,-0.055300657,-0.01959685,-0.030556101,0.02120712,-0.023247458,-0.014494606,0.065016694,-0.0016746514,-0.061295725,0.0024407424,-0.03746853,-0.024973892,0.007466909,0.050623313,0.034804907,0.034295592,0.058368023,-0.07003592,-0.084143154,-0.0030413375,-0.0012704082,-0.027413763,-0.016559377,0.019793916,-0.036064412,-0.021634683,0.022499504,-0.029707216,-0.009791753,-0.00044718737,0.015687346,0.006903626,-0.013775729,-0.044944733,0.014383729,0.019607075,-0.049326587,-0.0006506724,0.030714773,0.021295045,-0.03750167,-0.067284204,-0.07516193,-0.008255204,0.011009791,0.0050523924,0.031875886,0.03958514,-0.039403073,-0.0030943507,0.035910767,0.030915746,-0.030349994,-0.009646843,0.021663465,0.024879882,0.033607718,0.039952386,0.0157144,-0.10409061,0.019018922,-0.014618502,-0.07427135,0.09157182,0.0078412285,-0.023982005,-0.0190845,-0.00852638,0.017891008,0.0050837873,0.029978445,0.03097993,-0.01315767,0.027368926,-0.030748064,-0.05292082,-0.036061533,-0.033491947,0.017801156,-0.032804262,0.028803052,0.04623058,0.017103134,0.017309545,-0.03210668,-0.012246906,-0.020265732,0.004507841,0.06560822,-0.023040159,0.029608581,-0.0010658029,0.022087209,0.05803845,-0.013693519,0.0153009,-0.0070688147,0.015078356,0.01384415,-0.010505626,0.00064736785,-0.041675344,-0.06659621,0.009727183,0.031136284,0.012212435,0.012642738,-0.025084876,-0.03359177,-0.075387575,-0.04262538,0.002028515,0.01898489,0.030474706,0.022925546,-0.04244447,0.032078717,-0.03454388,0.061492488,0.03122435,0.03976627,-0.007254412,-0.01668792]	Keywords: Transformers, Long Sequences, Document Processing, Poolingformer, HIBERT, Adaptive Clustering Transformer, Architecture Search, Memory Efficiency\nKey Objects: Long sequences, Document-level processing\nRefers to Images: None\nHypothetical Questions:\n- What are some approaches for handling long sequences using Transformers?\n- What is Poolingformer and how does it relate to Transformers?\n- How do HIBERT and Adaptive Clustering Transformer contribute to document processing with Transformers?\n- What is the purpose of memory-efficient architecture search in the context of Transformers?\n---\nSummary:\nThis chunk presents a continuation of a survey on Transformers, focusing on methods for handling long sequences and document-level processing.  It highlights approaches like Poolingformer, HIBERT, and Adaptive Clustering Transformer, along with memory-efficient architecture search for Transformers.  The references are presented in a standard citation format.\nOriginal Text:\n- [165] Hang Zhang, Yeuyn Gong, Yelong Shen, Weisheng Li, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Poolingformer: Long Document Modeling with Pooling Attention. arXiv:2105.04371\n- [166] Xingxing Zhang, Furu Wei, and Ming Zhou. 2019. HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization. In Proceedings of ACL . Florence, Italy, 5059-5069. https://doi.org/10. 18653/v1/P19-1499\n- [167] Yuekai Zhao, Li Dong, Yelong Shen, Zhihua Zhang, Furu Wei, and Weizhu Chen. 2021. Memory-Efficient Differentiable Transformer Architecture Search. arXiv:2105.14669 [cs.LG]\n- [168] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. 2020. End-to-End Object Detection with Adaptive Clustering Transformer. CoRR abs/2011.09315 (2020). arXiv:2101.09315\nContextualized Text:\nThis is a continuation of a survey on Transformers, focusing on methods for handling long sequences and document-level processing.  It highlights approaches like Poolingformer, HIBERT, and Adaptive Clustering Transformer, along with memory-efficient architecture search for Transformers.  The references are presented in a standard citation format.	{"tags": ["Technical Report", "Survey", "Deep Learning", "Natural Language Processing"], "doc_id": "5dbcfb77-daf1-4f0b-b2b5-f89ccec52ae4", "summary": "This chunk presents a continuation of a survey on Transformers, focusing on methods for handling long sequences and document-level processing.  It highlights approaches like Poolingformer, HIBERT, and Adaptive Clustering Transformer, along with memory-efficient architecture search for Transformers.  The references are presented in a standard citation format.", "doc_type": "text", "entities": ["Poolingformer", "HIBERT", "Adaptive Clustering Transformer", "arXiv"], "keywords": ["Transformers", "Long Sequences", "Document Processing", "Poolingformer", "HIBERT", "Adaptive Clustering Transformer", "Architecture Search", "Memory Efficiency"], "key_objects": ["Long sequences", "Document-level processing"], "contextual_text": "This is a continuation of a survey on Transformers, focusing on methods for handling long sequences and document-level processing.  It highlights approaches like Poolingformer, HIBERT, and Adaptive Clustering Transformer, along with memory-efficient architecture search for Transformers.  The references are presented in a standard citation format.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some approaches for handling long sequences using Transformers?", "What is Poolingformer and how does it relate to Transformers?", "How do HIBERT and Adaptive Clustering Transformer contribute to document processing with Transformers?", "What is the purpose of memory-efficient architecture search in the context of Transformers?"]}
30564c33-84e4-49e1-a149-f8a74f34fd46	abe8c200-bfa1-4355-947e-23ea618c310d	[-0.0025007168,0.02376087,-0.011716623,0.038715936,0.010001084,0.06208365,0.018629082,0.051961616,0.052086875,-0.031648166,-0.015612209,0.024943009,-0.0003376093,-0.0094684325,-0.034132227,0.04332178,0.0021787887,0.05811942,0.038227674,-0.020845532,0.05829546,-0.008924293,0.019976376,-0.037113696,-0.004785833,0.024812067,0.03243057,-0.033105638,0.045420196,0.02726561,0.015584817,-0.06682631,0.023062749,-0.01239245,0.019724427,0.02311043,0.008007724,0.026037056,0.013783998,-0.019730998,-0.022717804,0.030524516,-0.057178017,0.00061248033,-0.0046418263,-0.050003488,-0.072409,-0.015475447,0.034399133,-0.024151608,-0.0264592,0.029635452,-0.028759658,0.0056102546,-0.0035231935,-0.020794382,-0.007482354,-0.029807774,0.026676076,-0.09918082,-0.067252845,0.03602709,-0.055013873,-0.023998676,0.008205579,-0.051637024,-0.03791555,-0.012256921,0.00765115,0.121989176,0.0029113975,0.022424953,-0.023201281,-0.045454774,0.116153665,0.04983902,-0.010409729,-0.011267915,-0.05191922,-0.021014884,-0.023463072,0.07073661,-0.02232135,-0.029187186,0.0933602,0.0025947033,-0.02990026,-0.04475049,0.09212462,-0.027789604,0.014280556,-0.026379444,0.017864332,0.047032423,-0.008687776,-0.08129793,0.029137217,-0.044223793,-0.005426705,-0.05260126,-0.022994034,-0.020693872,0.06097399,0.08418815,0.003986651,-0.02219083,-0.029144773,-0.052573312,-0.010010686,0.020786272,-0.006383119,-0.006584532,-0.010038376,0.009133977,0.0063918643,-0.04094355,-0.046876486,0.0032591394,0.0048759105,-0.0005536491,0.009085955,-0.01698951,0.014527178,0.025447503,0.070088,0.05485854,0.0317632,-0.008334343,-0.032468665,-0.0015233243,-0.0037966587,0.023431841,-0.019435469,0.013289737,0.020079965,0.017256746,0.08106525,-0.024558075,0.0060337964,0.018622251,0.020032726,-0.0043380386,-0.0008748325,0.029370949,-0.0086798025,0.014551278,-0.04790028,0.0088471025,-0.003866195,0.065589316,0.0070344475,0.012142694,0.010549946,0.02054491,0.023942595,-0.008600493,-0.00083379773,-0.06038926,-0.001704329,0.024001878,-0.012291533,-0.06713564,-0.031251453,0.03602937,0.04049205,0.13365072,0.013890904,-0.00400552,0.026318079,-0.0027166936,-0.032337762,0.011517075,-0.016526926,-0.030766504,0.015949348,-0.015034112,0.008358522,0.0066106804,-0.048949108,0.017314626,0.016646152,0.0035191788,-0.0013418595,0.049537763,-0.013071561,0.02514509,0.0044557243,0.06396889,-0.03886572,-0.034239933,0.044966392,-0.037785247,-0.018424502,0.018023318,0.03244263,0.017365208,0.0321534,-0.085346825,-0.03363917,-0.037341397,-0.028893344,0.018288434,0.012728664,0.009119669,0.022509933,0.003932222,-0.049644846,-0.009100811,-0.046970055,-0.014513849,-0.0055791154,-0.030015409,-0.035521295,0.06190739,0.042029213,-0.00558317,-0.017096898,0.003974143,0.023573704,-0.006328253,-0.07436598,0.001986653,-0.004379855,0.006468589,-0.054685857,-0.012519465,-0.037428252,0.035786316,0.032119412,-0.046273265,0.05951065,0.00523129,-0.035008423,0.040152133,-0.015272352,0.04833484,0.017343098,-0.00784383,0.04025113,0.052062627,0.029188752,0.06219183,0.026976459,0.055547994,0.016679607,-0.009653118,0.039154578,0.05493467,-0.0035044118,0.027265038,0.0095530385,0.04987051,0.042636044,0.012828607,0.0231631,-0.0363379,0.01899597,0.022455744,0.03450941,0.027086155,0.007209818,-0.017913034,0.007908743,-0.009548514,0.03877338,0.009695321,0.0028020805,0.059871323,0.028997218,0.002536039,0.02321347,0.05291626,-0.0007942447,-0.03296468,-0.051997785,0.016876953,0.010992212,0.0050731483,0.059247438,-0.036719926,0.0058319895,-0.01571603,0.030600946,-0.0023694802,0.038943842,-0.024633687,-0.020826755,-0.013428481,-0.06492027,-0.011256664,0.0066381292,-0.056209575,0.020245997,-0.029029256,-0.0031326218,0.032108136,0.0085428255,-0.026782695,-0.02922988,-0.0049494104,0.08936129,-0.04151102,0.025019558,-0.020540878,0.002385513,0.026541354,-0.06108841,0.014555102,0.04770216,0.0071140104,0.0053461706,-0.0143540455,-0.020436453,-0.017058372,0.02713833,0.09415826,0.056613076,-0.0236227,-0.008985758,0.011406727,0.03000446,0.0059590167,-0.008898564,0.040010884,0.0328926,0.0030307192,-0.008280165,-0.040935982,-0.09434574,0.06421171,-0.0736015,0.045879558,0.03356015,0.0237581,0.007160961,-0.0057990723,0.054763008,0.01652311,-0.009257097,0.011030588,0.012038493,-0.075100675,-0.007229968,-0.014569942,0.0096647395,0.021855753,0.003588798,0.026793642,0.033648323,-0.0031461115,0.018283773,0.049242117,0.049771097,0.03478058,-0.0055581816,-0.021948777,0.008033439,0.017091827,0.0019621141,0.029654918,0.066979654,-0.033847444,0.07775629,-0.026160816,-0.013459211,-0.01549032,-0.0649806,-0.044224337,0.062083207,-0.021700116,-0.008160503,-0.02530783,0.053699378,-0.04424047,-0.0408463,-0.018025761,0.03767729,0.026330356,-0.057712063,-0.019266054,-0.035796545,0.029131556,-0.033235088,-0.0076190303,-0.036726143,0.06516662,0.0007988504,0.0066834255,0.00043865593,0.041812662,-0.017153405,0.030543942,0.03516602,0.005222532,-0.009333403,-0.0031479502,0.040311113,0.018001463,0.026377747,-0.019373322,0.058511585,0.025138179,-0.042120498,0.024185235,0.053813923,0.017150084,-0.085410595,0.0032336495,0.05414163,-0.0009980408,0.038365964,-0.042576656,-0.015884176,0.033661127,-0.0141960075,-0.024982994,0.001747555,0.021068312,-0.0638115,0.024943253,-0.055032704,0.046540428,0.012885199,0.038549867,0.024487082,-0.043548655,-0.013144873,0.039548874,0.00801961,-0.033233166,0.019413445,-0.09342369,-0.026960842,0.04199529,0.051679112,0.018500775,-0.06843523,0.078918956,-0.09811299,0.0210529,0.0726847,-0.009345746,0.02193031,-0.022865485,0.0005664908,-0.027362635,-0.014581727,0.05062503,-0.000752484,-0.012852035,-0.008180234,0.04498068,0.006528608,0.005045779,-0.01089951,-0.022601748,-0.02683647,-0.0097501315,-0.042421464,0.019186085,-0.021151973,0.02086258,0.0009953998,-0.011880332,0.003611128,0.020464143,-0.051106855,-0.034481686,-0.04838461,-0.009624899,-0.0055828523,0.031712223,-0.039770547,0.027990054,-0.015891328,-0.02329067,-0.03146546,0.040632173,0.07654306,-0.024007604,-0.036828555,-0.0456695,0.01949125,0.054777794,-0.020491323,-0.030586183,-0.036429267,-0.015565044,-0.05286918,0.013112147,0.023820763,0.05794223,0.0445802,0.0338223,0.029466178,-0.018596364,-0.0012256777,0.0033054068,0.016414361,-0.04166242,-0.037823822,0.016519504,0.061779365,0.017993268,0.009655187,-0.0063849287,0.006794597,-0.020012982,0.044417407,0.058475766,0.037292458,0.013522165,-0.015506109,-0.006909528,-0.022483159,0.02614889,-0.007887127,-0.018389402,0.006588003,0.046997182,-0.0043460065,-0.024088513,0.0025298803,-0.025342284,-0.010104517,-0.019271778,0.041459598,0.03449888,0.05568794,-0.018300587,0.07048459,-0.00268167,-0.009096458,0.03278353,-0.053401757,-0.026600573,0.012804502,-0.0736601,0.009382261,0.02809008,-0.012435659,-0.019294888,0.032134954,-0.026279777,0.10297853,-0.057487603,0.0027441306,-0.03919101,-0.0036481265,0.08852337,0.025236165,0.005319795,-0.029934522,-0.016667448,-0.032486223,0.026860505,0.0935643,0.0069997488,-0.0038597218,0.05742351,0.0017273545,0.044910807,0.0028299429,-0.06329424,-0.016433708,-0.060264193,-0.057874434,0.005452794,-0.008723315,0.01708668,0.019549204,-0.013934135,0.0068037678,0.040999018,-0.034377113,0.015052072,0.037412424,-0.0050726393,0.025934612,-0.05188374,0.049159743,0.04379269,-0.020412587,-0.014778721,0.019839978,0.029893586,0.011706656,-0.03140482,0.0072789597,-0.0006648121,-0.020470636,-0.033375595,0.006615671,-0.0024379906,-0.049768254,-0.0066822898,0.0071137394,0.05064768,0.028289516,-0.013237304,-0.03373293,-0.02790268,-0.043995336,-0.018819373,0.011854239,0.01885586,0.0109927775,0.0035971664,-0.04149363,0.08663776,-0.08197319,-0.060577903,-0.039598394,0.038643178,-0.026902126,0.004813354,-0.016881894,-0.014104512,-0.008191858,-0.031252634,0.066559434,0.055642746,-0.042721864,0.003664051,-0.010003963,0.005035551,-0.0005096614,-0.006581019,-0.043055728,0.007548769,-0.0011161733,-0.016964348,0.10196149,-0.035898484,0.014458043,0.02091116,0.005149742,-0.08780056,-0.016446212,0.033887275,0.017804354,-0.018394182,-0.016975861,0.011150831,0.03209943,-0.05687819,-0.034281198,-0.08483608,0.048595462,-0.018092789,-0.0033910286,0.007455452,0.0027336814,-0.072357826,0.019912941,-0.014149893,-0.048031483,4.744542e-05,0.004913513,0.038662158,0.046956453,0.061101522,-0.047266364,-0.11724887,-0.024393942,-0.028761676,-0.025021024,-0.04044617,0.030434627,0.0035790836,-0.012368661,0.0149026485,-0.03539139,0.0052562603,0.0078045246,0.027259186,-0.02351824,-0.025364969,-0.00658945,-0.024592241,0.029786903,-0.0090305675,0.00678568,0.044894032,0.06543691,-0.06673146,-0.067729175,-0.036972854,-0.010319153,0.009393444,0.02042593,0.018396687,0.014853846,-0.016870165,0.015216726,0.06441842,0.015522022,0.0061482633,-0.0039651776,0.032807384,-0.034016345,0.057632968,-0.0014469663,-0.02538012,-0.08418742,0.01114753,-0.014975441,-0.066319495,0.048465814,0.0397286,0.0052195694,-0.03138181,-0.021756513,-0.0031716942,0.036440488,0.03825632,0.014694455,0.0017461914,0.012303558,-0.008960722,-0.04186419,-0.021221088,-0.042560704,0.035271082,-0.038357798,0.015963132,0.028496604,0.036270097,0.003428624,0.011005192,-0.028087007,-0.022783827,0.03995674,0.043560028,-0.042212054,0.06049766,-0.0112400185,0.034678355,0.054576166,-0.014838235,0.027029136,-0.024915949,0.015662163,-0.0022595108,0.016742103,0.007823431,-0.04368534,-0.09328564,0.008611059,0.015261381,0.0040583014,0.037035637,-0.0074018617,-0.029201586,-0.059317127,-0.03937473,-0.007897046,0.071251296,0.009215408,0.031885818,-0.027791955,0.005529887,-0.026605384,0.05832312,0.028276175,-0.0045380327,0.013818255,-0.025537582]	Keywords: transformers, neural networks, machine translation, object detection, speech synthesis, time-series forecasting, efficiency, long sequences, attention mechanism\nKey Objects: Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, Yibin Zheng, Xinhui Li, Fenglong Xie, Li Lu, Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang\nRefers to Images: None\nHypothetical Questions:\n- What are some applications of Transformer networks?\n- How do researchers try to improve the efficiency of Transformers?\n- What are some methods for handling long sequences with Transformer networks?\n---\nSummary:\nThis is a list of references related to Transformers, a type of neural network architecture.  The references span a wide range of applications including machine translation, object detection, speech synthesis, time-series forecasting, and more. Many entries also focus on efficiency improvements and techniques to handle longer sequences.\nOriginal Text:\n- [168] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong. 2020. End-to-End Object Detection with Adaptive Clustering Transformer. CoRR abs/2011.09315 (2020). arXiv:2101.09315\n- [169] Yibin Zheng, Xinhui Li, Fenglong Xie, and Li Lu. 2020. Improving End-to-End Speech Synthesis with Local Recurrent Neural Network Enhanced Transformer. In Proceedings of ICASSP . 6734-6738. https://doi.org/10.1109/ICASSP40776. 2020.9054148\n- [170] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. 2021. Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting. In Proceedings of AAAI .\n- [171] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, and Furu Wei. 2020. BERT Losses Patience: Fast and Robust Inference with Early Exit. arXiv:2006.04152\nContextualized Text:\nThe text is a list of references, presumably from a larger document or survey about Transformer architectures.	{"tags": ["reference list", "transformers architecture", "machine learning"], "doc_id": "30564c33-84e4-49e1-a149-f8a74f34fd46", "summary": "This is a list of references related to Transformers, a type of neural network architecture.  The references span a wide range of applications including machine translation, object detection, speech synthesis, time-series forecasting, and more. Many entries also focus on efficiency improvements and techniques to handle longer sequences.", "doc_type": "text", "entities": ["BERT", "Informer", "DETR", "Attention Mechanism"], "keywords": ["transformers", "neural networks", "machine translation", "object detection", "speech synthesis", "time-series forecasting", "efficiency", "long sequences", "attention mechanism"], "key_objects": ["Minghang Zheng", "Peng Gao", "Xiaogang Wang", "Hongsheng Li", "Yibin Zheng", "Xinhui Li", "Fenglong Xie", "Li Lu", "Haoyi Zhou", "Shanghang Zhang", "Jieqi Peng", "Shuai Zhang", "Jianxin Li", "Hui Xiong", "Wancai Zhang"], "contextual_text": "The text is a list of references, presumably from a larger document or survey about Transformer architectures.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are some applications of Transformer networks?", "How do researchers try to improve the efficiency of Transformers?", "What are some methods for handling long sequences with Transformer networks?"]}
acbb7dff-492b-4175-8454-c47f87aca0db	abe8c200-bfa1-4355-947e-23ea618c310d	[0.0044162176,0.025170328,-0.025528872,0.044513658,0.014614125,0.064524226,0.0031577586,0.07020108,0.06227133,-0.025775194,0.0003067094,-0.01509348,0.025110938,-0.0067873914,-0.06279211,0.043174405,0.009702142,0.037538514,0.042974014,-0.0063198777,0.04852409,0.014459332,-0.0061093466,-0.05750323,0.0029678987,0.020127771,0.011503967,-0.038485147,0.022444597,0.049809657,0.028636629,-0.06287541,0.00569674,-0.0055782553,0.014808846,0.002933382,0.0014285534,0.019736547,0.02026624,-0.03779378,-0.009427841,0.052549258,-0.03892466,-0.012975986,-0.011178712,-0.07340194,-0.056639276,-0.04576537,0.03762526,-0.005821627,-0.006946504,0.027903609,-0.018465396,0.0048790276,-0.020266023,-0.021981228,0.010596241,-0.043119397,0.02229889,-0.10659844,-0.055186626,0.04576621,-0.044375926,-0.029585302,0.027343694,-0.06420899,-0.049020417,-0.026240725,0.017702268,0.10845879,-0.009725763,0.017881049,-0.045805193,-0.064408466,0.101228155,0.04123939,-0.02615358,-0.0027723862,-0.07476702,-0.009969856,-0.026394334,0.051106595,-0.05528181,-0.024283953,0.07596672,-0.0010214193,-0.04324675,-0.047283404,0.077307194,-0.022634743,0.013917935,-0.047836468,0.013779791,0.08185629,-0.014833582,-0.058700673,0.04335721,-0.016556313,-0.013011883,-0.058647893,-0.04326115,-0.007911626,0.052559942,0.08064282,0.019264506,0.02488044,-0.023727259,-0.058422893,0.013180091,0.013480509,-0.014291538,-0.016772233,-0.0019793143,-0.0038519918,-0.012066059,-0.0556933,-0.04574106,-0.014029031,-0.0053130584,0.018659944,0.0064866636,-0.016548533,0.006159904,0.0050174776,0.07105476,0.07050048,-0.0133811375,0.0022222362,-0.011878089,-0.013953614,-0.016392086,0.005300832,-0.011566354,0.029840937,0.051864553,0.016700488,0.030388681,-0.049888182,-0.0036415372,-0.010959334,0.029267881,-0.018294988,-0.010755209,0.05071647,-0.023361381,0.0052506267,-0.05649225,0.019455466,-0.008748696,0.037260704,0.0076265847,0.021502294,0.019745959,0.02204193,0.029779268,-0.014400918,-0.0074397856,-0.07086708,0.002133748,0.01943967,-0.03922182,-0.0718247,-0.018830132,0.022952123,0.042219393,0.12196859,0.015951304,-0.0061841286,0.0034206011,0.0013622456,-0.017805485,0.016816111,-0.00979952,-0.023199825,-0.0028585952,-0.025201682,0.029144498,0.027087705,-0.044616994,0.015852163,0.0072491867,0.00905867,-0.008603843,0.048817318,-0.009815546,0.031091439,0.000108476874,0.075265184,-0.042833734,-0.040629376,0.026159948,-0.014500293,-0.011154484,0.003608136,0.031531002,0.015223393,0.010765187,-0.08397512,-0.048892803,-0.010347532,-0.051086955,0.0088607315,0.012234001,0.032158613,0.011676006,-0.013831439,-0.05333743,-0.005763708,-0.037129343,-0.039306723,0.008855281,-0.028170696,-0.02156517,0.071340494,0.034530755,-0.0052858624,-0.021513784,0.03207848,0.037216973,0.0030216465,-0.062902905,0.018339276,-0.016373007,0.011245002,-0.05831172,0.000903268,-0.052094728,0.03923574,0.022324096,-0.032426044,0.06613515,-0.009331925,0.0068994425,0.019350018,-0.008165699,0.024295386,-0.002136867,-0.012821071,0.027622556,0.056837235,0.041481618,0.06503035,0.026995616,0.068737835,0.019342037,-0.003318315,0.03600674,0.04880812,-0.015408601,0.028878197,-0.0065666684,0.030864954,0.037473,0.016802428,0.0063896296,-0.021618325,0.020451698,0.038950644,0.0588473,0.0058934875,0.019314712,-0.019435635,0.019767607,-0.020972185,0.03276552,-0.002355238,0.031636506,0.051122762,0.019804053,0.026852183,0.031226423,0.054304145,-0.027204258,-0.024192763,-0.121889874,0.013678548,-0.0029471628,0.039273992,0.025419582,0.0024530787,-0.0007232491,-0.03608214,0.024492973,-0.024323389,0.038979996,-0.026009472,-0.009014355,-0.026780654,-0.050976917,-0.012329857,0.0033034731,-0.0227646,0.022600614,-0.02168485,0.0067229695,0.02312899,0.011443911,-0.051939707,-0.017598718,0.0062275445,0.073646635,-0.036051635,0.020950876,-0.008822493,-0.010620598,0.032248877,-0.0776761,0.010868723,0.052661777,0.006577519,0.019412566,0.003109715,0.0033414753,-0.018021725,0.041875787,0.098073855,0.071829505,-0.01926282,-0.0066983523,0.014015386,0.0014314367,0.027673401,-0.0056717983,0.04257334,0.032363694,0.009138307,-0.014518051,-0.035321463,-0.10100373,0.06204231,-0.059547663,0.022013353,0.02580294,0.01594624,-0.00411288,-0.031553973,0.050433893,0.0330511,-0.0021254423,0.015794406,0.0055029998,-0.078631915,0.030430848,-0.008504461,-0.007703808,0.019392375,0.01569204,0.052726094,0.04465637,0.022102192,0.008690958,0.027304536,0.03889045,0.008575607,-0.003815583,-0.007369065,0.0012566156,0.014603863,0.0027548121,0.042047665,0.055645164,-0.031015547,0.07309408,-0.015945574,0.016048176,-0.008372533,-0.047154516,-0.044746928,0.03989068,-0.008740987,-0.0023645759,-0.036223624,0.034529693,-0.04548158,-0.024161808,-0.019473528,0.031711306,0.024873022,-0.050722934,-0.026192369,-0.036806587,0.0057690535,-0.05961078,-0.020887658,-0.03682018,0.06454473,0.019390881,0.010107014,0.0002932398,0.04170824,-0.028594594,0.028065698,0.041844487,-0.024256943,-0.0007234929,-0.009697467,0.042164348,0.026581842,0.012471639,-0.037366144,0.046954762,0.021078603,-0.045070548,0.028165681,0.055245914,0.02158259,-0.07802094,0.022232752,0.07068627,-0.0059158527,0.041934606,-0.050456587,-0.035402365,0.06642973,-0.016790824,-0.03466561,0.0187562,-0.0011271874,-0.05712764,-0.015936894,-0.06241524,0.063851856,0.011517752,0.05346555,0.028684294,-0.012778185,-0.033163704,0.01890496,-0.0033302526,-0.017590478,0.011339672,-0.08418304,0.0029753132,0.020021442,0.04646528,0.017979972,-0.04456475,0.04600568,-0.10665433,0.021868754,0.038160678,-0.024100188,0.00020395835,-0.048874144,-0.029971907,-0.025930589,-0.0184224,0.040453605,0.00018649743,-0.024531394,-0.02774167,0.055512797,0.015184686,0.0046974733,-0.009741521,-0.015303717,-0.04029186,-0.004331699,-0.06342547,0.003090728,-0.0126028,0.03858374,-0.0054563,-0.00348361,0.017124647,0.0022533706,-0.057146773,-0.011666502,-0.022899989,0.0034261763,0.003848178,0.023637738,-0.05272043,0.016701335,-0.018486409,-0.037953217,-0.025387693,0.038253367,0.09265222,-0.00819026,-0.063719384,-0.05106336,0.008917829,0.057437416,-0.040498286,-0.013276217,-0.02172159,-0.0486132,-0.04484314,0.019865507,0.030812409,0.07031238,0.030010473,0.02531916,0.015367571,-0.016770016,-0.012509566,0.020011863,0.016648483,-0.04796519,-0.03484018,-0.0036189896,0.053265452,-0.011690664,0.024835074,0.024802634,0.011053695,-0.0020920625,0.013262514,0.06487621,0.019971516,0.0059962184,-0.016534602,0.0036649832,-0.0098139895,0.04001951,-0.021880591,-0.0008478869,-0.0058412156,0.04059403,0.0019440886,-0.03471513,-0.011952354,-0.03353842,0.023785688,0.004842175,0.025559887,0.053983863,0.05576413,-0.026621088,0.07430774,-0.0035830168,-0.029006833,0.022040905,-0.05431966,-0.00039072317,-0.008308153,-0.08929591,-0.034906115,0.018611142,-0.04266753,0.010904327,0.004988205,-0.038870238,0.087018676,-0.034560394,0.01373891,-0.031192003,0.010107949,0.090606116,0.027195247,-0.014044482,-0.030658219,-0.007599656,-0.028555827,0.044495337,0.08128967,0.03226271,0.0065833563,0.06030375,0.009964465,0.046200596,0.011136051,-0.045585357,-0.008229103,-0.04459187,-0.052213784,-0.023214668,0.015538555,0.021143332,0.009116473,0.016116114,0.0049595004,0.023280287,-0.043968707,0.0061347014,0.04561261,-0.017845394,0.008590878,-0.04515244,0.075372666,0.05079181,-0.0033738106,-0.0036454967,0.026920844,0.045232043,0.012531209,-0.011702283,0.016657032,-0.0052509094,0.0001635068,-0.013556076,0.023227222,0.026850902,-0.05070271,-0.022132989,0.018469056,0.01857672,0.01018231,-0.047924966,-0.017081441,-0.014826873,-0.030171987,-0.023583973,0.04240635,-0.007775089,-0.012745448,-0.008280279,-0.032076184,0.0777757,-0.075925164,-0.04929058,-0.026353562,0.0071496996,-0.034177173,0.013026236,-0.011046972,-0.006475582,-0.014635592,-0.021599831,0.045667216,0.027911339,-0.044551034,-0.003579229,0.010079556,-0.018179175,-0.009378415,-0.021712326,-0.025562879,0.010180005,-0.010010821,-0.020390194,0.09751504,0.0011086997,-0.008519672,0.021035908,-0.014383811,-0.0898159,-0.01076432,0.037062798,0.010545317,-0.03148149,-0.025468431,0.034567744,0.014843299,-0.03977401,-0.020111436,-0.08477918,0.046577938,-0.0068760472,0.018427467,0.01687499,-0.0026813676,-0.0768709,0.0062750652,-0.02142871,-0.031452663,-0.018231694,0.04743263,0.056784283,0.042472955,0.07745313,-0.057212397,-0.09981832,-0.032573964,0.005622253,-0.013798081,-0.038810693,0.031676948,-0.013612686,-0.035850927,0.03331712,-0.012630758,0.0019423006,0.02096166,0.042044584,-0.043016125,-0.0050129564,-0.022520758,-0.03677485,0.023453338,-0.014342674,-0.005345579,0.03350071,0.01815737,-0.05714073,-0.07284409,-0.010090278,-0.014129536,0.0061748344,0.0087804,0.019208657,0.017924432,-0.00096386514,-0.012970166,0.076594085,0.03772095,-0.0052762013,-0.00096472166,0.03826715,-0.020523494,0.0595379,0.0048632943,-0.019413136,-0.06529117,0.0029195035,-0.019827606,-0.05073449,0.02377007,0.026385007,0.022957709,-0.030747728,-0.043463226,-0.003931841,0.01712178,0.05145406,0.02012005,0.023212375,-0.0022523657,-0.037116498,-0.0434771,-0.029148158,-0.025445983,0.017063107,-0.017453484,0.017780589,0.014063984,0.04191739,-0.003807306,0.008004323,-0.027043667,-0.021739399,0.015809372,0.014571242,-0.017053714,0.0676204,-0.027219528,-0.0052329903,0.049523648,-0.02995185,0.04352413,-0.025189193,-0.0037983004,-0.006371191,0.02386554,-0.020190822,-0.016402934,-0.088689804,-0.006192932,0.034078773,-0.0008946009,0.041879397,-0.019071303,-0.028024971,-0.06485851,-0.05345052,0.0019446108,0.0782754,0.04120854,0.009532057,-0.057160024,-0.00024732985,0.0029595636,0.030821076,0.023490295,0.005931399,-0.0063575055,-0.020404283]	Keywords: transformers, machine learning, natural language processing, object detection, speech synthesis, efficient algorithms, document modeling, inference\nKey Objects: Transformers, BERT Losses Patience, Deformable DETR\nRefers to Images: None\nHypothetical Questions:\n- What are the most recent innovations in transformer architectures?\n- How are transformers being used in computer vision tasks?\n- What are the challenges in applying transformers to long sequences?\n- What are the trade-offs between accuracy and efficiency when using transformers?\n---\nSummary:\nThis is a list of references related to transformers, likely from a survey or overview paper. The references cover a broad range of topics including efficient transformers, document modeling, object detection, speech synthesis, and inference techniques. Each entry includes author(s), publication year, title, and a unique identifier (e.g., arXiv identifier).\nOriginal Text:\n- [171] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, and Furu Wei. 2020. BERT Losses Patience: Fast and Robust Inference with Early Exit. arXiv:2006.04152\n- [172] Xizhou Zhu, Weijie Su, Lewei Li, Bin Li, Xiaogang Wang, and Jifeng Dai. 2020. Deformable DETR: Deformable Transformers for End-to-End Object Detection. CoRR abs/2010.04159 (2020). arXiv:2010.04159\nContextualized Text:\nThe list of references suggests a technical audience interested in advancements and variations within the transformer architecture.	{"tags": ["Deep Learning", "NLP", "CV"], "doc_id": "acbb7dff-492b-4175-8454-c47f87aca0db", "summary": "This is a list of references related to transformers, likely from a survey or overview paper. The references cover a broad range of topics including efficient transformers, document modeling, object detection, speech synthesis, and inference techniques. Each entry includes author(s), publication year, title, and a unique identifier (e.g., arXiv identifier).", "doc_type": "text", "entities": ["BERT", "DETR", "Informer", "LazyFormer", "Deformable DETR", "HiBERT", "Wangchunshu Zhou", "Xizhou Zhu"], "keywords": ["transformers", "machine learning", "natural language processing", "object detection", "speech synthesis", "efficient algorithms", "document modeling", "inference"], "key_objects": ["Transformers", "BERT Losses Patience", "Deformable DETR"], "contextual_text": "The list of references suggests a technical audience interested in advancements and variations within the transformer architecture.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What are the most recent innovations in transformer architectures?", "How are transformers being used in computer vision tasks?", "What are the challenges in applying transformers to long sequences?", "What are the trade-offs between accuracy and efficiency when using transformers?"]}
ecd4163c-ab35-4825-ac88-31d5a2a0c5ea	abe8c200-bfa1-4355-947e-23ea618c310d	[0.00072328554,0.013151909,-0.0033860155,0.011027713,-0.013809947,0.042123385,-0.019950857,0.053278416,0.02415493,-0.04657642,0.008992034,-0.0036476864,-0.0039901407,-0.0030605928,-0.0510914,0.044607375,0.0069354936,0.057644468,0.01631785,-0.008374939,0.055968966,0.014945367,0.0014823485,-0.046845704,-0.004325459,0.031979512,0.015821125,-0.052885517,0.031265102,0.0644213,0.03291705,-0.045663144,0.001220255,-0.009700681,0.04291567,0.035596836,0.022950849,0.011979755,0.043511618,-0.049783714,-0.02591842,0.054209135,-0.07368864,-0.008550461,-0.0020686348,-0.0863015,-0.044935834,-0.032030642,0.051829997,-0.0037153265,-0.019867932,0.021577246,-0.021791091,-0.015086949,0.00801026,-0.022306874,-0.011464589,-0.022316039,0.034085825,-0.09927061,-0.03488353,0.029923065,-0.03277186,0.00054927595,0.045502886,-0.06164165,-0.024545532,-0.023454979,0.01693874,0.11872836,-0.012353593,0.015148522,-0.026459195,-0.04247368,0.10191322,0.046136376,-0.011247251,-0.017920004,-0.06824561,0.0025428226,-0.02577903,0.05158261,-0.0806891,-0.0438409,0.084615976,0.0064296057,-0.05329216,-0.025643175,0.07079458,-0.014831044,0.026060065,-0.026353953,-0.011141207,0.08956793,-0.02922924,-0.067057006,0.022962403,-0.032873694,-0.022582242,-0.091344975,-0.016482156,-0.016965393,0.052842055,0.10288112,0.031837396,-0.0081305,-0.010598752,-0.05600387,-0.00021747556,0.038771693,0.0022692312,0.0014503922,0.015892494,0.023541858,0.001880665,0.010741926,-0.054967348,0.028786605,0.00035801902,0.006107882,0.015636176,-0.022590293,0.0048210607,0.02708726,0.08126189,0.048327565,-0.012028076,-0.019139748,-0.023327988,-0.010875305,-0.02200208,0.001360028,-0.0015743516,0.03290577,0.0073988466,0.002986135,0.07605906,-0.032370396,-0.011407658,0.0042849705,0.012035055,-0.04354056,-0.02836908,0.03660499,-0.027249584,0.027609453,-0.027107388,0.0011850409,-0.0067028007,0.039332394,0.01920817,0.033649817,0.031915538,0.033261057,0.03579754,-0.011981514,0.023831971,-0.03934952,0.021321073,-0.005655465,-0.0452697,-0.080902584,-0.02182946,0.02735567,0.025426146,0.15077396,-0.0070967036,-0.022667335,0.0041443245,0.007040129,-0.02431291,0.017796274,-0.01905084,-0.0297156,0.026529746,-0.0013690895,0.011078707,0.04311288,-0.037768558,0.020039221,-0.00065853965,0.006564881,-0.040765177,0.07155382,-0.010960939,0.04262928,0.0009904336,0.07222648,-0.012888427,-0.032843053,0.011565476,-0.039518073,0.015094124,0.012677008,0.026214108,0.013657822,0.02610325,-0.07842005,-0.03176013,-0.011670065,-0.020235114,0.015578394,0.01599219,0.025713889,0.01955771,0.00035645004,-0.047747497,-0.023706174,-0.032266594,-0.01482183,-0.023975385,-0.014673824,-0.0155205615,0.08129082,0.007763742,0.0050508576,-0.011224194,0.032532293,0.0166777,-0.013279063,-0.07920415,-0.021384098,-0.012981064,0.013953798,-0.05612224,0.0065016854,-0.03717299,0.059558883,0.011739666,-0.036591318,0.049658462,0.027375853,-0.009675263,-0.0044523994,-0.020292081,0.040530834,0.012900299,-0.010717489,0.00026190645,0.045960974,0.025303302,0.055338833,0.03250821,0.08635217,0.018410802,0.0028371376,0.0310037,0.045535967,-0.012625007,0.028476715,0.0090213055,0.04356378,0.03655463,0.00964046,-0.004489584,-0.029559385,0.03731456,0.06051372,0.07124409,-0.009557271,0.011125079,-0.03259411,0.016220897,-0.018648943,0.034889124,-0.0047536734,0.030870743,0.04424831,0.03986475,0.0031277095,0.032756466,0.024474436,-0.02677651,-0.032716934,-0.105001956,0.014666653,0.014859696,0.029270768,0.030427184,-0.0007860301,-0.0010401304,-0.025891678,0.014965218,-0.022795958,0.029877463,-0.04427318,0.001389748,-0.011456447,-0.062487025,-0.019631477,0.022858901,-0.033023383,0.015882747,-0.025917469,-0.0011274162,0.013011102,0.013143518,-0.052097976,-0.021032766,-0.015729163,0.091523506,-0.01816089,0.01326259,0.0022932272,0.0050360826,0.02147393,-0.055053767,0.008746075,0.025124274,0.013784221,0.02019466,-0.02966795,-0.015380472,-0.038014594,0.04684376,0.07778661,0.09339918,-0.014099842,-0.012033696,0.009575594,0.012553411,0.022476604,-0.0036608512,0.046710845,0.023047125,-0.006877849,0.005855515,-0.040220156,-0.08244026,0.06390365,-0.057317033,0.06621615,0.034501344,0.018690852,0.021085676,-0.019602183,0.050271988,0.0026505205,0.014669047,0.03555292,-0.010175486,-0.07793596,0.024653187,-0.025949838,-0.020882301,0.016211133,0.013305902,0.04618198,0.05910112,0.005905682,-0.016941622,0.026222942,0.040098637,0.002800209,0.006254249,-0.022707267,0.004568003,0.012138696,-0.015511035,0.05358674,0.05837504,-0.057436872,0.09993148,0.011567485,-0.0033744597,-0.021152355,-0.053951073,-0.03784945,0.063221365,-0.028257774,-0.0049796444,-0.02663682,0.047941655,-0.040141232,-0.04325245,-0.0027937908,-0.009882275,0.043077983,-0.054285962,-0.039459344,-0.0372851,0.0004084702,-0.053286444,-0.0064276694,-0.042376783,0.07183966,0.025016554,0.047648676,-0.024069078,0.033626046,-0.031807054,0.050641008,0.0366937,-0.024925144,-0.00029459552,-0.018166509,0.016522627,0.015962917,0.023719655,-0.05613341,0.023413079,0.013403044,-0.0529541,0.030456796,0.06574938,0.03573982,-0.094121225,0.036888223,0.092507824,-0.00071731576,0.023949727,-0.07005919,-0.009542374,0.023274867,-0.0014118786,-0.030024827,-0.015304616,0.01611635,-0.061179847,-0.00937194,-0.05902991,0.048666675,-0.013714619,0.045544386,0.040785376,-0.024147755,-0.042737816,0.016147915,0.011899857,-0.02423092,0.031998854,-0.07030408,0.02470129,0.026515938,-0.0020459506,0.037946932,-0.045857135,0.066220336,-0.08148941,0.018140234,0.033234637,-0.049530834,0.004121642,-0.026668765,-0.0054637864,-0.05463093,-0.004016218,0.038985305,-0.005719717,-0.014810715,0.003927486,0.03994278,0.045615215,-0.00053850486,0.007748037,0.012843592,-0.04053405,0.00048711526,-0.05061058,0.020091753,-0.006321537,0.045452956,-0.005638494,0.003203312,-0.004117899,-0.0063767415,-0.05541604,-0.04021481,-0.016827298,-0.0025377152,0.014577929,0.015779814,-0.046399664,0.006060729,-0.021032926,-0.01997466,0.009213424,0.036611166,0.064493544,0.025231909,-0.04962378,-0.040741276,0.035936795,0.06861105,-0.016899107,0.0042738453,-0.027043307,-0.03282258,-0.038814344,0.008357204,0.012978178,0.054089814,0.044389095,0.0031277759,0.008320746,-0.02917123,0.011909115,0.0012516747,0.004291614,-0.05196599,-0.031860452,-0.012637879,0.03487215,-0.0062700417,4.7290618e-05,-0.007693928,0.014382654,0.01017507,0.03057181,0.04738697,0.031050406,-0.0066344906,-0.012960754,-0.011712873,-0.01975656,0.017386895,-0.0133603895,-0.0018515905,0.00062533916,0.025433594,-0.010254644,-0.022095747,0.01767431,-0.02810239,0.019877022,-8.952048e-05,-0.017032988,0.044434927,0.045086637,-0.043580275,0.034652278,-0.003211549,-0.014160364,0.0387284,-0.03947336,-0.02419352,0.03806196,-0.06099834,0.0007158192,0.059901386,-0.032952584,-0.005063598,0.0017927994,-0.028564578,0.0690536,-0.038011186,0.007395685,-0.0072656274,0.017915495,0.13123293,0.040655077,0.004073546,-0.000247615,0.022124747,-0.041498218,0.061675582,0.06485565,0.023521308,0.0015212713,0.0765237,-0.00020155142,0.032087203,0.012219442,-0.028991915,-0.011261524,-0.029013013,-0.049330093,-0.0026828272,0.005677244,0.01659185,-0.007850553,0.0053821877,0.0057015195,0.024499478,-0.00013002804,0.00888498,0.031618156,-0.0049964795,0.028099112,-0.069355674,0.04154228,0.026170205,0.009267833,-0.005564456,0.009432579,0.010063616,-0.0040015006,-0.024886673,0.0013067757,-0.041397687,-0.0061104344,-0.02493032,0.015067212,0.011577498,-0.020460496,-0.011278396,0.03974838,0.038160287,0.03494355,-0.012753533,-0.026356261,0.0006205196,-0.032896034,-0.036436476,0.0063186027,-0.027722508,-0.01038335,-0.01182635,-0.01298279,0.099819094,-0.07622948,-0.052913617,-0.020866128,0.043918073,-0.040460184,0.009513341,-0.009038595,0.00012309915,-0.0053783176,-0.045289826,0.0696318,0.0149327805,-0.05033049,0.031348232,-0.0021192797,-0.012407953,-0.021082303,-0.0018116109,-0.040060043,-0.0026647805,-0.01546811,-0.026880827,0.083459385,-0.027158935,-0.0054281387,0.041527573,-0.018805834,-0.066557966,-0.031855397,0.016888177,0.0475686,-0.020494264,-0.029370619,0.01365825,0.026217125,-0.037848376,0.008088031,-0.06756336,0.043888174,-0.015127822,-0.008398831,0.01180209,-0.0065784384,-0.053999044,0.009939218,-0.0036341783,-0.031773448,-0.024655672,0.04309085,0.03890578,0.06099436,0.06490089,-0.050889783,-0.095002666,-0.040757857,-0.009726479,-0.010508207,-0.04815503,0.051733274,-0.0047316058,-0.015114638,0.027474107,-0.0116273295,0.004153195,0.0066878884,0.015717773,-0.009655849,-0.03436308,-0.015462762,-0.06346776,0.008416764,-0.0034853537,-0.010755039,0.035058286,0.025006363,-0.052144624,-0.04710568,-0.035591304,-0.038147133,-0.015643056,-0.0074281814,-0.0029128722,0.044109687,0.0009345571,-0.009945032,0.06992493,0.022006748,-0.010280912,-0.036256056,0.036171373,-0.005452959,0.051404405,0.008405107,-0.026625615,-0.10921432,0.004420933,-0.027342824,-0.055253748,0.04841034,-0.013929469,0.0075266864,-0.03908884,-0.033706576,-0.0017389569,0.029259846,0.032549944,0.011385413,0.002748012,0.01942815,-0.03264611,-0.034937818,-0.01652235,-0.040734198,0.011962892,-0.03144495,0.030110175,-0.014547567,0.024166925,-0.015606867,-0.0037678722,-0.038666002,-0.003916932,0.0010547541,0.029271983,-0.0045936108,0.058984537,-0.022141112,0.006157218,0.04859332,0.008085671,0.028981524,-0.025570113,0.0022898435,0.026724787,0.04522753,-0.012222888,-0.011402267,-0.09523069,-0.005877018,0.017413765,0.02102652,0.017351486,-0.0040076748,-0.03498192,-0.063154966,-0.025254622,-0.009980607,0.055555873,0.020507524,0.034517642,-0.04736554,0.011022312,0.008564511,0.019892901,0.02587372,-0.0022038945,0.009703116,-0.034862895]	Keywords: self-attention, transformer architectures, video representation, language representation, early exiting, efficiency, neural networks, sequence-to-sequence learning\nKey Objects: transformer architectures, video representation models, early exiting strategies, neural networks\nRefers to Images: None\nHypothetical Questions:\n- What is the significance of the references listed here?\n- Which areas of research are covered by these references?\n- Why are these references important in the field of transformer architectures?\n- What common themes emerge from these references?\n---\nSummary:\nThis section provides a list of references, numbering from 127 to 130, spanning various research areas including self-attention mechanisms, video and language representation, early exiting strategies for improved efficiency, and sequence-to-sequence learning with neural networks. The references showcase advancements in transformer architectures and their applications across diverse domains.\nOriginal Text:\n- [171] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, and Furu Wei. 2020. BERT Losses Patience: Fast and Robust Inference with Early Exit. arXiv:2006.04152\n- [172] Xizhou Zhu, Weijie Su, Lewei Li, Bin Li, Xiaogang Wang, and Jifeng Dai. 2020. Deformable DETR: Deformable Transformers for End-to-End Object Detection. CoRR abs/2010.04159 (2020). arXiv:2010.04159\nContextualized Text:\nThis section focuses on providing a comprehensive list of references pertinent to advancements in transformer architectures and their applications. The references are numbered sequentially from 127 to 130, detailing contributions to self-attention mechanisms, video and language representation learning, early exiting techniques for improved efficiency, and sequence-to-sequence learning methodologies employing neural networks.	{"tags": ["references", "transformer", "artificial intelligence", "machine learning"], "doc_id": "ecd4163c-ab35-4825-ac88-31d5a2a0c5ea", "summary": "This section provides a list of references, numbering from 127 to 130, spanning various research areas including self-attention mechanisms, video and language representation, early exiting strategies for improved efficiency, and sequence-to-sequence learning with neural networks. The references showcase advancements in transformer architectures and their applications across diverse domains.", "doc_type": "text", "entities": ["self-attention", "transformer architectures", "video representation", "language representation", "early exiting", "neural networks", "sequence-to-sequence learning"], "keywords": ["self-attention", "transformer architectures", "video representation", "language representation", "early exiting", "efficiency", "neural networks", "sequence-to-sequence learning"], "key_objects": ["transformer architectures", "video representation models", "early exiting strategies", "neural networks"], "contextual_text": "This section focuses on providing a comprehensive list of references pertinent to advancements in transformer architectures and their applications. The references are numbered sequentially from 127 to 130, detailing contributions to self-attention mechanisms, video and language representation learning, early exiting techniques for improved efficiency, and sequence-to-sequence learning methodologies employing neural networks.", "mentioned_images": [], "section_hierarchy": {"h1": "A Survey of Transformers", "h2": "REFERENCES"}, "hypothetical_questions": ["What is the significance of the references listed here?", "Which areas of research are covered by these references?", "Why are these references important in the field of transformer architectures?", "What common themes emerge from these references?"]}
\.


--
-- Name: langchain_pg_collection langchain_pg_collection_name_key; Type: CONSTRAINT; Schema: public; Owner: langchain
--

ALTER TABLE ONLY public.langchain_pg_collection
    ADD CONSTRAINT langchain_pg_collection_name_key UNIQUE (name);


--
-- Name: langchain_pg_collection langchain_pg_collection_pkey; Type: CONSTRAINT; Schema: public; Owner: langchain
--

ALTER TABLE ONLY public.langchain_pg_collection
    ADD CONSTRAINT langchain_pg_collection_pkey PRIMARY KEY (uuid);


--
-- Name: langchain_pg_embedding langchain_pg_embedding_pkey; Type: CONSTRAINT; Schema: public; Owner: langchain
--

ALTER TABLE ONLY public.langchain_pg_embedding
    ADD CONSTRAINT langchain_pg_embedding_pkey PRIMARY KEY (id);


--
-- Name: ix_cmetadata_gin; Type: INDEX; Schema: public; Owner: langchain
--

CREATE INDEX ix_cmetadata_gin ON public.langchain_pg_embedding USING gin (cmetadata jsonb_path_ops);


--
-- Name: langchain_pg_embedding langchain_pg_embedding_collection_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: langchain
--

ALTER TABLE ONLY public.langchain_pg_embedding
    ADD CONSTRAINT langchain_pg_embedding_collection_id_fkey FOREIGN KEY (collection_id) REFERENCES public.langchain_pg_collection(uuid) ON DELETE CASCADE;


--
-- PostgreSQL database dump complete
--

\unrestrict noEU2KJ5VvOfSiHBugyfFICbUwQSBcuaqdUUSbidQYHzaJ9gpxseIzYIg8XT1mR

